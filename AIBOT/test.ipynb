{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Paper Type: pdf\n",
      "machine learning\n",
      "\n",
      "“An astonishing machine learning book: intuitive, full of examples, fun to read but still comprehensive, strong, and deep! A great starting point for any univer- sity student—and a must-have for anybody in the field.” Jan Peters, Darmstadt University of Technology; Max-Planck Institute for Intelligent Systems\n",
      "\n",
      "Machine Learning A Probabilistic Perspective\n",
      "\n",
      "Kevin P. Murphy\n",
      "\n",
      "“Kevin Murphy excels at unraveling the complexities of machine learning methods while motivating the reader with a stream of illustrated examples and real-world case studies. The accompanying software package includes source code for many of the figures, making it both easy and very tempting to dive in and explore these methods for yourself. A must-buy for anyone interested in machine learning or curious about how to extract useful knowledge from big data.” John Winn, Microsoft Research\n",
      "\n",
      "Today’s Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, a unified, probabilistic approach.\n",
      "\n",
      "“This is a wonderful book that starts with basic topics in statistical modeling, culminating in the most ad- vanced topics. It provides both the theoretical foun- dations of probabilistic machine learning as well as practical tools, in the form of MATLAB code. The book should be on the shelf of any student interested in the topic, and any practitioner working in the field.” Yoram Singer, Google Research\n",
      "\n",
      "The coverage combines breadth and depth, offering necessary background material on such topics as probabili- ty, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional ran- dom fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify mod- els in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package—PMTK (probabilistic modeling toolkit)—that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.\n",
      "\n",
      "Kevin P. Murphy is a Research Scientist at Google. Previ- ously, he was Associate Professor of Computer Science and Statistics at the University of British Columbia.\n",
      "\n",
      "“This book will be an essential reference for practitio- ners of modern machine learning. It covers the basic concepts needed to understand the field as a whole, and the powerful modern methods that build on those concepts. In Machine Learning, the language of prob- ability and statistics reveals important connections be- tween seemingly disparate algorithms and strategies. Thus, its readers will become articulate in a holistic view of the state-of-the-art and poised to build the next generation of machine learning algorithms.” David Blei, Princeton University\n",
      "\n",
      "978-0-262-01802-9\n",
      "\n",
      "The MIT Press\n",
      "\n",
      "Massachusetts Institute of Technology\n",
      "\n",
      "Cambridge, Massachusetts 02142\n",
      "\n",
      "http://mitpress.mit.edu\n",
      "\n",
      "Machine Learning A Probabilistic Perspective\n",
      "\n",
      "Adaptive Computation and Machine Learning series\n",
      "\n",
      "The cover image is based on sequential Bayesian updating\n",
      "\n",
      "of a 2D Gaussian distribution. See Figure 7.11 for details.\n",
      "\n",
      "Kevin P. Murphy\n",
      "\n",
      "Machine Learning: A Probabilistic Perspective\n",
      "\n",
      "Machine Learning A Probabilistic Perspective\n",
      "\n",
      "Kevin P. Murphy\n",
      "\n",
      "The MIT Press Cambridge, Massachusetts London, England\n",
      "\n",
      "© 2012 Massachusetts Institute of Technology\n",
      "\n",
      "All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.\n",
      "\n",
      "For information about special quantity discounts, please email special_sales@mitpress.mit.edu\n",
      "\n",
      "This book was set in the LATEX programming language by the author. Printed and bound in the United States of America.\n",
      "\n",
      "Library of Congress Cataloging-in-Publication Information\n",
      "\n",
      "Murphy, Kevin P. Machine learning : a probabilistic perspective / Kevin P. Murphy. p. cm. — (Adaptive computation and machine learning series) Includes bibliographical references and index. ISBN 978-0-262-01802-9 (hardcover : alk. paper) 1. Machine learning. 2. Probabilities. I. Title. Q325.5.M87 2012 006.3’1—dc23 2012004558\n",
      "\n",
      "10\n",
      "\n",
      "9\n",
      "\n",
      "8\n",
      "\n",
      "7\n",
      "\n",
      "6\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "This book is dedicated to Alessandro, Michael and Stefano, and to the memory of Gerard Joseph Murphy.\n",
      "\n",
      "Contents\n",
      "\n",
      "Preface\n",
      "\n",
      "xxvii\n",
      "\n",
      "1\n",
      "\n",
      "Introduction 1.1\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "1.3\n",
      "\n",
      "Machine learning: what and why? 1.1.1 Supervised learning 1.2.1 1.2.2 Unsupervised learning 1.3.1 1.3.2 1.3.3 1.3.4 Some basic concepts in machine learning 1.4.1 1.4.2 1.4.3 1.4.4 1.4.5 1.4.6 1.4.7 1.4.8 1.4.9\n",
      "\n",
      "Types of machine learning\n",
      "\n",
      "Classiﬁcation Regression\n",
      "\n",
      "9 Discovering clusters Discovering latent factors Discovering graph structure Matrix completion\n",
      "\n",
      "Parametric vs non-parametric models A simple non-parametric classiﬁer: K-nearest neighbors The curse of dimensionality Parametric models for classiﬁcation and regression Linear regression Logistic regression Overﬁtting 22 Model selection 22 No free lunch theorem\n",
      "\n",
      "1\n",
      "\n",
      "3\n",
      "\n",
      "8\n",
      "\n",
      "3\n",
      "\n",
      "19\n",
      "\n",
      "21\n",
      "\n",
      "14\n",
      "\n",
      "10\n",
      "\n",
      "24\n",
      "\n",
      "1\n",
      "\n",
      "11\n",
      "\n",
      "2\n",
      "\n",
      "13\n",
      "\n",
      "18\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "19\n",
      "\n",
      "16\n",
      "\n",
      "2 Probability\n",
      "\n",
      "2.1 2.2\n",
      "\n",
      "27 Introduction 27 A brief review of probability theory Discrete random variables 2.2.1 Fundamental rules 2.2.2 28 Bayes rule 2.2.3 29 Independence and conditional independence 2.2.4 Continuous random variables 2.2.5\n",
      "\n",
      "28 28\n",
      "\n",
      "32\n",
      "\n",
      "30\n",
      "\n",
      "viii\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "2.3\n",
      "\n",
      "2.4\n",
      "\n",
      "2.5\n",
      "\n",
      "2.6\n",
      "\n",
      "2.7\n",
      "\n",
      "2.8\n",
      "\n",
      "Quantiles 2.2.6 2.2.7 33 Mean and variance Some common discrete distributions 2.3.1 2.3.2 2.3.3 2.3.4 Some common continuous distributions Gaussian (normal) distribution 2.4.1 Degenerate pdf 2.4.2 The Laplace distribution 2.4.3 The gamma distribution 2.4.4 The beta distribution 2.4.5 2.4.6 Pareto distribution Joint probability distributions 2.5.1 2.5.2 2.5.3 47 2.5.4 Transformations of random variables 2.6.1 2.6.2 2.6.3 Monte Carlo approximation 2.7.1 2.7.2 2.7.3 Information theory 2.8.1 2.8.2 2.8.3\n",
      "\n",
      "Entropy 56 KL divergence Mutual information\n",
      "\n",
      "Example: change of variables, the MC way 53 Example: estimating π by Monte Carlo integration Accuracy of Monte Carlo approximation\n",
      "\n",
      "Linear transformations General transformations Central limit theorem\n",
      "\n",
      "44 44 Covariance and correlation The multivariate Gaussian 46 Multivariate Student t distribution Dirichlet distribution\n",
      "\n",
      "The binomial and Bernoulli distributions The multinomial and multinoulli distributions The Poisson distribution The empirical distribution\n",
      "\n",
      "33\n",
      "\n",
      "56\n",
      "\n",
      "57\n",
      "\n",
      "39\n",
      "\n",
      "52\n",
      "\n",
      "43\n",
      "\n",
      "59\n",
      "\n",
      "42\n",
      "\n",
      "51\n",
      "\n",
      "49\n",
      "\n",
      "50\n",
      "\n",
      "41 41\n",
      "\n",
      "37\n",
      "\n",
      "37\n",
      "\n",
      "49\n",
      "\n",
      "34\n",
      "\n",
      "38 38\n",
      "\n",
      "46\n",
      "\n",
      "54\n",
      "\n",
      "34\n",
      "\n",
      "35\n",
      "\n",
      "54\n",
      "\n",
      "3 Generative models for discrete data\n",
      "\n",
      "65\n",
      "\n",
      "3.1 3.2\n",
      "\n",
      "3.3\n",
      "\n",
      "Introduction Bayesian concept learning 67 3.2.1 3.2.2 3.2.3 3.2.4 3.2.5 The beta-binomial model 3.3.1 3.3.2 3.3.3 3.3.4\n",
      "\n",
      "Likelihood Prior Posterior 68 Posterior predictive distribution A more complex prior 72\n",
      "\n",
      "Likelihood Prior Posterior 75 Posterior predictive distribution\n",
      "\n",
      "65\n",
      "\n",
      "67\n",
      "\n",
      "74\n",
      "\n",
      "73\n",
      "\n",
      "65\n",
      "\n",
      "72\n",
      "\n",
      "71\n",
      "\n",
      "77\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "ix\n",
      "\n",
      "3.4\n",
      "\n",
      "3.5\n",
      "\n",
      "The Dirichlet-multinomial model 3.4.1 3.4.2 3.4.3 3.4.4 Naive Bayes classiﬁers Model ﬁtting 3.5.1 Using the model for prediction 3.5.2 The log-sum-exp trick 3.5.3 Feature selection using mutual information 3.5.4 Classifying documents using bag of words 3.5.5\n",
      "\n",
      "Likelihood Prior Posterior Posterior predictive 82 83\n",
      "\n",
      "79\n",
      "\n",
      "79\n",
      "\n",
      "79\n",
      "\n",
      "81\n",
      "\n",
      "86\n",
      "\n",
      "78\n",
      "\n",
      "85\n",
      "\n",
      "86 87\n",
      "\n",
      "4 Gaussian models Introduction 4.1.1 4.1.2 4.1.3 4.1.4 Gaussian discriminant analysis 4.2.1 4.2.2 4.2.3 4.2.4 4.2.5 4.2.6 4.2.7 4.2.8 Inference in jointly Gaussian distributions Statement of the result 4.3.1 Examples 4.3.2 111 Information form 4.3.3 Proof of the result * 4.3.4 119 Linear Gaussian systems Statement of the result 4.4.1 Examples 4.4.2 124 Proof of the result * 4.4.3 Digression: The Wishart distribution * Inverse Wishart distribution 4.5.1 4.5.2 Visualizing the Wishart distribution * Inferring the parameters of an MVN Posterior distribution of μ 4.6.1 Posterior distribution of Σ * 4.6.2 128 Posterior distribution of μ and Σ * 4.6.3 Sensor fusion with unknown precisions * 4.6.4\n",
      "\n",
      "4.5\n",
      "\n",
      "4.4\n",
      "\n",
      "4.3\n",
      "\n",
      "4.2\n",
      "\n",
      "4.6\n",
      "\n",
      "4.1\n",
      "\n",
      "Quadratic discriminant analysis (QDA) Linear discriminant analysis (LDA) Two-class LDA MLE for discriminant analysis Strategies for preventing overﬁtting Regularized LDA * Diagonal LDA 108 Nearest shrunken centroids classiﬁer *\n",
      "\n",
      "Notation Basics MLE for an MVN 99 Maximum entropy derivation of the Gaussian *\n",
      "\n",
      "97 97\n",
      "\n",
      "97\n",
      "\n",
      "97\n",
      "\n",
      "120\n",
      "\n",
      "104\n",
      "\n",
      "115\n",
      "\n",
      "107\n",
      "\n",
      "116\n",
      "\n",
      "101\n",
      "\n",
      "111\n",
      "\n",
      "119\n",
      "\n",
      "127 128\n",
      "\n",
      "125 126\n",
      "\n",
      "106\n",
      "\n",
      "110\n",
      "\n",
      "103\n",
      "\n",
      "106\n",
      "\n",
      "132\n",
      "\n",
      "127\n",
      "\n",
      "102\n",
      "\n",
      "109\n",
      "\n",
      "138\n",
      "\n",
      "101\n",
      "\n",
      "x\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "5 Bayesian statistics 149 Introduction 149 Summarizing posterior distributions MAP estimation 5.2.1 Credible intervals 5.2.2 5.2.3 Inference for a difference in proportions Bayesian model selection 5.3.1 5.3.2 5.3.3 5.3.4 Priors 5.4.1 5.4.2 5.4.3 5.4.4 Hierarchical Bayes 5.5.1 Empirical Bayes 5.6.1 5.6.2 Bayesian decision theory 5.7.1 5.7.2 5.7.3\n",
      "\n",
      "5.4\n",
      "\n",
      "5.6\n",
      "\n",
      "5.3\n",
      "\n",
      "5.5\n",
      "\n",
      "5.7\n",
      "\n",
      "5.1 5.2\n",
      "\n",
      "Example: beta-binomial model Example: Gaussian-Gaussian model\n",
      "\n",
      "155 Bayesian Occam’s razor Computing the marginal likelihood (evidence) Bayes factors Jeffreys-Lindley paradox * 165 Uninformative priors Jeffreys priors * Robust priors Mixtures of conjugate priors 171\n",
      "\n",
      "Example: modeling related cancer rates\n",
      "\n",
      "176 Bayes estimators for common loss functions The false positive vs false negative tradeoff 184 Other topics *\n",
      "\n",
      "172\n",
      "\n",
      "163\n",
      "\n",
      "168\n",
      "\n",
      "166\n",
      "\n",
      "149\n",
      "\n",
      "152\n",
      "\n",
      "165\n",
      "\n",
      "156\n",
      "\n",
      "164\n",
      "\n",
      "149\n",
      "\n",
      "168\n",
      "\n",
      "173\n",
      "\n",
      "173\n",
      "\n",
      "171\n",
      "\n",
      "154\n",
      "\n",
      "177 180\n",
      "\n",
      "158\n",
      "\n",
      "6 Frequentist statistics\n",
      "\n",
      "191\n",
      "\n",
      "6.1 6.2\n",
      "\n",
      "6.3\n",
      "\n",
      "6.4\n",
      "\n",
      "6.5\n",
      "\n",
      "191 Introduction Sampling distribution of an estimator 6.2.1 6.2.2 Frequentist decision theory Bayes risk 6.3.1 195 Minimax risk 6.3.2 196 6.3.3 Admissible estimators Desirable properties of estimators 6.4.1 6.4.2 6.4.3 6.4.4 Empirical risk minimization 6.5.1 6.5.2 6.5.3 6.5.4\n",
      "\n",
      "Bootstrap Large sample theory for the MLE *\n",
      "\n",
      "200 Consistent estimators Unbiased estimators 200 Minimum variance estimators The bias-variance tradeoff 204\n",
      "\n",
      "Regularized risk minimization Structural risk minimization Estimating the risk using cross validation Upper bounding the risk using statistical learning theory *\n",
      "\n",
      "192\n",
      "\n",
      "194\n",
      "\n",
      "197\n",
      "\n",
      "200\n",
      "\n",
      "202\n",
      "\n",
      "191\n",
      "\n",
      "206\n",
      "\n",
      "201\n",
      "\n",
      "205\n",
      "\n",
      "193\n",
      "\n",
      "206\n",
      "\n",
      "209\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "xi\n",
      "\n",
      "6.6\n",
      "\n",
      "6.5.5 Pathologies of frequentist statistics * 6.6.1 6.6.2 6.6.3 6.6.4\n",
      "\n",
      "Surrogate loss functions\n",
      "\n",
      "Counter-intuitive behavior of conﬁdence intervals p-values considered harmful The likelihood principle 214 Why isn’t everyone a Bayesian?\n",
      "\n",
      "210\n",
      "\n",
      "211\n",
      "\n",
      "213\n",
      "\n",
      "215\n",
      "\n",
      "212\n",
      "\n",
      "7\n",
      "\n",
      "Linear regression 217 Introduction 7.1 217 Model speciﬁcation 7.2 Maximum likelihood estimation (least squares) 7.3 7.3.1 7.3.2 7.3.3 Robust linear regression * Ridge regression 225 7.5.1 7.5.2 7.5.3 7.5.4 Bayesian linear regression 7.6.1 7.6.2 7.6.3 7.6.4\n",
      "\n",
      "7.4 7.5\n",
      "\n",
      "7.6\n",
      "\n",
      "Derivation of the MLE Geometric interpretation Convexity\n",
      "\n",
      "Basic idea 225 Numerically stable computation * Connection with PCA * Regularization effects of big data\n",
      "\n",
      "231 Computing the posterior Computing the posterior predictive 233 Bayesian inference when σ2 is unknown * EB for linear regression (evidence procedure)\n",
      "\n",
      "221\n",
      "\n",
      "217\n",
      "\n",
      "223\n",
      "\n",
      "219\n",
      "\n",
      "228\n",
      "\n",
      "220\n",
      "\n",
      "232\n",
      "\n",
      "230\n",
      "\n",
      "227\n",
      "\n",
      "217\n",
      "\n",
      "234\n",
      "\n",
      "238\n",
      "\n",
      "8 Logistic regression Introduction 8.1 8.2 Model speciﬁcation 8.3 Model ﬁtting MLE 8.3.1 Steepest descent 8.3.2 Newton’s method 8.3.3 Iteratively reweighted least squares (IRLS) 8.3.4 Quasi-Newton (variable metric) methods 8.3.5 (cid:4)2 regularization 8.3.6 Multi-class logistic regression 8.3.7 Bayesian logistic regression 8.4.1 8.4.2 8.4.3 8.4.4 8.4.5 Online learning and stochastic optimization 8.5.1\n",
      "\n",
      "8.5\n",
      "\n",
      "8.4\n",
      "\n",
      "Online learning and regret minimization\n",
      "\n",
      "Laplace approximation Derivation of the BIC Gaussian approximation for logistic regression Approximating the posterior predictive 256 Residual analysis (outlier detection) *\n",
      "\n",
      "245 245\n",
      "\n",
      "245 246\n",
      "\n",
      "245\n",
      "\n",
      "247 249\n",
      "\n",
      "252\n",
      "\n",
      "254\n",
      "\n",
      "255\n",
      "\n",
      "255\n",
      "\n",
      "252\n",
      "\n",
      "261\n",
      "\n",
      "260\n",
      "\n",
      "250 251\n",
      "\n",
      "262\n",
      "\n",
      "256\n",
      "\n",
      "xii\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "8.6\n",
      "\n",
      "8.5.2 8.5.3 8.5.4 8.5.5 Generative vs discriminative classiﬁers 8.6.1 8.6.2 8.6.3\n",
      "\n",
      "Stochastic optimization and risk minimization The LMS algorithm The perceptron algorithm 266 A Bayesian view\n",
      "\n",
      "Pros and cons of each approach Dealing with missing data Fisher’s linear discriminant analysis (FLDA) *\n",
      "\n",
      "264\n",
      "\n",
      "265\n",
      "\n",
      "269\n",
      "\n",
      "267\n",
      "\n",
      "268\n",
      "\n",
      "262\n",
      "\n",
      "271\n",
      "\n",
      "9 Generalized linear models and the exponential family 281\n",
      "\n",
      "9.1 9.2\n",
      "\n",
      "9.4\n",
      "\n",
      "9.3\n",
      "\n",
      "Introduction The exponential family 9.2.1 9.2.2 9.2.3 9.2.4 9.2.5 9.2.6 Generalized linear models (GLMs) 9.3.1 9.3.2 9.3.3 Probit regression 9.4.1 9.4.2 9.4.3 9.4.4\n",
      "\n",
      "ML/MAP estimation using gradient-based optimization Latent variable interpretation Ordinal probit regression * Multinomial probit models *\n",
      "\n",
      "Deﬁnition Examples 284 Log partition function MLE for the exponential family Bayes for the exponential family * Maximum entropy derivation of the exponential family *\n",
      "\n",
      "290 Basics ML and MAP estimation Bayesian inference\n",
      "\n",
      "293\n",
      "\n",
      "282 282\n",
      "\n",
      "281\n",
      "\n",
      "293\n",
      "\n",
      "290\n",
      "\n",
      "292\n",
      "\n",
      "295\n",
      "\n",
      "295\n",
      "\n",
      "294\n",
      "\n",
      "286\n",
      "\n",
      "287\n",
      "\n",
      "281\n",
      "\n",
      "289\n",
      "\n",
      "294\n",
      "\n",
      "9.5 Multi-task learning\n",
      "\n",
      "296\n",
      "\n",
      "9.6\n",
      "\n",
      "9.7\n",
      "\n",
      "9.5.1 9.5.2 9.5.3 9.5.4 Generalized linear mixed models * 9.6.1 9.6.2 Learning to rank * 9.7.1 9.7.2 9.7.3 9.7.4\n",
      "\n",
      "Hierarchical Bayes for multi-task learning Application to personalized email spam ﬁltering Application to domain adaptation Other kinds of prior\n",
      "\n",
      "Example: semi-parametric GLMMs for medical data Computational issues\n",
      "\n",
      "The pointwise approach The pairwise approach The listwise approach Loss functions for ranking\n",
      "\n",
      "300\n",
      "\n",
      "297\n",
      "\n",
      "300\n",
      "\n",
      "301 302\n",
      "\n",
      "301\n",
      "\n",
      "298\n",
      "\n",
      "303\n",
      "\n",
      "297\n",
      "\n",
      "296\n",
      "\n",
      "296\n",
      "\n",
      "298\n",
      "\n",
      "10 Directed graphical models (Bayes nets)\n",
      "\n",
      "307\n",
      "\n",
      "10.1\n",
      "\n",
      "Introduction 10.1.1 10.1.2\n",
      "\n",
      "307 Chain rule Conditional independence\n",
      "\n",
      "307\n",
      "\n",
      "308\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "xiii\n",
      "\n",
      "10.2\n",
      "\n",
      "10.3 10.4\n",
      "\n",
      "10.5\n",
      "\n",
      "10.6\n",
      "\n",
      "10.1.3 10.1.4 10.1.5 Examples 10.2.1 10.2.2 10.2.3 10.2.4 10.2.5 Inference Learning 10.4.1 10.4.2 10.4.3 Conditional independence properties of DGMs 10.5.1\n",
      "\n",
      "10.5.2 10.5.3 Inﬂuence (decision) diagrams *\n",
      "\n",
      "Graphical models Graph terminology 309 Directed graphical models\n",
      "\n",
      "Naive Bayes classiﬁers Markov and hidden Markov models 313 Medical diagnosis Genetic linkage analysis * Directed Gaussian graphical models *\n",
      "\n",
      "d-separation and the Bayes Ball algorithm (global Markov properties) 324 Other Markov properties of DGMs Markov blanket and full conditionals\n",
      "\n",
      "319 320 Plate notation Learning from complete data Learning with missing and/or latent variables 324\n",
      "\n",
      "311\n",
      "\n",
      "320\n",
      "\n",
      "308\n",
      "\n",
      "328\n",
      "\n",
      "311\n",
      "\n",
      "315\n",
      "\n",
      "310\n",
      "\n",
      "322\n",
      "\n",
      "327\n",
      "\n",
      "312\n",
      "\n",
      "327\n",
      "\n",
      "318\n",
      "\n",
      "323\n",
      "\n",
      "11 Mixture models and the EM algorithm Latent variable models\n",
      "\n",
      "11.1 11.2 Mixture models\n",
      "\n",
      "337\n",
      "\n",
      "337\n",
      "\n",
      "337\n",
      "\n",
      "11.3\n",
      "\n",
      "11.4\n",
      "\n",
      "11.2.1 11.2.2 11.2.3 11.2.4 Parameter estimation for mixture models 11.3.1 11.3.2 The EM algorithm 11.4.1 11.4.2 11.4.3 11.4.4 11.4.5 11.4.6 11.4.7 11.4.8 11.4.9\n",
      "\n",
      "Mixtures of Gaussians Mixture of multinoullis Using mixture models for clustering Mixtures of experts\n",
      "\n",
      "Basic idea EM for GMMs EM for mixture of experts EM for DGMs with hidden variables EM for the Student distribution * EM for probit regression * Theoretical basis for EM * Online EM 365 Other EM variants *\n",
      "\n",
      "Unidentiﬁability Computing a MAP estimate is non-convex\n",
      "\n",
      "348\n",
      "\n",
      "349\n",
      "\n",
      "350\n",
      "\n",
      "346\n",
      "\n",
      "342\n",
      "\n",
      "367\n",
      "\n",
      "339 340\n",
      "\n",
      "362 363\n",
      "\n",
      "357\n",
      "\n",
      "345\n",
      "\n",
      "359\n",
      "\n",
      "340\n",
      "\n",
      "358\n",
      "\n",
      "347\n",
      "\n",
      "11.5 Model selection for latent variable models\n",
      "\n",
      "11.6\n",
      "\n",
      "11.5.1 11.5.2 Fitting models with missing data\n",
      "\n",
      "370 Model selection for probabilistic models Model selection for non-probabilistic methods\n",
      "\n",
      "372\n",
      "\n",
      "370\n",
      "\n",
      "370\n",
      "\n",
      "xiv\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "11.6.1\n",
      "\n",
      "EM for the MLE of an MVN with missing data\n",
      "\n",
      "373\n",
      "\n",
      "12 Latent linear models Factor analysis 12.1.1 12.1.2 12.1.3 12.1.4 12.1.5 12.1.6 Principal components analysis (PCA) Classical PCA: statement of the theorem 12.2.1 Proof * 12.2.2 Singular value decomposition (SVD) 12.2.3 Probabilistic PCA 12.2.4 12.2.5 EM algorithm for PCA Choosing the number of latent dimensions 12.3.1 12.3.2 PCA for categorical data PCA for paired and multi-view data 12.5.1 Supervised PCA (latent factor regression) 12.5.2 Partial least squares Canonical correlation analysis 12.5.3 Independent Component Analysis (ICA) 12.6.1 12.6.2 12.6.3 12.6.4\n",
      "\n",
      "12.6\n",
      "\n",
      "12.4 12.5\n",
      "\n",
      "12.2\n",
      "\n",
      "12.3\n",
      "\n",
      "12.1\n",
      "\n",
      "Model selection for FA/PPCA Model selection for PCA 402\n",
      "\n",
      "Maximum likelihood estimation The FastICA algorithm Using EM Other estimation principles *\n",
      "\n",
      "FA is a low rank parameterization of an MVN Inference of the latent factors Unidentiﬁability Mixtures of factor analysers EM for factor analysis models Fitting FA models with missing data 387\n",
      "\n",
      "381 381\n",
      "\n",
      "389\n",
      "\n",
      "414\n",
      "\n",
      "383\n",
      "\n",
      "395\n",
      "\n",
      "406\n",
      "\n",
      "396\n",
      "\n",
      "411\n",
      "\n",
      "399\n",
      "\n",
      "404\n",
      "\n",
      "385\n",
      "\n",
      "398\n",
      "\n",
      "415\n",
      "\n",
      "382\n",
      "\n",
      "386\n",
      "\n",
      "407 407\n",
      "\n",
      "410\n",
      "\n",
      "398\n",
      "\n",
      "392\n",
      "\n",
      "387\n",
      "\n",
      "387\n",
      "\n",
      "405\n",
      "\n",
      "381\n",
      "\n",
      "13 Sparse linear models\n",
      "\n",
      "421\n",
      "\n",
      "13.1 13.2\n",
      "\n",
      "13.3\n",
      "\n",
      "13.4\n",
      "\n",
      "Introduction Bayesian variable selection 13.2.1 13.2.2 13.2.3 (cid:4)1 regularization: basics 13.3.1 13.3.2 13.3.3 13.3.4 13.3.5 13.3.6 (cid:4)1 regularization: algorithms Coordinate descent 13.4.1\n",
      "\n",
      "422 The spike and slab model From the Bernoulli-Gaussian model to (cid:4)0 regularization Algorithms\n",
      "\n",
      "Why does (cid:4)1 regularization yield sparse solutions? Optimality conditions for lasso Comparison of least squares, lasso, ridge and subset selection Regularization path Model selection Bayesian inference for linear models with Laplace priors\n",
      "\n",
      "421\n",
      "\n",
      "426\n",
      "\n",
      "429\n",
      "\n",
      "439\n",
      "\n",
      "441 441\n",
      "\n",
      "436\n",
      "\n",
      "424\n",
      "\n",
      "431\n",
      "\n",
      "430\n",
      "\n",
      "425\n",
      "\n",
      "440\n",
      "\n",
      "435\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "xv\n",
      "\n",
      "13.5\n",
      "\n",
      "13.4.2 LARS and other homotopy methods 13.4.3 Proximal and gradient projection methods 13.4.4 EM for lasso (cid:4)1 regularization: extensions 449 Group Lasso 13.5.1 Fused lasso 13.5.2 454 Elastic net (ridge and lasso combined) 13.5.3\n",
      "\n",
      "447\n",
      "\n",
      "449\n",
      "\n",
      "441\n",
      "\n",
      "455\n",
      "\n",
      "442\n",
      "\n",
      "13.6 Non-convex regularizers\n",
      "\n",
      "457\n",
      "\n",
      "13.7\n",
      "\n",
      "13.8\n",
      "\n",
      "13.6.1 13.6.2 13.6.3 Automatic relevance determination (ARD)/sparse Bayesian learning (SBL) 13.7.1 13.7.2 13.7.3 13.7.4 13.7.5 Sparse coding * 13.8.1 13.8.2 13.8.3 13.8.4\n",
      "\n",
      "Bridge regression 458 Hierarchical adaptive lasso Other hierarchical priors\n",
      "\n",
      "ARD for linear regression Whence sparsity? 465 Connection to MAP estimation Algorithms for ARD * 466 ARD for logistic regression 468\n",
      "\n",
      "Learning a sparse coding dictionary Results of dictionary learning from image patches Compressed sensing Image inpainting and denoising\n",
      "\n",
      "472\n",
      "\n",
      "462\n",
      "\n",
      "463\n",
      "\n",
      "458\n",
      "\n",
      "468\n",
      "\n",
      "465\n",
      "\n",
      "472\n",
      "\n",
      "469\n",
      "\n",
      "470\n",
      "\n",
      "463\n",
      "\n",
      "14 Kernels 14.1 14.2\n",
      "\n",
      "14.5\n",
      "\n",
      "14.4\n",
      "\n",
      "14.3\n",
      "\n",
      "Introduction Kernel functions 14.2.1 14.2.2 14.2.3 14.2.4 14.2.5 14.2.6 14.2.7 14.2.8 486 Using kernels inside GLMs 486 Kernel machines 14.3.1 14.3.2 L1VMs, RVMs, and other sparse vector machines The kernel trick Kernelized nearest neighbor classiﬁcation 14.4.1 Kernelized K-medoids clustering 489 14.4.2 Kernelized ridge regression 14.4.3 14.4.4 Kernel PCA Support vector machines (SVMs) 14.5.1 14.5.2\n",
      "\n",
      "479\n",
      "\n",
      "RBF kernels Kernels for comparing documents Mercer (positive deﬁnite) kernels 482 Linear kernels 482 Matern kernels 483 String kernels Pyramid match kernels Kernels derived from probabilistic generative models\n",
      "\n",
      "SVMs for regression SVMs for classiﬁcation\n",
      "\n",
      "479\n",
      "\n",
      "488\n",
      "\n",
      "479\n",
      "\n",
      "493\n",
      "\n",
      "480\n",
      "\n",
      "497\n",
      "\n",
      "498\n",
      "\n",
      "484\n",
      "\n",
      "496\n",
      "\n",
      "492\n",
      "\n",
      "481\n",
      "\n",
      "480\n",
      "\n",
      "489\n",
      "\n",
      "487\n",
      "\n",
      "485\n",
      "\n",
      "xvi\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "14.6 14.7\n",
      "\n",
      "14.5.3 14.5.4 14.5.5 Comparison of discriminative kernel methods Kernels for building generative models Smoothing kernels 14.7.1 Kernel density estimation (KDE) 14.7.2 From KDE to KNN 14.7.3 Kernel regression 14.7.4 Locally weighted regression 14.7.5\n",
      "\n",
      "Choosing C Summary of key points A probabilistic interpretation of SVMs\n",
      "\n",
      "504\n",
      "\n",
      "510\n",
      "\n",
      "507\n",
      "\n",
      "509\n",
      "\n",
      "504\n",
      "\n",
      "512\n",
      "\n",
      "507\n",
      "\n",
      "508\n",
      "\n",
      "505\n",
      "\n",
      "505\n",
      "\n",
      "15 Gaussian processes Introduction GPs for regression 15.2.1 15.2.2 15.2.3 15.2.4 15.2.5 15.2.6 GPs meet GLMs 15.3.1 15.3.2 15.3.3 Connection with other methods 15.4.1 15.4.2 15.4.3 15.4.4 15.4.5 15.4.6 15.4.7 GP latent variable model Approximation methods for large datasets\n",
      "\n",
      "15.1 15.2\n",
      "\n",
      "15.4\n",
      "\n",
      "15.5 15.6\n",
      "\n",
      "15.3\n",
      "\n",
      "532 Linear models compared to GPs Linear smoothers compared to GPs SVMs compared to GPs L1VM and RVMs compared to GPs Neural networks compared to GPs Smoothing splines compared to GPs * RKHS methods compared to GPs *\n",
      "\n",
      "Binary classiﬁcation Multi-class classiﬁcation GPs for Poisson regression\n",
      "\n",
      "Predictions using noise-free observations Predictions using noisy observations Effect of the kernel parameters Estimating the kernel parameters Computational and numerical issues * Semi-parametric GPs *\n",
      "\n",
      "515\n",
      "\n",
      "515\n",
      "\n",
      "525\n",
      "\n",
      "516\n",
      "\n",
      "540\n",
      "\n",
      "525\n",
      "\n",
      "524\n",
      "\n",
      "534\n",
      "\n",
      "528\n",
      "\n",
      "531\n",
      "\n",
      "519\n",
      "\n",
      "532\n",
      "\n",
      "542\n",
      "\n",
      "521\n",
      "\n",
      "534 535\n",
      "\n",
      "538\n",
      "\n",
      "533\n",
      "\n",
      "518\n",
      "\n",
      "536\n",
      "\n",
      "524\n",
      "\n",
      "517\n",
      "\n",
      "16 Adaptive basis function models\n",
      "\n",
      "543\n",
      "\n",
      "16.1 16.2\n",
      "\n",
      "16.3\n",
      "\n",
      "Introduction Classiﬁcation and regression trees (CART) 16.2.1 16.2.2 16.2.3 16.2.4 16.2.5 16.2.6 Generalized additive models\n",
      "\n",
      "Basics 544 Growing a tree 545 549 Pruning a tree Pros and cons of trees Random forests CART compared to hierarchical mixture of experts * 552\n",
      "\n",
      "543\n",
      "\n",
      "550\n",
      "\n",
      "550\n",
      "\n",
      "544\n",
      "\n",
      "551\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "xvii\n",
      "\n",
      "16.4\n",
      "\n",
      "16.5\n",
      "\n",
      "16.6\n",
      "\n",
      "16.7\n",
      "\n",
      "16.8\n",
      "\n",
      "16.3.1 16.3.2 16.3.3 Boosting 16.4.1 16.4.2 16.4.3 16.4.4 16.4.5 16.4.6 16.4.7 16.4.8 Why does boosting work so well? 16.4.9 Feedforward neural networks (multilayer perceptrons) 16.5.1 16.5.2 16.5.3 16.5.4 16.5.5 16.5.6 16.5.7 580 Ensemble learning Stacking 16.6.1 580 Error-correcting output codes 16.6.2 16.6.3 Ensemble learning is not equivalent to Bayes model averaging Experimental comparison 16.7.1 16.7.2 Interpreting black-box models\n",
      "\n",
      "Backﬁtting Computational efficiency Multivariate adaptive regression splines (MARS)\n",
      "\n",
      "582 Low-dimensional features High-dimensional features\n",
      "\n",
      "Forward stagewise additive modeling L2boosting 557 AdaBoost LogitBoost Boosting as functional gradient descent Sparse boosting 561 Multivariate adaptive regression trees (MART)\n",
      "\n",
      "Convolutional neural networks Other kinds of neural networks A brief history of the ﬁeld The backpropagation algorithm Identiﬁability Regularization Bayesian inference *\n",
      "\n",
      "A Bayesian view\n",
      "\n",
      "554\n",
      "\n",
      "558 559\n",
      "\n",
      "552\n",
      "\n",
      "572 572\n",
      "\n",
      "563\n",
      "\n",
      "576\n",
      "\n",
      "585\n",
      "\n",
      "553\n",
      "\n",
      "582 583\n",
      "\n",
      "568\n",
      "\n",
      "581\n",
      "\n",
      "564 568\n",
      "\n",
      "569\n",
      "\n",
      "562\n",
      "\n",
      "555\n",
      "\n",
      "560\n",
      "\n",
      "563\n",
      "\n",
      "562\n",
      "\n",
      "553\n",
      "\n",
      "581\n",
      "\n",
      "17 Markov and hidden Markov models\n",
      "\n",
      "589\n",
      "\n",
      "17.1 17.2 Markov models\n",
      "\n",
      "Introduction\n",
      "\n",
      "589\n",
      "\n",
      "589\n",
      "\n",
      "17.2.1 17.2.2 17.2.3 17.2.4\n",
      "\n",
      "Transition matrix Application: Language modeling Stationary distribution of a Markov chain * Application: Google’s PageRank algorithm for web page ranking *\n",
      "\n",
      "589\n",
      "\n",
      "591\n",
      "\n",
      "596\n",
      "\n",
      "600\n",
      "\n",
      "17.3 Hidden Markov models\n",
      "\n",
      "17.4\n",
      "\n",
      "17.3.1 Inference in HMMs 17.4.1 17.4.2 17.4.3 17.4.4 17.4.5\n",
      "\n",
      "603 Applications of HMMs\n",
      "\n",
      "Types of inference problems for temporal models The forwards algorithm The forwards-backwards algorithm The Viterbi algorithm Forwards ﬁltering, backwards sampling\n",
      "\n",
      "606\n",
      "\n",
      "612\n",
      "\n",
      "604\n",
      "\n",
      "609\n",
      "\n",
      "610\n",
      "\n",
      "616\n",
      "\n",
      "606\n",
      "\n",
      "xviii\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "17.5\n",
      "\n",
      "17.6\n",
      "\n",
      "617 Learning for HMMs Training with fully observed data 17.5.1 EM for HMMs (the Baum-Welch algorithm) 17.5.2 Bayesian methods for “ﬁtting” HMMs * 17.5.3 Discriminative training 17.5.4 621 17.5.5 Model selection 621 Generalizations of HMMs 17.6.1 17.6.2 17.6.3 17.6.4 17.6.5 17.6.6 17.6.7\n",
      "\n",
      "Variable duration (semi-Markov) HMMs Hierarchical HMMs Input-output HMMs Auto-regressive and buried HMMs Factorial HMM Coupled HMM and the inﬂuence model Dynamic Bayesian networks (DBNs)\n",
      "\n",
      "627\n",
      "\n",
      "624 625\n",
      "\n",
      "620\n",
      "\n",
      "617\n",
      "\n",
      "626\n",
      "\n",
      "628\n",
      "\n",
      "620\n",
      "\n",
      "622\n",
      "\n",
      "628\n",
      "\n",
      "618\n",
      "\n",
      "18 State space models 631 Introduction 631 Applications of SSMs 18.2.1 18.2.2 18.2.3 18.2.4 Inference in LG-SSM 18.3.1 18.3.2 Learning for LG-SSM 18.4.1 18.4.2 18.4.3 18.4.4 18.4.5 Approximate online inference for non-linear, non-Gaussian SSMs 18.5.1 18.5.2 18.5.3\n",
      "\n",
      "18.1 18.2\n",
      "\n",
      "18.6 Hybrid discrete/continuous SSMs Inference Application: data association and multi-target tracking Application: fault diagnosis 659 Application: econometric forecasting\n",
      "\n",
      "18.5\n",
      "\n",
      "18.4\n",
      "\n",
      "18.3\n",
      "\n",
      "18.6.1 18.6.2 18.6.3 18.6.4\n",
      "\n",
      "Extended Kalman ﬁlter (EKF) Unscented Kalman ﬁlter (UKF) Assumed density ﬁltering (ADF) 655\n",
      "\n",
      "The Kalman ﬁltering algorithm The Kalman smoothing algorithm\n",
      "\n",
      "646 Identiﬁability and numerical stability Training with fully observed data EM for LG-SSM Subspace methods Bayesian methods for “ﬁtting” LG-SSMs\n",
      "\n",
      "SSMs for object tracking Robotic SLAM Online parameter learning using recursive least squares SSM for time series forecasting *\n",
      "\n",
      "656\n",
      "\n",
      "640\n",
      "\n",
      "632\n",
      "\n",
      "633\n",
      "\n",
      "647\n",
      "\n",
      "647\n",
      "\n",
      "632\n",
      "\n",
      "648\n",
      "\n",
      "650 652\n",
      "\n",
      "640\n",
      "\n",
      "637\n",
      "\n",
      "647\n",
      "\n",
      "643\n",
      "\n",
      "660\n",
      "\n",
      "646\n",
      "\n",
      "647\n",
      "\n",
      "636\n",
      "\n",
      "647\n",
      "\n",
      "658\n",
      "\n",
      "19 Undirected graphical models (Markov random ﬁelds) 661\n",
      "\n",
      "19.1 19.2\n",
      "\n",
      "Introduction Conditional independence properties of UGMs 19.2.1\n",
      "\n",
      "Key properties\n",
      "\n",
      "661\n",
      "\n",
      "661\n",
      "\n",
      "661\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "xix\n",
      "\n",
      "19.3\n",
      "\n",
      "19.4\n",
      "\n",
      "19.5\n",
      "\n",
      "19.6\n",
      "\n",
      "19.7\n",
      "\n",
      "19.2.2 19.2.3 Parameterization of MRFs 19.3.1 19.3.2 Examples of MRFs 19.4.1 19.4.2 19.4.3 19.4.4 19.4.5 Learning 19.5.1 19.5.2 19.5.3 19.5.4 19.5.5 19.5.6 19.5.7 Conditional random ﬁelds (CRFs) 19.6.1 19.6.2 19.6.3 Structural SVMs 19.7.1 19.7.2 19.7.3 19.7.4 19.7.5\n",
      "\n",
      "693 SSVMs: a probabilistic view SSVMs: a non-probabilistic view 695 Cutting plane methods for ﬁtting SSVMs Online algorithms for ﬁtting SSVMs Latent structural SVMs\n",
      "\n",
      "684 Chain-structured CRFs, MEMMs and the label-bias problem Applications of CRFs 692 CRF training\n",
      "\n",
      "Ising model Hopﬁeld networks Potts model Gaussian MRFs 672 Markov logic networks *\n",
      "\n",
      "The Hammersley-Clifford theorem Representing potential functions 668\n",
      "\n",
      "Training maxent models using gradient methods Training partially observed maxent models Approximate methods for computing the MLEs of MRFs Pseudo likelihood Stochastic maximum likelihood 679 Feature induction for maxent models * Iterative proportional ﬁtting (IPF) *\n",
      "\n",
      "An undirected alternative to d-separation Comparing directed and undirected graphical models\n",
      "\n",
      "676\n",
      "\n",
      "668\n",
      "\n",
      "671\n",
      "\n",
      "665\n",
      "\n",
      "678\n",
      "\n",
      "669\n",
      "\n",
      "686\n",
      "\n",
      "701\n",
      "\n",
      "674\n",
      "\n",
      "693\n",
      "\n",
      "667\n",
      "\n",
      "665\n",
      "\n",
      "681\n",
      "\n",
      "700\n",
      "\n",
      "680\n",
      "\n",
      "698\n",
      "\n",
      "663\n",
      "\n",
      "677\n",
      "\n",
      "676\n",
      "\n",
      "664\n",
      "\n",
      "678\n",
      "\n",
      "684\n",
      "\n",
      "20 Exact inference for graphical models\n",
      "\n",
      "707\n",
      "\n",
      "20.1 20.2\n",
      "\n",
      "20.3\n",
      "\n",
      "20.4\n",
      "\n",
      "Introduction Belief propagation for trees 20.2.1 20.2.2 20.2.3 20.2.4 The variable elimination algorithm 20.3.1 20.3.2 20.3.3 The junction tree algorithm * 20.4.1 Creating a junction tree 20.4.2 Message passing on a junction tree 20.4.3\n",
      "\n",
      "Serial protocol Parallel protocol Gaussian BP * Other BP variants *\n",
      "\n",
      "714 The generalized distributive law * Computational complexity of VE A weakness of VE\n",
      "\n",
      "Computational complexity of JTA\n",
      "\n",
      "707\n",
      "\n",
      "710\n",
      "\n",
      "707\n",
      "\n",
      "709\n",
      "\n",
      "707\n",
      "\n",
      "720\n",
      "\n",
      "712\n",
      "\n",
      "720\n",
      "\n",
      "720\n",
      "\n",
      "717 717\n",
      "\n",
      "722\n",
      "\n",
      "725\n",
      "\n",
      "xx\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "20.4.4\n",
      "\n",
      "JTA generalizations *\n",
      "\n",
      "726\n",
      "\n",
      "20.5 Computational intractability of exact inference in the worst case\n",
      "\n",
      "726\n",
      "\n",
      "20.5.1\n",
      "\n",
      "Approximate inference\n",
      "\n",
      "727\n",
      "\n",
      "21 Variational inference\n",
      "\n",
      "731\n",
      "\n",
      "21.1 21.2\n",
      "\n",
      "21.3\n",
      "\n",
      "21.4\n",
      "\n",
      "21.5\n",
      "\n",
      "21.6\n",
      "\n",
      "21.7 21.8\n",
      "\n",
      "Introduction 731 Variational inference 21.2.1 21.2.2 The mean ﬁeld method 21.3.1 21.3.2 Structured mean ﬁeld * 21.4.1 Variational Bayes 21.5.1 21.5.2 Variational Bayes EM 21.6.1 Variational message passing and VIBES Local variational bounds * 21.8.1 21.8.2 21.8.3 21.8.4 21.8.5\n",
      "\n",
      "Alternative interpretations of the variational objective Forward or reverse KL? *\n",
      "\n",
      "Motivating applications Bohning’s quadratic bound to the log-sum-exp function Bounds for the sigmoid function Other bounds and approximations to the log-sum-exp function * Variational inference based on upper bounds\n",
      "\n",
      "Example: VB for a univariate Gaussian Example: VB for linear regression\n",
      "\n",
      "Derivation of the mean ﬁeld update equations Example: mean ﬁeld for the Ising model 739\n",
      "\n",
      "Example: VBEM for mixtures of Gaussians *\n",
      "\n",
      "Example: factorial HMM\n",
      "\n",
      "742\n",
      "\n",
      "732\n",
      "\n",
      "749\n",
      "\n",
      "735\n",
      "\n",
      "756\n",
      "\n",
      "756\n",
      "\n",
      "740\n",
      "\n",
      "733\n",
      "\n",
      "756\n",
      "\n",
      "760\n",
      "\n",
      "746\n",
      "\n",
      "742\n",
      "\n",
      "737\n",
      "\n",
      "750\n",
      "\n",
      "763\n",
      "\n",
      "736\n",
      "\n",
      "733\n",
      "\n",
      "758\n",
      "\n",
      "762\n",
      "\n",
      "22 More variational inference Introduction 767 Loopy belief propagation: algorithmic issues 22.2.1 22.2.2 22.2.3 22.2.4 22.2.5 22.2.6 Loopy belief propagation: theoretical issues * UGMs represented in exponential family form 22.3.1 The marginal polytope 22.3.2 Exact inference as a variational optimization problem 22.3.3 Mean ﬁeld as a variational optimization problem 22.3.4 LBP as a variational optimization problem 22.3.5 22.3.6 Loopy BP vs mean ﬁeld Extensions of belief propagation * 22.4.1\n",
      "\n",
      "22.1 22.2\n",
      "\n",
      "22.4\n",
      "\n",
      "22.3\n",
      "\n",
      "A brief history LBP on pairwise models LBP on a factor graph Convergence Accuracy of LBP Other speedup tricks for LBP *\n",
      "\n",
      "Generalized belief propagation\n",
      "\n",
      "767\n",
      "\n",
      "771\n",
      "\n",
      "767\n",
      "\n",
      "774\n",
      "\n",
      "769\n",
      "\n",
      "777\n",
      "\n",
      "783 783\n",
      "\n",
      "768\n",
      "\n",
      "783\n",
      "\n",
      "775\n",
      "\n",
      "767\n",
      "\n",
      "776\n",
      "\n",
      "779\n",
      "\n",
      "776\n",
      "\n",
      "779\n",
      "\n",
      "778\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "xxi\n",
      "\n",
      "22.5\n",
      "\n",
      "22.4.2 Expectation propagation 22.5.1 22.5.2 22.5.3 22.5.4 22.5.5 22.5.6\n",
      "\n",
      "Convex belief propagation\n",
      "\n",
      "EP as a variational inference problem Optimizing the EP objective using moment matching EP for the clutter problem LBP is a special case of EP Ranking players using TrueSkill Other applications of EP\n",
      "\n",
      "787\n",
      "\n",
      "799\n",
      "\n",
      "785\n",
      "\n",
      "791 792\n",
      "\n",
      "793\n",
      "\n",
      "788\n",
      "\n",
      "789\n",
      "\n",
      "22.6 MAP state estimation\n",
      "\n",
      "799\n",
      "\n",
      "22.6.1 Linear programming relaxation 22.6.2 Max-product belief propagation 22.6.3 22.6.4 22.6.5\n",
      "\n",
      "Graphcuts Experimental comparison of graphcuts and BP Dual decomposition\n",
      "\n",
      "801\n",
      "\n",
      "806\n",
      "\n",
      "799 800\n",
      "\n",
      "804\n",
      "\n",
      "23 Monte Carlo inference\n",
      "\n",
      "815\n",
      "\n",
      "23.1 23.2\n",
      "\n",
      "23.3\n",
      "\n",
      "23.4\n",
      "\n",
      "23.5\n",
      "\n",
      "23.6\n",
      "\n",
      "Introduction 815 Sampling from standard distributions 23.2.1 23.2.2 Rejection sampling Basic idea 23.3.1 Example 23.3.2 Application to Bayesian statistics 23.3.3 Adaptive rejection sampling 23.3.4 Rejection sampling in high dimensions 23.3.5 Importance sampling 820 23.4.1 23.4.2 23.4.3 23.4.4 Particle ﬁltering 23.5.1 23.5.2 23.5.3 23.5.4 23.5.5 23.5.6 23.5.7 Rao-Blackwellised particle ﬁltering (RBPF) 23.6.1 23.6.2 23.6.3\n",
      "\n",
      "RBPF for switching LG-SSMs Application: tracking a maneuvering target Application: Fast SLAM\n",
      "\n",
      "Using the cdf Sampling from a Gaussian (Box-Muller method)\n",
      "\n",
      "Sequential importance sampling The degeneracy problem 825 The resampling step The proposal distribution Application: robot localization Application: visual object tracking Application: time series forecasting\n",
      "\n",
      "Basic idea Handling unnormalized distributions Importance sampling for a DGM: likelihood weighting Sampling importance resampling (SIR)\n",
      "\n",
      "823\n",
      "\n",
      "818\n",
      "\n",
      "817 817\n",
      "\n",
      "820\n",
      "\n",
      "815\n",
      "\n",
      "825\n",
      "\n",
      "834\n",
      "\n",
      "827\n",
      "\n",
      "819\n",
      "\n",
      "815\n",
      "\n",
      "831\n",
      "\n",
      "828\n",
      "\n",
      "831\n",
      "\n",
      "824\n",
      "\n",
      "819\n",
      "\n",
      "828 831\n",
      "\n",
      "821\n",
      "\n",
      "822\n",
      "\n",
      "820\n",
      "\n",
      "832\n",
      "\n",
      "817\n",
      "\n",
      "822\n",
      "\n",
      "24 Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "837\n",
      "\n",
      "xxii\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "24.1 24.2 Gibbs sampling\n",
      "\n",
      "Introduction\n",
      "\n",
      "837\n",
      "\n",
      "838\n",
      "\n",
      "24.2.1 24.2.2 24.2.3 24.2.4 24.2.5 24.2.6 24.2.7 24.2.8\n",
      "\n",
      "Basic idea Example: Gibbs sampling for the Ising model Example: Gibbs sampling for inferring the parameters of a GMM Collapsed Gibbs sampling * Gibbs sampling for hierarchical GLMs BUGS and JAGS The Imputation Posterior (IP) algorithm Blocking Gibbs sampling\n",
      "\n",
      "838\n",
      "\n",
      "846\n",
      "\n",
      "847\n",
      "\n",
      "841\n",
      "\n",
      "844\n",
      "\n",
      "847\n",
      "\n",
      "838\n",
      "\n",
      "840\n",
      "\n",
      "24.3 Metropolis Hastings algorithm\n",
      "\n",
      "848\n",
      "\n",
      "24.4\n",
      "\n",
      "24.5\n",
      "\n",
      "24.3.1 24.3.2 24.3.3 24.3.4 24.3.5 24.3.6 Why MH works * 24.3.7 Speed and accuracy of MCMC 24.4.1 24.4.2 24.4.3 24.4.4 24.4.5 Auxiliary variable MCMC * 24.5.1 24.5.2 24.5.3 24.5.4\n",
      "\n",
      "Basic idea Gibbs sampling is a special case of MH Proposal distributions Adaptive MCMC Initialization and mode hopping\n",
      "\n",
      "863 Auxiliary variable sampling for logistic regression Slice sampling Swendsen Wang Hybrid/Hamiltonian MCMC *\n",
      "\n",
      "Reversible jump (trans-dimensional) MCMC *\n",
      "\n",
      "The burn-in phase Mixing rates of Markov chains * Practical convergence diagnostics Accuracy of MCMC How many chains?\n",
      "\n",
      "848\n",
      "\n",
      "864\n",
      "\n",
      "853\n",
      "\n",
      "866\n",
      "\n",
      "854\n",
      "\n",
      "856\n",
      "\n",
      "860 862\n",
      "\n",
      "856\n",
      "\n",
      "850\n",
      "\n",
      "868\n",
      "\n",
      "857\n",
      "\n",
      "854\n",
      "\n",
      "858\n",
      "\n",
      "849\n",
      "\n",
      "855\n",
      "\n",
      "863\n",
      "\n",
      "24.6 Annealing methods\n",
      "\n",
      "868\n",
      "\n",
      "24.7\n",
      "\n",
      "Simulated annealing 24.6.1 Annealed importance sampling 24.6.2 24.6.3 Parallel tempering Approximating the marginal likelihood 24.7.1 24.7.2 24.7.3\n",
      "\n",
      "The candidate method Harmonic mean estimate Annealed importance sampling\n",
      "\n",
      "871\n",
      "\n",
      "869\n",
      "\n",
      "872\n",
      "\n",
      "872\n",
      "\n",
      "871\n",
      "\n",
      "872\n",
      "\n",
      "873\n",
      "\n",
      "25 Clustering\n",
      "\n",
      "875\n",
      "\n",
      "25.1\n",
      "\n",
      "Introduction 25.1.1 25.1.2\n",
      "\n",
      "Measuring (dis)similarity Evaluating the output of clustering methods *\n",
      "\n",
      "875\n",
      "\n",
      "875\n",
      "\n",
      "876\n",
      "\n",
      "25.2 Dirichlet process mixture models\n",
      "\n",
      "879\n",
      "\n",
      "25.2.1 25.2.2\n",
      "\n",
      "From ﬁnite to inﬁnite mixture models The Dirichlet process\n",
      "\n",
      "882\n",
      "\n",
      "879\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "xxiii\n",
      "\n",
      "25.2.3 25.2.4\n",
      "\n",
      "Applying Dirichlet processes to mixture modeling Fitting a DP mixture model\n",
      "\n",
      "886\n",
      "\n",
      "885\n",
      "\n",
      "25.3 Affinity propagation 25.4\n",
      "\n",
      "Spectral clustering 25.4.1 25.4.2 25.4.3\n",
      "\n",
      "890 Graph Laplacian Normalized graph Laplacian 893 Example\n",
      "\n",
      "887\n",
      "\n",
      "891\n",
      "\n",
      "892\n",
      "\n",
      "25.5 Hierarchical clustering\n",
      "\n",
      "893\n",
      "\n",
      "25.5.1 25.5.2 25.5.3 25.5.4\n",
      "\n",
      "Agglomerative clustering Divisive clustering Choosing the number of clusters Bayesian hierarchical clustering\n",
      "\n",
      "898\n",
      "\n",
      "895\n",
      "\n",
      "899\n",
      "\n",
      "899\n",
      "\n",
      "25.6 Clustering datapoints and features\n",
      "\n",
      "901\n",
      "\n",
      "25.6.1 903 Biclustering 25.6.2 Multi-view clustering\n",
      "\n",
      "903\n",
      "\n",
      "26 Graphical model structure learning\n",
      "\n",
      "907\n",
      "\n",
      "26.1 26.2\n",
      "\n",
      "26.3\n",
      "\n",
      "26.4\n",
      "\n",
      "26.5\n",
      "\n",
      "26.6\n",
      "\n",
      "26.7\n",
      "\n",
      "Introduction Structure learning for knowledge discovery 26.2.1 26.2.2 Learning tree structures 26.3.1 26.3.2 26.3.3 26.3.4 Mixtures of trees Learning DAG structures Markov equivalence 26.4.1 Exact structural inference 26.4.2 26.4.3 920 Scaling up to larger graphs Learning DAG structure with latent variables 26.5.1 26.5.2 26.5.3 26.5.4 26.5.5 Learning causal DAGs 26.6.1 26.6.2 26.6.3 Learning undirected Gaussian graphical models 26.7.1 26.7.2 26.7.3 26.7.4\n",
      "\n",
      "MLE for a GGM Graphical lasso Bayesian inference for GGM structure * Handling non-Gaussian data using copulas *\n",
      "\n",
      "Relevance networks Dependency networks 910\n",
      "\n",
      "Directed or undirected tree? Chow-Liu algorithm for ﬁnding the ML tree structure Finding the MAP forest\n",
      "\n",
      "Causal interpretation of DAGs Using causal DAGs to resolve Simpson’s paradox Learning causal DAG structures\n",
      "\n",
      "Approximating the marginal likelihood when we have missing data Structural EM Discovering hidden variables Case study: Google’s Rephil Structural equation models *\n",
      "\n",
      "907\n",
      "\n",
      "931\n",
      "\n",
      "925\n",
      "\n",
      "938 939\n",
      "\n",
      "914\n",
      "\n",
      "914\n",
      "\n",
      "908\n",
      "\n",
      "914\n",
      "\n",
      "909\n",
      "\n",
      "912\n",
      "\n",
      "916\n",
      "\n",
      "926 928 929\n",
      "\n",
      "911\n",
      "\n",
      "931\n",
      "\n",
      "935\n",
      "\n",
      "908\n",
      "\n",
      "922\n",
      "\n",
      "938\n",
      "\n",
      "941\n",
      "\n",
      "942\n",
      "\n",
      "933\n",
      "\n",
      "912\n",
      "\n",
      "922\n",
      "\n",
      "xxiv\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "26.8\n",
      "\n",
      "Learning undirected discrete graphical models 26.8.1 26.8.2\n",
      "\n",
      "Graphical lasso for MRFs/CRFs Thin junction trees\n",
      "\n",
      "944\n",
      "\n",
      "942\n",
      "\n",
      "942\n",
      "\n",
      "27 Latent variable models for discrete data\n",
      "\n",
      "945\n",
      "\n",
      "27.1 27.2 Distributed state LVMs for discrete data Mixture models 946 Exponential family PCA LDA and mPCA 948 GaP model and non-negative matrix factorization\n",
      "\n",
      "27.7\n",
      "\n",
      "27.6\n",
      "\n",
      "27.5\n",
      "\n",
      "27.3\n",
      "\n",
      "27.4\n",
      "\n",
      "27.2.1 27.2.2 27.2.3 27.2.4 Latent Dirichlet allocation (LDA) 27.3.1 27.3.2 27.3.3 27.3.4 27.3.5 27.3.6 27.3.7 27.3.8 Extensions of LDA 27.4.1 27.4.2 27.4.3 27.4.4 LVMs for graph-structured data Stochastic block model 27.5.1 Mixed membership stochastic block model 27.5.2 Relational topic model 27.5.3 LVMs for relational data 975 Inﬁnite relational model 27.6.1 Probabilistic matrix factorization for collaborative ﬁltering 27.6.2 Restricted Boltzmann machines (RBMs) 27.7.1 27.7.2 27.7.3\n",
      "\n",
      "Introduction\n",
      "\n",
      "Correlated topic model Dynamic topic model LDA-HMM Supervised LDA\n",
      "\n",
      "Basics Unsupervised discovery of topics Quantitatively evaluating LDA as a language model Fitting using (collapsed) Gibbs sampling Example Fitting using batch variational inference Fitting using online variational inference Determining the number of topics\n",
      "\n",
      "Varieties of RBMs Learning RBMs Applications of RBMs\n",
      "\n",
      "945\n",
      "\n",
      "950\n",
      "\n",
      "956\n",
      "\n",
      "961\n",
      "\n",
      "963\n",
      "\n",
      "987\n",
      "\n",
      "967\n",
      "\n",
      "985\n",
      "\n",
      "991\n",
      "\n",
      "961 962\n",
      "\n",
      "970 971\n",
      "\n",
      "974\n",
      "\n",
      "950\n",
      "\n",
      "947\n",
      "\n",
      "976\n",
      "\n",
      "983\n",
      "\n",
      "946\n",
      "\n",
      "953\n",
      "\n",
      "960\n",
      "\n",
      "957 959\n",
      "\n",
      "955\n",
      "\n",
      "973\n",
      "\n",
      "949\n",
      "\n",
      "953\n",
      "\n",
      "979\n",
      "\n",
      "28 Deep learning\n",
      "\n",
      "995\n",
      "\n",
      "28.1 28.2 Deep generative models\n",
      "\n",
      "Introduction\n",
      "\n",
      "28.2.1 28.2.2 28.2.3 28.2.4\n",
      "\n",
      "995 Deep directed networks Deep Boltzmann machines Deep belief networks Greedy layer-wise learning of DBNs\n",
      "\n",
      "995\n",
      "\n",
      "997\n",
      "\n",
      "996\n",
      "\n",
      "996\n",
      "\n",
      "998\n",
      "\n",
      "28.3 Deep neural networks\n",
      "\n",
      "999\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "xxv\n",
      "\n",
      "28.3.1 28.3.2 28.3.3\n",
      "\n",
      "Deep multi-layer perceptrons Deep auto-encoders 1000 Stacked denoising auto-encoders\n",
      "\n",
      "999\n",
      "\n",
      "1001\n",
      "\n",
      "28.4 Applications of deep networks\n",
      "\n",
      "1001\n",
      "\n",
      "28.4.1 28.4.2 28.4.3 28.4.4 28.4.5 28.5 Discussion\n",
      "\n",
      "Handwritten digit classiﬁcation using DBNs Data visualization and feature discovery using deep auto-encoders Information retrieval using deep auto-encoders (semantic hashing) Learning audio features using 1d convolutional DBNs Learning image features using 2d convolutional DBNs\n",
      "\n",
      "1005\n",
      "\n",
      "1001\n",
      "\n",
      "1004 1005\n",
      "\n",
      "1002 1003\n",
      "\n",
      "Notation\n",
      "\n",
      "1009\n",
      "\n",
      "Bibliography\n",
      "\n",
      "1015\n",
      "\n",
      "Indexes\n",
      "\n",
      "1047 Index to code Index to keywords\n",
      "\n",
      "1047\n",
      "\n",
      "1050\n",
      "\n",
      "Preface\n",
      "\n",
      "Introduction\n",
      "\n",
      "With the ever increasing amounts of data in electronic form, the need for automated methods for data analysis continues to grow. The goal of machine learning is to develop methods that can automatically detect patterns in data, and then to use the uncovered patterns to predict future data or other outcomes of interest. Machine learning is thus closely related to the ﬁelds of statistics and data mining, but differs slightly in terms of its emphasis and terminology. This book provides a detailed introduction to the ﬁeld, and includes worked examples drawn from application domains such as molecular biology, text processing, computer vision, and robotics.\n",
      "\n",
      "Target audience\n",
      "\n",
      "This book is suitable for upper-level undergraduate students and beginning graduate students in computer science, statistics, electrical engineering, econometrics, or any one else who has the appropriate mathematical background. Speciﬁcally, the reader is assumed to already be familiar with basic multivariate calculus, probability, linear algebra, and computer programming. Prior exposure to statistics is helpful but not necessary.\n",
      "\n",
      "A probabilistic approach\n",
      "\n",
      "This books adopts the view that the best way to make machines that can learn from data is to use the tools of probability theory, which has been the mainstay of statistics and engineering for centuries. Probability theory can be applied to any problem involving uncertainty. In machine learning, uncertainty comes in many forms: what is the best prediction (or decision) given some data? what is the best model given some data? what measurement should I perform next? etc. including inferring parameters of statistical models, is sometimes called a Bayesian approach. However, this term tends to elicit very strong reactions (either positive or negative, depending on who you ask), so we prefer the more neutral term “probabilistic approach”. Besides, we will often use techniques such as maximum likelihood estimation, which are not Bayesian methods, but certainly fall within the probabilistic paradigm.\n",
      "\n",
      "The systematic application of probabilistic reasoning to all inferential problems,\n",
      "\n",
      "Rather than describing a cookbook of different heuristic methods, this book stresses a princi- pled model-based approach to machine learning. For any given model, a variety of algorithms\n",
      "\n",
      "xxviii\n",
      "\n",
      "Preface\n",
      "\n",
      "can often be applied. Conversely, any given algorithm can often be applied to a variety of models. This kind of modularity, where we distinguish model from algorithm, is good pedagogy and good engineering.\n",
      "\n",
      "We will often use the language of graphical models to specify our models in a concise and intuitive way. In addition to aiding comprehension, the graph structure aids in developing efficient algorithms, as we will see. However, this book is not primarily about graphical models; it is about probabilistic modeling in general.\n",
      "\n",
      "A practical approach\n",
      "\n",
      "Nearly all of the methods described in this book have been implemented in a MATLAB software package called PMTK, which stands for probabilistic modeling toolkit. This is freely available from pmtk3.googlecode.com (the digit 3 refers to the third edition of the toolkit, which is the one used in this version of the book). There are also a variety of supporting ﬁles, written by other people, available at pmtksupport.googlecode.com. These will be downloaded automatically, if you follow the setup instructions described on the PMTK website.\n",
      "\n",
      "MATLAB is a high-level, interactive scripting language ideally suited to numerical computation and data visualization, and can be purchased from www.mathworks.com. Some of the code requires the Statistics toolbox, which needs to be purchased separately. There is also a free version of Matlab called Octave, available at http://www.gnu.org/software/octave/, which supports most of the functionality of MATLAB. Some (but not all) of the code in this book also works in Octave. See the PMTK website for details.\n",
      "\n",
      "PMTK was used to generate many of the ﬁgures in this book; the source code for these ﬁgures is included on the PMTK website, allowing the reader to easily see the effects of changing the data or algorithm or parameter settings. The book refers to ﬁles by name, e.g., naiveBayesFit. In order to ﬁnd the corresponding ﬁle, you can use two methods: within Matlab you can type which naiveBayesFit and it will return the full path to the ﬁle; or, if you do not have Matlab but want to read the source code anyway, you can use your favorite search engine, which should return the corresponding ﬁle from the pmtk3.googlecode.com website.\n",
      "\n",
      "Details on how to use PMTK can be found on the website, which will be udpated over time.\n",
      "\n",
      "Details on the underlying theory behind these methods can be found in this book.\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "A book this large is obviously a team effort. I would especially like to thank the following people: my wife Margaret, for keeping the home ﬁres burning as I toiled away in my office for the last six years; Matt Dunham, who created many of the ﬁgures in this book, and who wrote much of the code in PMTK; Baback Moghaddam, who gave extremely detailed feedback on every page of an earlier draft of the book; Chris Williams, who also gave very detailed feedback; Cody Severinski and Wei-Lwun Lu, who assisted with ﬁgures; generations of UBC students, who gave helpful comments on earlier drafts; Daphne Koller, Nir Friedman, and Chris Manning, for letting me use their latex style ﬁles; Stanford University, Google Research and Skyline College for hosting me during part of my sabbatical; and various Canadian funding agencies (NSERC, CRC and CIFAR) who have supported me ﬁnancially over the years.\n",
      "\n",
      "In addition, I would like to thank the following people for giving me helpful feedback on\n",
      "\n",
      "Preface\n",
      "\n",
      "xxix\n",
      "\n",
      "parts of the book, and/or for sharing ﬁgures, code, exercises or even (in some cases) text: David Blei, Hannes Bretschneider, Greg Corrado, Arnaud Doucet, Mario Figueiredo, Nando de Freitas, Mark Girolami, Gabriel Goh, Tom Griffiths, Katherine Heller, Geoff Hinton, Aapo Hyvarinen, Tommi Jaakkola, Mike Jordan, Charles Kemp, Emtiyaz Khan, Bonnie Kirkpatrick, Daphne Koller, Zico Kolter, Honglak Lee, Julien Mairal, Andrew McPherson, Tom Minka, Ian Nabney, Arthur Pope, Carl Rassmussen, Ryan Rifkin, Ruslan Salakhutdinov, Mark Schmidt, Daniel Selsam, David Sontag, Erik Sudderth, Josh Tenenbaum, Kai Yu, Martin Wainwright, Yair Weiss.\n",
      "\n",
      "Kevin Patrick Murphy Palo Alto, California June 2012\n",
      "\n",
      "1 Introduction\n",
      "\n",
      "1.1 Machine learning: what and why?\n",
      "\n",
      "We are drowning in information and starving for knowledge. — John Naisbitt.\n",
      "\n",
      "We are entering the era of big data. For example, there are about 1 trillion web pages1; one hour of video is uploaded to YouTube every second, amounting to 10 years of content every day2; the genomes of 1000s of people, each of which has a length of 3.8 × 109 base pairs, have been sequenced by various labs; Walmart handles more than 1M transactions per hour and has databases containing more than 2.5 petabytes (2.5 × 1015) of information (Cukier 2010); and so on.\n",
      "\n",
      "This deluge of data calls for automated methods of data analysis, which is what machine learning provides. In particular, we deﬁne machine learning as a set of methods that can automatically detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty (such as planning how to collect more data!).\n",
      "\n",
      "This books adopts the view that the best way to solve such problems is to use the tools of probability theory. Probability theory can be applied to any problem involving uncertainty. In machine learning, uncertainty comes in many forms: what is the best prediction about the future given some past data? what is the best model to explain some data? what measurement should I perform next? etc. The probabilistic approach to machine learning is closely related to the ﬁeld of statistics, but differs slightly in terms of its emphasis and terminology3.\n",
      "\n",
      "We will describe a wide variety of probabilistic models, suitable for a wide variety of data and tasks. We will also describe a wide variety of algorithms for learning and using such models. The goal is not to develop a cook book of ad hoc techiques, but instead to present a uniﬁed view of the ﬁeld through the lens of probabilistic modeling and inference. Although we will pay attention to computational efficiency, details on how to scale these methods to truly massive datasets are better described in other books, such as (Rajaraman and Ullman 2011; Bekkerman et al. 2011).\n",
      "\n",
      "1. http://googleblog.blogspot.com/2008/07/we-knew-web-was-big.html 2. Source: http://www.youtube.com/t/press_statistics. 3. Rob Tibshirani, a statistician at Stanford university, has created an amusing comparison between machine learning and statistics, available at http://www-stat.stanford.edu/~tibs/stat315a/glossary.pdf.\n",
      "\n",
      "2\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "It should be noted, however, that even when one has an apparently massive data set, the effective number of data points for certain cases of interest might be quite small. In fact, data across a variety of domains exhibits a property known as the long tail, which means that a few things (e.g., words) are very common, but most things are quite rare (see Section 2.4.6 for details). For example, 20% of Google searches each day have never been seen before4. This means that the core statistical issues that we discuss in this book, concerning generalizing from relatively small samples sizes, are still very relevant even in the big data era.\n",
      "\n",
      "1.1.1\n",
      "\n",
      "Types of machine learning\n",
      "\n",
      "In the predictive or supervised Machine learning is usually divided into two main types. learning approach, the goal is to learn a mapping from inputs x to outputs y, given a labeled set of input-output pairs D = {(xi, yi)}N i=1. Here D is called the training set, and N is the number of training examples.\n",
      "\n",
      "In the simplest setting, each training input xi is a D-dimensional vector of numbers, rep- resenting, say, the height and weight of a person. These are called features, attributes or covariates. In general, however, xi could be a complex structured object, such as an image, a sentence, an email message, a time series, a molecular shape, a graph, etc.\n",
      "\n",
      "Similarly the form of the output or response variable can in principle be anything, but most methods assume that yi is a categorical or nominal variable from some ﬁnite set, yi ∈ {1, . . . , C} (such as male or female), or that yi is a real-valued scalar (such as income level). When yi is categorical, the problem is known as classiﬁcation or pattern recognition, and when yi is real-valued, the problem is known as regression. Another variant, known as ordinal regression, occurs where label space Y has some natural ordering, such as grades A–F. The second main type of machine learning is the descriptive or unsupervised learning approach. Here we are only given inputs, D = {xi}N i=1, and the goal is to ﬁnd “interesting patterns” in the data. This is sometimes called knowledge discovery. This is a much less well-deﬁned problem, since we are not told what kinds of patterns to look for, and there is no obvious error metric to use (unlike supervised learning, where we can compare our prediction of y for a given x to the observed value).\n",
      "\n",
      "There is a third type of machine learning, known as reinforcement learning, which is somewhat less commonly used. This is useful for learning how to act or behave when given occasional reward or punishment signals. (For example, consider how a baby learns to walk.) Unfortunately, RL is beyond the scope of this book, although we do discuss decision theory in Section 5.7, which is the basis of RL. See e.g., (Kaelbling et al. 1996; Sutton and Barto 1998; Russell and Norvig 2010; Szepesvari 2010; Wiering and van Otterlo 2012) for more information on RL.\n",
      "\n",
      "4. http://certifiedknowledge.org/blog/are-search-queries-becoming-even-more-unique-statistic s-from-google.\n",
      "\n",
      "1.2. Supervised learning\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.1 Left: Some labeled training examples of colored shapes, along with 3 unlabeled test cases. Right: Representing the training data as an N × D design matrix. Row i represents the feature vector xi. The last column is the label, yi ∈ {0, 1}. Based on a ﬁgure by Leslie Kaelbling.\n",
      "\n",
      "1.2\n",
      "\n",
      "Supervised learning\n",
      "\n",
      "We begin our investigation of machine learning by discussing supervised learning, which is the form of ML most widely used in practice.\n",
      "\n",
      "1.2.1\n",
      "\n",
      "Classiﬁcation\n",
      "\n",
      "In this section, we discuss classiﬁcation. Here the goal is to learn a mapping from inputs x to outputs y, where y ∈ {1, . . . , C}, with C being the number of classes. If C = 2, this is called binary classiﬁcation (in which case we often assume y ∈ {0, 1}); if C > 2, this is called multiclass classiﬁcation. If the class labels are not mutually exclusive (e.g., somebody may be classiﬁed as tall and strong), we call it multi-label classiﬁcation, but this is best viewed as predicting multiple related binary class labels (a so-called multiple output model). When we use the term “classiﬁcation”, we will mean multiclass classiﬁcation with a single output, unless we state otherwise.\n",
      "\n",
      "One way to formalize the problem is as function approximation. We assume y = f (x) for some unknown function f , and the goal of learning is to estimate the function f given a labeled training set, and then to make predictions using ˆy = ˆf (x). (We use the hat symbol to denote an estimate.) Our main goal is to make predictions on novel inputs, meaning ones that we have not seen before (this is called generalization), since predicting the response on the training set is easy (we can just look up the answer).\n",
      "\n",
      "1.2.1.1\n",
      "\n",
      "Example\n",
      "\n",
      "As a simple toy example of classiﬁcation, consider the problem illustrated in Figure 1.1(a). We have two classes of object which correspond to labels 0 and 1. The inputs are colored shapes. These have been described by a set of D features or attributes, which are stored in an N × D design matrix X, shown in Figure 1.1(b). The input features x can be discrete, continuous or a combination of the two. In addition to the inputs, we have a vector of training labels y.\n",
      "\n",
      "In Figure 1.1, the test cases are a blue crescent, a yellow circle and a blue arrow. None of these have been seen before. Thus we are required to generalize beyond the training set. A\n",
      "\n",
      "4\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "reasonable guess is that blue crescent should be y = 1, since all blue shapes are labeled 1 in the training set. The yellow circle is harder to classify, since some yellow things are labeled y = 1 and some are labeled y = 0, and some circles are labeled y = 1 and some y = 0. Consequently it is not clear what the right label should be in the case of the yellow circle. Similarly, the correct label for the blue arrow is unclear.\n",
      "\n",
      "1.2.1.2\n",
      "\n",
      "The need for probabilistic predictions\n",
      "\n",
      "To handle ambiguous cases, such as the yellow circle above, it is desirable to return a probability. The reader is assumed to already have some familiarity with basic concepts in probability. If not, please consult Chapter 2 for a refresher, if necessary.\n",
      "\n",
      "We will denote the probability distribution over possible labels, given the input vector x and training set D by p(y|x, D). In general, this represents a vector of length C. (If there are just two classes, it is sufficient to return the single number p(y = 1|x, D), since p(y = 1|x, D) + p(y = 0|x, D) = 1.) In our notation, we make explicit that the probability is conditional on the test input x, as well as the training set D, by putting these terms on the right hand side of the conditioning bar |. We are also implicitly conditioning on the form of model that we use to make predictions. When choosing between different models, we will make this assumption explicit by writing p(y|x, D, M ), where M denotes the model. However, if the model is clear from context, we will drop M from our notation for brevity.\n",
      "\n",
      "Given a probabilistic output, we can always compute our “best guess” as to the “true label”\n",
      "\n",
      "using\n",
      "\n",
      "ˆy = ˆf (x) =\n",
      "\n",
      "C argmax c=1\n",
      "\n",
      "p(y = c|x, D)\n",
      "\n",
      "(1.1)\n",
      "\n",
      "This corresponds to the most probable class label, and is called the mode of the distribution p(y|x, D); it is also known as a MAP estimate (MAP stands for maximum a posteriori). Using the most probable label makes intuitive sense, but we will give a more formal justiﬁcation for this procedure in Section 5.7.\n",
      "\n",
      "In such a case we are not very conﬁdent of our answer, so it might be better to say “I don’t know” instead of returning an answer that we don’t really trust. This is particularly important in domains such as medicine and ﬁnance where we may be risk averse, as we explain in Section 5.7. Another application where it is important to assess risk is when playing TV game shows, such as Jeopardy. In this game, contestants have to solve various word puzzles and answer a variety In 2011, IBM unveiled a of trivia questions, but if they answer incorrectly, they lose money. computer system called Watson which beat the top human Jeopardy champion. Watson uses a variety of interesting techniques (Ferrucci et al. 2010), but the most pertinent one for our present purposes is that it contains a module that estimates how conﬁdent it is of its answer. The system only chooses to “buzz in” its answer if sufficiently conﬁdent it is correct. Similarly, Google has a system known as SmartASS (ad selection system) that predicts the probability you will click on an ad based on your search history and other user and ad-speciﬁc features (Metz 2010). This probability is known as the click-through rate or CTR, and can be used to maximize expected proﬁt. We will discuss some of the basic principles behind systems such as SmartASS later in this book.\n",
      "\n",
      "Now consider a case such as the yellow circle, where p(ˆy|x, D) is far from 1.0.\n",
      "\n",
      "1.2. Supervised learning\n",
      "\n",
      "5\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "s t n e m u c o d\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "600\n",
      "\n",
      "700\n",
      "\n",
      "800\n",
      "\n",
      "900\n",
      "\n",
      "1000\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50 words\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "Figure 1.2 Subset of size 16242 x 100 of the 20-newsgroups data. We only show 1000 rows, for clarity. Each row is a document (represented as a bag-of-words bit vector), each column is a word. The red lines separate the 4 classes, which are (in descending order) comp, rec, sci, talk (these are the titles of USENET groups). We can see that there are subsets of words whose presence or absence is indicative of the class. The data is available from http://cs.nyu.edu/~roweis/data.html. Figure generated by newsgroupsVisualize.\n",
      "\n",
      "1.2.1.3\n",
      "\n",
      "Real-world applications\n",
      "\n",
      "Classiﬁcation is probably the most widely used form of machine learning, and has been used to solve many interesting and often difficult real-world problems. We have already mentioned some important applciations. We give a few more examples below.\n",
      "\n",
      "Document classiﬁcation and email spam ﬁltering\n",
      "\n",
      "In document classiﬁcation, the goal is to classify a document, such as a web page or email message, into one of C classes, that is, to compute p(y = c|x, D), where x is some represen- tation of the text. A special case of this is email spam ﬁltering, where the classes are spam y = 1 or ham y = 0.\n",
      "\n",
      "Most classiﬁers assume that the input vector x has a ﬁxed size. A common way to represent variable-length documents in feature-vector format is to use a bag of words representation. This is explained in detail in Section 3.4.4.1, but the basic idea is to deﬁne xij = 1 iff word j occurs in document i. If we apply this transformation to every document in our data set, we get a binary document × word co-occurrence matrix: see Figure 1.2 for an example. Essentially the document classiﬁcation problem has been reduced to one that looks for subtle changes in the pattern of bits. For example, we may notice that most spam messages have a high probability of containing the words “buy”, “cheap”, “viagra”, etc. In Exercise 8.1 and Exercise 8.2, you will get hands-on experience applying various classiﬁcation techniques to the spam ﬁltering problem.\n",
      "\n",
      "6\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 1.3 Three types of iris ﬂowers: setosa, versicolor and virginica. Source: http://www.statlab.u ni-heidelberg.de/data/iris/ . Used with kind permission of Dennis Kramb and SIGNA.\n",
      "\n",
      "sepal length\n",
      "\n",
      "sepal width\n",
      "\n",
      "petal length\n",
      "\n",
      "petal width\n",
      "\n",
      "h\n",
      "\n",
      "t\n",
      "\n",
      "g n e\n",
      "\n",
      "l\n",
      "\n",
      "l\n",
      "\n",
      "a p e s\n",
      "\n",
      "h\n",
      "\n",
      "t\n",
      "\n",
      "d w\n",
      "\n",
      "i\n",
      "\n",
      "l\n",
      "\n",
      "a p e s\n",
      "\n",
      "h\n",
      "\n",
      "t\n",
      "\n",
      "g n e\n",
      "\n",
      "l\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "\n",
      "t\n",
      "\n",
      "e p\n",
      "\n",
      "h d w\n",
      "\n",
      "t\n",
      "\n",
      "i\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "\n",
      "t\n",
      "\n",
      "e p\n",
      "\n",
      "Figure 1.4 Visualization of the Iris data as a pairwise scatter plot. The diagonal plots the marginal histograms of the 4 features. The off diagonals contain scatterplots of all possible pairs of features. Red circle = setosa, green diamond = versicolor, blue star = virginica. Figure generated by fisheririsDemo.\n",
      "\n",
      "Classifying ﬂowers\n",
      "\n",
      "Figure 1.3 gives another example of classiﬁcation, due to the statistician Ronald Fisher. The goal is to learn to distinguish three different kinds of iris ﬂower, called setosa, versicolor and virginica. Fortunately, rather than working directly with images, a botanist has already extracted 4 useful (Such feature features or characteristics: sepal length and width, and petal length and width. extraction is an important, but difficult, task. Most machine learning methods use features chosen by some human. Later we will discuss some methods that can learn good features from the data.) If we make a scatter plot of the iris data, as in Figure 1.4, we see that it is easy to distinguish setosas (red circles) from the other two classes by just checking if their petal length\n",
      "\n",
      "1.2. Supervised learning\n",
      "\n",
      "7\n",
      "\n",
      "true class = 7\n",
      "\n",
      "true class = 2\n",
      "\n",
      "true class = 1\n",
      "\n",
      "true class = 7\n",
      "\n",
      "true class = 2\n",
      "\n",
      "true class = 1\n",
      "\n",
      "true class = 0\n",
      "\n",
      "true class = 4\n",
      "\n",
      "true class = 1\n",
      "\n",
      "true class = 0\n",
      "\n",
      "true class = 4\n",
      "\n",
      "true class = 1\n",
      "\n",
      "true class = 4\n",
      "\n",
      "true class = 9\n",
      "\n",
      "true class = 5\n",
      "\n",
      "true class = 4\n",
      "\n",
      "true class = 9\n",
      "\n",
      "true class = 5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.5 (b) Same as (a), but with the features permuted randomly. Classiﬁcation performance is identical on both versions of the data (assuming the training data is permuted in an identical way). Figure generated by shuffledDigitsDemo.\n",
      "\n",
      "(a) First 9 test MNIST gray-scale images.\n",
      "\n",
      "or width is below some threshold. However, distinguishing versicolor from virginica is slightly (It is always a good idea harder; any decision will need to be based on at least two features. to perform exploratory data analysis, such as plotting the data, before applying a machine learning method.)\n",
      "\n",
      "Image classiﬁcation and handwriting recognition\n",
      "\n",
      "Now consider the harder problem of classifying images directly, where a human has not pre- processed the data. We might want to classify the image as a whole, e.g., is it an indoors or outdoors scene? is it a horizontal or vertical photo? does it contain a dog or not? This is called image classiﬁcation.\n",
      "\n",
      "In the special case that the images consist of isolated handwritten letters and digits, for example, in a postal or ZIP code on a letter, we can use classiﬁcation to perform handwriting recognition. A standard dataset used in this area is known as MNIST, which stands for “Modiﬁed National Institute of Standards”5. (The term “modiﬁed” is used because the images have been preprocessed to ensure the digits are mostly in the center of the image.) This dataset contains 60,000 training images and 10,000 test images of the digits 0 to 9, as written by various people. The images are size 28 × 28 and have grayscale values in the range 0 : 255. See Figure 1.5(a) for some example images.\n",
      "\n",
      "Many generic classiﬁcation methods ignore any structure in the input features, such as spatial layout. Consequently, they can also just as easily handle data that looks like Figure 1.5(b), which is the same data except we have randomly permuted the order of all the features. (You will verify this in Exercise 1.1.) This ﬂexibility is both a blessing (since the methods are general purpose) and a curse (since the methods ignore an obviously useful source of information). We will discuss methods for exploiting structure in the input features later in the book.\n",
      "\n",
      "5. Available from http://yann.lecun.com/exdb/mnist/.\n",
      "\n",
      "8\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.6 Example of face detection. (a) Input image (Murphy family, photo taken 5 August 2010). Used with kind permission of Bernard Diedrich of Sherwood Studios. (b) Output of classiﬁer, which detected 5 faces at different poses. This was produced using the online demo at http://demo.pittpatt.com/. The classiﬁer was trained on 1000s of manually labeled images of faces and non-faces, and then was applied to a dense set of overlapping patches in the test image. Only the patches whose probability of containing a face was sufficiently high were returned. Used with kind permission of Pittpatt.com\n",
      "\n",
      "Face detection and recognition\n",
      "\n",
      "A harder problem is to ﬁnd objects within an image; this is called object detection or object localization. An important special case of this is face detection. One approach to this problem is to divide the image into many small overlapping patches at different locations, scales and orientations, and to classify each such patch based on whether it contains face-like texture or not. This is called a sliding window detector. The system then returns those locations where the probability of face is sufficiently high. See Figure 1.6 for an example. Such face detection systems are built-in to most modern digital cameras; the locations of the detected faces are used to determine the center of the auto-focus. Another application is automatically blurring out faces in Google’s StreetView system.\n",
      "\n",
      "Having found the faces, one can then proceed to perform face recognition, which means estimating the identity of the person (see Figure 1.10(a)). In this case, the number of class labels might be very large. Also, the features one should use are likely to be different than in the face detection problem: for recognition, subtle differences between faces such as hairstyle may be important for determining identity, but for detection, it is important to be invariant to such details, and to just focus on the differences between faces and non-faces. For more information about visual object detection, see e.g., (Szeliski 2010).\n",
      "\n",
      "1.2.2\n",
      "\n",
      "Regression\n",
      "\n",
      "Regression is just like classiﬁcation except the response variable is continuous. Figure 1.7 shows a simple example: we have a single real-valued input xi ∈ R, and a single real-valued response yi ∈ R. We consider ﬁtting two models to the data: a straight line and a quadratic function. (We explain how to ﬁt such models below.) Various extensions of this basic problem can arise, such as having high-dimensional inputs, outliers, non-smooth responses, etc. We will discuss ways to handle such problems later in the book.\n",
      "\n",
      "1.3. Unsupervised learning\n",
      "\n",
      "9\n",
      "\n",
      "degree 1\n",
      "\n",
      "degree 2\n",
      "\n",
      "15\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "−5\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.7 Figure generated by linregPolyVsDegree.\n",
      "\n",
      "(a) Linear regression on some 1d data.\n",
      "\n",
      "(b) Same data with polynomial regression (degree 2).\n",
      "\n",
      "Here are some examples of real-world regression problems.\n",
      "\n",
      "Predict tomorrow’s stock market price given current market conditions and other possible\n",
      "\n",
      "side information.\n",
      "\n",
      "Predict the age of a viewer watching a given video on YouTube.\n",
      "\n",
      "Predict the location in 3d space of a robot arm end effector, given control signals (torques)\n",
      "\n",
      "sent to its various motors.\n",
      "\n",
      "Predict the amount of prostate speciﬁc antigen (PSA) in the body as a function of a number\n",
      "\n",
      "of different clinical measurements.\n",
      "\n",
      "Predict the temperature at any location inside a building using weather data, time, door\n",
      "\n",
      "sensors, etc.\n",
      "\n",
      "1.3 Unsupervised learning\n",
      "\n",
      "We now consider unsupervised learning, where we are just given output data, without any inputs. The goal is to discover “interesting structure” in the data; this is sometimes called knowledge discovery. Unlike supervised learning, we are not told what the desired output is Instead, we will formalize our task as one of density estimation, that is, we for each input. want to build models of the form p(xi|θ). There are two differences from the supervised case. First, we have written p(xi|θ) instead of p(yi|xi, θ); that is, supervised learning is conditional density estimation, whereas unsupervised learning is unconditional density estimation. Second, xi is a vector of features, so we need to create multivariate probability models. By contrast, in supervised learning, yi is usually just a single variable that we are trying to predict. This means that for most supervised learning problems, we can use univariate probability models (with input-dependent parameters), which signiﬁcantly simpliﬁes the problem. (We will discuss multi-output classiﬁcation in Chapter 19, where we will see that it also involves multivariate probability models.)\n",
      "\n",
      "It is also more widely applicable than supervised learning, since it does not require a human expert to\n",
      "\n",
      "Unsupervised learning is arguably more typical of human and animal learning.\n",
      "\n",
      "10\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "280\n",
      "\n",
      "280\n",
      "\n",
      "K=2\n",
      "\n",
      "260\n",
      "\n",
      "260\n",
      "\n",
      "240\n",
      "\n",
      "240\n",
      "\n",
      "220\n",
      "\n",
      "220\n",
      "\n",
      "200\n",
      "\n",
      "200\n",
      "\n",
      "t h g e w\n",
      "\n",
      "i\n",
      "\n",
      "180\n",
      "\n",
      "t h g e w\n",
      "\n",
      "i\n",
      "\n",
      "180\n",
      "\n",
      "160\n",
      "\n",
      "160\n",
      "\n",
      "140\n",
      "\n",
      "140\n",
      "\n",
      "120\n",
      "\n",
      "120\n",
      "\n",
      "100\n",
      "\n",
      "100\n",
      "\n",
      "80\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "80\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "height\n",
      "\n",
      "height\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.8 (a) The height and weight of some people. Figure generated by kmeansHeightWeight.\n",
      "\n",
      "(b) A possible clustering using K = 2 clusters.\n",
      "\n",
      "manually label the data. Labeled data is not only expensive to acquire6, but it also contains relatively little information, certainly not enough to reliably estimate the parameters of complex models. Geoff Hinton, who is a famous professor of ML at the University of Toronto, has said:\n",
      "\n",
      "When we’re learning to see, nobody’s telling us what the right answers are — we just look. Every so often, your mother says “that’s a dog”, but that’s very little information. You’d be lucky if you got a few bits of information — even one bit per second — that way. The brain’s visual system has 1014 neural connections. And you only live for 109 seconds. So it’s no use learning one bit per second. You need more like 105 bits per second. And there’s only one place you can get that much information: from the input itself. — Geoffrey Hinton, 1996 (quoted in (Gorder 2006)).\n",
      "\n",
      "Below we describe some canonical examples of unsupervised learning.\n",
      "\n",
      "1.3.1\n",
      "\n",
      "Discovering clusters\n",
      "\n",
      "As a canonical example of unsupervised learning, consider the problem of clustering data into groups. For example, Figure 1.8(a) plots some 2d data, representing the height and weight of a group of 210 people. It seems that there might be various clusters, or subgroups, although it is not clear how many. Let K denote the number of clusters. Our ﬁrst goal is to estimate the distribution over the number of clusters, p(K|D); this tells us if there are subpopulations within the data. For simplicity, we often approximate the distribution p(K|D) by its mode, K ∗ = arg maxK p(K|D). In the supervised case, we were told that there are two classes (male and female), but in the unsupervised case, we are free to choose as many or few clusters as we like. Picking a model of the “right” complexity is called model selection, and will be discussed in detail below.\n",
      "\n",
      "represent the cluster to which data point i is assigned.\n",
      "\n",
      "Our second goal is to estimate which cluster each point belongs to. Let zi ∈ {1, . . . , K} (zi is an example of a hidden or\n",
      "\n",
      "6. The advent of crowd sourcing web sites such as Mechanical Turk, (https://www.mturk.com/mturk/welcome), which outsource data processing tasks to humans all over the world, has reduced the cost of labeling data. Nevertheless, the amount of unlabeled data is still orders of magnitude larger than the amount of labeled data.\n",
      "\n",
      "1.3. Unsupervised learning\n",
      "\n",
      "11\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "−2\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "−4\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.9 (a) A set of points that live on a 2d linear subspace embedded in 3d. The solid red line is the ﬁrst principal component direction. The dotted black line is the second PC direction. (b) 2D representation of the data. Figure generated by pcaDemo3d.\n",
      "\n",
      "latent variable, since it is never observed in the training set.) We can infer which cluster each data point belongs to by computing z∗ i = argmaxk p(zi = k|xi, D). This is illustrated in Figure 1.8(b), where we use different colors to indicate the assignments, assuming K = 2.\n",
      "\n",
      "In this book, we focus on model based clustering, which means we ﬁt a probabilistic model to the data, rather than running some ad hoc algorithm. The advantages of the model-based approach are that one can compare different kinds of models in an objective way (in terms of the likelihood they assign to the data), we can combine them together into larger systems, etc.\n",
      "\n",
      "Here are some real world applications of clustering.\n",
      "\n",
      "\n",
      "\n",
      "In astronomy, the autoclass system (Cheeseman et al. 1988) discovered a new type of star, based on clustering astrophysical measurements.\n",
      "\n",
      "\n",
      "\n",
      "In e-commerce, it is common to cluster users into groups, based on their purchasing or web-surﬁng behavior, and then to send customized targeted advertising to each group (see e.g., (Berkhin 2006)).\n",
      "\n",
      "\n",
      "\n",
      "In biology, it is common to cluster ﬂow-cytometry data into groups, to discover different sub-populations of cells (see e.g., (Lo et al. 2009)).\n",
      "\n",
      "1.3.2\n",
      "\n",
      "Discovering latent factors\n",
      "\n",
      "When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the “essence” of the data. This is called dimensionality reduction. A simple example is shown in Figure 1.9, where we project some 3d data down to a 2d plane. The 2d approximation is quite good, since most points lie close to this subspace. Reducing to 1d would involve projecting points onto the red line in Figure 1.9(a); this would be a rather poor approximation. (We will make this notion precise in Chapter 12.)\n",
      "\n",
      "The motivation behind this technique is that although the data may appear high dimensional, there may only be a small number of degrees of variability, corresponding to latent factors. For example, when modeling the appearance of face images, there may only be a few underlying identity, etc, as latent factors which describe most of the variability, such as lighting, pose, illustrated in Figure 1.10.\n",
      "\n",
      "12\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.10 a) 25 randomly chosen 64 × 64 pixel images from the Olivetti face database. and the ﬁrst three principal component basis vectors (eigenfaces). Figure generated by pcaImageDemo.\n",
      "\n",
      "(b) The mean\n",
      "\n",
      "When used as input to other statistical models, such low dimensional representations often result in better predictive accuracy, because they focus on the “essence” of the object, ﬁltering out inessential features. Also, low dimensional representations are useful for enabling fast nearest neighbor searches and two dimensional projections are very useful for visualizing high dimensional data.\n",
      "\n",
      "The most common approach to dimensionality reduction is called principal components analysis or PCA. This can be thought of as an unsupervised version of (multi-output) linear regression, where we observe the high-dimensional response y, but not the low-dimensional “cause” z. Thus the model has the form z → y; we have to “invert the arrow”, and infer the latent low-dimensional z from the observed high-dimensional y. See Section 12.1 for details.\n",
      "\n",
      "Dimensionality reduction, and PCA in particular, has been applied in many different areas.\n",
      "\n",
      "Some examples include the following:\n",
      "\n",
      "\n",
      "\n",
      "In biology, it is common to use PCA to interpret gene microarray data, to account for the fact that each measurement is usually the result of many genes which are correlated in their behavior by the fact that they belong to different biological pathways.\n",
      "\n",
      "\n",
      "\n",
      "In natural language processing, it is common to use a variant of PCA called latent semantic analysis for document retrieval (see Section 27.2.2).\n",
      "\n",
      "\n",
      "\n",
      "In signal processing (e.g., of acoustic or neural signals), it is common to use ICA (which is a variant of PCA) to separate signals into their different sources (see Section 12.6).\n",
      "\n",
      "\n",
      "\n",
      "In computer graphics, it is common to project motion capture data to a low dimensional space, and use it to create animations. See Section 15.5 for one way to tackle such problems.\n",
      "\n",
      "1.3. Unsupervised learning\n",
      "\n",
      "13\n",
      "\n",
      "Figure 1.11 A sparse undirected Gaussian graphical model learned using graphical lasso (Section 26.7.2) applied to some ﬂow cytometry data (from (Sachs et al. 2005)), which measures the phosphorylation status of 11 proteins. Figure generated by ggmLassoDemo.\n",
      "\n",
      "1.3.3\n",
      "\n",
      "Discovering graph structure\n",
      "\n",
      "Sometimes we measure a set of correlated variables, and we would like to discover which ones are most correlated with which others. This can be represented by a graph G, in which nodes represent variables, and edges represent direct dependence between variables (we will make this precise in Chapter 10, when we discuss graphical models). We can then learn this graph structure from data, i.e., we compute ˆG = argmax p(G|D).\n",
      "\n",
      "As with unsupervised learning in general, there are two main applications for learning sparse graphs: to discover new knowledge, and to get better joint probability density estimators. We now give somes example of each.\n",
      "\n",
      "Much of the motivation for learning sparse graphical models comes from the systems biology community. For example, suppose we measure the phosphorylation status of some proteins in a cell (Sachs et al. 2005). Figure 1.11 gives an example of a graph structure that was learned from this data (using methods discussed in Section 26.7.2). As another example, Smith et al. (2006) showed that one can recover the neural “wiring diagram” of a certain kind of bird from time-series EEG data. The recovered structure closely matched the known functional connectivity of this part of the bird brain.\n",
      "\n",
      "\n",
      "\n",
      "In some cases, we are not interested in interpreting the graph structure, we just want to use it to model correlations and to make predictions. One example of this is in ﬁnancial portfolio management, where accurate models of the covariance between large numbers of different stocks is important. Carvalho and West (2007) show that by learning a sparse graph, and then using this as the basis of a trading strategy, it is possible to outperform (i.e., make more money than) methods that do not exploit sparse graphs. Another example is predicting traffic jams on the freeway. Horvitz et al. (2005) describe a deployed system called JamBayes for predicting traffic ﬂow in the Seattle area; predictions are made using a graphical model whose structure was learned from data.\n",
      "\n",
      "14\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.12 (a) A noisy image with an occluder. (b) An estimate of the underlying pixel intensities, based on a pairwise MRF model. Source: Figure 8 of (Felzenszwalb and Huttenlocher 2006). Used with kind permission of Pedro Felzenszwalb.\n",
      "\n",
      "1.3.4 Matrix completion\n",
      "\n",
      "Sometimes we have missing data, that is, variables whose values are unknown. For example, we might have conducted a survey, and some people might not have answered certain questions. Or we might have various sensors, some of which fail. The corresponding design matrix will then have “holes” in it; these missing entries are often represented by NaN, which stands for “not a number”. The goal of imputation is to infer plausible values for the missing entries. This is sometimes called matrix completion. Below we give some example applications.\n",
      "\n",
      "1.3.4.1\n",
      "\n",
      "Image inpainting\n",
      "\n",
      "An interesting example of an imputation-like task is known as image inpainting. The goal is to “ﬁll in” holes (e.g., due to scratches or occlusions) in an image with realistic texture. This is illustrated in Figure 1.12, where we denoise the image, as well as impute the pixels hidden behind the occlusion. This can be tackled by building a joint probability model of the pixels, given a set of clean images, and then inferring the unknown variables (pixels) given the known variables (pixels). This is somewhat like masket basket analysis, except the data is real-valued and spatially structured, so the kinds of probability models we use are quite different. See Sections 19.6.2.7 and 13.8.4 for some possible choices.\n",
      "\n",
      "1.3.4.2\n",
      "\n",
      "Collaborative ﬁltering\n",
      "\n",
      "Another interesting example of an imputation-like task is known as collaborative ﬁltering. A common example of this concerns predicting which movies people will want to watch based on how they, and other people, have rated movies which they have already seen. The key idea is that the prediction is not based on features of the movie or user (although it could be), but merely on a ratings matrix. More precisely, we have a matrix X where X(m, u) is the rating\n",
      "\n",
      "1.3. Unsupervised learning\n",
      "\n",
      "15\n",
      "\n",
      "(cid:3)(cid:3)(cid:88)(cid:86)(cid:72)(cid:85)(cid:86)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:24)(cid:34)\n",
      "\n",
      "(cid:80)(cid:82)(cid:89)(cid:76)(cid:72)(cid:86)\n",
      "\n",
      "(cid:34)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:24)(cid:34)\n",
      "\n",
      "Figure 1.13 Example of movie-rating data. Training data is in red, test data is denoted by ?, empty cells are unknown.\n",
      "\n",
      "(say an integer between 1 and 5, where 1 is dislike and 5 is like) by user u of movie m. Note that most of the entries in X will be missing or unknown, since most users will not have rated most movies. Hence we only observe a tiny subset of the X matrix, and we want to predict In particular, for any given user u, we might want to predict which of the a different subset. unrated movies he/she is most likely to want to watch.\n",
      "\n",
      "In order to encourage research in this area, the DVD rental company Netﬂix created a com- launched in 2006, with a $1M USD prize (see http://netflixprize.com/). petition, In particular, they provided a large matrix of ratings, on a scale of 1 to 5, for ∼ 18k movies created by ∼ 500k users. The full matrix would have ∼ 9 × 109 entries, but only about 1% of the entries are observed, so the matrix is extremely sparse. A subset of these are used for training, and the rest for testing, as shown in Figure 1.13. The goal of the competition was to predict more accurately than Netﬂix’s existing system. On 21 September 2009, the prize was awarded to a team of researchers known as “BellKor’s Pragmatic Chaos”. Section 27.6.2 discusses some of their methodology. Further details on the teams and their methods can be found at http://www.netflixprize.com/community/viewtopic.php?id=1537.\n",
      "\n",
      "1.3.4.3 Market basket analysis\n",
      "\n",
      "In commercial data mining, there is much interest in a task called market basket analysis. The data consists of a (typically very large but sparse) binary matrix, where each column represents an item or product, and each row represents a transaction. We set xij = 1 if item j was purchased on the i’th transaction. Many items are purchased together (e.g., bread and butter), so there will be correlations amongst the bits. Given a new partially observed bit vector, representing a subset of items that the consumer has bought, the goal is to predict which other bits are likely to turn on, representing other items the consumer might be likely to buy. (Unlike collaborative ﬁltering, we often assume there is no missing data in the training data, since we know the past shopping behavior of each customer.)\n",
      "\n",
      "This task arises in other domains besides modeling purchasing patterns. For example, similar techniques can be used to model dependencies between ﬁles in complex software systems. In this case, the task is to predict, given a subset of ﬁles that have been changed, which other ones need to be updated to ensure consistency (see e.g., (Hu et al. 2010)).\n",
      "\n",
      "It is common to solve such tasks using frequent itemset mining, which create association rules (see e.g., (Hastie et al. 2009, sec 14.2) for details). Alternatively, we can adopt a probabilistic approach, and ﬁt a joint density model p(x1, . . . , xD) to the bit vectors, see e.g., (Hu et al.\n",
      "\n",
      "16\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(a) Illustration of a K-nearest neighbors classiﬁer in 2d for K = 3. The 3 nearest neighbors Figure 1.14 of test point x1 have labels 1, 1 and 0, so we predict p(y = 1|x1, D, K = 3) = 2/3. The 3 nearest neighbors of test point x2 have labels 0, 0, and 0, so we predict p(y = 1|x2, D, K = 3) = 0/3. (b) Illustration of the Voronoi tesselation induced by 1-NN. Based on Figure 4.13 of (Duda et al. 2001). Figure generated by knnVoronoi.\n",
      "\n",
      "2010). Such models often have better predictive acccuracy than association rules, although they may be less interpretible. This is typical of the difference between data mining and machine learning: in data mining, there is more emphasis on interpretable models, whereas in machine learning, there is more emphasis on accurate models.\n",
      "\n",
      "1.4\n",
      "\n",
      "Some basic concepts in machine learning\n",
      "\n",
      "In this Section, we provide an introduction to some key ideas in machine learning. We will expand on these concepts later in the book, but we introduce them brieﬂy here, to give a ﬂavor of things to come.\n",
      "\n",
      "1.4.1\n",
      "\n",
      "Parametric vs non-parametric models\n",
      "\n",
      "In this book, we will be focussing on probabilistic models of the form p(y|x) or p(x), depending on whether we are interested in supervised or unsupervised learning respectively. There are many ways to deﬁne such models, but the most important distinction is this: does the model have a ﬁxed number of parameters, or does the number of parameters grow with the amount of training data? The former is called a parametric model, and the latter is called a non- parametric model. Parametric models have the advantage of often being faster to use, but the disadvantage of making stronger assumptions about the nature of the data distributions. Non- parametric models are more ﬂexible, but often computationally intractable for large datasets. We will give examples of both kinds of models in the sections below. We focus on supervised learning for simplicity, although much of our discussion also applies to unsupervised learning.\n",
      "\n",
      "1.4.2\n",
      "\n",
      "A simple non-parametric classiﬁer: K-nearest neighbors A simple example of a non-parametric classiﬁer is the K nearest neighbor (KNN) classiﬁer. This simply “looks at” the K points in the training set that are nearest to the test input x,\n",
      "\n",
      "1.4. Some basic concepts in machine learning\n",
      "\n",
      "17\n",
      "\n",
      "train\n",
      "\n",
      "p(y=1|data,K=10)\n",
      "\n",
      "5\n",
      "\n",
      "120\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "4\n",
      "\n",
      "100\n",
      "\n",
      "0.8\n",
      "\n",
      "3\n",
      "\n",
      "80\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "2\n",
      "\n",
      "60\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "0.4\n",
      "\n",
      "0\n",
      "\n",
      "40\n",
      "\n",
      "0.3\n",
      "\n",
      "−1\n",
      "\n",
      "20\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "0\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "120\n",
      "\n",
      "p(y=2|data,K=10)\n",
      "\n",
      "1\n",
      "\n",
      "5\n",
      "\n",
      "predicted label, K=10\n",
      "\n",
      "0.9\n",
      "\n",
      "100\n",
      "\n",
      "0.8\n",
      "\n",
      "4\n",
      "\n",
      "80\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "60\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "1\n",
      "\n",
      "40\n",
      "\n",
      "0.3\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "c1 c2 c3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 1.15 (c) Probability of class 2. (d) MAP estimate of class label. Figure generated by knnClassifyDemo.\n",
      "\n",
      "(a) Some synthetic 3-class training data in 2d. (b) Probability of class 1 for KNN with K = 10.\n",
      "\n",
      "counts how many members of each class are in this set, and returns that empirical fraction as the estimate, as illustrated in Figure 1.14. More formally,\n",
      "\n",
      "p(y = c|x, D, K) =\n",
      "\n",
      "1 K\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i∈NK (x,D)\n",
      "\n",
      "I(yi = c)\n",
      "\n",
      "(1.2)\n",
      "\n",
      "where NK(x, D) are the (indices of the) K nearest points to x in D and I(e) is the indicator function deﬁned as follows: if e is true if e is false\n",
      "\n",
      "I(e) =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "1 0\n",
      "\n",
      "(1.3)\n",
      "\n",
      "This method is an example of memory-based learning or instance-based learning. It can be derived from a probabilistic framework as explained in Section 14.7.3. The most common\n",
      "\n",
      "18\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "d=10 d=7 d=5\n",
      "\n",
      "e b u c\n",
      "\n",
      "f o\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "d=3\n",
      "\n",
      "h t g n e\n",
      "\n",
      "l\n",
      "\n",
      "0.5\n",
      "\n",
      "e g d E\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "d=1\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "s\n",
      "\n",
      "0.1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "Fraction of data in neighborhood\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.16 Illustration of the curse of dimensionality. (a) We embed a small cube of side s inside a larger unit cube. (b) We plot the edge length of a cube needed to cover a given volume of the unit cube as a function of the number of dimensions. Based on Figure 2.6 from (Hastie et al. 2009). Figure generated by curseDimensionality.\n",
      "\n",
      "distance metric to use is Euclidean distance (which limits the applicability of the technique to data which is real-valued), although other metrics can be used.\n",
      "\n",
      "Figure 1.15 gives an example of the method in action, where the input is two dimensional, we have three classes, and K = 10. (We discuss the effect of K below.) Panel (a) plots the training data. Panel (b) plots p(y = 1|x, D) where x is evaluated on a grid of points. Panel (c) plots p(y = 2|x, D). We do not need to plot p(y = 3|x, D), since probabilities sum to one. Panel (d) plots the MAP estimate ˆy(x) = argmaxc(y = c|x, D).\n",
      "\n",
      "A KNN classiﬁer with K = 1 induces a Voronoi tessellation of the points (see Figure 1.14(b)). This is a partition of space which associates a region V (xi) with each point xi in such a way that all points in V (xi) are closer to xi than to any other point. Within each cell, the predicted label is the label of the corresponding training point.\n",
      "\n",
      "1.4.3\n",
      "\n",
      "The curse of dimensionality\n",
      "\n",
      "The KNN classiﬁer is simple and can work quite well, provided it is given a good distance metric and has enough labeled training data. In fact, it can be shown that the KNN classiﬁer can come within a factor of 2 of the best possible performance if N → ∞ (Cover and Hart 1967).\n",
      "\n",
      "However, the main problem with KNN classiﬁers is that they do not work well with high dimensional inputs. The poor performance in high dimensional settings is due to the curse of dimensionality.\n",
      "\n",
      "To explain the curse, we give some examples from (Hastie et al. 2009, p22). Consider applying a KNN classiﬁer to data where the inputs are uniformly distributed in the D-dimensional unit cube. Suppose we estimate the density of class labels around a test point x by “growing” a hyper-cube around x until it contains a desired fraction f of the data points. The expected edge length of this cube will be eD(f ) = f 1/D. If D = 10, and we want to base our estimate on 10%\n",
      "\n",
      "1.4. Some basic concepts in machine learning\n",
      "\n",
      "19\n",
      "\n",
      "PDF\n",
      "\n",
      "0.4\n",
      "\n",
      "0.35\n",
      "\n",
      "0.3\n",
      "\n",
      "0.25\n",
      "\n",
      "0.2\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "0 −3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.17 (b) Visualization of the conditional density model p(y|x, θ) = N (y|w0 + w1x, σ2). The density falls off exponentially fast as we move away from the regression line. Figure generated by linregWedgeDemo2.\n",
      "\n",
      "(a) A Gaussian pdf with mean 0 and variance 1. Figure generated by gaussPlotDemo.\n",
      "\n",
      "of the data, we have e10(0.1) = 0.8, so we need to extend the cube 80% along each dimension around x. Even if we only use 1% of the data, we ﬁnd e10(0.01) = 0.63: see Figure 1.16. Since the entire range of the data is only 1 along each dimension, we see that the method is no longer very local, despite the name “nearest neighbor”. The trouble with looking at neighbors that are so far away is that they may not be good predictors about the behavior of the input-output function at a given point.\n",
      "\n",
      "1.4.4\n",
      "\n",
      "Parametric models for classiﬁcation and regression\n",
      "\n",
      "The main way to combat the curse of dimensionality is to make some assumptions about the nature of the data distribution (either p(y|x) for a supervised problem or p(x) for an unsupervised problem). These assumptions, known as inductive bias, are often embodied in the form of a parametric model, which is a statistical model with a ﬁxed number of parameters. Below we brieﬂy describe two widely used examples; we will revisit these and other models in much greater depth later in the book.\n",
      "\n",
      "1.4.5\n",
      "\n",
      "Linear regression\n",
      "\n",
      "One of the most widely used models for regression is known as linear regression. This asserts that the response is a linear function of the inputs. This can be written as follows:\n",
      "\n",
      "y(x) = wT x + (cid:5) =\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "wjxj + (cid:5)\n",
      "\n",
      "(1.4)\n",
      "\n",
      "j=1\n",
      "\n",
      "where wT x represents the inner or scalar product between the input vector x and the model’s weight vector w7, and (cid:5) is the residual error between our linear predictions and the true response.\n",
      "\n",
      "7. In statistics, it is more common to denote the regression weights by β.\n",
      "\n",
      "20\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "degree 14\n",
      "\n",
      "degree 20\n",
      "\n",
      "15\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "−5\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.18 Polynomial of degrees 14 and 20 ﬁt by least squares to 21 data points. Figure generated by linregPolyVsDegree.\n",
      "\n",
      "We often assume that (cid:5) has a Gaussian8 or normal distribution. We denote this by (cid:5) ∼ N (μ, σ2), where μ is the mean and σ2 is the variance (see Chapter 2 for details). When we plot this distribution, we get the well-known bell curve shown in Figure 1.17(a).\n",
      "\n",
      "To make the connection between linear regression and Gaussians more explicit, we can rewrite\n",
      "\n",
      "the model in the following form:\n",
      "\n",
      "p(y|x, θ) = N (y|μ(x), σ2(x))\n",
      "\n",
      "(1.5)\n",
      "\n",
      "This makes it clear that the model is a conditional probability density. In the simplest case, we assume μ is a linear function of x, so μ = wT x, and that the noise is ﬁxed, σ2(x) =σ 2. In this case, θ = (w, σ2) are the parameters of the model.\n",
      "\n",
      "For example, suppose the input is 1 dimensional. We can represent the expected response as\n",
      "\n",
      "follows:\n",
      "\n",
      "μ(x) = w0 + w1x = wT x\n",
      "\n",
      "(1.6)\n",
      "\n",
      "where w0 is the intercept or bias term, w1 is the slope, and where we have deﬁned the vector x = (1, x). (Prepending a constant 1 term to an input vector is a common notational trick which If w1 is positive, allows us to combine the intercept term with the other terms in the model.) it means we expect the output to increase as the input increases. This is illustrated in 1d in Figure 1.17(b); a more conventional plot, of the mean response vs x, is shown in Figure 1.7(a).\n",
      "\n",
      "non-linear function of the inputs, φ(x). That is, we use\n",
      "\n",
      "Linear regression can be made to model non-linear relationships by replacing x with some\n",
      "\n",
      "p(y|x, θ) = N (y|wT φ(x), σ2)\n",
      "\n",
      "(1.7)\n",
      "\n",
      "This is known as basis function expansion. For example, Figure 1.18 illustrates the case where φ(x) = [1, x, x2, . . . , xd], for d = 14 and d = 20; this is known as polynomial regression. In fact, many popular We will consider other kinds of basis functions later in the book. machine learning methods — such as support vector machines, neural networks, classiﬁcation and regression trees, etc. — can be seen as just different ways of estimating basis functions from data, as we discuss in Chapters 14 and 16.\n",
      "\n",
      "8. Carl Friedrich Gauss (1777–1855) was a German mathematician and physicist.\n",
      "\n",
      "1.4. Some basic concepts in machine learning\n",
      "\n",
      "21\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "0 −10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "460\n",
      "\n",
      "480\n",
      "\n",
      "500\n",
      "\n",
      "520\n",
      "\n",
      "540\n",
      "\n",
      "560\n",
      "\n",
      "580\n",
      "\n",
      "600\n",
      "\n",
      "620\n",
      "\n",
      "640\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.19 (a) The sigmoid or logistic function. We have sigm(−∞) = 0, sigm(0) = 0.5, and sigm(∞) = 1. Figure generated by sigmoidPlot. (b) Logistic regression for SAT scores. Solid black dots are the data. The open red circles are the predicted probabilities. The green crosses denote two students with the same SAT score of 525 (and hence same input representation x) but with different training labels (one student passed, y = 1, the other failed, y = 0). Hence this data is not perfectly separable using just the SAT feature. Figure generated by logregSATdemo.\n",
      "\n",
      "1.4.6\n",
      "\n",
      "Logistic regression\n",
      "\n",
      "We can generalize linear regression to the (binary) classiﬁcation setting by making two changes. First we replace the Gaussian distribution for y with a Bernoulli distribution9,which is more appropriate for the case when the response is binary, y ∈ {0, 1}. That is, we use\n",
      "\n",
      "p(y|x, w) = Ber(y|μ(x))\n",
      "\n",
      "(1.8)\n",
      "\n",
      "where μ(x) = E [y|x] =p(y = 1|x). Second, we compute a linear combination of the inputs, as before, but then we pass this through a function that ensures 0 ≤ μ(x) ≤ 1 by deﬁning\n",
      "\n",
      "μ(x) = sigm(wT x)\n",
      "\n",
      "(1.9)\n",
      "\n",
      "where sigm(η) refers to the sigmoid function, also known as the logistic or logit function. This is deﬁned as\n",
      "\n",
      "sigm(η) (cid:2)\n",
      "\n",
      "1 1 + exp(−η)\n",
      "\n",
      "=\n",
      "\n",
      "eη eη + 1\n",
      "\n",
      "(1.10)\n",
      "\n",
      "The term “sigmoid” means S-shaped: see Figure 1.19(a) for a plot. It is also known as a squashing function, since it maps the whole real line to [0, 1], which is necessary for the output to be interpreted as a probability.\n",
      "\n",
      "Putting these two steps together we get\n",
      "\n",
      "p(y|x, w) = Ber(y|sigm(wT x))\n",
      "\n",
      "(1.11)\n",
      "\n",
      "This is called logistic regression due to its similarity to linear regression (although it is a form of classiﬁcation, not regression!).\n",
      "\n",
      "9. Daniel Bernoulli (1700–1782) was a Dutch-Swiss mathematician and physicist.\n",
      "\n",
      "22\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "A simple example of logistic regression is shown in Figure 1.19(b), where we plot\n",
      "\n",
      "p(yi = 1|xi, w) = sigm(w0 + w1xi)\n",
      "\n",
      "(1.12)\n",
      "\n",
      "where xi is the SAT10 score of student i and yi is whether they passed or failed a class. The solid black dots show the training data, and the red circles plot p(y = 1|xi, ˆw), where ˆw are the parameters estimated from the training data (we discuss how to compute these estimates in Section 8.3.4).\n",
      "\n",
      "If we threshold the output probability at 0.5, we can induce a decision rule of the form\n",
      "\n",
      "ˆy(x) = 1 ⇐⇒ p(y = 1|x) > 0.5\n",
      "\n",
      "(1.13)\n",
      "\n",
      "By looking at Figure 1.19(b), we see that sigm(w0 + w1x) = 0.5 for x ≈ 545 = x∗. We can imagine drawing a vertical line at x = x∗; this is known as a decision boundary. Everything to the left of this line is classiﬁed as a 0, and everything to the right of the line is classiﬁed as a 1. We notice that this decision rule has a non-zero error rate even on the training set. This is because the data is not linearly separable, i.e., there is no straight line we can draw to separate the 0s from the 1s. We can create models with non-linear decision boundaries using basis function expansion, just as we did with non-linear regression. We will see many examples of this later in the book.\n",
      "\n",
      "1.4.7\n",
      "\n",
      "Overﬁtting\n",
      "\n",
      "When we ﬁt highly ﬂexible models, we need to be careful that we do not overﬁt the data, that is, we should avoid trying to model every minor variation in the input, since this is more likely to be noise than true signal. This is illustrated in Figure 1.18(b), where we see that using a high degree polynomial results in a curve that is very “wiggly”. It is unlikely that the true function has such extreme oscillations. Thus using such a model might result in accurate predictions of future outputs.\n",
      "\n",
      "As another example, consider the KNN classiﬁer. The value of K can have a large effect on the behavior of this model. When K = 1, the method makes no errors on the training set (since we just return the labels of the original training points), but the resulting prediction surface is very “wiggly” (see Figure 1.20(a)). Therefore the method may not work well at predicting future In Figure 1.20(b), we see that using K = 5 results in a smoother prediction surface, data. because we are averaging over a larger neighborhood. As K increases, the predictions becomes smoother until, in the limit of K = N , we end up predicting the majority label of the whole data set. Below we discuss how to pick the “right” value of K.\n",
      "\n",
      "1.4.8 Model selection\n",
      "\n",
      "When we have a variety of models of different complexity (e.g., linear or logistic regression models with different degree polynomials, or KNN classiﬁers with different values of K), how should we pick the right one? A natural approach is to compute the misclassiﬁcation rate on\n",
      "\n",
      "10. SAT stands for “Scholastic Aptitude Test”. This is a standardized test for college admissions used in the United States (the data in this example is from (Johnson and Albert 1999, p87)).\n",
      "\n",
      "1.4. Some basic concepts in machine learning\n",
      "\n",
      "23\n",
      "\n",
      "predicted label, K=1\n",
      "\n",
      "predicted label, K=5\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "c1 c2 c3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "c1 c2 c3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1.20 Prediction surface for KNN on the data in Figure 1.15(a). (a) K=1. (b) K=5. Figure generated by knnClassifyDemo.\n",
      "\n",
      "the training set for each method. This is deﬁned as follows:\n",
      "\n",
      "err(f, D) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "I(f (xi) (cid:10)= yi)\n",
      "\n",
      "(1.14)\n",
      "\n",
      "where f (x) is our classiﬁer. In Figure 1.21(a), we plot this error rate vs K for a KNN classiﬁer (dotted blue line). We see that increasing K increases our error rate on the training set, because we are over-smoothing. As we said above, we can get minimal error on the training set by using K = 1, since this model is just memorizing the data.\n",
      "\n",
      "However, what we care about is generalization error, which is the expected value of the misclassiﬁcation rate when averaged over future data (see Section 6.3 for details). This can be approximated by computing the misclassiﬁcation rate on a large independent test set, not used during model training. We plot the test error vs K in Figure 1.21(a) in solid red (upper curve). Now we see a U-shaped curve: for complex models (small K), the method overﬁts, and for simple models (big K), the method underﬁts. Therefore, an obvious way to pick K is to pick the value with the minimum error on the test set (in this example, any value between 10 and 100 should be ﬁne).\n",
      "\n",
      "Unfortunately, when training the model, we don’t have access to the test set (by assumption), so we cannot use the test set to pick the model of the right complexity.11 However, we can create a test set by partitioning the training set into two: the part used for training the model, and a second part, called the validation set, used for selecting the model complexity. We then ﬁt all the models on the training set, and evaluate their performance on the validation set, and pick the best. Once we have picked the best, we can reﬁt it to all the available data. If we have a separate test set, we can evaluate performance on this, in order to estimate the accuracy of our method. (We discuss this in more detail in Section 6.5.3.)\n",
      "\n",
      "Often we use about 80% of the data for the training set, and 20% for the validation set. But if the number of training cases is small, this technique runs into problems, because the model\n",
      "\n",
      "11. In academic settings, we usually do have access to the test set, but we should not use it for model ﬁtting or model selection, otherwise we will get an unrealistically optimistic estimate of performance of our method. This is one of the “golden rules” of machine learning research.\n",
      "\n",
      "24\n",
      "\n",
      "Chapter 1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "0.35\n",
      "\n",
      "0.3\n",
      "\n",
      "train test\n",
      "\n",
      "e t a r\n",
      "\n",
      "0.25\n",
      "\n",
      "n o i t a c i f i s s a c s m\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "0.15\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "0 0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60 K\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "120\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(a) Misclassiﬁcation rate vs K in a K-nearest neighbor classiﬁer. On the left, where K is Figure 1.21 small, the model is complex and hence we overﬁt. On the right, where K is large, the model is simple and we underﬁt. Dotted blue line: training set (size 200). Solid red line: test set (size 500). (b) Schematic of 5-fold cross validation. Figure generated by knnClassifyDemo.\n",
      "\n",
      "won’t have enough data to train on, and we won’t have enough data to make a reliable estimate of the future performance.\n",
      "\n",
      "A simple but popular solution to this is to use cross validation (CV). The idea is simple: we split the training data into K folds; then, for each fold k ∈ {1, . . . , K}, we train on all the folds but the k’th, and test on the k’th, in a round-robin fashion, as sketched in Figure 1.21(b). We then compute the error averaged over all the folds, and use this as a proxy for the test error. (Note that each point gets predicted only once, although it will be used for training K −1 times.) It is common to use K = 5; this is called 5-fold CV. If we set K = N , then we get a method called leave-one out cross validation, or LOOCV, since in fold i, we train on all the data cases except for i, and then test on i. Exercise 1.3 asks you to compute the 5-fold CV estimate of the test error vs K, and to compare it to the empirical test error in Figure 1.21(a).\n",
      "\n",
      "Choosing K for a KNN classiﬁer is a special case of a more general problem known as model selection, where we have to choose between models with different degrees of ﬂexibility. Cross- validation is widely used for solving such problems, although we will discuss other approaches later in the book.\n",
      "\n",
      "1.4.9\n",
      "\n",
      "No free lunch theorem\n",
      "\n",
      "All models are wrong, but some models are useful. — George Box (Box and Draper 1987, p424).12\n",
      "\n",
      "Much of machine learning is concerned with devising different models, and different algorithms to ﬁt them. We can use methods such as cross validation to empirically choose the best method for our particular problem. However, there is no universally best model — this is sometimes called the no free lunch theorem (Wolpert 1996). The reason for this is that a set of assumptions that works well in one domain may work poorly in another.\n",
      "\n",
      "12. George Box is a retired statistics professor at the University of Wisconsin.\n",
      "\n",
      "1.4. Some basic concepts in machine learning\n",
      "\n",
      "25\n",
      "\n",
      "As a consequence of the no free lunch theorem, we need to develop many different types of models, to cover the wide variety of data that occurs in the real world. And for each model, there may be many different algorithms we can use to train the model, which make different speed-accuracy-complexity tradeoffs. It is this combination of data, models and algorithms that we will be studying in the subsequent chapters.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 1.1 KNN classiﬁer on shuffled MNIST data Run mnist1NNdemo and verify that the misclassiﬁcation rate (on the ﬁrst 1000 test cases) of MNIST of a 1-NN classiﬁer is 3.8%. (If you run it all on all 10,000 test cases, the error rate is 3.09%.) Modify the code so that you ﬁrst randomly permute the features (columns of the training and test design matrices), as in shuffledDigitsDemo, and then apply the classiﬁer. Verify that the error rate is not changed.\n",
      "\n",
      "Exercise 1.2 Approximate KNN classiﬁers Use the Matlab/C++ code at http://people.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN to per- form approximate nearest neighbor search, and combine it with mnist1NNdemo to classify the MNIST data set. How much speedup do you get, and what is the drop (if any) in accuracy?\n",
      "\n",
      "Exercise 1.3 CV for KNN Use knnClassifyDemo to plot the CV estimate of the misclassiﬁcation rate on the test set. Compare this to Figure 1.21(a). Discuss the similarities and differences to the test error rate.\n",
      "\n",
      "2 Probability\n",
      "\n",
      "2.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Probability theory is nothing but common sense reduced to calculation. — Pierre Laplace, 1812\n",
      "\n",
      "In the previous chapter, we saw how probability can play a useful role in machine learning. In this chapter, we discuss probability theory in more detail. We do not have to space to go into great detail — for that, you are better off consulting some of the excellent textbooks available on this topic, such as (Jaynes 2003; Bertsekas and Tsitsiklis 2008; Wasserman 2004). But we will brieﬂy review many of the key ideas you will need in later chapters.\n",
      "\n",
      "Before we start with the more technical material, let us pause and ask: what is probability? We are all familiar with the phrase “the probability that a coin will land heads is 0.5”. But what does this mean? There are actually at least two different interpretations of probability. One is called the frequentist interpretation. In this view, probabilities represent long run frequencies of events. For example, the above statement means that, if we ﬂip the coin many times, we expect it to land heads about half the time.1\n",
      "\n",
      "In this view, probability is used to quantify our uncertainty about something; hence it is fundamentally related to information rather than repeated trials (Jaynes 2003). In the Bayesian view, the above statement means we believe the coin is equally likely to land heads or tails on the next toss.\n",
      "\n",
      "The other interpretation is called the Bayesian interpretation of probability.\n",
      "\n",
      "One big advantage of the Bayesian interpretation is that it can be used to model our uncer- tainty about events that do not have long term frequencies. For example, we might want to compute the probability that the polar ice cap will melt by 2020 CE. This event will happen zero or one times, but cannot happen repeatedly. Nevertheless, we ought to be able to quantify our uncertainty about this event; based on how probable we think this event is, we will (hopefully!) take appropriate actions (see Section 5.7 for a discussion of optimal decision making under uncertainty). To give some more machine learning oriented examples, we might have received a speciﬁc email message, and want to compute the probability it is spam. Or we might have observed a “blip” on our radar screen, and want to compute the probability distribution over the location of the corresponding target (be it a bird, plane, or missile). In all these cases, the idea of repeated trials does not make sense, but the Bayesian interpretation is valid and indeed\n",
      "\n",
      "1. Actually, the Stanford statistician (and former professional magician) Persi Diaconis has shown that a coin is about 51% likely to land facing the same way up as it started, due to the physics of the problem (Diaconis et al. 2007).\n",
      "\n",
      "28\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.75\n",
      "\n",
      "0.75\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.25\n",
      "\n",
      "0.25\n",
      "\n",
      "0 0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0 0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2.1 p(x) = 1 if x = 1 and p(x) = 0 if x ∈ {2, 3, 4}. Figure generated by discreteProbDistFig.\n",
      "\n",
      "(A) a uniform distribution on {1, 2, 3, 4}, with p(x = k) = 1/4. (b) a degenerate distribution\n",
      "\n",
      "quite natural. We shall therefore adopt the Bayesian interpretation in this book. Fortunately, the basic rules of probability theory are the same, no matter which interpretation is adopted.\n",
      "\n",
      "2.2\n",
      "\n",
      "A brief review of probability theory\n",
      "\n",
      "This section is a very brief review of the basics of probability theory, and is merely meant as a refresher for readers who may be “rusty”. Readers who are already familiar with these basics may safely skip this section.\n",
      "\n",
      "2.2.1\n",
      "\n",
      "Discrete random variables\n",
      "\n",
      "The expression p(A) denotes the probability that the event A is true. For example, A might be the logical expression “it will rain tomorrow”. We require that 0 ≤ p(A) ≤ 1, where p(A) = 0 means the event deﬁnitely will not happen, and p(A) = 1 means the event deﬁnitely will happen. We write p(A) to denote the probability of the event not A; this is deﬁned to p(A) = 1 − p(A). We will often write A = 1 to mean the event A is true, and A = 0 to mean the event A is false.\n",
      "\n",
      "We can extend the notion of binary events by deﬁning a discrete random variable X, which can take on any value from a ﬁnite or countably inﬁnite set X . We denote the probability of the event that X = x by p(X = x), or just p(x) for short. Here p() is called a probability mass function or pmf. This satisﬁes the properties 0 ≤ p(x) ≤ 1 and x∈X p(x) = 1. Figure 2.1 shows two pmf’s deﬁned on the ﬁnite state space X = {1, 2, 3, 4, 5}. On the left we have a uniform distribution, p(x) = 1/5, and on the right, we have a degenerate distribution, p(x) = I(x = 1), where I() is the binary indicator function. This distribution represents the fact that X is always equal to the value 1, in other words, it is a constant.\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "2.2.2\n",
      "\n",
      "Fundamental rules\n",
      "\n",
      "In this section, we review the basic rules of probability.\n",
      "\n",
      "2.2. A brief review of probability theory\n",
      "\n",
      "29\n",
      "\n",
      "2.2.2.1\n",
      "\n",
      "Probability of a union of two events\n",
      "\n",
      "Given two events, A and B, we deﬁne the probability of A or B as follows:\n",
      "\n",
      "p(A ∨ B) =p( A) +p( B) − p(A ∧ B)\n",
      "\n",
      "(2.1)\n",
      "\n",
      "= p(A) +p( B) if A and B are mutually exclusive\n",
      "\n",
      "(2.2)\n",
      "\n",
      "2.2.2.2\n",
      "\n",
      "Joint probabilities\n",
      "\n",
      "We deﬁne the probability of the joint event A and B as follows:\n",
      "\n",
      "p(A, B) = p(A ∧ B) = p(A|B)p(B)\n",
      "\n",
      "(2.3)\n",
      "\n",
      "This is sometimes called the product rule. Given a joint distribution on two events p(A, B), we deﬁne the marginal distribution as follows:\n",
      "\n",
      "p(A) =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "p(A, B) =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "p(A|B = b)p(B = b)\n",
      "\n",
      "(2.4)\n",
      "\n",
      "b\n",
      "\n",
      "b\n",
      "\n",
      "where we are summing over all possible states of B. We can deﬁne p(B) similarly. This is sometimes called the sum rule or the rule of total probability.\n",
      "\n",
      "The product rule can be applied multiple times to yield the chain rule of probability:\n",
      "\n",
      "p(X1:D) = p(X1)p(X2|X1)p(X3|X2, X1)p(X4|X1, X2, X3) . . . p(XD|X1:D−1)\n",
      "\n",
      "(2.5)\n",
      "\n",
      "where we introduce the Matlab-like notation 1 :D to denote the set {1, 2, . . . , D}.\n",
      "\n",
      "2.2.2.3\n",
      "\n",
      "Conditional probability\n",
      "\n",
      "We deﬁne the conditional probability of event A, given that event B is true, as follows:\n",
      "\n",
      "p(A|B) =\n",
      "\n",
      "p(A, B) p(B)\n",
      "\n",
      "if p(B) > 0\n",
      "\n",
      "(2.6)\n",
      "\n",
      "2.2.3\n",
      "\n",
      "Bayes rule\n",
      "\n",
      "Combining the deﬁnition of conditional probability with the product and sum rules yields Bayes rule, also called Bayes Theorem2:\n",
      "\n",
      "p(X = x|Y = y) =\n",
      "\n",
      "p(X = x, Y = y) p(Y = y)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(X = x)p(Y = y|X = x) x(cid:2) p(X = x(cid:3))p(Y = y|X = x(cid:3))\n",
      "\n",
      "(2.7)\n",
      "\n",
      "2.2.3.1\n",
      "\n",
      "Example: medical diagnosis\n",
      "\n",
      "As an example of how to use this rule, consider the following medical diagonsis problem. Suppose you are a woman in your 40s, and you decide to have a medical test for breast cancer called a mammogram. If the test is positive, what is the probability you have cancer? That obviously depends on how reliable the test is. Suppose you are told the test has a sensitivity\n",
      "\n",
      "2. Thomas Bayes (1702–1761) was an English mathematician and Presbyterian minister.\n",
      "\n",
      "30\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "of 80%, which means, if you have cancer, the test will be positive with probability 0.8. In other words,\n",
      "\n",
      "p(x = 1|y = 1) = 0.8\n",
      "\n",
      "(2.8)\n",
      "\n",
      "where x = 1 is the event the mammogram is positive, and y = 1 is the event you have breast cancer. Many people conclude they are therefore 80% likely to have cancer. But this is false! It ignores the prior probability of having breast cancer, which fortunately is quite low:\n",
      "\n",
      "p(y = 1) = 0.004\n",
      "\n",
      "(2.9)\n",
      "\n",
      "Ignoring this prior is called the base rate fallacy. We also need to take into account the fact that the test may be a false positive or false alarm. Unfortunately, such false positives are quite likely (with current screening technology):\n",
      "\n",
      "p(x = 1|y = 0) = 0.1\n",
      "\n",
      "(2.10)\n",
      "\n",
      "Combining these three terms using Bayes rule, we can compute the correct answer as follows:\n",
      "\n",
      "p(y = 1|x = 1) =\n",
      "\n",
      "p(x = 1|y = 1)p(y = 1) p(x = 1|y = 1)p(y = 1) + p(x = 1|y = 0)p(y = 0)\n",
      "\n",
      "(2.11)\n",
      "\n",
      "=\n",
      "\n",
      "0.8 × 0.004 0.8 × 0.004 + 0.1 × 0.996\n",
      "\n",
      "= 0.031\n",
      "\n",
      "(2.12)\n",
      "\n",
      "where p(y = 0) = 1 − p(y = 1) = 0.996. about a 3% chance of actually having breast cancer!3\n",
      "\n",
      "In other words, if you test positive, you only have\n",
      "\n",
      "2.2.3.2\n",
      "\n",
      "Example: Generative classiﬁers\n",
      "\n",
      "We can generalize the medical diagonosis example to classify feature vectors x of arbitrary type as follows:\n",
      "\n",
      "p(y = c|x, θ) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(y = c|θ)p(x|y = c, θ) c(cid:2) p(y = c(cid:3)|θ)p(x|y = c(cid:3), θ)\n",
      "\n",
      "(2.13)\n",
      "\n",
      "This is called a generative classiﬁer, since it speciﬁes how to generate the data using the class- conditional density p(x|y = c) and the class prior p(y = c). We discuss such models in detail in Chapters 3 and 4. An alternative approach is to directly ﬁt the class posterior, p(y = c|x); this is known as a discriminative classiﬁer. We discuss the pros and cons of the two approaches in Section 8.6.\n",
      "\n",
      "2.2.4\n",
      "\n",
      "Independence and conditional independence\n",
      "\n",
      "We say X and Y are unconditionally independent or marginally independent, denoted X ⊥ Y , if we can represent the joint as the product of the two marginals (see Figure 2.2), i.e.,\n",
      "\n",
      "X ⊥ Y ⇐⇒ p(X, Y ) = p(X)p(Y )\n",
      "\n",
      "(2.14)\n",
      "\n",
      "2.2. A brief review of probability theory\n",
      "\n",
      "31\n",
      "\n",
      "(cid:6)(cid:2)(cid:7)(cid:4)(cid:1)(cid:8)(cid:3)\n",
      "\n",
      "(cid:6)(cid:2)(cid:8)(cid:3)\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "(cid:6)(cid:2)(cid:7)(cid:3)\n",
      "\n",
      "Figure 2.2 Computing p(x, y) = p(x)p(y), where X ⊥ Y . Here X and Y are discrete random variables; X has 6 possible states (values) and Y has 5 possible states. A general joint distribution on two such variables would require (6 × 5) − 1 = 29 parameters to deﬁne it (we subtract 1 because of the sum-to-one constraint). By assuming (unconditional) independence, we only need (6 − 1) + (5 − 1) = 9 parameters to deﬁne p(x, y).\n",
      "\n",
      "In general, we say a set of variables is mutually independent if the joint can be written as a product of marginals.\n",
      "\n",
      "Unfortunately, unconditional independence is rare, because most variables can inﬂuence most other variables. However, usually this inﬂuence is mediated via other variables rather than being direct. We therefore say X and Y are conditionally independent (CI) given Z iff the conditional joint can be written as a product of conditional marginals:\n",
      "\n",
      "X ⊥ Y |Z ⇐⇒ p(X, Y |Z) = p(X|Z)p(Y |Z)\n",
      "\n",
      "(2.15)\n",
      "\n",
      "When we discuss graphical models in Chapter 10, we will see that we can write this assumption as a graph X −Z −Y , which captures the intuition that all the dependencies between X and Y are mediated via Z. For example, the probability it will rain tomorrow (event X) is independent of whether the ground is wet today (event Y ), given knowledge of whether it is raining today (event Z). Intuitively, this is because Z “causes” both X and Y , so if we know Z, we do not need to know about Y in order to predict X or vice versa. We shall expand on this concept in Chapter 10.\n",
      "\n",
      "Another characterization of CI is this:\n",
      "\n",
      "Theorem 2.2.1. X ⊥ Y |Z iff there exist function g and h such that\n",
      "\n",
      "p(x, y|z) = g(x, z)h(y, z)\n",
      "\n",
      "(2.16)\n",
      "\n",
      "for all x, y, z such that p(z) > 0.\n",
      "\n",
      "3. These numbers are from (McGrayne 2011, p257). Based on this analysis, the US government decided not to recommend annual mammogram screening to women in their 40s: the number of false alarms would cause needless worry and stress amongst women, and result in unnecesssary, expensive, and potentially harmful followup tests. See Section 5.7 for the optimal way to trade off risk reverse reward in the face of uncertainty.\n",
      "\n",
      "32\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "See Exercise 2.8 for the proof. CI assumptions allow us to build large probabilistic models from small pieces. We will see many examples of this throughout the book. In particular, in Section 3.5, we discuss naive Bayes classiﬁers, in Section 17.2, we discuss Markov models, and in Chapter 10 we discuss graphical models; all of these models heavily exploit CI properties.\n",
      "\n",
      "2.2.5\n",
      "\n",
      "Continuous random variables\n",
      "\n",
      "So far, we have only considered reasoning about uncertain discrete quantities. We will now show (following (Jaynes 2003, p107)) how to extend probability to reason about uncertain continuous quantities.\n",
      "\n",
      "Suppose X is some uncertain continuous quantity. The probability that X lies in any interval a ≤ X ≤ b can be computed as follows. Deﬁne the events A = (X ≤ a), B = (X ≤ b) and W = (a < X ≤ b). We have that B = A ∨ W , and since A and W are mutually exclusive, the sum rules gives\n",
      "\n",
      "p(B) = p(A) +p( W )\n",
      "\n",
      "(2.17)\n",
      "\n",
      "and hence\n",
      "\n",
      "p(W ) = p(B) − p(A)\n",
      "\n",
      "(2.18)\n",
      "\n",
      "Deﬁne the function F (q) (cid:2) p(X ≤ q). This is called the cumulative distribution function or cdf of X. This is obviously a monotonically increasing function. See Figure 2.3(a) for an example. Using this notation we have\n",
      "\n",
      "p(a < X ≤ b) = F (b) − F (a)\n",
      "\n",
      "(2.19)\n",
      "\n",
      "Now deﬁne f (x) = d dx F (x) (we assume this derivative exists); this is called the probability density function or pdf. See Figure 2.3(b) for an example. Given a pdf, we can compute the probability of a continuous variable being in a ﬁnite interval as follows:\n",
      "\n",
      "(cid:4) b\n",
      "\n",
      "P (a < X ≤ b) =\n",
      "\n",
      "a\n",
      "\n",
      "f (x)dx\n",
      "\n",
      "(2.20)\n",
      "\n",
      "As the size of the interval gets smaller, we can write\n",
      "\n",
      "P (x ≤ X ≤ x + dx) ≈ p(x)dx\n",
      "\n",
      "(2.21)\n",
      "\n",
      "We require p(x) ≥ 0, but it is possible for p(x) > 1 for any given x, so long as the density\n",
      "\n",
      "integrates to 1. As an example, consider the uniform distribution Unif(a, b):\n",
      "\n",
      "1 b − a If we set a = 0 and b = 1\n",
      "\n",
      "Unif(x|a, b) =\n",
      "\n",
      "I(a ≤ x ≤ b)\n",
      "\n",
      "2 , we havep( x) = 2 for any x ∈ [0, 1 2 ].\n",
      "\n",
      "(2.22)\n",
      "\n",
      "2.2. A brief review of probability theory\n",
      "\n",
      "33\n",
      "\n",
      "CDF\n",
      "\n",
      "100\n",
      "\n",
      "80\n",
      "\n",
      "60\n",
      "\n",
      "40\n",
      "\n",
      "α/2\n",
      "\n",
      "α/2\n",
      "\n",
      "20\n",
      "\n",
      "0 −3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "Φ−1(α/2)\n",
      "\n",
      "0\n",
      "\n",
      "Φ−1(1−α/2)\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2.3 (a) Plot of the cdf for the standard normal, N (0, 1). (b) Corresponding pdf. The shaded regions each contain α/2 of the probability mass. Therefore the nonshaded region contains 1 − α of the probability mass. If the distribution is Gaussian N (0, 1), then the leftmost cutoff point is Φ−1(α/2), where Φ is the cdf of the Gaussian. By symmetry, the rightost cutoff point is Φ−1(1 − α/2) = −Φ−1(α/2). If α = 0.05, the central interval is 95%, and the left cutoff is -1.96 and the right is 1.96. Figure generated by quantileDemo.\n",
      "\n",
      "2.2.6\n",
      "\n",
      "Quantiles\n",
      "\n",
      "Since the cdf F is a monotonically increasing function, it has an inverse; let us denote this by F −1. If F is the cdf of X, then F −1(α) is the value of xα such that P (X ≤ xα) = α; this is called the α quantile of F . The value F −1(0.5) is the median of the distribution, with half of the probability mass on the left, and half on the right. The values F −1(0.25) and F −1(0.75) are the lower and upper quartiles.\n",
      "\n",
      "We can also use the inverse cdf to compute tail area probabilities. For example, if Φ is the cdf of the Gaussian distribution N (0, 1), then points to the left of Φ−1(α)/2) contain α/2 probability mass, as illustrated in Figure 2.3(b). By symmetry, points to the right of Φ−1(1−α/2) also contain α/2 of the mass. Hence the central interval (Φ−1(α/2), Φ−1(1 − α/2)) contains 1 − α of the mass. If we set α = 0.05, the central 95% interval is covered by the range\n",
      "\n",
      "(Φ−1(0.025), Φ−1(0.975)) = (−1.96, 1.96)\n",
      "\n",
      "(2.23)\n",
      "\n",
      "If the distribution is N (μ, σ2), then the 95% interval becomes (μ − 1.96σ, μ + 1.96σ). This is sometimes approximated by writing μ ± 2σ.\n",
      "\n",
      "2.2.7 Mean and variance\n",
      "\n",
      "The most familiar property of a distribution is its mean, orexpected value, denoted by μ. For discrete rv’s, it is deﬁned as E [X] (cid:2) x∈X x p(x), and for continuous rv’s, it is deﬁned as E [X] (cid:2) X x p(x)dx. If this integral is not ﬁnite, the mean is not deﬁned (we will see some examples of this later).\n",
      "\n",
      "The variance is a measure of the “spread” of a distribution, denoted by σ2. This is deﬁned\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "34\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "as follows:\n",
      "\n",
      "var [X] (cid:2) E (cid:4)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(X − μ)2\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(x − μ)2p(x)dx (cid:4) (cid:4)\n",
      "\n",
      "x2p(x)dx + μ2\n",
      "\n",
      "p(x)dx − 2μ\n",
      "\n",
      "xp(x)dx = E\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "X 2\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "− μ2\n",
      "\n",
      "(2.24)\n",
      "\n",
      "(2.25)\n",
      "\n",
      "from which we derive the useful result\n",
      "\n",
      "E\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "X 2\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "= μ2 + σ2\n",
      "\n",
      "(2.26)\n",
      "\n",
      "The standard deviation is deﬁned as\n",
      "\n",
      "std [X] (cid:2)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "var [X]\n",
      "\n",
      "(2.27)\n",
      "\n",
      "This is useful since it has the same units as X itself.\n",
      "\n",
      "2.3\n",
      "\n",
      "Some common discrete distributions\n",
      "\n",
      "In this section, we review some commonly used parametric distributions deﬁned on discrete state spaces, both ﬁnite and countably inﬁnite.\n",
      "\n",
      "2.3.1\n",
      "\n",
      "The binomial and Bernoulli distributions\n",
      "\n",
      "Suppose we toss a coin n times. Let X ∈ {0, . . . , n} be the number of heads. If the probability of heads is θ, then we say X has a binomial distribution, written as X ∼ Bin(n, θ). The pmf is given by\n",
      "\n",
      "Bin(k|n, θ) (cid:2)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "n k\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "θk(1 − θ)n−k\n",
      "\n",
      "(2.28)\n",
      "\n",
      "where (cid:9)\n",
      "\n",
      "n k\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "n! (n − k)!k!\n",
      "\n",
      "(2.29)\n",
      "\n",
      "is the number of ways to choose k items from n (this is known as the binomial coefficient, and is pronounced “n choose k”). See Figure 2.4 for some examples of the binomial distribution. This distribution has the following mean and variance:\n",
      "\n",
      "mean = θ, var = nθ(1 − θ) (2.30) Now suppose we toss a coin only once. Let X ∈ {0, 1} be a binary random variable, with probability of “success” or “heads” of θ. We say that X has a Bernoulli distribution. This is written as X ∼ Ber(θ), where the pmf is deﬁned as\n",
      "\n",
      "Ber(x|θ) = θI(x=1)(1 − θ)I(x=0)\n",
      "\n",
      "(2.31)\n",
      "\n",
      "In other words,\n",
      "\n",
      "Ber(x|θ) =\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "θ 1 − θ\n",
      "\n",
      "if x = 1 if x = 0\n",
      "\n",
      "(2.32)\n",
      "\n",
      "This is obviously just a special case of a Binomial distribution with n = 1.\n",
      "\n",
      "2.3. Some common discrete distributions\n",
      "\n",
      "35\n",
      "\n",
      "θ=0.250\n",
      "\n",
      "θ=0.900\n",
      "\n",
      "0.35\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.35\n",
      "\n",
      "0.25\n",
      "\n",
      "0.3\n",
      "\n",
      "0.25\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.15\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "0.05\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2.4 by binomDistPlot.\n",
      "\n",
      "Illustration of the binomial distribution with n = 10 and θ ∈ {0.25, 0.9}. Figure generated\n",
      "\n",
      "2.3.2\n",
      "\n",
      "The multinomial and multinoulli distributions\n",
      "\n",
      "The binomial distribution can be used to model the outcomes of coin tosses. To model the outcomes of tossing a K-sided die, we can use the multinomial distribution. This is deﬁned as let x = (x1, . . . , xK) be a random vector, where xj is the number of times side j of follows: the die occurs. Then x has the following pmf:\n",
      "\n",
      "Mu(x|n, θ) (cid:2)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "n x1 . . . xK\n",
      "\n",
      "(cid:10) K(cid:12)\n",
      "\n",
      "j=1\n",
      "\n",
      "θ\n",
      "\n",
      "xj j\n",
      "\n",
      "(2.33)\n",
      "\n",
      "where θj is the probability that side j shows up, and\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "n x1 . . . xK\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "n! x1!x2! · · ·x K!\n",
      "\n",
      "(2.34)\n",
      "\n",
      "is the multinomial coefficient (the number of ways to divide a set of size n = subsets with sizes x1 up to xK).\n",
      "\n",
      "Now suppose n = 1. This is like rolling a K-sided dice once, so x will be a vector of 0s and 1s (a bit vector), in which only one bit can be turned on. Speciﬁcally, if the dice shows up as face k, then the k’th bit will be on. In this case, we can think of x as being a scalar categorical random variable with K states (values), and x is its dummy encoding, that is, x = [I(x = 1), . . . , I(x = K)]. For example, if K = 3, we encode the states 1, 2 and 3 as (1, 0, 0), (0, 1, 0), and (0, 0, 1). This is also called a one-hot encoding, since we imagine that only one of the K “wires” is “hot” or on. In this case, the pmf becomes\n",
      "\n",
      "(cid:2)K\n",
      "\n",
      "k=1 xk into\n",
      "\n",
      "Mu(x|1, θ) =\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "θ\n",
      "\n",
      "I(xj =1) j\n",
      "\n",
      "(2.35)\n",
      "\n",
      "j=1\n",
      "\n",
      "See Figure 2.1(b-c) for an example. This very common special case is known as a categorical or discrete distribution. (Gustavo Lacerda suggested we call it the multinoulli distribution, by analogy with the Binomial/ Bernoulli distinction, a term which we shall adopt in this book.) We\n",
      "\n",
      "36\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "Name Multinomial Multinoulli Binomial Bernoulli\n",
      "\n",
      "n K x - - - 1 1 - 1 1\n",
      "\n",
      "x ∈ {0, 1, . . . , n}K, x ∈ {0, 1}K, x ∈ {0, 1, . . . , n} x ∈ {0, 1}\n",
      "\n",
      "(cid:2)K\n",
      "\n",
      "k=1 xk = 1 (1-of-K encoding)\n",
      "\n",
      "(cid:2)K\n",
      "\n",
      "k=1 xk = n\n",
      "\n",
      "Table 2.1\n",
      "\n",
      "Summary of the multinomial and related distributions.\n",
      "\n",
      "a t a g c c g g t a c g g c a t t a g c t g c a a c c g c a t c a g c c a c t a g a g c a a t a a c c g c g a c c g c a t t a g c c g c t a a g g t a t a a g c c t c g t a c g t a t t a g c c g t t a c g g c c a t a t c c g g t a c a g t a a t a g c a g g t a c c g a a a c a t c c g t g a c g g a a\n",
      "\n",
      "s t i\n",
      "\n",
      "B\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6 8 Sequence Position\n",
      "\n",
      "7\n",
      "\n",
      "9\n",
      "\n",
      "10 11 12 13 14 15\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2.5 seqlogoDemo.\n",
      "\n",
      "(a) Some aligned DNA sequences.\n",
      "\n",
      "(b) The corresponding sequence logo. Figure generated by\n",
      "\n",
      "will use the following notation for this case:\n",
      "\n",
      "Cat(x|θ) (cid:2) Mu(x|1, θ)\n",
      "\n",
      "(2.36)\n",
      "\n",
      "In otherwords, if x ∼ Cat(θ), then p(x = j|θ) = θj. See Table 2.1 for a summary.\n",
      "\n",
      "2.3.2.1\n",
      "\n",
      "Application: DNA sequence motifs\n",
      "\n",
      "An interesting application of multinomial models arises in biosequence analysis. Suppose we have a set of (aligned) DNA sequences, such as in Figure 2.5(a), where there are 10 rows (sequences) and 15 columns (locations along the genome). We see that several locations are con- served by evolution (e.g., because they are part of a gene coding region), since the corresponding columns tend to be “pure”. For example, column 7 is all G’s.\n",
      "\n",
      "One way to visually summarize the data is by using a sequence logo: see Figure 2.5(b). We plot the letters A, C, G and T with a fontsize proportional to their empirical probability, and with the most probable letter on the top. The empirical probability distribution at location t, ˆθt, is gotten by normalizing the vector of counts (see Equation 3.48 ):\n",
      "\n",
      "Nt =\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "N(cid:3)\n",
      "\n",
      "I(Xit = 1),\n",
      "\n",
      "N(cid:3)\n",
      "\n",
      "I(Xit = 2),\n",
      "\n",
      "N(cid:3)\n",
      "\n",
      "I(Xit = 3),\n",
      "\n",
      "N(cid:3)\n",
      "\n",
      "I(Xit = 4)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(2.37)\n",
      "\n",
      "i=1 ˆθt = Nt/N\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "(2.38)\n",
      "\n",
      "This distribution is known as a motif. We can also compute the most probable letter in each location; this is called the consensus sequence.\n",
      "\n",
      "2.3. Some common discrete distributions\n",
      "\n",
      "37\n",
      "\n",
      "Poi(λ=1.000)\n",
      "\n",
      "Poi(λ=10.000)\n",
      "\n",
      "0.4\n",
      "\n",
      "0.14\n",
      "\n",
      "0.35\n",
      "\n",
      "0.12\n",
      "\n",
      "0.3\n",
      "\n",
      "0.1\n",
      "\n",
      "0.25\n",
      "\n",
      "0.08\n",
      "\n",
      "0.2\n",
      "\n",
      "0.06\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "0.04\n",
      "\n",
      "0.05\n",
      "\n",
      "0.02\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2.6 Illustration of some Poisson distributions for λ ∈ {1, 10}. We have truncated the x-axis to 25 for clarity, but the support of the distribution is over all the non-negative integers. Figure generated by poissonPlotDemo.\n",
      "\n",
      "2.3.3\n",
      "\n",
      "The Poisson distribution\n",
      "\n",
      "We say that X ∈ {0, 1, 2, . . .} has a Poisson distribution with parameter λ >0, written X ∼ Poi(λ), if its pmf is Poi(x|λ) =e −λ λx x!\n",
      "\n",
      "(2.39)\n",
      "\n",
      "The ﬁrst term is just the normalization constant, required to ensure the distribution sums to 1. The Poisson distribution is often used as a model for counts of rare events like radioactive\n",
      "\n",
      "decay and traffic accidents. See Figure 2.6 for some plots.\n",
      "\n",
      "2.3.4\n",
      "\n",
      "The empirical distribution\n",
      "\n",
      "Given a set of data, D = {x1, . . . , xN }, we deﬁne the empirical distribution, also called the empirical measure, as follows:\n",
      "\n",
      "pemp(A) (cid:2)\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:3)\n",
      "\n",
      "i=1\n",
      "\n",
      "δxi (A)\n",
      "\n",
      "(2.40)\n",
      "\n",
      "where δx(A) is the Dirac measure, deﬁned by if x (cid:12)∈ A if x ∈ A\n",
      "\n",
      "δx(A) =\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "0 1\n",
      "\n",
      "(2.41)\n",
      "\n",
      "In general, we can associate “weights” with each sample:\n",
      "\n",
      "N(cid:3)\n",
      "\n",
      "p(x) =\n",
      "\n",
      "wiδxi (x)\n",
      "\n",
      "(2.42)\n",
      "\n",
      "where we require 0 ≤ wi ≤ 1 and i=1 wi = 1. We can think of this as a histogram, with “spikes” at the data points xi, where wi determines the height of spike i. This distribution assigns 0 probability to any point not in the data set.\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:2)N\n",
      "\n",
      "38\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "2.4\n",
      "\n",
      "Some common continuous distributions\n",
      "\n",
      "In this section we present some commonly used univariate (one-dimensional) continuous prob- ability distributions.\n",
      "\n",
      "2.4.1\n",
      "\n",
      "Gaussian (normal) distribution\n",
      "\n",
      "The most widely used distribution in statistics and machine learning is the Gaussian or normal distribution. Its pdf is given by\n",
      "\n",
      "Here μ = E [X] is the mean (and mode), and σ2 = var [X] is the variance. normalization constant needed to ensure the density integrates to 1 (see Exercise 2.11).\n",
      "\n",
      "If X ∼ N (0, 1), we say X follows a standard normal distribution. See Figure 2.3(b) for a plot of this pdf; this is sometimes called the bell curve.\n",
      "\n",
      "We write X ∼ N (μ, σ2) to denote that p(X = x) =N (x|μ, σ2).\n",
      "\n",
      "N (x|μ, σ2) (cid:2)\n",
      "\n",
      "1√\n",
      "\n",
      "2πσ2\n",
      "\n",
      "e− 1\n",
      "\n",
      "2σ2 (x−μ)2\n",
      "\n",
      "√\n",
      "\n",
      "2πσ2 is the\n",
      "\n",
      "(2.43)\n",
      "\n",
      "λ = 1/σ2. A high precision means a narrow distribution (low variance) centered on μ.4\n",
      "\n",
      "We will often talk about the precision of a Gaussian, by which we mean the inverse variance:\n",
      "\n",
      "density at its center, x = μ. We have N (μ|μ, σ2) = (σ p(x) > 1.\n",
      "\n",
      "Note that, since this is a pdf, we can have p(x) > 1. To see this, consider evaluating the 2π, we have\n",
      "\n",
      "√\n",
      "\n",
      "2π)−1e0, so if σ < 1/\n",
      "\n",
      "√\n",
      "\n",
      "The cumulative distribution function or cdf of the Gaussian is deﬁned as\n",
      "\n",
      "Φ(x; μ, σ2) (cid:2)\n",
      "\n",
      "(cid:4) x\n",
      "\n",
      "−∞\n",
      "\n",
      "N (z|μ, σ2)dz\n",
      "\n",
      "(2.44)\n",
      "\n",
      "See Figure 2.3(a) for a plot of this cdf when μ = 0, σ2 = 1. This integral has no closed form expression, but is built in to most software packages. In particular, we can compute it in terms of the error function (erf):\n",
      "\n",
      "Φ(x; μ, σ) =\n",
      "\n",
      "1 2\n",
      "\n",
      "[1 + erf(z/\n",
      "\n",
      "√\n",
      "\n",
      "2)]\n",
      "\n",
      "(2.45)\n",
      "\n",
      "where z = (x − μ)/σ and\n",
      "\n",
      "erf(x) (cid:2) 2√ π\n",
      "\n",
      "(cid:4) x\n",
      "\n",
      "0\n",
      "\n",
      "e−t2\n",
      "\n",
      "dt\n",
      "\n",
      "(2.46)\n",
      "\n",
      "The Gaussian distribution is the most widely used distribution in statistics. There are several reasons for this. First, it has two parameters which are easy to interpret, and which capture some of the most basic properties of a distribution, namely its mean and variance. Second, the central limit theorem (Section 2.6.3) tells us that sums of independent random variables have an approximately Gaussian distribution, making it a good choice for modeling residual errors or “noise”. Third, the Gaussian distribution makes the least number of assumptions (has\n",
      "\n",
      "4. The symbol λ will have many different meanings in this book, in order to be consistent with the rest of the literature. The intended meaning should be clear from context.\n",
      "\n",
      "2.4. Some common continuous distributions\n",
      "\n",
      "39\n",
      "\n",
      "maximum entropy), subject to the constraint of having a speciﬁed mean and variance, as we show in Section 9.2.6; this makes it a good default choice in many cases. Finally, it has a simple mathematical form, which results in easy to implement, but often highly effective, methods, as we will see. See (Jaynes 2003, ch 7) for a more extensive discussion of why Gaussians are so widely used.\n",
      "\n",
      "2.4.2\n",
      "\n",
      "Degenerate pdf In the limit that σ2 → 0, the Gaussian becomes an inﬁnitely tall and inﬁnitely thin “spike” centered at μ:\n",
      "\n",
      "lim σ2→0\n",
      "\n",
      "N (x|μ, σ2) = δ(x − μ)\n",
      "\n",
      "(2.47)\n",
      "\n",
      "where δ is called a Dirac delta function, and is deﬁned as\n",
      "\n",
      "δ(x) =\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "∞ if x = 0 if x (cid:12)= 0 0\n",
      "\n",
      "(2.48)\n",
      "\n",
      "such that (cid:4) ∞\n",
      "\n",
      "−∞\n",
      "\n",
      "δ(x)dx = 1\n",
      "\n",
      "(2.49)\n",
      "\n",
      "A useful property of delta functions is the sifting property, which selects out a single term\n",
      "\n",
      "from a sum or integral:\n",
      "\n",
      "(cid:4) ∞\n",
      "\n",
      "−∞\n",
      "\n",
      "f (x)δ(x − μ)dx = f (μ)\n",
      "\n",
      "(2.50)\n",
      "\n",
      "since the integrand is only non-zero if x − μ = 0.\n",
      "\n",
      "One problem with the Gaussian distribution is that it is sensitive to outliers, since the log- probability only decays quadratically with distance from the center. A more robust distribution is the Student t distribution5 Its pdf is as follows: (cid:16)−( ν+1 (cid:15) 2 )\n",
      "\n",
      "T (x|μ, σ2, ν) ∝\n",
      "\n",
      "1 +\n",
      "\n",
      "1 ν\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "x − μ σ\n",
      "\n",
      "(cid:10)2\n",
      "\n",
      "(2.51)\n",
      "\n",
      "where μ is the mean, σ2 > 0 is the scale parameter, and ν > 0 is called the degrees of freedom. See Figure 2.7 for some plots. For later reference, we note that the distribution has the following properties:\n",
      "\n",
      "mean = μ, mode = μ, var =\n",
      "\n",
      "νσ2 (ν − 2)\n",
      "\n",
      "(2.52)\n",
      "\n",
      "5. This distribution has a colourful etymology. It was ﬁrst published in 1908 by William Sealy Gosset, who worked at the Guinness brewery in Dublin. Since his employer would not allow him to use his own name, he called it the “Student” distribution. The origin of the term t seems to have arisen in the context of Tables of the Student distribution, used by Fisher when developing the basis of classical statistical inference. See http://jeff560.tripod.com/s.html for more historical details.\n",
      "\n",
      "40\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "Gauss Student Laplace\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "Gauss Student Laplace\n",
      "\n",
      "0.6\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "0.5\n",
      "\n",
      "−4\n",
      "\n",
      "0.4\n",
      "\n",
      "−5\n",
      "\n",
      "0.3\n",
      "\n",
      "−6\n",
      "\n",
      "0.2\n",
      "\n",
      "−7\n",
      "\n",
      "0.1\n",
      "\n",
      "−8\n",
      "\n",
      "0 −4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "−9\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "2). The mean is 0 and the variance Figure 2.7 is 1 for both the Gaussian and Laplace. The mean and variance of the Student is undeﬁned when ν = 1. (b) Log of these pdf’s. Note that the Student distribution is not log-concave for any parameter value, unlike the Laplace distribution, which is always log-concave (and log-convex...) Nevertheless, both are unimodal. Figure generated by studentLaplacePdfPlot.\n",
      "\n",
      "(a) The pdf’s for a N (0, 1), T (0, 1, 1) and Lap(0, 1/\n",
      "\n",
      "√\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "gaussian student T laplace\n",
      "\n",
      "gaussian student T laplace\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "0 −5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2.8 Illustration of the effect of outliers on ﬁtting Gaussian, Student and Laplace distributions. (a) No outliers (the Gaussian and Student curves are on top of each other). (b) With outliers. We see that the Gaussian is more affected by outliers than the Student and Laplace distributions. Based on Figure 2.16 of (Bishop 2006a). Figure generated by robustDemo.\n",
      "\n",
      "The variance is only deﬁned if ν > 2. The mean is only deﬁned if ν > 1.\n",
      "\n",
      "As an illustration of the robustness of the Student distribution, consider Figure 2.8. On the left, we show a Gaussian and a Student ﬁt to some data with no outliers. On the right, we add some outliers. We see that the Gaussian is affected a lot, whereas the Student distribution hardly changes. This is because the Student has heavier tails, at least for small ν (see Figure 2.7). If ν = 1, this distribution is known as the Cauchy or Lorentz distribution. This is notable\n",
      "\n",
      "for having such heavy tails that the integral that deﬁnes the mean does not converge.\n",
      "\n",
      "It is common to use ν = 4, which gives good performance in a range of problems (Lange et al. 1989). For ν (cid:17) 5, the Student distribution rapidly approaches a Gaussian distribution and loses its robustness properties.\n",
      "\n",
      "To ensure ﬁnite variance, we require ν > 2.\n",
      "\n",
      "2.4. Some common continuous distributions\n",
      "\n",
      "41\n",
      "\n",
      "Gamma distributions\n",
      "\n",
      "3.5\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "a=1.0,b=1.0 a=1.5,b=1.0 a=2.0,b=1.0\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "0.6\n",
      "\n",
      "2\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "1.5\n",
      "\n",
      "0.3\n",
      "\n",
      "1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "2.5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "If a ≤ 1, the mode is at 0, otherwise it is > 0. As Figure 2.9 (a) Some Ga(a, b = 1) distributions. we increase the rate b, we reduce the horizontal scale, thus squeezing everything leftwards and upwards. Figure generated by gammaPlotDemo. (b) An empirical pdf of some rainfall data, with a ﬁtted Gamma distribution superimposed. Figure generated by gammaRainfallDemo.\n",
      "\n",
      "2.4.3\n",
      "\n",
      "The Laplace distribution\n",
      "\n",
      "Another distribution with heavy tails is the Laplace distribution6, also known as the double sided exponential distribution. This has the following pdf: (cid:9)\n",
      "\n",
      "Lap(x|μ, b) (cid:2)\n",
      "\n",
      "1 2b\n",
      "\n",
      "exp\n",
      "\n",
      "−\n",
      "\n",
      "|x − μ| b\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(2.53)\n",
      "\n",
      "Here μ is a location parameter and b >0 is a scale parameter. See Figure 2.7 for a plot. This distribution has the following properties: mean = μ, mode = μ, var = 2b2\n",
      "\n",
      "(2.54)\n",
      "\n",
      "Its robustness to outliers is illustrated in Figure 2.8. It also put mores probability density at 0 than the Gaussian. This property is a useful way to encourage sparsity in a model, as we will see in Section 13.3.\n",
      "\n",
      "2.4.4\n",
      "\n",
      "The gamma distribution\n",
      "\n",
      "The gamma distribution is a ﬂexible distribution for positive real valued rv’s, x > 0. deﬁned in terms of two parameters, called the shape a >0 and the rate b >0: 7 ba Γ(a)\n",
      "\n",
      "Ga(T |shape = a, rate = b) (cid:2)\n",
      "\n",
      "T a−1e−T b\n",
      "\n",
      "It is\n",
      "\n",
      "(2.55)\n",
      "\n",
      "6. Pierre-Simon Laplace (1749–1827) was a French mathematician, who played a key role in creating the ﬁeld of Bayesian statistics. 7. There is an alternative parameterization, where we use the scale parameter instead of the rate: Gas(T |a, b) (cid:2) Ga(T |a, 1/b). This version is the one used by Matlab’s gampdf, although in this book will use the rate parameterization unless otherwise speciﬁed.\n",
      "\n",
      "42\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "where Γ(a) is the gamma function:\n",
      "\n",
      "Γ(x) (cid:2)\n",
      "\n",
      "(cid:4) ∞\n",
      "\n",
      "ux−1e−udu\n",
      "\n",
      "0\n",
      "\n",
      "(2.56)\n",
      "\n",
      "See Figure 2.9 for some plots. For later reference, we note that the distribution has the following properties:\n",
      "\n",
      "mean =\n",
      "\n",
      "a b\n",
      "\n",
      ", mode =\n",
      "\n",
      "a − 1 b\n",
      "\n",
      ", var =\n",
      "\n",
      "a b2\n",
      "\n",
      "(2.57)\n",
      "\n",
      "There are several distributions which are just special cases of the Gamma, which we discuss\n",
      "\n",
      "below.\n",
      "\n",
      "Exponential distribution This is deﬁned by Expon(x|λ) (cid:2) Ga(x|1, λ), where λ is the rate parameter. This distribution describes the times between events in a Poisson process, i.e. a process in which events occur continuously and independently at a constant average rate λ. It is common to ﬁx a = 2, yielding the one-parameter Erlang distribution, Erlang(x|λ) = Ga(x|2, λ), whereλ is the rate parameter.\n",
      "\n",
      "Erlang distribution This is the same as the Gamma distribution where a is an integer.\n",
      "\n",
      "Chi-squared distribution This is deﬁned by χ2(x|ν) (cid:2) Ga(x| ν\n",
      "\n",
      "2 ). This is the distribution of the sum of squared Gaussian random variables. More precisely, if Zi ∼ N (0, 1), and S =\n",
      "\n",
      "(cid:2)ν\n",
      "\n",
      "i=1 Z 2\n",
      "\n",
      "i , then S ∼ χ2 ν.\n",
      "\n",
      "2 , 1\n",
      "\n",
      "that 1\n",
      "\n",
      "Another useful result is the following:\n",
      "\n",
      "IG(x|shape = a, scale = b) (cid:2)\n",
      "\n",
      "X ∼ IG(a, b), whereIG is the inverse gamma distribution deﬁned by ba Γ(a)\n",
      "\n",
      "If X ∼ Ga(a, b), then one can show (Exercise 2.10)\n",
      "\n",
      "x−(a+1)e−b/x\n",
      "\n",
      "(2.58)\n",
      "\n",
      "The distribution has these properties\n",
      "\n",
      "mean =\n",
      "\n",
      "b a − 1\n",
      "\n",
      ", mode =\n",
      "\n",
      "b a + 1\n",
      "\n",
      ", var =\n",
      "\n",
      "b2 (a − 1)2(a − 2)\n",
      "\n",
      ",\n",
      "\n",
      "(2.59)\n",
      "\n",
      "The mean only exists if a >1. The variance only exists if a >2.\n",
      "\n",
      "We will see applications of these distributions later on.\n",
      "\n",
      "2.4.5\n",
      "\n",
      "The beta distribution\n",
      "\n",
      "The beta distribution has support over the interval [0, 1] and is deﬁned as follows:\n",
      "\n",
      "Beta(x|a, b) =\n",
      "\n",
      "1 B(a, b)\n",
      "\n",
      "xa−1(1 − x)b−1\n",
      "\n",
      "(2.60)\n",
      "\n",
      "Here B(p, q) is the beta function,\n",
      "\n",
      "B(a, b) (cid:2)\n",
      "\n",
      "Γ(a)Γ(b) Γ(a + b)\n",
      "\n",
      "(2.61)\n",
      "\n",
      "See Figure 2.10 for plots of some beta distributions. We require a, b > 0 to ensure the distribution is integrable (i.e., to ensure B(a, b) exists). If\n",
      "\n",
      "If a = b = 1, we get the uniform distirbution.\n",
      "\n",
      "2.4. Some common continuous distributions\n",
      "\n",
      "43\n",
      "\n",
      "beta distributions\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "a=0.1, b=0.1 a=1.0, b=1.0 a=2.0, b=3.0 a=8.0, b=4.0\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "Figure 2.10 Some beta distributions. Figure generated by betaPlotDemo.\n",
      "\n",
      "a and b are both less than 1, we get a bimodal distribution with “spikes” at 0 and 1; if a and b are both greater than 1, the distribution is unimodal. For later reference, we note that the distribution has the following properties (Exercise 2.16):\n",
      "\n",
      "mean =\n",
      "\n",
      "a a + b\n",
      "\n",
      ", mode =\n",
      "\n",
      "a − 1 a + b − 2\n",
      "\n",
      ", var =\n",
      "\n",
      "ab (a + b)2(a + b + 1)\n",
      "\n",
      "(2.62)\n",
      "\n",
      "2.4.6\n",
      "\n",
      "Pareto distribution\n",
      "\n",
      "The Pareto distribution is used to model the distribution of quantities that exhibit long tails, also called heavy tails. For example, it has been observed that the most frequent word in English (“the”) occurs approximately twice as often as the second most frequent word (“of”), which occurs twice as often as the fourth most frequent word, etc. If we plot the frequency of words vs their rank, we will get a power law; this is known as Zipf’s law. Wealth has a similarly skewed distribution, especially in plutocracies such as the USA.8\n",
      "\n",
      "The Pareto pdf is deﬁned as follow: Pareto(x|k, m) = kmkx−(k+1)I(x ≥ m)\n",
      "\n",
      "(2.63)\n",
      "\n",
      "This density asserts that x must be greater than some constant m, but not too much greater, where k controls what is “too much”. As k → ∞, the distribution approaches δ(x − m). See If we plot the distibution on a log-log scale, it forms a straight Figure 2.11(a) for some plots. line, of the form log p(x) = a log x + c for some constants a and c. See Figure 2.11(b) for an illustration (this is known as a power law). This distribution has the following properties\n",
      "\n",
      "mean =\n",
      "\n",
      "km k − 1\n",
      "\n",
      "if k > 1 , mode = m, var =\n",
      "\n",
      "m2k (k − 1)2(k − 2)\n",
      "\n",
      "if k > 2\n",
      "\n",
      "(2.64)\n",
      "\n",
      "8. In the USA, http://www.politifact.com/wisconsin/statements/2011/mar/10/michael-moore/michael-moore-s ays-400-americans-have-more-wealth-.) See (Hacker and Pierson 2010) for a political analysis of how such an extreme distribution of income has arisen in a democratic country.\n",
      "\n",
      "400 Americans have more wealth than half of\n",
      "\n",
      "all Americans\n",
      "\n",
      "combined.\n",
      "\n",
      "(Source:\n",
      "\n",
      "44\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "Pareto distribution\n",
      "\n",
      "1 10\n",
      "\n",
      "Pareto(m=1, k) on log scale\n",
      "\n",
      "2\n",
      "\n",
      "1.8\n",
      "\n",
      "m=0.01, k=0.10 m=0.00, k=0.50 m=1.00, k=1.00\n",
      "\n",
      "k=1.0 k=2.0 k=3.0\n",
      "\n",
      "1.6\n",
      "\n",
      "0 10\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "1\n",
      "\n",
      "10\n",
      "\n",
      "−1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "10\n",
      "\n",
      "−2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "2.5\n",
      "\n",
      "3\n",
      "\n",
      "3.5\n",
      "\n",
      "4\n",
      "\n",
      "4.5\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "−3\n",
      "\n",
      "0 10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2.11 generated by paretoPlot.\n",
      "\n",
      "(a) The Pareto distribution Pareto(x|m, k) for m = 1. (b) The pdf on a log-log scale. Figure\n",
      "\n",
      "2.5\n",
      "\n",
      "Joint probability distributions\n",
      "\n",
      "So far, we have been mostly focusing on modeling univariate probability distributions. In this section, we start our discussion of the more challenging problem of building joint probability distributions on multiple related random variables; this will be a central topic in this book.\n",
      "\n",
      "A joint probability distribution has the form p(x1, . . . , xD) for a set ofD > 1 variables, and models the (stochastic) relationships between the variables. If all the variables are discrete, we can represent the joint distribution as a big multi-dimensional array, with one variable per dimension. However, the number of parameters needed to deﬁne such a model is O(K D), where K is the number of states for each variable.\n",
      "\n",
      "We can deﬁne high dimensional joint distributions using fewer parameters by making con- ditional independence assumptions, as we explain in Chapter 10. In the case of continuous distributions, an alternative approach is to restrict the form of the pdf to certain functional forms, some of which we will examine below.\n",
      "\n",
      "2.5.1\n",
      "\n",
      "Covariance and correlation\n",
      "\n",
      "The covariance between two rv’s X and Y measures the degree to which X and Y are (linearly) related. Covariance is deﬁned as\n",
      "\n",
      "cov [X, Y ] (cid:2) E [(X − E [X])(Y − E [Y ])] = E [XY ] − E [X] E [Y ]\n",
      "\n",
      "(2.65)\n",
      "\n",
      "2.5.\n",
      "\n",
      "Joint probability distributions\n",
      "\n",
      "45\n",
      "\n",
      "Figure 2.12 Several sets of (x, y) points, with the correlation coefficient of x and y for each set. Note that the correlation reﬂects the noisiness and direction of a linear relationship (top row), but not the slope of that relationship (middle), nor many aspects of nonlinear relationships (bottom). N.B.: the ﬁgure in the center has a slope of 0 but in that case the correlation coefficient is undeﬁned because the variance of Y is zero. Source: http://en.wikipedia.org/wiki/File:Correlation_examples.png\n",
      "\n",
      "If x is a d-dimensional random vector, its covariance matrix is deﬁned to be the following symmetric, positive deﬁnite matrix:\n",
      "\n",
      "cov [x] (cid:2) E ⎛\n",
      "\n",
      "=\n",
      "\n",
      "⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "var [X1] cov [X2, X1] ... cov [Xd, X1]\n",
      "\n",
      "(x − E [x])(x − E [x])\n",
      "\n",
      "cov [X1, X2] var [X2] ... cov [Xd, X2]\n",
      "\n",
      "T\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "· · · · · . . . · · ·\n",
      "\n",
      "cov [X1, Xd] cov [X2, Xd] ... var [Xd]\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "(2.66)\n",
      "\n",
      "(2.67)\n",
      "\n",
      "Covariances can be between 0 and inﬁnity. Sometimes it is more convenient to work with a normalized measure, with a ﬁnite upper bound. The (Pearson) correlation coefficient between X and Y is deﬁned as\n",
      "\n",
      "corr [X, Y ] (cid:2)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "var [X] var [Y ]\n",
      "\n",
      "cov [X, Y ]\n",
      "\n",
      "(2.68)\n",
      "\n",
      "A correlation matrix has the form\n",
      "\n",
      "R =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎝\n",
      "\n",
      "corr [X1, X1] ... corr [Xd, X1]\n",
      "\n",
      "corr [X1, X2] ... corr [Xd, X2]\n",
      "\n",
      "· · . . . · · ·\n",
      "\n",
      "corr [X1, Xd] ... corr [Xd, Xd]\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎠\n",
      "\n",
      "(2.69)\n",
      "\n",
      "One can show (Exercise 4.3) that −1 ≤ corr [X, Y ] ≤ 1. Hence in a correlation matrix, each\n",
      "\n",
      "entry on the diagonal is 1, and the other entries are between -1 and 1.\n",
      "\n",
      "One can also show that corr [X, Y ] = 1 if and only if Y = aX + b for some parameters a and b, i.e., if there is a linear relationship between X and Y (see Exercise 4.4). Intuitively one\n",
      "\n",
      "46\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "might expect the correlation coefficient to be related to the slope of the regression line, i.e., the coefficient a in the expression Y = aX + b. However, as we show in Equation 7.99 later, the regression coefficient is in fact given by a = cov [X, Y ] /var [X]. A better way to think of the correlation coefficient is as a degree of linearity: see Figure 2.12.\n",
      "\n",
      "If X and Y are independent, meaning p(X, Y ) =p( X)p(Y ) (see Section 2.2.4), then cov [X, Y ] = 0, and hence corr [X, Y ] = 0 so they are uncorrelated. However, the con- verse is not true: uncorrelated does not imply independent. For example, let X ∼ U (−1, 1) and Y = X 2. Clearly Y is dependent on X (in fact, Y is uniquely determined by X), yet one can show (Exercise 4.1) that corr [X, Y ] = 0. Some striking examples of this fact are shown in Figure 2.12. This shows several data sets where there is clear dependendence between X and Y , and yet the correlation coefficient is 0. A more general measure of dependence between random variables is mutual information, discussed in Section 2.8.3. This is only zero if the variables truly are independent.\n",
      "\n",
      "2.5.2\n",
      "\n",
      "The multivariate Gaussian\n",
      "\n",
      "The multivariate Gaussian or multivariate normal (MVN) is the most widely used joint prob- ability density function for continuous variables. We discuss MVNs in detail in Chapter 4; here we just give some deﬁnitions and plots.\n",
      "\n",
      "The pdf of the MVN in D dimensions is deﬁned by the following:\n",
      "\n",
      "N (x|μ, Σ) (cid:2)\n",
      "\n",
      "(2π)D/2|Σ|1/2 exp\n",
      "\n",
      "1\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(x − μ)T Σ−1(x − μ)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "(2.70)\n",
      "\n",
      "where μ = E [x] ∈ RD is the mean vector, and Σ = cov [x] is the D × D covariance matrix. Sometimes we will work in terms of the precision matrix or concentration matrix instead. This is just the inverse covariance matrix, Λ = Σ−1. The normalization constant (2π)−D/2|Λ|1/2 just ensures that the pdf integrates to 1 (see Exercise 4.5).\n",
      "\n",
      "Figure 2.13 plots some MVN densities in 2d for three different kinds of covariance matrices. A full covariance matrix has D(D + 1)/2 parameters (we divide by 2 since Σ is symmetric). A diagonal covariance matrix has D parameters, and has 0s in the off-diagonal terms. A spherical or isotropic covariance, Σ = σ2ID, has one free parameter.\n",
      "\n",
      "2.5.3 Multivariate Student t distribution\n",
      "\n",
      "A more robust alternative to the MVN is the multivariate Student t distribution, whose pdf is given by\n",
      "\n",
      "T (x|μ, Σ, ν) =\n",
      "\n",
      "=\n",
      "\n",
      "Γ(ν/2 + D/2) Γ(ν/2) Γ(ν/2 + D/2) Γ(ν/2)\n",
      "\n",
      "|Σ|−1/2 νD/2πD/2\n",
      "\n",
      "×\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "1 +\n",
      "\n",
      "1 ν\n",
      "\n",
      "(x − μ)T Σ−1(x − μ)\n",
      "\n",
      "(cid:26)−( ν+D\n",
      "\n",
      "2\n",
      "\n",
      ") (2.71)\n",
      "\n",
      "|πV|−1/2 ×\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "1 + (x − μ)T V−1(x − μ)\n",
      "\n",
      "(cid:7)−( ν+D\n",
      "\n",
      "2\n",
      "\n",
      ")\n",
      "\n",
      "(2.72)\n",
      "\n",
      "where Σ is called the scale matrix (since it is not exactly the covariance matrix) and V = νΣ. This has fatter tails than a Gaussian. The smaller ν is, the fatter the tails. As ν → ∞, the\n",
      "\n",
      "2.5.\n",
      "\n",
      "Joint probability distributions\n",
      "\n",
      "47\n",
      "\n",
      "6\n",
      "\n",
      "full\n",
      "\n",
      "10\n",
      "\n",
      "diagonal\n",
      "\n",
      "8\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "−10\n",
      "\n",
      "−5\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "spherical\n",
      "\n",
      "spherical\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "0.2\n",
      "\n",
      "2\n",
      "\n",
      "0.15\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "−1\n",
      "\n",
      "0.05\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "0 5\n",
      "\n",
      "5\n",
      "\n",
      "−4\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "−5\n",
      "\n",
      "−5\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 2.13 We show the level sets for 2d Gaussians. (a) A full covariance matrix has elliptical contours. (b) A diagonal covariance matrix is an axis aligned ellipse. (c) A spherical covariance matrix has a circular shape. (d) Surface plot for the spherical Gaussian in (c). Figure generated by gaussPlot2Ddemo.\n",
      "\n",
      "distribution tends towards a Gaussian. The distribution has the following properties\n",
      "\n",
      "mean = μ, mode = μ, Cov =\n",
      "\n",
      "ν ν − 2\n",
      "\n",
      "Σ\n",
      "\n",
      "(2.73)\n",
      "\n",
      "2.5.4\n",
      "\n",
      "Dirichlet distribution\n",
      "\n",
      "A multivariate generalization of the beta distribution is the Dirichlet distribution9, which has support over the probability simplex, deﬁned by\n",
      "\n",
      "SK = {x : 0 ≤ xk ≤ 1,\n",
      "\n",
      "K(cid:3)\n",
      "\n",
      "xk = 1}\n",
      "\n",
      "(2.74)\n",
      "\n",
      "k=1\n",
      "\n",
      "The pdf is deﬁned as follows:\n",
      "\n",
      "Dir(x|α) (cid:2)\n",
      "\n",
      "1 B(α)\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "k=1\n",
      "\n",
      "xαk−1 k\n",
      "\n",
      "I(x ∈ SK)\n",
      "\n",
      "(2.75)\n",
      "\n",
      "9. Johann Dirichlet was a German mathematician, 1805–1859.\n",
      "\n",
      "48\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "α=0.10\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "p\n",
      "\n",
      "5\n",
      "\n",
      "0 1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(a) The Dirichlet distribution when K = 3 deﬁnes a distribution over the simplex, which Figure 2.14 can be represented by the triangular surface. Points on this surface satisfy 0 ≤ θk ≤ 1 and k=1 θk = 1. Figure generated by visDirichletGui, by Jonathan Huang. (d) α = (0.1, 0.1, 0.1). (The comb-like structure on the edges is a plotting artifact.) Figure generated by dirichlet3dPlot.\n",
      "\n",
      "(b) Plot of the Dirichlet density when α = (2, 2, 2).\n",
      "\n",
      "(c) α = (20, 2, 2).\n",
      "\n",
      "(cid:2)3\n",
      "\n",
      "Samples from Dir (alpha=0.1)\n",
      "\n",
      "Samples from Dir (alpha=1)\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2.15 (a) α = (0.1, . . . , 0.1). This results in very sparse distributions, with many 0s. results in more uniform (and dense) distributions. Figure generated by dirichletHistogramDemo.\n",
      "\n",
      "Samples from a 5-dimensional symmetric Dirichlet distribution for different parameter values. (b) α = (1, . . . , 1). This\n",
      "\n",
      "2.6. Transformations of random variables\n",
      "\n",
      "49\n",
      "\n",
      "where B(α1, . . . , αK) is the natural generalization of the beta function to K variables:\n",
      "\n",
      "B(α) (cid:2)\n",
      "\n",
      "(cid:27)K\n",
      "\n",
      "k=1 Γ(αk) Γ(α0)\n",
      "\n",
      "(2.76)\n",
      "\n",
      "where α0 (cid:2)\n",
      "\n",
      "(cid:2)K\n",
      "\n",
      "k=1 αk.\n",
      "\n",
      "Figure 2.14 shows some plots of the Dirichlet when K = 3, and Figure 2.15 for some sampled probability vectors. We see that α0 = k=1 αk controls the strength of the distribution (how peaked it is), and the αk control where the peak occurs. For example, Dir(1, 1, 1) is a uniform distribution, Dir(2, 2, 2) is a broad distribution centered at (1/3, 1/3, 1/3), and Dir(20, 20, 20) is a narrow distribution centered at (1/3, 1/3, 1/3). If αk < 1 for all k, we get “spikes” at the corner of the simplex.\n",
      "\n",
      "(cid:2)K\n",
      "\n",
      "For future reference, the distribution has these properties\n",
      "\n",
      "where α0 = case, the mean becomes 1/K, and the variance becomes var [xk] = K−1 increases the precision (decreases the variance) of the distribution.\n",
      "\n",
      "E [xk] =\n",
      "\n",
      "αk α0 (cid:2)\n",
      "\n",
      "k αk. Often we use a symmetric Dirichlet prior of the form αk = α/K. In this K2(α+1) . So increasing α\n",
      "\n",
      ", mode [xk] =\n",
      "\n",
      "αk − 1 α0 − K\n",
      "\n",
      ", var [xk] =\n",
      "\n",
      "αk(α0 − αk) α2 0(α0 + 1)\n",
      "\n",
      "(2.77)\n",
      "\n",
      "2.6\n",
      "\n",
      "Transformations of random variables\n",
      "\n",
      "If x ∼ p() is some random variable, and y = f (x), what is the distribution of y? This is the question we address in this section.\n",
      "\n",
      "2.6.1\n",
      "\n",
      "Linear transformations\n",
      "\n",
      "Suppose f () is a linear function:\n",
      "\n",
      "y = f (x) = Ax + b\n",
      "\n",
      "(2.78)\n",
      "\n",
      "In this case, we can easily derive the mean and covariance of y as follows. First, for the mean, we have\n",
      "\n",
      "E [y] = E [Ax + b] = Aμ + b\n",
      "\n",
      "(2.79)\n",
      "\n",
      "where μ = E [x]. This is called the linearity of expectation. If f () is a scalar-valued function, f (x) = aT x + b, the corresponding result is = aT μ + b\n",
      "\n",
      "E\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "aT x + b\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(2.80)\n",
      "\n",
      "For the covariance, we have cov [y] = cov [Ax + b] = AΣAT\n",
      "\n",
      "(2.81)\n",
      "\n",
      "where Σ = cov [x]. We leave the proof of this as an exercise. If f () is scalar valued, the result becomes\n",
      "\n",
      "var [y] = var\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "aT x + b\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "= aT Σa\n",
      "\n",
      "(2.82)\n",
      "\n",
      "50\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "We will use both of these results extensively in later chapters. Note, however, that the mean and covariance only completely deﬁne the distribution of y if x is Gaussian. In general we must use the techniques described below to derive the full distribution of y, as opposed to just its ﬁrst two moments.\n",
      "\n",
      "2.6.2\n",
      "\n",
      "General transformations\n",
      "\n",
      "If X is a discrete rv, we can derive the pmf for y by simply summing up the probability mass for all the x’s such that f (x) =y : (cid:3)\n",
      "\n",
      "py(y) =\n",
      "\n",
      "px(x)\n",
      "\n",
      "(2.83)\n",
      "\n",
      "x:f (x)=y\n",
      "\n",
      "For example, if f (X) = 1 if X is even and f (X) = 0 otherwise, and px(X) is uniform on the set {1, . . . , 10}, then py(1) = x∈{2,4,6,8,10} px(x) = 0.5, and py(0) = 0.5 similarly. Note that in this example, f is a many-to-one function.\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "If X is continuous, we cannot use Equation 2.83 since px(x) is a density, not a pmf, and we\n",
      "\n",
      "cannot sum up densities. Instead, we work with cdf’s, and write\n",
      "\n",
      "Py(y) (cid:2) P (Y ≤ y) = P (f (X) ≤ y) = P (X ∈ {x|f (x) ≤ y})\n",
      "\n",
      "(2.84)\n",
      "\n",
      "We can derive the pdf of y by differentiating the cdf.\n",
      "\n",
      "In the case of monotonic and hence invertible functions, we can write Py(y) = P (f (X) ≤ y) = P (X ≤ f −1(y)) = Px(f −1(y))\n",
      "\n",
      "(2.85)\n",
      "\n",
      "Taking derivatives we get\n",
      "\n",
      "d dx where x = f −1(y). We can think of dx as a measure of volume in the x-space; similarly dy measures volume in y space. Thus dx dy measures the change in volume. Since the sign of this change is not important, we take the absolute value to get the general expression:\n",
      "\n",
      "py(y) (cid:2)\n",
      "\n",
      "d dy\n",
      "\n",
      "Py(y) =\n",
      "\n",
      "d dy\n",
      "\n",
      "Px(f −1(y)) =\n",
      "\n",
      "dx dy\n",
      "\n",
      "Px(x) =\n",
      "\n",
      "dx dy\n",
      "\n",
      "px(x)\n",
      "\n",
      "(2.86)\n",
      "\n",
      "py(y) = px(x)\n",
      "\n",
      "(cid:28) (cid:28) dx dy\n",
      "\n",
      "(cid:28) (cid:28)\n",
      "\n",
      "(2.87)\n",
      "\n",
      "This is called change of variables formula. We can understand this result more intuitively as follows. Observations falling in the range (x, x + δx) will get transformed into (y, y + δy), where px(x)δx ≈ py(y)δy. Hence py(y) ≈ px(x)| δx δy |. For example, suppose X ∼ U (−1, 1), and Y = X 2. Then py(y) = 1\n",
      "\n",
      "2 y− 1\n",
      "\n",
      "2 . See also Exercise 2.10.\n",
      "\n",
      "2.6.2.1 Multivariate change of variables *\n",
      "\n",
      "We can extend the previous results to multivariate distributions as follows. Let f be a function that maps Rn to Rn, and let y = f (x). Then its Jacobian matrix J is given by · · · . . . · · ·\n",
      "\n",
      "Jx→y (cid:2)\n",
      "\n",
      "∂(y1, . . . , yn) ∂(x1, . . . , xn)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎝\n",
      "\n",
      "∂y1 ∂x1\n",
      "\n",
      "∂yn ∂x1\n",
      "\n",
      "...\n",
      "\n",
      "∂y1 ∂xn\n",
      "\n",
      "∂yn ∂xn\n",
      "\n",
      "...\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎠\n",
      "\n",
      "(2.88)\n",
      "\n",
      "2.6. Transformations of random variables\n",
      "\n",
      "51\n",
      "\n",
      "| det J| measures how much a unit cube changes in volume when we apply f .\n",
      "\n",
      "Jacobian of the inverse mapping y → x: (cid:28) (cid:28) det\n",
      "\n",
      "If f is an invertible mapping, we can deﬁne the pdf of the transformed variables using the\n",
      "\n",
      "py(y) = px(x)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "∂x ∂y\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:28) (cid:28) = px(x)| det Jy→x|\n",
      "\n",
      "(2.89)\n",
      "\n",
      "In Exercise 4.5 you will use this formula to derive the normalization constant for a multivariate Gaussian.\n",
      "\n",
      "to polar coordinates y = (r, θ), wherex 1 = r cos θ and x2 = r sin θ. Then\n",
      "\n",
      "As a simple example, consider transforming a density from Cartesian coordinates x = (x1, x2)\n",
      "\n",
      "Jy→x =\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "∂x1 ∂r ∂x2 ∂r\n",
      "\n",
      "∂x1 ∂θ ∂x2 ∂θ\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "cos θ −r sin θ r cos θ sin θ\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(2.90)\n",
      "\n",
      "and\n",
      "\n",
      "| det J| = |r cos2 θ + r sin2 θ| = |r|\n",
      "\n",
      "(2.91)\n",
      "\n",
      "Hence\n",
      "\n",
      "py(y) =p x(x)| det J|\n",
      "\n",
      "(2.92)\n",
      "\n",
      "pr,θ(r, θ) =p x1,x2 (x1, x2)r = px1,x2 (r cos θ, r sin θ)r\n",
      "\n",
      "(2.93)\n",
      "\n",
      "To see this geometrically, notice that the area of the shaded patch in Figure 2.16 is given by\n",
      "\n",
      "P (r ≤ R ≤ r + dr, θ ≤ Θ ≤ θ + dθ) =p r,θ(r, θ)drdθ\n",
      "\n",
      "(2.94)\n",
      "\n",
      "In the limit, this is equal to the density at the center of the patch, p(r, θ), times the size of the patch, r dr dθ. Hence\n",
      "\n",
      "pr,θ(r, θ)drdθ = px1,x2 (r cos θ, r sin θ)r dr dθ\n",
      "\n",
      "(2.95)\n",
      "\n",
      "2.6.3\n",
      "\n",
      "Central limit theorem\n",
      "\n",
      "Now consider N random variables with pdf’s (not necessarily Gaussian) p(xi), each with mean μ and variance σ2. We assume each variable is independent and identically distributed or iid for short. Let SN = i=1 Xi be the sum of the rv’s. This is a simple but widely used transformation of rv’s. One can show that, as N increases, the distribution of this sum approaches\n",
      "\n",
      "p(SN = s) =\n",
      "\n",
      "1√\n",
      "\n",
      "2πN σ2\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:2)N\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "−\n",
      "\n",
      "(s − N μ)2 2N σ2\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(2.96)\n",
      "\n",
      "Hence the distribution of the quantity\n",
      "\n",
      "X − μ √ N σ/ converges to the standard normal, where X = 1 N the central limit theorem. See e.g., (Jaynes 2003, p222) or (Rice 1995, p169) for a proof.\n",
      "\n",
      "ZN (cid:2)\n",
      "\n",
      "SN − N μ √\n",
      "\n",
      "σ\n",
      "\n",
      "N\n",
      "\n",
      "=\n",
      "\n",
      "(cid:2)N\n",
      "\n",
      "i=1 xi is the sample mean. This is called\n",
      "\n",
      "(2.97)\n",
      "\n",
      "In Figure 2.17 we give an example in which we compute the mean of rv’s drawn from a beta distribution. We see that the sampling distribution of the mean value rapidly converges to a Gaussian distribution.\n",
      "\n",
      "52\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "Figure 2.16 Change of variables from polar to Cartesian. The area of the shaded patch is r dr dθ. Based on (Rice 1995) Figure 3.16.\n",
      "\n",
      "N = 1\n",
      "\n",
      "N = 5\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 2.17 The central limit theorem in pictures. We plot a histogram of 1 N Beta(1, 5), for j = 1 : 10000. As N → ∞, the distribution tends towards a Gaussian. N = 5. Based on Figure 2.6 of (Bishop 2006a). Figure generated by centralLimitDemo.\n",
      "\n",
      "(cid:2)N\n",
      "\n",
      "i=1 xij, where xij ∼ (a) N = 1. (b)\n",
      "\n",
      "2.7 Monte Carlo approximation\n",
      "\n",
      "In general, computing the distribution of a function of an rv using the change of variables formula can be difficult. One simple but powerful alternative is as follows. First we generate S samples from the distribution, call them x1, . . . , xS. (There are many ways to generate such samples; one popular method, for high dimensional distributions, is called Markov chain Monte Carlo or MCMC; this will be explained in Chapter 24.) Given the samples, we can approximate the distribution of f (X) by using the empirical distribution of {f (xs)}S s=1. This is called a Monte Carlo approximation, named after a city in Europe known for its plush gambling casinos. Monte Carlo techniques were ﬁrst developed in the area of statistical physics — in particular, during development of the atomic bomb — but are now widely used in statistics and machine learning as well.\n",
      "\n",
      "We can use Monte Carlo to approximate the expected value of any function of a random\n",
      "\n",
      "2.7. Monte Carlo approximation\n",
      "\n",
      "53\n",
      "\n",
      "1.5\n",
      "\n",
      "6\n",
      "\n",
      "0.25\n",
      "\n",
      "1\n",
      "\n",
      "4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.15\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "Figure 2.18 Computing the distribution of y = x2, where p(x) is uniform (left). The analytic result is shown in the middle, and the Monte Carlo approximation is shown on the right. Figure generated by changeOfVarsDemo1d.\n",
      "\n",
      "variable. We simply draw samples, and then compute the arithmetic mean of the function applied to the samples. This can be written as follows:\n",
      "\n",
      "E [f (X)] =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "f (x)p(x)dx ≈\n",
      "\n",
      "1 S\n",
      "\n",
      "S(cid:3)\n",
      "\n",
      "s=1\n",
      "\n",
      "f (xs)\n",
      "\n",
      "(2.98)\n",
      "\n",
      "where xs ∼ p(X). This is called Monte Carlo integration, and has the advantage over numerical integration (which is based on evaluating the function at a ﬁxed grid of points) that the function is only evaluated in places where there is non-negligible probability.\n",
      "\n",
      "By varying the function f (), we can approximate many quantities of interest, such as\n",
      "\n",
      "x = 1 S (cid:2)S\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 S 1 S #{xs ≤ c} →P (X ≤ c)\n",
      "\n",
      "s=1(xs − x)2 → var [X]\n",
      "\n",
      "(cid:2)S\n",
      "\n",
      "s=1 xs → E [X]\n",
      "\n",
      "median{x1, . . . , xS} →median( X)\n",
      "\n",
      "We give some examples below, and will see many more in later chapters.\n",
      "\n",
      "2.7.1\n",
      "\n",
      "Example: change of variables, the MC way\n",
      "\n",
      "In Section 2.6.2, we discussed how to analytically compute the distribution of a function of a random variable, y = f (x). A much simpler approach is to use a Monte Carlo approximation. For example, suppose x ∼ Unif(−1, 1) and y = x2. We can approximate p(y) by drawing many samples from p(x), squaring them, and computing the resulting empirical distribution. See Figure 2.18 for an illustration. We will use this technique extensively in later chapters. See also Figure 5.2.\n",
      "\n",
      "54\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "Figure 2.19 Estimating π by Monte Carlo integration. Blue points are inside the circle, red crosses are outside. Figure generated by mcEstimatePi.\n",
      "\n",
      "2.7.2\n",
      "\n",
      "Example: estimating π by Monte Carlo integration\n",
      "\n",
      "MC approximation can be used for many applications, not just statistical ones. Suppose we want to estimate π. We know that the area of a circle with radius r is πr2, but it is also equal to the following deﬁnite integral: (cid:4) r\n",
      "\n",
      "I =\n",
      "\n",
      "(cid:4) r\n",
      "\n",
      "−r\n",
      "\n",
      "−r\n",
      "\n",
      "I(x2 + y2 ≤ r2)dxdy\n",
      "\n",
      "(2.99)\n",
      "\n",
      "Hence π = I/(r2). Let f (x, y) = I(x2 + y2 ≤ r2) be an indicator function that is 1 for points inside the circle, and 0 outside, and let p(x) and p(y) be uniform distributions on [−r, r], sop( x) =p( y) = 1/(2r). Then\n",
      "\n",
      "(cid:4) (cid:4)\n",
      "\n",
      "Let us approximate this by Monte Carlo integration.\n",
      "\n",
      "I = (2r)(2r) (cid:4) (cid:4)\n",
      "\n",
      "f (x, y)p(x)p(y)dxdy\n",
      "\n",
      "(2.100)\n",
      "\n",
      "= 4r2\n",
      "\n",
      "f (x, y)p(x)p(y)dxdy\n",
      "\n",
      "(2.101)\n",
      "\n",
      "≈ 4r2 1 S\n",
      "\n",
      "S(cid:3)\n",
      "\n",
      "s=1\n",
      "\n",
      "f (xs, ys)\n",
      "\n",
      "(2.102)\n",
      "\n",
      "We ﬁnd ˆπ = 3.1416 with standard error 0.09 (see Section 2.7.3 for a discussion of standard errors). We can plot the points that are accepted/ rejected as in Figure 2.19.\n",
      "\n",
      "2.7.3\n",
      "\n",
      "Accuracy of Monte Carlo approximation\n",
      "\n",
      "The accuracy of an MC approximation increases with sample size. This is illustrated in Fig- ure 2.20, On the top line, we plot a histogram of samples from a Gaussian distribution. On the bottom line, we plot a smoothed version of these samples, created using a kernel density estimate (Section 14.7.2). This smoothed distribution is then evaluated on a dense grid of points\n",
      "\n",
      "2.7. Monte Carlo approximation\n",
      "\n",
      "55\n",
      "\n",
      "6\n",
      "\n",
      "10 samples\n",
      "\n",
      "2\n",
      "\n",
      "100 samples\n",
      "\n",
      "1.8\n",
      "\n",
      "5\n",
      "\n",
      "1.6\n",
      "\n",
      "4\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "3\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "2\n",
      "\n",
      "0.6\n",
      "\n",
      "1\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0 0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "2.5\n",
      "\n",
      "0 0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "2.5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "10 samples\n",
      "\n",
      "100 samples\n",
      "\n",
      "2.5\n",
      "\n",
      "1.8\n",
      "\n",
      "1.6\n",
      "\n",
      "2\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0 0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "2.5\n",
      "\n",
      "0 0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "2.5\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 2.20 10 and 100 samples from a Gaussian distribution, N (μ = 1.5, σ2 = 0.25). Solid red line is true pdf. Top line: histogram of samples. Bottom line: kernel density estimate derived from samples in dotted blue, solid red line is true pdf. Based on Figure 4.1 of (Hoff 2009). Figure generated by mcAccuracyDemo.\n",
      "\n",
      "and plotted. Note that this smoothing is just for the purposes of plotting, it is not used for the Monte Carlo estimate itself.\n",
      "\n",
      "If we denote the exact mean by μ = E [f (X)], and the MC approximation by ˆμ, one can\n",
      "\n",
      "show that, with independent samples,\n",
      "\n",
      "(ˆμ − μ) → N (0,\n",
      "\n",
      "σ2 S\n",
      "\n",
      ")\n",
      "\n",
      "(2.103)\n",
      "\n",
      "where\n",
      "\n",
      "σ2 = var [f (X)] = E\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "f (X)2\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "− E [f (X)]\n",
      "\n",
      "2\n",
      "\n",
      "(2.104)\n",
      "\n",
      "This is a consequence of the central-limit theorem. Of course, σ2 is unknown in the above expression, but it can also be estimated by MC:\n",
      "\n",
      "ˆσ2 =\n",
      "\n",
      "1 S\n",
      "\n",
      "S(cid:3)\n",
      "\n",
      "(f (xs) − ˆμ)2\n",
      "\n",
      "s=1\n",
      "\n",
      "(2.105)\n",
      "\n",
      "Then we have (cid:11)\n",
      "\n",
      "P\n",
      "\n",
      "μ − 1.96\n",
      "\n",
      "ˆσ√ S\n",
      "\n",
      "≤ ˆμ ≤ μ + 1.96\n",
      "\n",
      "ˆσ√ S\n",
      "\n",
      "(cid:29)\n",
      "\n",
      "≈ 0.95\n",
      "\n",
      "(2.106)\n",
      "\n",
      "56\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "The term uncertainty about our estimate of μ. (See Section 6.2 for more discussion on standard errors.)\n",
      "\n",
      "we need to use a number of samples S which satisﬁes 1.96 the 1.96 factor by 2, yielding S ≥ 4ˆσ2 (cid:9)2 .\n",
      "\n",
      "If we want to report an answer which is accurate to within ±(cid:11) with probability at least 95%, ˆσ2/S ≤ (cid:11). We can approximate\n",
      "\n",
      "ˆσ2 S is called the (numerical or empirical) standard error, and is an estimate of our\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "2.8\n",
      "\n",
      "Information theory\n",
      "\n",
      "information theory is concerned with representing data in a compact fashion (a task known as data compression or source coding), as well as with transmitting and storing it in a way that is robust to errors (a task known as error correction or channel coding). At ﬁrst, this seems far removed from the concerns of probability theory and machine learning, but in fact there is an intimate connection. To see this, note that compactly representing data requires allocating short codewords to highly probable bit strings, and reserving longer codewords to less probable bit strings. This is similar to the situation in natural language, where common words (such as “a”, “the”, “and”) are generally much shorter than rare words. Also, decoding messages sent over noisy channels requires having a good probability model of the kinds of messages that people tend to send. In both cases, we need a model that can predict which kinds of data are likely and which unlikely, which is also a central problem in machine learning (see (MacKay 2003) for more details on the connection between information theory and machine learning). Obviously we cannot go into the details of information theory here (see e.g.,\n",
      "\n",
      "(Cover and Thomas 2006) if you are interested to learn more). However, we will introduce a few basic concepts that we will need later in the book.\n",
      "\n",
      "2.8.1\n",
      "\n",
      "Entropy\n",
      "\n",
      "The entropy of a random variable X with distribution p, denoted by H (X) or sometimes H (p), is a measure of its uncertainty. In particular, for a discrete variable with K states, it is deﬁned by\n",
      "\n",
      "H (X) (cid:2) −\n",
      "\n",
      "K(cid:3)\n",
      "\n",
      "p(X = k) log2 p(X = k)\n",
      "\n",
      "(2.107)\n",
      "\n",
      "k=1\n",
      "\n",
      "Usually we use log base 2, in which case the units are called bits (short for binary digits). If we use log base e, the units are called nats. For example, if X ∈ {1, . . . , 5} with histogram distribution p = [0.25, 0.25, 0.2, 0.15, 0.15], we ﬁnd H = 2.2855. The discrete distribution with maximum entropy is the uniform distribution (see Section 9.2.6 for a proof). Hence for a K-ary random variable, the entropy is maximized if p(x = k) = 1/K; in this case, H (X) = log2 K. Conversely, the distribution with minimum entropy (which is zero) is any delta-function that In Figure 2.5(b), where puts all its mass on one state. Such a distribution has no uncertainty. we plotted a DNA sequence logo, the height of each bar is deﬁned to be 2 − H, where H is the entropy of that distribution, and 2 is the maximum possible entropy. Thus a bar of height 0 corresponds to a uniform distribution, whereas a bar of height 2 corresponds to a deterministic distribution.\n",
      "\n",
      "2.8.\n",
      "\n",
      "Information theory\n",
      "\n",
      "57\n",
      "\n",
      "1\n",
      "\n",
      ")\n",
      "\n",
      "X H\n",
      "\n",
      "(\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5 p(X = 1)\n",
      "\n",
      "1\n",
      "\n",
      "Figure 2.21 Entropy of a Bernoulli random variable as a function of θ. The maximum entropy is log2 2 = 1. Figure generated by bernoulliEntropyFig.\n",
      "\n",
      "and p(X = 0) = 1 − θ. Hence the entropy becomes\n",
      "\n",
      "For the special case of binary random variables, X ∈ {0, 1}, we can write p(X = 1) = θ\n",
      "\n",
      "H (X) =−[ p(X = 1) log2 p(X = 1) + p(X = 0) log2 p(X = 0)]\n",
      "\n",
      "(2.108)\n",
      "\n",
      "= −[θ log2 θ + (1− θ) log2(1 − θ)]\n",
      "\n",
      "(2.109)\n",
      "\n",
      "This is called the binary entropy function, and is also written H (θ). We plot this in Figure 2.21. We see that the maximum value of 1 occurs when the distribution is uniform, θ = 0.5.\n",
      "\n",
      "2.8.2\n",
      "\n",
      "KL divergence\n",
      "\n",
      "One way to measure the dissimilarity of two probability distributions, p and q, is known as the Kullback-Leibler divergence (KL divergence) orrelative entropy. This is deﬁned as follows:\n",
      "\n",
      "KL (p||q) (cid:2)\n",
      "\n",
      "K(cid:3)\n",
      "\n",
      "k=1\n",
      "\n",
      "pk log\n",
      "\n",
      "pk qk\n",
      "\n",
      "(2.110)\n",
      "\n",
      "where the sum gets replaced by an integral for pdfs.10 We can rewrite this as\n",
      "\n",
      "KL (p||q) =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "pk log pk −\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "pk log qk = −H (p) +H (p, q)\n",
      "\n",
      "(2.111)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "where H (p, q) is called the cross entropy,\n",
      "\n",
      "H (p, q) (cid:2) −\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "pk log qk\n",
      "\n",
      "(2.112)\n",
      "\n",
      "k\n",
      "\n",
      "One can show (Cover and Thomas 2006) that the cross entropy is the average number of bits needed to encode data coming from a source with distribution p when we use model q to\n",
      "\n",
      "10. The KL divergence is not a distance, since it is asymmetric. One symmetric version of the KL divergence is the Jensen-Shannon divergence, deﬁned as JS(p1, p2) = 0.5KL (p1||q) + 0.5KL (p2||q), whereq = 0.5p1 + 0.5p2.\n",
      "\n",
      "58\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "deﬁne our codebook. Hence the “regular” entropy H (p) = H (p, p), deﬁned in Section 2.8.1, is the expected number of bits if we use the true model, so the KL divergence is the difference between these. In other words, the KL divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution q to encode the data instead of the true distribution p.\n",
      "\n",
      "The “extra number of bits” interpretation should make it clear that KL (p||q) ≥ 0, and that\n",
      "\n",
      "the KL is only equal to zero iff q = p. We now give a proof of this important result.\n",
      "\n",
      "Theorem 2.8.1. (Information inequality) KL (p||q) ≥ 0 with equality iff p = q.\n",
      "\n",
      "Proof. To prove the theorem, we need to use Jensen’s inequality. This states that, for any convex function f , we have that (cid:14)\n",
      "\n",
      "f\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "n(cid:3)\n",
      "\n",
      "λixi\n",
      "\n",
      "≤\n",
      "\n",
      "n(cid:3)\n",
      "\n",
      "λif (xi)\n",
      "\n",
      "(2.113)\n",
      "\n",
      "where λi ≥ 0 and can be proved by induction for n >2.\n",
      "\n",
      "p(x) > 0} be the support of p(x). Then\n",
      "\n",
      "Let us now prove the main theorem, following (Cover and Thomas 2006, p28). Let A = {x :\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:2)n\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1 λi = 1. This is clearly true for n = 2 (by deﬁnition of convexity), and\n",
      "\n",
      "−KL (p||q) =−\n",
      "\n",
      "≤ log\n",
      "\n",
      "≤ log\n",
      "\n",
      "x∈A (cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "x∈A (cid:3)\n",
      "\n",
      "p(x) log\n",
      "\n",
      "p(x)\n",
      "\n",
      "q(x) = log 1 = 0\n",
      "\n",
      "q(x) p(x)\n",
      "\n",
      "p(x) q(x)\n",
      "\n",
      "= log\n",
      "\n",
      "=\n",
      "\n",
      "x∈A (cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "x∈A\n",
      "\n",
      "p(x) log\n",
      "\n",
      "q(x)\n",
      "\n",
      "q(x) p(x)\n",
      "\n",
      "(2.114)\n",
      "\n",
      "(2.115)\n",
      "\n",
      "(2.116)\n",
      "\n",
      "x∈X\n",
      "\n",
      "where the ﬁrst inequality follows from Jensen’s. Since log(x) is a strictly concave function, we have equality in Equation 2.115 iff p(x) =cq (x) for some c. We have equality in Equation 2.116 x∈X q(x) = 1, which implies c = 1. Hence KL (p||q) = 0 iff p(x) = q(x) iff for all x.\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "x∈A q(x) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "One important consequence of this result is that the discrete distribution with the maximum entropy is the uniform distribution. More precisely, H (X) ≤ log |X |, where |X | is the number of states for X, with equality iff p(x) is uniform. To see this, let u(x) = 1/|X |. Then\n",
      "\n",
      "0 ≤ KL (p||u) = (cid:3)\n",
      "\n",
      "=\n",
      "\n",
      "p(x) log p(x) −\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "x\n",
      "\n",
      "p(x) log\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "p(x) log u(x) =− H (X) + log |X |\n",
      "\n",
      "p(x) u(x)\n",
      "\n",
      "(2.117)\n",
      "\n",
      "(2.118)\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "This is a formulation of Laplace’s principle of insufficient reason, which argues in favor of using uniform distributions when there are no other reasons to favor one distribution over another. See Section 9.2.6 for a discussion of how to create distributions that satisfy certain constraints, but otherwise are as least-commital as possible. (For example, the Gaussian satisﬁes ﬁrst and second moment constraints, but otherwise has maximum entropy.)\n",
      "\n",
      "2.8.\n",
      "\n",
      "Information theory\n",
      "\n",
      "59\n",
      "\n",
      "2.8.3 Mutual information\n",
      "\n",
      "Consider two random variables, X and Y . Suppose we want to know how much knowing one variable tells us about the other. We could compute the correlation coefficient, but this is only deﬁned for real-valued random variables, and furthermore, this is a very limited measure of dependence, as we saw in Figure 2.12. A more general approach is to determine how similar the joint distribution p(X, Y ) is to the factored distribution p(X)p(Y ). This is called the mutual information or MI, and is deﬁned as follows:\n",
      "\n",
      "I (X; Y ) (cid:2) KL (p(X, Y )||p(X)p(Y )) =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "x\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "y\n",
      "\n",
      "p(x, y) log\n",
      "\n",
      "p(x, y) p(x)p(y)\n",
      "\n",
      "(2.119)\n",
      "\n",
      "We have I (X; Y ) ≥ 0 with equality iff p(X, Y ) = p(X)p(Y ). That is, the MI is zero iff the variables are independent.\n",
      "\n",
      "To gain insight into the meaning of MI, it helps to re-express it in terms of joint and conditional entropies. One can show (Exercise 2.12) that the above expression is equivalent to the following:\n",
      "\n",
      "where H (Y |X) is the conditional entropy, deﬁned as H (Y |X) = x p(x)H (Y |X = x). Thus we can interpret the MI between X and Y as the reduction in uncertainty about X after observing Y , or, by symmetry, the reduction in uncertainty about Y after observing X. We will encounter several applications of MI later in the book. See also Exercises 2.13 and 2.14 for the connection between MI and correlation coefficients.\n",
      "\n",
      "I (X; Y ) = H (X) − H (X|Y ) = H (Y ) − H (Y |X)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(2.120)\n",
      "\n",
      "A quantity which is closely related to MI is the pointwise mutual information or PMI. For\n",
      "\n",
      "two events (not random variables) x and y, this is deﬁned as\n",
      "\n",
      "PMI(x, y) (cid:2) log\n",
      "\n",
      "p(x, y) p(x)p(y)\n",
      "\n",
      "= log\n",
      "\n",
      "p(x|y) p(x)\n",
      "\n",
      "= log\n",
      "\n",
      "p(y|x) p(y)\n",
      "\n",
      "(2.121)\n",
      "\n",
      "This measures the discrepancy between these events occuring together compared to what would be expected by chance. Clearly the MI of X and Y is just the expected value of the PMI. Interestingly, we can rewrite the PMI as follows:\n",
      "\n",
      "PMI(x, y) = log\n",
      "\n",
      "p(x|y) p(x)\n",
      "\n",
      "= log\n",
      "\n",
      "p(y|x) p(y)\n",
      "\n",
      "(2.122)\n",
      "\n",
      "This is the amount we learn from updating the prior p(x) into the posterior p(x|y), or equiva- lently, updating the prior p(y) into the posterior p(y|x).\n",
      "\n",
      "2.8.3.1 Mutual information for continuous random variables *\n",
      "\n",
      "The above formula for MI is deﬁned for discrete random variables. For continuous random variables, it is common to ﬁrst discretize or quantize them, by dividing the ranges of each variable into bins, and computing how many values fall in each histogram bin (Scott 1979). We can then easily compute the MI using the formula above (see mutualInfoAllPairsMixed for some code, and miMixedDemo for a demo).\n",
      "\n",
      "Unfortunately, the number of bins used, and the location of the bin boundaries, can have a signiﬁcant effect on the results. One way around this is to try to estimate the MI directly,\n",
      "\n",
      "60\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "\"\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "#\n",
      "\n",
      "(cid:8)(cid:3)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(cid:3) (cid:17) (cid:2) (cid:1) (cid:16)\n",
      "\n",
      "(cid:12) (cid:8) (cid:7) (cid:10) (cid:9) (cid:9)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:8) (cid:13) (cid:4) (cid:12) (cid:13) (cid:10) (cid:16) (cid:6) (cid:8) (cid:14) (cid:14) (cid:13) (cid:4) (cid:1) (cid:12) (cid:13) (cid:15) (cid:14) (cid:6) (cid:8) (cid:5)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:3)/(cid:7)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "5(cid:3)/(cid:7)\n",
      "\n",
      "5(cid:6)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "\"\n",
      "\n",
      "&* &’ )\n",
      "\n",
      "(\n",
      "\n",
      "%\n",
      "\n",
      "$\n",
      "\n",
      "(cid:3)/(cid:9)(cid:7)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:3)/(cid:7) 2(cid:27)3(cid:17)+$(cid:26)(cid:21)(cid:12)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "#\n",
      "\n",
      "(cid:3)/4(cid:7)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(cid:23) (cid:16) (cid:21) (cid:18) (cid:17) (cid:30)\n",
      "\n",
      "(cid:19) (cid:17) (cid:16) (cid:12) (cid:15) (cid:21) (cid:29) (cid:13)\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(cid:27) (cid:17)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "(cid:14) (cid:17) (cid:14) (cid:16) (cid:26) (cid:24)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:25) (cid:15)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:17) (cid:23)\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "(cid:19) (cid:17) (cid:18) (cid:14) (cid:15) (cid:16) (cid:12) ! <\n",
      "\n",
      "(cid:17) (cid:23) (cid:12) %\n",
      "\n",
      "\n",
      "\n",
      "(cid:12) 6 (cid:19) (cid:17) (cid:14) - (cid:29) & *\n",
      "\n",
      "(cid:10)(cid:3)\n",
      "\n",
      "(cid:9)(cid:3)\n",
      "\n",
      "(cid:6)(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "4(cid:7)\n",
      "\n",
      "(cid:7)(cid:3)\n",
      "\n",
      "(cid:9)(cid:7)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:6)(cid:4) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:14)(cid:17)(cid:11)(cid:12)(cid:13)(cid:16)(cid:15)(cid:14)(cid:18)(cid:17)(cid:19)(cid:20)(cid:12)(cid:21)(cid:17)(cid:6)(cid:3)(cid:22)(cid:3)(cid:3)(cid:3)(cid:23)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "(cid:6)(cid:9)\n",
      "\n",
      "(cid:9)(cid:3)(cid:22)(cid:3)(cid:3)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)(cid:22)(cid:3)(cid:3)(cid:3)\n",
      "\n",
      "(cid:27)(cid:13)$(cid:26) (cid:12)(cid:17))(cid:17)\"(cid:12)(cid:21)(cid:16)(cid:26)(cid:13)(cid:17)(cid:19)(cid:27)(cid:13)(cid:14)1(cid:23)(cid:17)\n",
      "\n",
      "(cid:17) (cid:23) (cid:16) (cid:21) % (cid:12) 9\n",
      "\n",
      "(cid:19) (cid:17) (cid:18) $ (cid:13) % (cid:14) $ (cid:12) (cid:20) , . (cid:17) (cid:12) (cid:25) (cid:15) (cid:24)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "(cid:17) (cid:23)\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "(cid:19) (cid:17) (cid:18) (cid:14) (cid:15) (cid:21) % (cid:20) (cid:16) (cid:11)\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "(cid:17) /\n",
      "\n",
      "(cid:27) (cid:17) (cid:16) (cid:12) (cid:16) % (cid:12) 2\n",
      "\n",
      "\n",
      "\n",
      "(cid:4)(cid:3)\n",
      "\n",
      "(cid:10)(cid:3)\n",
      "\n",
      "(cid:4)(cid:3)\n",
      "\n",
      "(cid:10)(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:9) 3#(cid:15)-&(cid:21)(cid:12)(cid:13)(cid:17)\"(cid:12)(cid:21)(cid:17)=(cid:26) %(cid:13)(cid:17)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:6)(cid:7)(cid:3)\n",
      "\n",
      "(cid:10)(cid:3)(cid:3)\n",
      "\n",
      "’(cid:12)%-(cid:14)#(cid:17).,(cid:20)/(cid:17))(cid:17)\"(cid:12)(cid:21)(cid:16)(cid:26)(cid:13)(cid:17)(cid:19)0+1(cid:23)(cid:17)\n",
      "\n",
      "+ (cid:11) * ( ’ (cid:26)\n",
      "\n",
      "(cid:27)\n",
      "\n",
      ")\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(cid:14) (cid:17)\n",
      "\n",
      "(cid:12) (cid:29) &\n",
      "\n",
      "(cid:17) (cid:16) #\n",
      "\n",
      "(cid:6)(cid:4)(cid:3)(cid:3)(cid:17)\n",
      "\n",
      "(cid:5)(cid:3)(cid:3)(cid:17)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "% (cid:12) (cid:11)\n",
      "\n",
      "(cid:3)(cid:17)\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(cid:23) 1 (cid:17) (cid:14) (cid:13) (cid:27) (cid:19) (cid:17) (cid:13) (cid:26) (cid:16) (cid:21) (cid:12) \"\n",
      "\n",
      "(cid:17) ) (cid:17) / (cid:20) , . (cid:17) # (cid:14) - % (cid:12) ’\n",
      "\n",
      "(cid:4)(cid:3)(cid:3)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)(cid:3)(cid:3)\n",
      "\n",
      "(cid:9)(cid:3)(cid:3)(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:3)(cid:17)\n",
      "\n",
      "(cid:6),(cid:6)(cid:3)(cid:7)\n",
      "\n",
      "(cid:9),(cid:6)(cid:3)(cid:7)\n",
      "\n",
      "(cid:9),(cid:6)(cid:3)(cid:4)(cid:17)\n",
      "\n",
      "(cid:31)(cid:29) !(cid:12)(cid:21)(cid:17)(cid:26)(cid:25)(cid:17)\"#(cid:18)(cid:16)(cid:15)$(cid:15)%(cid:13)(cid:16)\n",
      "\n",
      "(cid:3) >(cid:21)(cid:26)(cid:16)(cid:16)(cid:17)(cid:31)%(cid:14)?-(cid:17)(cid:27)(cid:13)$(cid:17))(cid:17)\"(cid:12)(cid:21)(cid:16)(cid:26)(cid:13)(cid:17)(cid:19)(cid:27)(cid:13)(cid:14)1(cid:23)(cid:17)\n",
      "\n",
      "(cid:9)(cid:3)(cid:22)(cid:3)(cid:3)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)(cid:22)(cid:3)(cid:3)(cid:3)\n",
      "\n",
      "Figure 2.22 Left: Correlation coefficient vs maximal information criterion (MIC) for all pairwise relation- ships in the WHO data. Right: scatter plots of certain pairs of variables. The red lines are non-parametric smoothing regressions (Section 15.4.6) ﬁt separately to each trend. Source: Figure 4 of (Reshed et al. 2011) . Used with kind permission of David Reshef and the American Association for the Advancement of Science.\n",
      "\n",
      "without ﬁrst performing density estimation (Learned-Miller 2004). Another approach is to try many different bin sizes and locations, and to compute the maximum MI achieved. This statistic, appropriately normalized, is known as the maximal information coefficient (MIC) (Reshed et al. 2011). More precisely, deﬁne\n",
      "\n",
      "m(x, y) =\n",
      "\n",
      "maxG∈G(x,y) I (X(G); Y (G)) log min(x, y)\n",
      "\n",
      "(2.123)\n",
      "\n",
      "where G(x, y) is the set of 2d grids of size x×y, and X(G), Y (G) represents a discretization of the variables onto this grid. (The maximization over bin locations can be performed efficiently using dynamic programming (Reshed et al. 2011).) Now deﬁne the MIC as\n",
      "\n",
      "MIC (cid:2) max\n",
      "\n",
      "x,y:xy<B\n",
      "\n",
      "m(x, y)\n",
      "\n",
      "(2.124)\n",
      "\n",
      "where B is some sample-size dependent bound on the number of bins we can use and still reliably estimate the distribution ((Reshed et al. 2011) suggest B = N 0.6). It can be shown that the MIC lies in the range [0, 1], where 0 represents no relationship between the variables, and 1 represents a noise-free relationship of any form, not just linear.\n",
      "\n",
      "Figure 2.22 gives an example of this statistic in action. The data consists of 357 variables measuring a variety of social, economic, health and political indicators, collected by the World Health Organization (WHO). On the left of the ﬁgure, we see the correlation coefficient (CC) plotted against the MIC for all 63,566 variable pairs. On the right of the ﬁgure, we see scatter plots for particular pairs of variables, which we now discuss:\n",
      "\n",
      "The point marked C has a low CC and a low MIC. The corresponding scatter plot makes it\n",
      "\n",
      "2.8.\n",
      "\n",
      "Information theory\n",
      "\n",
      "61\n",
      "\n",
      "clear that there is no relationship between these two variables (percentage of lives lost to injury and density of dentists in the population).\n",
      "\n",
      "The points marked D and H have high CC (in absolute value) and high MIC, because they\n",
      "\n",
      "represent nearly linear relationships.\n",
      "\n",
      "The points marked E, F, and G have low CC but high MIC. This is because they correspond to non-linear (and sometimes, as in the case of E and F, non-functional, i.e., one-to-many) relationships between the variables.\n",
      "\n",
      "In summary, we see that statistics (such as MIC) based on mutual information can be used to discover interesting relationships between variables in a way that simpler measures, such as correlation coefficients, cannot. For this reason, the MIC has been called “a correlation for the 21st century” (Speed 2011).\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 2.1 Probabilities are sensitive to the form of the question that was used to generate the answer\n",
      "\n",
      "(Source: Minka.) My neighbor has two children. Assuming that the gender of a child is like a coin ﬂip, it is most likely, a priori, that my neighbor has one boy and one girl, with probability 1/2. The other possibilities—two boys or two girls—have probabilities 1/4 and 1/4.\n",
      "\n",
      "a. Suppose I ask him whether he has any boys, and he says yes. What is the probability that one child is\n",
      "\n",
      "a girl?\n",
      "\n",
      "b. Suppose instead that I happen to see one of his children run by, and it is a boy. What is the probability\n",
      "\n",
      "that the other child is a girl?\n",
      "\n",
      "Exercise 2.2 Legal reasoning (Source: Peter Lee.) Suppose a crime has been committed. Blood is found at the scene for which there is no innocent explanation. It is of a type which is present in 1% of the population.\n",
      "\n",
      "a. The prosecutor claims: “There is a 1% chance that the defendant would have the crime blood type if he were innocent. Thus there is a 99% chance that he guilty”. This is known as the prosecutor’s fallacy. What is wrong with this argument?\n",
      "\n",
      "b. The defender claims: “The crime occurred in a city of 800,000 people. The blood type would be found in approximately 8000 people. The evidence has provided a probability of just 1 in 8000 that the defendant is guilty, and thus has no relevance.” This is known as the defender’s fallacy. What is wrong with this argument?\n",
      "\n",
      "Exercise 2.3 Variance of a sum Show that the variance of a sum is var [X + Y ] = var [X] + var [Y ] + 2cov [X, Y ] , where cov [X, Y ] is the covariance between X and Y\n",
      "\n",
      "Exercise 2.4 Bayes rule for medical diagnosis (Source: Koller.) After your yearly checkup, the doctor has bad news and good news. The bad news is that you tested positive for a serious disease, and that the test is 99% accurate (i.e., the probability of testing positive given that you have the disease is 0.99, as is the probability of tetsing negative given that you don’t have the disease). The good news is that this is a rare disease, striking only one in 10,000 people. What are the chances that you actually have the disease? (Show your calculations as well as giving the ﬁnal result.)\n",
      "\n",
      "62\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "Exercise 2.5 The Monty Hall problem (Source: Mackay.) On a game show, a contestant is told the rules as follows:\n",
      "\n",
      "There are three doors, labelled 1, 2, 3. A single prize has been hidden behind one of them. You get to select one door. Initially your chosen door will not be opened. Instead, the gameshow host will open one of the other two doors, and he will do so in such a way as not to reveal the prize. For example, if you ﬁrst choose door 1, he will then open one of doors 2 and 3, and it is guaranteed that he will choose which one to open so that the prize will not be revealed. At this point, you will be given a fresh choice of door: you can either stick with your ﬁrst choice, or you can switch to the other closed door. All the doors will then be opened and you will receive whatever is behind your ﬁnal choice of door.\n",
      "\n",
      "Imagine that the contestant chooses door 1 ﬁrst; then the gameshow host opens door 3, revealing nothing behind the door, as promised. Should the contestant (a) stick with door 1, or (b) switch to door 2, or (c) does it make no difference? You may assume that initially, the prize is equally likely to be behind any of the 3 doors. Hint: use Bayes rule.\n",
      "\n",
      "Exercise 2.6 Conditional independence (Source: Koller.)\n",
      "\n",
      "a. Let H ∈ {1, . . . , K} be a discrete random variable, and let e1 and e2 be the observed values of two\n",
      "\n",
      "other random variables E1 and E2. Suppose we wish to calculate the vector\n",
      "\n",
      "(cid:8)P (H|e1, e2) = (P (H = 1|e1, e2), . . . , P (H = K|e1, e2))\n",
      "\n",
      "Which of the following sets of numbers are sufficient for the calculation? i. P (e1, e2), P (H), P (e1|H), P (e2|H) ii. P (e1, e2), P (H), P (e1, e2|H) iii. P (e1|H), P (e2|H), P (H)\n",
      "\n",
      "b. Now suppose we now assume E1 ⊥ E2|H (i.e., E1 and E2 are conditionally independent given H).\n",
      "\n",
      "Which of the above 3 sets are sufficent now?\n",
      "\n",
      "Show your calculations as well as giving the ﬁnal result. Hint: use Bayes rule.\n",
      "\n",
      "Exercise 2.7 Pairwise independence does not imply mutual independence We say that two random variables are pairwise independent if\n",
      "\n",
      "p(X2|X1) = p(X2)\n",
      "\n",
      "(2.125)\n",
      "\n",
      "and hence\n",
      "\n",
      "p(X2, X1) = p(X1)p(X2|X1) = p(X1)p(X2)\n",
      "\n",
      "(2.126)\n",
      "\n",
      "We say that n random variables are mutually independent if\n",
      "\n",
      "p(Xi|XS) = p(Xi) ∀S ⊆ {1, . . . , n} \\ {i}\n",
      "\n",
      "(2.127)\n",
      "\n",
      "and hence\n",
      "\n",
      "n(cid:3)\n",
      "\n",
      "p(X1:n) =\n",
      "\n",
      "p(Xi)\n",
      "\n",
      "(2.128)\n",
      "\n",
      "i=1\n",
      "\n",
      "Show that pairwise independence between all pairs of variables does not necessarily imply mutual inde- pendence. It suffices to give a counter example.\n",
      "\n",
      "2.8.\n",
      "\n",
      "Information theory\n",
      "\n",
      "63\n",
      "\n",
      "Exercise 2.8 Conditional independence iff joint factorizes In the text we said X ⊥ Y |Z iff\n",
      "\n",
      "p(x, y|z) = p(x|z)p(y|z)\n",
      "\n",
      "(2.129)\n",
      "\n",
      "for all x, y, z such that p(z) > 0. Now prove the following alternative deﬁnition: X ⊥ Y |Z iff there exist function g and h such that\n",
      "\n",
      "p(x, y|z) = g(x, z)h(y, z)\n",
      "\n",
      "(2.130)\n",
      "\n",
      "for all x, y, z such that p(z) > 0.\n",
      "\n",
      "Exercise 2.9 Conditional independence (Source: Koller.) Are the following properties true? Prove or disprove. Note that we are not restricting attention to distributions that can be represented by a graphical model.\n",
      "\n",
      "a. True or false? (X ⊥ W |Z, Y ) ∧ (X ⊥ Y |Z) ⇒ (X ⊥ Y, W |Z) b. True or false? (X ⊥ Y |Z) ∧ (X ⊥ Y |W ) ⇒ (X ⊥ Y |Z, W )\n",
      "\n",
      "Exercise 2.10 Deriving the inverse gamma density Let X ∼ Ga(a, b), i.e.\n",
      "\n",
      "Ga(x|a, b) =\n",
      "\n",
      "ba Γ(a)\n",
      "\n",
      "xa−1e−xb\n",
      "\n",
      "(2.131)\n",
      "\n",
      "Let Y = 1/X. Show that Y ∼ IG(a, b), i.e.,\n",
      "\n",
      "IG(x|shape = a, scale = b) =\n",
      "\n",
      "ba Γ(a)\n",
      "\n",
      "x−(a+1)e−b/x\n",
      "\n",
      "(2.132)\n",
      "\n",
      "Hint: use the change of variables formula.\n",
      "\n",
      "Exercise 2.11 Normalization constant for a 1D Gaussian The normalization constant for a zero-mean Gaussian is given by\n",
      "\n",
      "Z =\n",
      "\n",
      "(cid:4) b\n",
      "\n",
      "a\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "− x2 2σ2\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "dx\n",
      "\n",
      "(2.133)\n",
      "\n",
      "where a = −∞ and b = ∞. To compute this, consider its square\n",
      "\n",
      "Z 2 =\n",
      "\n",
      "(cid:4) b\n",
      "\n",
      "a\n",
      "\n",
      "(cid:4) b\n",
      "\n",
      "a\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "− x2 + y2 2σ2\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "dxdy\n",
      "\n",
      "(2.134)\n",
      "\n",
      "Let us change variables from cartesian (x, y) to polar (r, θ) using x = r cos θ and y = r sin θ. Since dxdy = rdrdθ, and cos2θ + sin2 θ = 1, we have\n",
      "\n",
      "Z 2 =\n",
      "\n",
      "(cid:4) 2π\n",
      "\n",
      "0\n",
      "\n",
      "(cid:4) ∞\n",
      "\n",
      "0\n",
      "\n",
      "r exp\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "− r2 2σ2\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "drdθ\n",
      "\n",
      "(2.135)\n",
      "\n",
      "Evaluate this integral and hence show Z = σ two terms, the ﬁrst of which (involving dθ) du/dr = − 1\n",
      "\n",
      "σ2 re−r2/2σ2\n",
      "\n",
      ", so the second integral is also easy (since\n",
      "\n",
      "is constant, so is easy. Hint 2:\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(2π). Hint 1: separate the integral into a product of then\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "u(cid:3)(r)dr = u(r)).\n",
      "\n",
      "if u = e−r2/2σ2\n",
      "\n",
      "64\n",
      "\n",
      "Chapter 2. Probability\n",
      "\n",
      "Exercise 2.12 Expressing mutual information in terms of entropies Show that\n",
      "\n",
      "I(X, Y ) = H(X) − H(X|Y ) = H(Y ) − H(Y |X)\n",
      "\n",
      "(2.136)\n",
      "\n",
      "Exercise 2.13 Mutual information for correlated normals (Source: (Cover and Thomas 1991, Q9.3).) Find the mutual information I(X1, X2) where X has a bivariate normal distribution: (cid:5) (cid:6)\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "X1 X2\n",
      "\n",
      "∼ N\n",
      "\n",
      "0,\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "σ2 ρσ2\n",
      "\n",
      "ρσ2 σ2\n",
      "\n",
      "(cid:6)(cid:6)\n",
      "\n",
      "(2.137)\n",
      "\n",
      "Evaluate I(X1, X2) for ρ = 1, ρ = 0 and ρ = −1 and comment. Hint: The (differential) entropy of a d-dimensional Gaussian is\n",
      "\n",
      "h(X) =\n",
      "\n",
      "1 2\n",
      "\n",
      "log2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(2πe)d det Σ\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(2.138)\n",
      "\n",
      "In the 1d case, this becomes 2πeσ2(cid:12)\n",
      "\n",
      "h(X) =\n",
      "\n",
      "1 2\n",
      "\n",
      "log2\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(2.139)\n",
      "\n",
      "Hint: log(0) = ∞.\n",
      "\n",
      "Exercise 2.14 A measure of correlation (normalized mutual information) (Source: (Cover and Thomas 1991, Q2.20).) Let X and Y be discrete random variables which are identically distributed (so H(X) = H(Y )) but not necessarily independent. Deﬁne\n",
      "\n",
      "r = 1 − H(Y |X) H(X)\n",
      "\n",
      "(2.140)\n",
      "\n",
      "a. Show r = I(X,Y ) H(X) b. Show 0 ≤ r ≤ 1 c. When is r = 0? d. When is r = 1?\n",
      "\n",
      "Exercise 2.15 MLE minimizes KL divergence to the empirical distribution Let pemp(x) be the empirical distribution, and let q(x|θ) be some model. Show that argminq KL (pemp||q) is obtained by q(x) =q (x; ˆθ), where ˆθ is the MLE. Hint: use non-negativity of the KL divergence.\n",
      "\n",
      "Exercise 2.16 Mean, mode, variance for the beta distribution Suppose θ ∼ Beta(a, b). Derive the mean, mode and variance.\n",
      "\n",
      "Exercise 2.17 Expected value of the minimum Suppose X, Y are two points sampled independently and uniformly at random from the interval [0, 1]. What is the expected location of the left most point?\n",
      "\n",
      "3 Generative models for discrete data\n",
      "\n",
      "3.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In Section 2.2.3.2, we discussed how to classify a feature vector x by applying Bayes rule to a generative classiﬁer of the form\n",
      "\n",
      "p(y = c|x, θ) ∝ p(x|y = c, θ)p(y = c|θ)\n",
      "\n",
      "(3.1)\n",
      "\n",
      "The key to using such models is specifying a suitable form for the class-conditional density p(x|y = c, θ), which deﬁnes what kind of data we expect to see in each class. In this chapter, we focus on the case where the observed data are discrete symbols. We also discuss how to infer the unknown parameters θ of such models.\n",
      "\n",
      "3.2\n",
      "\n",
      "Bayesian concept learning\n",
      "\n",
      "Consider how a child learns to understand the meaning of a word, such as “dog”. Presumably the child’s parents point out positive examples of this concept, saying such things as, “look at the cute dog!”, or “mind the doggy”, etc. However, it is very unlikely that they provide negative examples, by saying “look at that non-dog”. Certainly, negative examples may be obtained during an active learning process — the child says “look at the dog” and the parent says “that’s a cat, dear, not a dog” — but psychological research has shown that people can learn concepts from positive examples alone (Xu and Tenenbaum 2007).\n",
      "\n",
      "We can think of learning the meaning of a word as equivalent to concept learning, which in turn is equivalent to binary classiﬁcation. To see this, deﬁne f (x) = 1 if x is an example of the concept C, and f (x) = 0 otherwise. Then the goal is to learn the indicator function f , which just deﬁnes which elements are in the set C. By allowing for uncertainty about the deﬁnition of f , or equivalently the elements of C, we can emulate fuzzy set theory, but using standard probability calculus. Note that standard binary classiﬁcation techniques require positive and negative examples. By contrast, we will devise a way to learn from positive examples alone.\n",
      "\n",
      "For pedagogical purposes, we will consider a very simple example of concept learning called the number game, based on part of Josh Tenenbaum’s PhD thesis (Tenenbaum 1999). The game proceeds as follows. I choose some simple arithmetical concept C, such as “prime number” or “a number between 1 and 10”. I then give you a series of randomly chosen positive examples D = {x1, . . . , xN } drawn from C, and ask you whether some new test case ˜x belongs to C, i.e., I ask you to classify ˜x.\n",
      "\n",
      "66\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "Examples\n",
      "\n",
      "16\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100\n",
      "\n",
      "60\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100\n",
      "\n",
      "16 8 2 64\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100\n",
      "\n",
      "16 23 19 20\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100\n",
      "\n",
      "Figure 3.1 Empirical predictive distribution averaged over 8 humans in the number game. First two rows: after seeing D = {16} and D = {60}. This illustrates diffuse similarity. Third row: after seeing D = {16, 8, 2, 64}. This illustrates rule-like behavior (powers of 2). Bottom row: after seeing D = {16, 23, 19, 20}. This illustrates focussed similarity (numbers near 20). Source: Figure 5.5 of (Tenenbaum 1999). Used with kind permission of Josh Tenenbaum.\n",
      "\n",
      "Suppose, for simplicity, that all numbers are integers between 1 and 100. Now suppose I tell you “16” is a positive example of the concept. What other numbers do you think are positive? 17? 6? 32? 99? It’s hard to tell with only one example, so your predictions will be quite vague. Presumably numbers that are similar in some sense to 16 are more likely. But similar in what way? 17 is similar, because it is “close by”, 6 is similar because it has a digit in common, 32 is similar because it is also even and a power of 2, but 99 does not seem similar. Thus some numbers are more likely than others. We can represent this as a probability distribution, p(˜x|D), which is the probability that ˜x ∈ C given the data D for any ˜x ∈ {1, . . . , 100}. This is called the posterior predictive distribution. Figure 3.1(top) shows the predictive distribution of people derived from a lab experiment. We see that people predict numbers that are similar to 16, under a variety of kinds of similarity.\n",
      "\n",
      "Now suppose I tell you that 8, 2 and 64 are also positive examples. Now you may guess that the hidden concept is “powers of two”. This is an example of induction. Given this hypothesis, the predictive distribution is quite speciﬁc, and puts most of its mass on powers of 2, as shown If instead I tell you the data is D = {16, 23, 19, 20}, you will get a in Figure 3.1(third row). different kind of generalization gradient, as shown in Figure 3.1(bottom).\n",
      "\n",
      "How can we explain this behavior and emulate it in a machine? The classic approach to induction is to suppose we have a hypothesis space of concepts, H, such as: odd numbers, even numbers, all numbers between 1 and 100, powers of two, all numbers ending in j (for\n",
      "\n",
      "3.2. Bayesian concept learning\n",
      "\n",
      "67\n",
      "\n",
      "0 ≤ j ≤ 9), etc. The subset of H that is consistent with the data D is called the version space. As we see more examples, the version space shrinks and we become increasingly certain about the concept (Mitchell 1997).\n",
      "\n",
      "However, the version space is not the whole story. After seeing D = {16}, there are many consistent rules; how do you combine them to predict if ˜x ∈ C? Also, after seeing D = {16, 8, 2, 64}, why did you choose the rule “powers of two” and not, say, “all even numbers”, or “powers of two except for 32”, both of which are equally consistent with the evidence? We will now provide a Bayesian explanation for this.\n",
      "\n",
      "3.2.1\n",
      "\n",
      "Likelihood\n",
      "\n",
      "We must explain why we chose htwo (cid:2)“powers of two”, and not, say, heven (cid:2) “even numbers” after seeing D = {16, 8, 2, 64}, given that both hypotheses are consistent with the evidence. The key intuition is that we want to avoid suspicious coincidences. If the true concept was even numbers, how come we only saw numbers that happened to be powers of two?\n",
      "\n",
      "To formalize this, let us assume that examples are sampled uniformly at random from the extension of a concept. (The extension of a concept is just the set of numbers that belong to it, e.g., the extension of heven is {2, 4, 6, . . . , 98, 100}; the extension of “numbers ending in 9” is {9, 19, . . . , 99}.) Tenenbaum calls this the strong sampling assumption. Given this assumption, the probability of independently sampling N items (with replacement) from h is given by\n",
      "\n",
      "p(D|h) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "1 size(h)\n",
      "\n",
      "(cid:3)N\n",
      "\n",
      "=\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "1 |h|\n",
      "\n",
      "(cid:3)N\n",
      "\n",
      "(3.2)\n",
      "\n",
      "This crucial equation embodies what Tenenbaum calls the size principle, which means the model favors the simplest (smallest) hypothesis consistent with the data. This is more commonly known as Occam’s razor.1\n",
      "\n",
      "To see how it works, let D = {16}. Then p(D|htwo) = 1/6, since there are only 6 powers of two less than 100, but p(D|heven) = 1/50, since there are 50 even numbers. So the likelihood that h = htwo is higher than if h = heven. After 4 examples, the likelihood of htwo is (1/6)4 = 7.7 × 10−4, whereas the likelihood of heven is (1/50)4 = 1.6 × 10−7. This is a likelihood ratio of almost 5000:1 in favor of htwo. This quantiﬁes our earlier intuition that D = {16, 8, 2, 64} would be a very suspicious coincidence if generated by heven.\n",
      "\n",
      "3.2.2\n",
      "\n",
      "Prior Suppose D = {16, 8, 2, 64}. Given this data, the concept h(cid:2) =“powers of two except 32” is more likely than h =“powers of two”, since h(cid:2) does not need to explain the coincidence that 32 is missing from the set of examples.\n",
      "\n",
      "However, the hypothesis h(cid:2) =“powers of two except 32” seems “conceptually unnatural”. We can capture such intution by assigning low prior probability to unnatural concepts. Of course, your prior might be different than mine. This subjective aspect of Bayesian reasoning is a source of much controversy, since it means, for example, that a child and a math professor\n",
      "\n",
      "1. William of Occam (also spelt Ockham) was an English monk and philosopher, 1288–1348.\n",
      "\n",
      "68\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "will reach different answers. In fact, they presumably not only have different priors, but also different hypothesis spaces. However, we can ﬁnesse that by deﬁning the hypothesis space of the child and the math professor to be the same, and then setting the child’s prior weight to be zero on certain “advanced” concepts. Thus there is no sharp distinction between the prior and the hypothesis space.\n",
      "\n",
      "If you are told the numbers are from some arithmetic rule, then given 1200, 1500, 900 and 1400, you may think 400 is likely but 1183 is unlikely. But if you are told that the numbers are examples of healthy cholesterol levels, you would probably think 400 is unlikely and 1183 is likely. Thus we see that the prior is the mechanism by which background knowledge can be brought to bear on a problem. Without this, rapid learning (i.e., from small samples sizes) is impossible.\n",
      "\n",
      "Although the subjectivity of the prior is controversial, it is actually quite useful.\n",
      "\n",
      "let us use a simple prior which puts uniform probability on 30 simple arithmetical concepts, such as “even numbers”, “odd numbers”, “prime numbers”, “numbers ending in 9”, etc. To make things more interesting, we make the concepts even and odd more likely apriori. We also include two “unnatural” concepts, namely “powers of 2, plus 37” and “powers of 2, except 32”, but give them low prior weight. See Figure 3.2(a) for a plot of this prior. We will consider a slightly more sophisticated prior later on.\n",
      "\n",
      "So, what prior should we use? For illustration purposes,\n",
      "\n",
      "3.2.3\n",
      "\n",
      "Posterior\n",
      "\n",
      "The posterior is simply the likelihood times the prior, normalized. In this context we have\n",
      "\n",
      "p(h|D) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(D|h)p(h) h(cid:2)∈H p(D, h(cid:2))\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(h)I(D ∈ h)/|h|N h(cid:2)∈H p(h(cid:2))I(D ∈ h(cid:2))/|h(cid:2)|N\n",
      "\n",
      "(3.3)\n",
      "\n",
      "where I(D ∈ h) is 1 iff (iff and only if) all the data are in the extension of the hypothesis h. Figure 3.2 plots the prior, likelihood and posterior after seeing D = {16}. We see that the posterior is a combination of prior and likelihood. In the case of most of the concepts, the prior is uniform, so the posterior is proportional to the likelihood. However, the “unnatural” concepts of “powers of 2, plus 37” and “powers of 2, except 32” have low posterior support, despite having high likelihood, due to the low prior. Conversely, the concept of odd numbers has low posterior support, despite having a high prior, due to the low likelihood.\n",
      "\n",
      "Figure 3.3 plots the prior, likelihood and posterior after seeing D = {16, 8, 2, 64}. Now the likelihood is much more peaked on the powers of two concept, so this dominates the posterior. Essentially the learner has an aha moment, and ﬁgures out the true concept. (Here we see the need for the low prior on the unnatural concepts, otherwise we would have overﬁt the data and picked “powers of 2, except for 32”.)\n",
      "\n",
      "In general, when we have enough data, the posterior p(h|D) becomes peaked on a single\n",
      "\n",
      "concept, namely the MAP estimate, i.e.,\n",
      "\n",
      "p(h|D) → δˆhM AP (h)\n",
      "\n",
      "(3.4)\n",
      "\n",
      "where ˆhM AP = argmaxh p(h|D) is the posterior mode, and where δ is the Dirac measure deﬁned by\n",
      "\n",
      "δx(A) =\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "1 0\n",
      "\n",
      "if x ∈ A if x (cid:6)∈ A\n",
      "\n",
      "(3.5)\n",
      "\n",
      "3.2. Bayesian concept learning\n",
      "\n",
      "69\n",
      "\n",
      "data = 16\n",
      "\n",
      "35\n",
      "\n",
      "35\n",
      "\n",
      "even odd squares mult of 3 mult of 4 mult of 5 mult of 6 mult of 7 mult of 8 mult of 9 mult of 10 ends in 1 ends in 2 ends in 3 ends in 4 ends in 5 ends in 6 ends in 7 ends in 8 ends in 9 powers of 2 powers of 3 powers of 4 powers of 5 powers of 6 powers of 7 powers of 8 powers of 9 powers of 10 all powers of 2 + {37} powers of 2 − {32}\n",
      "\n",
      "30\n",
      "\n",
      "25\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "30\n",
      "\n",
      "25\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0.1 prior\n",
      "\n",
      "0 0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2 lik\n",
      "\n",
      "0 0.4\n",
      "\n",
      "0\n",
      "\n",
      "0.2 post\n",
      "\n",
      "0.4\n",
      "\n",
      "Figure 3.2 Prior, likelihood and posterior for D = {16}. Based on (Tenenbaum 1999). Figure generated by numbersGame.\n",
      "\n",
      "Note that the MAP estimate can be written as\n",
      "\n",
      "ˆhM AP = argmax\n",
      "\n",
      "h\n",
      "\n",
      "p(D|h)p(h) = argmax\n",
      "\n",
      "h\n",
      "\n",
      "[log p(D|h) + log p(h)]\n",
      "\n",
      "(3.6)\n",
      "\n",
      "Since the likelihood term depends exponentially on N , and the prior stays constant, as we get more and more data, the MAP estimate converges towards the maximum likelihood estimate or MLE:\n",
      "\n",
      "ˆhmle (cid:2) argmax\n",
      "\n",
      "h\n",
      "\n",
      "p(D|h) = argmax\n",
      "\n",
      "h\n",
      "\n",
      "log p(D|h)\n",
      "\n",
      "(3.7)\n",
      "\n",
      "In other words, if we have enough data, we see that the data overwhelms the prior.\n",
      "\n",
      "In this\n",
      "\n",
      "70\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "data = 16 8 2 64\n",
      "\n",
      "35\n",
      "\n",
      "35\n",
      "\n",
      "even odd squares mult of 3 mult of 4 mult of 5 mult of 6 mult of 7 mult of 8 mult of 9 mult of 10 ends in 1 ends in 2 ends in 3 ends in 4 ends in 5 ends in 6 ends in 7 ends in 8 ends in 9 powers of 2 powers of 3 powers of 4 powers of 5 powers of 6 powers of 7 powers of 8 powers of 9 powers of 10 all powers of 2 + {37} powers of 2 − {32}\n",
      "\n",
      "30\n",
      "\n",
      "25\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "30\n",
      "\n",
      "25\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0.1 prior\n",
      "\n",
      "0 0.2\n",
      "\n",
      "0\n",
      "\n",
      "1 lik\n",
      "\n",
      "x 10\n",
      "\n",
      "2 −3\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5 post\n",
      "\n",
      "1\n",
      "\n",
      "Figure 3.3 Prior, likelihood and posterior for D = {16, 8, 2, 64}. Based on (Tenenbaum 1999). Figure generated by numbersGame.\n",
      "\n",
      "case, the MAP estimate converges towards the MLE.\n",
      "\n",
      "If the true hypothesis is in the hypothesis space, then the MAP/ ML estimate will converge upon this hypothesis. Thus we say that Bayesian inference (and ML estimation) are consistent estimators (see Section 6.4.1 for details). We also say that the hypothesis space is identiﬁable in the limit, meaning we can recover the truth in the limit of inﬁnite data. If our hypothesis class is not rich enough to represent the “truth” (which will usually be the case), we will converge on the hypothesis that is as close as possible to the truth. However, formalizing this notion of “closeness” is beyond the scope of this chapter.\n",
      "\n",
      "3.2. Bayesian concept learning\n",
      "\n",
      "71\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "8\n",
      "\n",
      "12\n",
      "\n",
      "16\n",
      "\n",
      "20\n",
      "\n",
      "24\n",
      "\n",
      "28\n",
      "\n",
      "32\n",
      "\n",
      "36\n",
      "\n",
      "40\n",
      "\n",
      "44\n",
      "\n",
      "48\n",
      "\n",
      "52\n",
      "\n",
      "56\n",
      "\n",
      "60\n",
      "\n",
      "64\n",
      "\n",
      "68\n",
      "\n",
      "72\n",
      "\n",
      "76\n",
      "\n",
      "80\n",
      "\n",
      "84\n",
      "\n",
      "88\n",
      "\n",
      "92\n",
      "\n",
      "96 100\n",
      "\n",
      "powers of 4\n",
      "\n",
      "powers of 2\n",
      "\n",
      "ends in 6\n",
      "\n",
      "squares\n",
      "\n",
      "even\n",
      "\n",
      "mult of 8\n",
      "\n",
      "mult of 4\n",
      "\n",
      "all\n",
      "\n",
      "powers of 2 − {32}\n",
      "\n",
      "powers of 2 + {37}\n",
      "\n",
      "0\n",
      "\n",
      "0.5 p(h | 16 )\n",
      "\n",
      "1\n",
      "\n",
      "Figure 3.4 Posterior over hypotheses and the corresponding predictive distribution after seeing one example, D = {16}. A dot means this number is consistent with this hypothesis. The graph p(h|D) on the right is the weight given to hypothesis h. By taking a weighed sum of dots, we get p(˜x ∈ C|D) (top). Based on Figure 2.9 of (Tenenbaum 1999). Figure generated by numbersGame.\n",
      "\n",
      "3.2.4\n",
      "\n",
      "Posterior predictive distribution\n",
      "\n",
      "The posterior is our internal belief state about the world. The way to test if our beliefs are justiﬁed is to use them to predict objectively observable quantities (this is the basis of the scientiﬁc method). Speciﬁcally, the posterior predictive distribution in this context is given by\n",
      "\n",
      "p(˜x ∈ C|D) =\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "p(y = 1|˜x, h)p(h|D)\n",
      "\n",
      "(3.8)\n",
      "\n",
      "h\n",
      "\n",
      "This is just a weighted average of the predictions of each individual hypothesis and is called Bayes model averaging (Hoeting et al. 1999). This is illustrated in Figure 3.4. The dots at the bottom show the predictions from each hypothesis; the vertical curve on the right shows the weight associated with each hypothesis. If we multiply each row by its weight and add up, we get the distribution at the top.\n",
      "\n",
      "When we have a small and/or ambiguous dataset, the posterior p(h|D) is vague, which induces a broad predictive distribution. However, once we have “ﬁgured things out”, the posterior becomes a delta function centered at the MAP estimate. In this case, the predictive distribution\n",
      "\n",
      "72\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "becomes\n",
      "\n",
      "p(˜x ∈ C|D) =\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "p(˜x|h)δˆh(h) = p(˜x|ˆh)\n",
      "\n",
      "(3.9)\n",
      "\n",
      "h\n",
      "\n",
      "This is called a plug-in approximation to the predictive density and is very widely used, due to its simplicity. However, in general, this under-represents our uncertainty, and our predictions will not be as “smooth” as when using BMA. We will see more examples of this later in the book. Although MAP learning is simple, it cannot explain the gradual shift from similarity-based For reasoning (with uncertain posteriors) to rule-based reasoning (with certain posteriors). example, suppose we observe D = {16}. If we use the simple prior above, the minimal consistent hypothesis is “all powers of 4”, so only 4 and 16 get a non-zero probability of being predicted. This is of course an example of overﬁtting. Given D = {16, 8, 2, 64}, the MAP hypothesis is “all powers of two”. Thus the plug-in predictive distribution gets broader (or stays the same) as we see more data: it starts narrow, but is forced to broaden as it seems more data. In contrast, in the Bayesian approach, we start broad and then narrow down as we learn more, In particular, given D = {16}, there are many hypotheses which makes more intuitive sense. with non-negligible posterior support, so the predictive distribution is broad. However, when we see D = {16, 8, 2, 64}, the posterior concentrates its mass on one hypothesis, so the predictive distribution becomes narrower. So the predictions made by a plug-in approach and a Bayesian approach are quite different in the small sample regime, although they converge to the same answer as we see more data.\n",
      "\n",
      "3.2.5\n",
      "\n",
      "A more complex prior\n",
      "\n",
      "To model human behavior, Tenenbaum used a slightly more sophisticated prior which was de- rived by analysing some experimental data of how people measure similarity between numbers; see (Tenenbaum 1999, p208) for details. The result is a set of arithmetical concepts similar to those mentioned above, plus all intervals between n and m for 1 ≤ n, m ≤ 100. (Note that these hypotheses are not mutually exclusive.) Thus the prior is a mixture of two priors, one over arithmetical rules, and one over intervals: interval(h)\n",
      "\n",
      "p(h) = π0p\n",
      "\n",
      "rules(h) + (1− π0)p\n",
      "\n",
      "(3.10)\n",
      "\n",
      "The only free parameter in the model is the relative weight, π0, given to these two parts of the prior. The results are not very sensitive to this value, so long as π0 > 0.5, reﬂecting the fact that people are more likely to think of concepts deﬁned by rules. The predictive distribution of the model, using this larger hypothesis space, is shown in Figure 3.5. It is strikingly similar to the human predictive distribution, shown in Figure 3.1, even though it was not ﬁt to human data (modulo the choice of hypothesis space).\n",
      "\n",
      "3.3\n",
      "\n",
      "The beta-binomial model\n",
      "\n",
      "The number game involved inferring a distribution over a discrete variable drawn from a ﬁnite hypothesis space, h ∈ H, given a series of discrete observations. This made the computations particularly simple: we just needed to sum, multiply and divide. However, in many applications, the unknown parameters are continuous, so the hypothesis space is (some subset) of RK, where\n",
      "\n",
      "3.3. The beta-binomial model\n",
      "\n",
      "73\n",
      "\n",
      "Examples\n",
      "\n",
      "16\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100\n",
      "\n",
      "60\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100\n",
      "\n",
      "16 8 2 64\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100\n",
      "\n",
      "16 23 19 20\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 68 72 76 80 84 88 92 96 100\n",
      "\n",
      "Figure 3.5 Predictive distributions for the model using the full hypothesis space. Compare to Figure 3.1. The predictions of the Bayesian model are only plotted for those values of ˜x for which human data is available; this is why the top line looks sparser than Figure 3.4. Source: Figure 5.6 of (Tenenbaum 1999). Used with kind permission of Josh Tenenbaum.\n",
      "\n",
      "K is the number of parameters. This complicates the mathematics, since we have to replace sums with integrals. However, the basic ideas are the same.\n",
      "\n",
      "We will illustrate this by considering the problem of inferring the probability that a coin shows up heads, given a series of observed coin tosses. Although this might seem trivial, it turns out that this model forms the basis of many of the methods we will consider later in this book, including naive Bayes classiﬁers, Markov models, etc. It is historically important, since it was the example which was analyzed in Bayes’ original paper of 1763. (Bayes’ analysis was subsequently generalized by Pierre-Simon Laplace, creating what we now call “Bayes rule” — see (Stigler 1986) for further historical details.)\n",
      "\n",
      "We will follow our now-familiar recipe of specifying the likelihood and prior, and deriving the\n",
      "\n",
      "posterior and posterior predictive.\n",
      "\n",
      "3.3.1\n",
      "\n",
      "Likelihood\n",
      "\n",
      "Suppose Xi ∼ Ber(θ), where Xi = 1 represents “heads”, Xi = 0 represents “tails”, and θ ∈ [0, 1] is the rate parameter (probability of heads). If the data are iid, the likelihood has the form\n",
      "\n",
      "p(D|θ) = θN1 (1 − θ)N0\n",
      "\n",
      "(3.11)\n",
      "\n",
      "74\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "where we haveN 1 = i=1 I(xi = 0) tails. These two counts are called the sufficient statistics of the data, since this is all we need to know about D to infer θ. (An alternative set of sufficient statistics are N1 and N = N0 + N1.)\n",
      "\n",
      "More formally, we say s(D) is a sufficient statistic for data D if p(θ|D) =p( θ|s(data)). If we use a uniform prior, this is equivalent to saying p(D|θ ∝ p(s(D)|θ). Consequently, if we have two datasets with the same sufficient statistics, we will infer the same value for θ.\n",
      "\n",
      "Now suppose the data consists of the count of the number of heads N1 observed in a ﬁxed number N = N1 + N0 of trials. In this case, we have N1 ∼ Bin(N, θ), where Bin represents the binomial distribution, which has the following pmf:\n",
      "\n",
      "Bin(k|n, θ) (cid:2)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "n k\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "θk(1 − θ)n−k\n",
      "\n",
      "i=1 I(xi = 1) heads and N0 =\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      "(3.12)\n",
      "\n",
      "is a constant independent of θ, the likelihood for the binomial sampling model is the Since same as the likelihood for the Bernoulli model. So any inferences we make about θ will be the same whether we observe the counts, D = (N1, N ), or a sequence of trials, D = {x1, . . . , xN }.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "n k\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "3.3.2\n",
      "\n",
      "Prior\n",
      "\n",
      "We need a prior which has support over the interval [0, 1]. To make the math easier, it would convenient if the prior had the same form as the likelihood, i.e., if the prior looked like\n",
      "\n",
      "p(θ) ∝ θγ1 (1 − θ)γ2\n",
      "\n",
      "(3.13)\n",
      "\n",
      "for some prior parameters γ1 and γ2. posterior by simply adding up the exponents:\n",
      "\n",
      "If this were the case, then we could easily evaluate the\n",
      "\n",
      "p(θ) ∝ p(D|θ)p(θ) = θN1 (1 − θ)N0 θγ1 (1 − θ)γ2 = θN1+γ1 (1 − θ)N0+γ2\n",
      "\n",
      "(3.14)\n",
      "\n",
      "When the prior and the posterior have the same form, we say that the prior is a conjugate prior for the corresponding likelihood. Conjugate priors are widely used because they simplify computation, and are easy to interpret, as we see below.\n",
      "\n",
      "In the case of the Bernoulli, the conjugate prior is the beta distribution, which we encountered\n",
      "\n",
      "in Section 2.4.5:\n",
      "\n",
      "Beta(θ|a, b) ∝ θa−1(1 − θ)b−1\n",
      "\n",
      "(3.15)\n",
      "\n",
      "The parameters of the prior are called hyper-parameters. We can set them in order to encode our prior beliefs. For example, to encode our beliefs that θ has mean 0.7 and standard deviation 0.2, we set a = 2.975 and b = 1.275 (Exercise 3.15). Or to encode our beliefs that θ has mean 0.15 and that we think it lives in the interval (0.05, 0.30) with probability, then we ﬁnd a = 4.5 and b = 25.5 (Exercise 3.16).\n",
      "\n",
      "If we know “nothing” about θ, except that it lies in the interval [0, 1], we can use a uni- form prior, which is a kind of uninformative prior (see Section 5.4.2 for details). The uniform distribution can be represented by a beta distribution with a = b = 1.\n",
      "\n",
      "3.3. The beta-binomial model\n",
      "\n",
      "75\n",
      "\n",
      "6\n",
      "\n",
      "4.5\n",
      "\n",
      "5\n",
      "\n",
      "prior Be(2.0, 2.0) lik Be(4.0, 18.0) post Be(5.0, 19.0)\n",
      "\n",
      "4\n",
      "\n",
      "prior Be(5.0, 2.0) lik Be(12.0, 14.0) post Be(16.0, 15.0)\n",
      "\n",
      "3.5\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 3.6 (a) Updating a Beta(2, 2) prior with a Binomial likelihood with sufficient statistics N1 = 3, N0 = 17 to yield a Beta(5,19) posterior. likeli- hood with sufficient statistics N1 = 11,N0 = 13 to yield a Beta(16, 15) posterior. Figure generated by binomialBetaPosteriorDemo.\n",
      "\n",
      "(b) Updating a Beta(5, 2) prior with a Binomial\n",
      "\n",
      "3.3.3\n",
      "\n",
      "Posterior\n",
      "\n",
      "If we multiply the likelihood by the beta prior we get the following posterior (following Equa- tion 3.14):\n",
      "\n",
      "p(θ|D) ∝ Bin(N1|θ, N0 + N1)Beta(θ|a, b)Beta(θ|N1 + a, N0 + b)\n",
      "\n",
      "(3.16)\n",
      "\n",
      "In particular, the posterior is obtained by adding the prior hyper-parameters to the empirical counts. For this reason, the hyper-parameters are known as pseudo counts. The strength of the prior, also known as the effective sample size of the prior, is the sum of the pseudo counts, a + b; this plays a role analogous to the data set size, N1 + N0 = N .\n",
      "\n",
      "Figure 3.6(a) gives an example where we update a weak Beta(2,2) prior with a peaked likelihood function, corresponding to a large sample size; we see that the posterior is essentially identical to the likelihood: since the data has overwhelmed the prior. Figure 3.6(b) gives an example where we update a strong Beta(5,2) prior with a peaked likelihood function; now we see that the posterior is a “compromise” between the prior and likelihood.\n",
      "\n",
      "To see this, suppose we have two data sets Da and Db with sufficient statistics N a N b 0 . Let N1 = N a datasets. In batch mode we have\n",
      "\n",
      "1 , N b\n",
      "\n",
      "Note that updating the posterior sequentially is equivalent to updating in a single batch. 0 and 0 be the sufficient statistics of the combined\n",
      "\n",
      "1 + N b\n",
      "\n",
      "1 and N0 = N a\n",
      "\n",
      "0 + N b\n",
      "\n",
      "1 , N a\n",
      "\n",
      "p(θ|Da, Db) ∝ Bin(N1|θ, N1 + N0)Beta(θ|a, b) ∝ Beta(θ|N1 + a, N0 + b)\n",
      "\n",
      "(3.17)\n",
      "\n",
      "In sequential mode, we have\n",
      "\n",
      "p(θ|Da, Db) ∝ p(Db|θ)p(θ|Da)\n",
      "\n",
      "(3.18)\n",
      "\n",
      "∝ Bin(N b ∝ Beta(θ| N a\n",
      "\n",
      "1 |θ, N b\n",
      "\n",
      "1 + N b\n",
      "\n",
      "1 + N b\n",
      "\n",
      "1 + a, N a\n",
      "\n",
      "0 )Beta(θ|N a 0 + N b\n",
      "\n",
      "1 + a, N a 0 + b)\n",
      "\n",
      "0 + b)\n",
      "\n",
      "(3.19)\n",
      "\n",
      "(3.20)\n",
      "\n",
      "This makes Bayesian inference particularly well-suited to online learning, as we will see later.\n",
      "\n",
      "76\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "3.3.3.1\n",
      "\n",
      "Posterior mean and mode\n",
      "\n",
      "From Equation 2.62, the MAP estimate is given by a + N1 − 1 a + b + N − 2\n",
      "\n",
      "ˆθM AP =\n",
      "\n",
      "(3.21)\n",
      "\n",
      "If we use a uniform prior, then the MAP estimate reduces to the MLE, which is just the empirical fraction of heads: N1 N\n",
      "\n",
      "ˆθM LE =\n",
      "\n",
      "(3.22)\n",
      "\n",
      "This makes intuitive sense, but it can also be derived by applying elementary calculus to maximize the likelihood function in Equation 3.11. (Exercise 3.1).\n",
      "\n",
      "By contrast, the posterior mean is given by,\n",
      "\n",
      "θ =\n",
      "\n",
      "a + N1 a + b + N\n",
      "\n",
      "(3.23)\n",
      "\n",
      "This difference between the mode and the mean will prove important later.\n",
      "\n",
      "We will now show that the posterior mean is convex combination of the prior mean and the MLE, which captures the notion that the posterior is a compromise between what we previously believed and what the data is telling us.\n",
      "\n",
      "let the prior mean be m1 = a/α0. Then the posterior mean is given by\n",
      "\n",
      "Let α0 = a + b be the equivalent sample size of the prior, which controls its strength, and\n",
      "\n",
      "N N + α0 where λ = α0 N +α0 is the ratio of the prior to posterior equivalent sample size. So the weaker the prior, the smaller is λ, and hence the closer the posterior mean is to the MLE. One can show similarly that the posterior mode is a convex combination of the prior mode and the MLE, and that it too converges to the MLE.\n",
      "\n",
      "E [θ|D] =\n",
      "\n",
      "α0m1 + N1 N + α0\n",
      "\n",
      "=\n",
      "\n",
      "α0 N + α0\n",
      "\n",
      "m1 +\n",
      "\n",
      "N1 N\n",
      "\n",
      "= λm1 + (1− λ)ˆθM LE\n",
      "\n",
      "(3.24)\n",
      "\n",
      "3.3.3.2\n",
      "\n",
      "Posterior variance\n",
      "\n",
      "The mean and mode are point estimates, but it is useful to know how much we can trust them. The variance of the posterior is one way to measure this. The variance of the Beta posterior is given by\n",
      "\n",
      "(a + N1)(b + N0) (a + N1 + b + N0)2(a + N1 + b + N0 + 1) We can simplify this formidable expression in the case that N (cid:8) a, b, to get\n",
      "\n",
      "var [θ|D] =\n",
      "\n",
      "(3.25)\n",
      "\n",
      "var [θ|D] ≈\n",
      "\n",
      "N1N0 N N N\n",
      "\n",
      "=\n",
      "\n",
      "ˆθ(1 − ˆθ) N\n",
      "\n",
      "(3.26)\n",
      "\n",
      "where ˆθ is the MLE. Hence the “error bar” in our estimate (i.e., the posterior standard deviation), is given by\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "σ =\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "var [θ|D] ≈\n",
      "\n",
      "ˆθ(1 − ˆθ) N\n",
      "\n",
      "(3.27)\n",
      "\n",
      "3.3. The beta-binomial model\n",
      "\n",
      "77\n",
      "\n",
      "We see that the uncertainty goes down at a rate of 1/ N . Note, however, that the uncertainty (variance) is maximized when ˆθ = 0.5, and is minimized when ˆθ is close to 0 or 1. This means it is easier to be sure that a coin is biased than to be sure that it is fair.\n",
      "\n",
      "√\n",
      "\n",
      "3.3.4\n",
      "\n",
      "Posterior predictive distribution\n",
      "\n",
      "So far, we have been focusing on inference of the unknown parameter(s). Let us now turn our attention to prediction of future observable data.\n",
      "\n",
      "Consider predicting the probability of heads in a single future trial under a Beta(a, b) poste-\n",
      "\n",
      "rior. We have\n",
      "\n",
      "(cid:11) 1\n",
      "\n",
      "p(˜x = 1|D) =\n",
      "\n",
      "=\n",
      "\n",
      "0 (cid:11) 1\n",
      "\n",
      "0\n",
      "\n",
      "p(x = 1|θ)p(θ|D)dθ\n",
      "\n",
      "θ Beta(θ|a, b)dθ = E [θ|D] =\n",
      "\n",
      "a a + b\n",
      "\n",
      "(3.28)\n",
      "\n",
      "(3.29)\n",
      "\n",
      "Thus we see that the mean of the posterior predictive distribution is equivalent (in this case) to plugging in the posterior mean parameters: p(˜x|D) = Ber(˜x|E [θ|D]).\n",
      "\n",
      "3.3.4.1\n",
      "\n",
      "Overﬁtting and the black swan paradox Suppose instead that we plug-in the MLE, i.e., we use p(˜x|D) ≈ Ber(˜x|ˆθM LE). Unfortunately, this approximation can perform quite poorly when the sample size is small. For example, suppose we have seen N = 3 tails in a row. The MLE is ˆθ = 0/3 = 0, since this makes the observed data as probable as possible. However, using this estimate, we predict that heads are impossible. This is called the zero count problem or the sparse data problem, and frequently occurs when estimating counts from small amounts of data. One might think that in the era of “big data”, such concerns are irrelevant, but note that once we partition the data based on certain criteria — such as the number of times a speciﬁc person has engaged in a speciﬁc activity — the sample sizes can become much smaller. This problem arises, for example, when trying to perform personalized recommendation of web pages. Thus Bayesian methods are still useful, even in the big data regime (Jordan 2011).\n",
      "\n",
      "The zero-count problem is analogous to a problem in philosophy called the black swan paradox. This is based on the ancient Western conception that all swans were white. In that context, a black swan was a metaphor for something that could not exist. (Black swans were discovered in Australia by European explorers in the 17th Century.) The term “black swan paradox” was ﬁrst coined by the famous philosopher of science Karl Popper; the term has also been used as the title of a recent popular book (Taleb 2007). This paradox was used to illustrate the problem of induction, which is the problem of how to draw general conclusions about the future from speciﬁc observations from the past.\n",
      "\n",
      "a = b = 1. In this case, plugging in the posterior mean gives Laplace’s rule of succession\n",
      "\n",
      "Let us now derive a simple Bayesian solution to the problem. We will use a uniform prior, so\n",
      "\n",
      "p(˜x = 1|D) =\n",
      "\n",
      "N1 + 1 N1 + N0 + 2\n",
      "\n",
      "(3.30)\n",
      "\n",
      "This justiﬁes the common practice of adding 1 to the empirical counts, normalizing and then plugging them in, a technique known as add-one smoothing. (Note that plugging in the MAP\n",
      "\n",
      "78\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "parameters would not have this smoothing effect, since the mode has the form ˆθ = N1+a−1 N +a+b−2 , which becomes the MLE if a = b = 1.)\n",
      "\n",
      "3.3.4.2\n",
      "\n",
      "Predicting the outcome of multiple future trials\n",
      "\n",
      "Suppose now we were interested in predicting the number of heads, x, in M future trials. This is given by\n",
      "\n",
      "(cid:11) 1\n",
      "\n",
      "p(x|D, M ) =\n",
      "\n",
      "=\n",
      "\n",
      "0 (cid:7) M x\n",
      "\n",
      "Bin(x|θ, M )Beta(θ|a, b)dθ (cid:8) (cid:11) 1\n",
      "\n",
      "1 B(a, b)\n",
      "\n",
      "0\n",
      "\n",
      "θx(1 − θ)M −xθa−1(1 − θ)b−1dθ\n",
      "\n",
      "(3.31)\n",
      "\n",
      "(3.32)\n",
      "\n",
      "We recognize the integral as the normalization constant for a Beta(a+x, M −x+b) distribution. Hence (cid:11) 1\n",
      "\n",
      "0\n",
      "\n",
      "θx(1 − θ)M −xθa−1(1 − θ)b−1dθ = B(x + a, M − x + b)\n",
      "\n",
      "(3.33)\n",
      "\n",
      "Thus we ﬁnd that the posterior predictive is given by the following, known as the (compound) beta-binomial distribution: (cid:7)\n",
      "\n",
      "Bb(x|a, b, M ) (cid:2)\n",
      "\n",
      "M x\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "B(x + a, M − x + b) B(a, b)\n",
      "\n",
      "(3.34)\n",
      "\n",
      "This distribution has the following mean and variance\n",
      "\n",
      "(a + b + M ) a + b + 1 If M = 1, and hence x ∈ {0, 1}, we see that the mean becomes E [x|D] = p(x = 1|D) = a which is consistent with Equation 3.29.\n",
      "\n",
      "E [x] =M\n",
      "\n",
      "a a + b\n",
      "\n",
      ", var [x] =\n",
      "\n",
      "M ab (a + b)2\n",
      "\n",
      "(3.35)\n",
      "\n",
      "a+b ,\n",
      "\n",
      "This process is illustrated in Figure 3.7(a). We start with a Beta(2,2) prior, and plot the posterior predictive density after seeing N1 = 3 heads and N0 = 17 tails. Figure 3.7(b) plots a plug-in approximation using a MAP estimate. We see that the Bayesian prediction has longer tails, spreading its probablity mass more widely, and is therefore less prone to overﬁtting and blackswan type paradoxes.\n",
      "\n",
      "3.4\n",
      "\n",
      "The Dirichlet-multinomial model\n",
      "\n",
      "In the previous section, we discussed how to infer the probability that a coin comes up heads. In this section, we generalize these results to infer the probability that a dice with K sides comes up as face k. This might seem like another toy exercise, but the methods we will study are widely used to analyse text data, biosequence data, etc., as we will see later.\n",
      "\n",
      "3.4. The Dirichlet-multinomial model\n",
      "\n",
      "79\n",
      "\n",
      "0.35\n",
      "\n",
      "posterior predictive\n",
      "\n",
      "0.35\n",
      "\n",
      "plugin predictive\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.25\n",
      "\n",
      "0.25\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.15\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "0.05\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 3.7 Figure generated by betaBinomPostPredDemo.\n",
      "\n",
      "(a) Posterior predictive distributions after seeing N1 = 3, N0 = 17. (b) Plugin approximation.\n",
      "\n",
      "3.4.1\n",
      "\n",
      "Likelihood\n",
      "\n",
      "Suppose we observe N dice rolls, D = {x1, . . . , xN }, where xi ∈ {1, . . . , K}. the data is iid, the likelihood has the form\n",
      "\n",
      "If we assume\n",
      "\n",
      "p(D|θ) =\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "θNk k\n",
      "\n",
      "(3.36)\n",
      "\n",
      "k=1\n",
      "\n",
      "i=1 I(yi = k) is the number of times event k occured (these are the sufficient where Nk = statistics for this model). The likelihood for the multinomial model has the same form, up to an irrelevant constant factor.\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      "3.4.2\n",
      "\n",
      "Prior\n",
      "\n",
      "Since the parameter vector lives in the K-dimensional probability simplex, we need a prior that has support over this simplex. Ideally it would also be conjugate. Fortunately, the Dirichlet distribution (Section 2.5.4) satisﬁes both criteria. So we will use the following prior:\n",
      "\n",
      "Dir(θ|α) =\n",
      "\n",
      "1 B(α)\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "k=1\n",
      "\n",
      "θαk−1 k\n",
      "\n",
      "I(x ∈ SK)\n",
      "\n",
      "(3.37)\n",
      "\n",
      "3.4.3\n",
      "\n",
      "Posterior\n",
      "\n",
      "Multiplying the likelihood by the prior, we ﬁnd that the posterior is also Dirichlet:\n",
      "\n",
      "p(θ|D) ∝ p(D|θ)p(θ)\n",
      "\n",
      "(3.38)\n",
      "\n",
      "∝\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "k θαk−1 θNk\n",
      "\n",
      "k\n",
      "\n",
      "=\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "θαk+Nk−1 k\n",
      "\n",
      "(3.39)\n",
      "\n",
      "k=1\n",
      "\n",
      "k=1\n",
      "\n",
      "= Dir(θ|α1 + N1, . . . , αK + NK)\n",
      "\n",
      "(3.40)\n",
      "\n",
      "80\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "We see that the posterior is obtained by adding the prior hyper-parameters (pseudo-counts) αk to the empirical counts Nk.\n",
      "\n",
      "We can derive the mode of this posterior (i.e., the MAP estimate) by using calculus. However, k θk = 1.2. We can do this by using a Lagrange we must enforce the constraint that multiplier. The constrained objective function, or Lagrangian, is given by the log likelihood plus log prior plus the constraint:\n",
      "\n",
      "(cid:9)(θ, λ) =\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "Nk log θk +\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(αk − 1) log θk + λ\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "1 −\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "θk\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(3.41)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "To simplify notation, we deﬁne N (cid:2) the original constraint:\n",
      "\n",
      "∂(cid:9) ∂λ\n",
      "\n",
      "=\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "1 −\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "k\n",
      "\n",
      "θk\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "= 0\n",
      "\n",
      "k (cid:2) Nk + αk − 1. Taking derivatives with respect to λ yields\n",
      "\n",
      "(3.42)\n",
      "\n",
      "Taking derivatives with respect to θk yields\n",
      "\n",
      "∂(cid:9) ∂θk N (cid:2)\n",
      "\n",
      "N (cid:2) k θk k = λθk\n",
      "\n",
      "=\n",
      "\n",
      "− λ = 0\n",
      "\n",
      "(3.43)\n",
      "\n",
      "(3.44)\n",
      "\n",
      "We can solve for λ using the sum-to-one constraint: (cid:6)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "N (cid:2)\n",
      "\n",
      "k = λ\n",
      "\n",
      "θk\n",
      "\n",
      "(3.45)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "where α0 (cid:2) given by\n",
      "\n",
      "N + α0 − K = λ\n",
      "\n",
      "(cid:4)K\n",
      "\n",
      "k=1 αk is the equivalent sample size of the prior. Thus the MAP estimate is\n",
      "\n",
      "(3.46)\n",
      "\n",
      "ˆθk =\n",
      "\n",
      "Nk + αk − 1 N + α0 − K\n",
      "\n",
      "(3.47)\n",
      "\n",
      "which is consistent with Equation 2.77. If we use a uniform prior, αk = 1, we recover the MLE:\n",
      "\n",
      "ˆθk = Nk/N\n",
      "\n",
      "(3.48)\n",
      "\n",
      "This is just the empirical fraction of times face k shows up.\n",
      "\n",
      "2. We do not need to explicitly enforce the constraint that θk ≥ 0 since the gradient of the objective has the form Nk/θk − λ; so negative values would reduce the objective, rather than maximize it. (Of course, this does not preclude setting θk = 0, and indeed this is the optimal solution if Nk = 0 and αk = 1.)\n",
      "\n",
      "3.4. The Dirichlet-multinomial model\n",
      "\n",
      "81\n",
      "\n",
      "3.4.4\n",
      "\n",
      "Posterior predictive\n",
      "\n",
      "The posterior predictive distribution for a single multinoulli trial expression:\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "is given by the following\n",
      "\n",
      "p(X = j|D) =\n",
      "\n",
      "p(X = j|θ)p(θ|D)dθ\n",
      "\n",
      "(3.49)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:2)(cid:11)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "=\n",
      "\n",
      "p(X = j|θj)\n",
      "\n",
      "p(θ−j, θj|D)dθ−j\n",
      "\n",
      "dθj\n",
      "\n",
      "(3.50)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "θjp(θj|D)dθj = E [θj|D] =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "αj + Nj k(αk + Nk)\n",
      "\n",
      "=\n",
      "\n",
      "αj + Nj α0 + N\n",
      "\n",
      "(3.51)\n",
      "\n",
      "where θ−j are all the components of θ except θj. See also Exercise 3.13.\n",
      "\n",
      "The above expression avoids the zero-count problem, just as we saw in Section 3.3.4.1. In fact, this form of Bayesian smoothing is even more important in the multinomial case than the binary case, since the likelihood of data sparsity increases once we start partitioning the data into many categories.\n",
      "\n",
      "3.4.4.1 Worked example: language models using bag of words\n",
      "\n",
      "One application of Bayesian smoothing using the Dirichlet-multinomial model is to language modeling, which means predicting which words might occur next in a sequence. Here we will take a very simple-minded approach, and assume that the i’th word, Xi ∈ {1, . . . , K}, is sampled independently from all the other words using a Cat(θ) distribution. This is called the bag of words model. Given a past sequence of words, how can we predict which one is likely to come next?\n",
      "\n",
      "For example, suppose we observe the following sequence (part of a children’s nursery rhyme):\n",
      "\n",
      "Mary had a little lamb, little lamb, little lamb, Mary had a little lamb, its fleece as white as snow\n",
      "\n",
      "Furthermore, suppose our vocabulary consists of the following words:\n",
      "\n",
      "mary lamb little big fleece white black snow rain unk 1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "Here unk stands for unknown, and represents all other words that do not appear elsewhere on the list. To encode each line of the nursery rhyme, we ﬁrst strip off punctuation, and remove any stop words such as “a”, “as”, “the”, etc. We can also perform stemming, which means reducing words to their base form, such as stripping off the ﬁnal s in plural words, or the ing from verbs (e.g., running becomes run). In this example, no words need stemming. Finally, we replace each word by its index into the vocabulary to get:\n",
      "\n",
      "1 10 3 2 3 2 3 2 1 10 3 2 10 5 10 6 8\n",
      "\n",
      "We now ignore the word order, and count how often each word occurred, resulting in a\n",
      "\n",
      "histogram of word counts:\n",
      "\n",
      "82\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "Token Word mary Count\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "2 lamb 4\n",
      "\n",
      "3 little 4\n",
      "\n",
      "4 big 0\n",
      "\n",
      "5 ﬂeece white 1\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      "7 black 0\n",
      "\n",
      "8 snow rain 1\n",
      "\n",
      "9\n",
      "\n",
      "0\n",
      "\n",
      "10 unk 4\n",
      "\n",
      "Denote the above counts by Nj.\n",
      "\n",
      "If we use a Dir(α) prior for θ, the posterior predictive is\n",
      "\n",
      "just\n",
      "\n",
      "p( ˜X = j|D) = E[θj|D] =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "αj + Nj j(cid:2) αj(cid:2) + Nj(cid:2)\n",
      "\n",
      "=\n",
      "\n",
      "1 + Nj 10 + 17\n",
      "\n",
      "(3.52)\n",
      "\n",
      "If we set αj = 1, we get\n",
      "\n",
      "(3.53) The modes of the predictive distribution are X = 2 (“lamb”) and X = 10 (“unk”). Note that the words “big”, “black” and “rain” are predicted to occur with non-zero probability in the future, even though they have never been seen before. Later on we will see more sophisticated language models.\n",
      "\n",
      "p( ˜X = j|D) = (3/27, 5/27, 5/27, 1/27, 2/27, 2/27, 1/27, 2/27, 1/27, 5/27)\n",
      "\n",
      "3.5 Naive Bayes classiﬁers\n",
      "\n",
      "In this section, we discuss how to classify vectors of discrete-valued features, x ∈ {1, . . . , K}D, where K is the number of values for each feature, and D is the number of features. We will use a generative approach. This requires us to specify the class conditional distribution, p(x|y = c). The simplest approach is to assume the features are conditionally independent given the class label. This allows us to write the class conditional density as a product of one dimensional densities:\n",
      "\n",
      "p(x|y = c, θ) =\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "p(xj|y = c, θjc)\n",
      "\n",
      "(3.54)\n",
      "\n",
      "j=1\n",
      "\n",
      "The resulting model is called a naive Bayes classiﬁer (NBC).\n",
      "\n",
      "The model is called “naive” since we do not expect the features to be independent, even conditional on the class label. However, even if the naive Bayes assumption is not true, it often results in classiﬁers that work well (Domingos and Pazzani 1997). One reason for this is that the model is quite simple (it only has O(CD) parameters, for C classes and D features), and hence it is relatively immune to overﬁtting.\n",
      "\n",
      "The form of the class-conditional density depends on the type of each feature. We give some\n",
      "\n",
      "possibilities below:\n",
      "\n",
      "\n",
      "\n",
      "In the case of real-valued features, we can use the Gaussian distribution: p(x|y = c, θ) = (cid:15)D jc is its\n",
      "\n",
      "j=1 N (xj|μjc, σ2\n",
      "\n",
      "jc), where μjc is the mean of feature j in objects of class c, and σ2\n",
      "\n",
      "variance.\n",
      "\n",
      "\n",
      "\n",
      "In the case of binary features, xj ∈ {0, 1}, we can use the Bernoulli distribution: p(x|y = c, θ) = j=1 Ber(xj|μjc), where μjc is the probability that feature j occurs in class c. This is sometimes called the multivariate Bernoulli naive Bayes model. We will see an application of this below.\n",
      "\n",
      "(cid:15)D\n",
      "\n",
      "3.5. Naive Bayes classiﬁers\n",
      "\n",
      "83\n",
      "\n",
      "\n",
      "\n",
      "In the case of categorical features, xj ∈ {1, . . . , K}, we can model use the multinoulli distribution: p(x|y = c, θ) = j=1 Cat(xj|μjc), where μjc is a histogram over the K possible values for xj in class c.\n",
      "\n",
      "(cid:15)D\n",
      "\n",
      "Obviously we can handle other kinds of features, or use different distributional assumptions. Also, it is easy to mix and match features of different types.\n",
      "\n",
      "3.5.1 Model ﬁtting\n",
      "\n",
      "We now discuss how to “train” a naive Bayes classiﬁer. This usually means computing the MLE or the MAP estimate for the parameters. However, we will also discuss how to compute the full posterior, p(θ|D).\n",
      "\n",
      "3.5.1.1 MLE for NBC\n",
      "\n",
      "The probability for a single data case is given by\n",
      "\n",
      "p(xi, yi|θ) = p(yi|π)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(xij|θj) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "πI(yi=c)\n",
      "\n",
      "c\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(xij|θjc)I(yi=c)\n",
      "\n",
      "(3.55)\n",
      "\n",
      "j\n",
      "\n",
      "c\n",
      "\n",
      "j\n",
      "\n",
      "c\n",
      "\n",
      "Hence the log-likelihood is given by\n",
      "\n",
      "log p(D|θ) =\n",
      "\n",
      "C(cid:6)\n",
      "\n",
      "Nc log πc +\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "C(cid:6)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "log p(xij|θjc)\n",
      "\n",
      "(3.56)\n",
      "\n",
      "c=1\n",
      "\n",
      "j=1\n",
      "\n",
      "c=1\n",
      "\n",
      "i:yi=c\n",
      "\n",
      "We see that this expression decomposes into a series of terms, one concerning π, and DC terms containing the θjc’s. Hence we can optimize all these parameters separately.\n",
      "\n",
      "From Equation 3.48, the MLE for the class prior is given by\n",
      "\n",
      "ˆπc =\n",
      "\n",
      "where Nc (cid:2)\n",
      "\n",
      "Nc N (cid:4)\n",
      "\n",
      "i I(yi = c) is the number of examples in class c.\n",
      "\n",
      "(3.57)\n",
      "\n",
      "The MLE for the likelihood depends on the type of distribution we choose to use for each feature. For simplicity, let us suppose all features are binary, so xj|y = c ∼ Ber(θjc). In this case, the MLE becomes Njc Nc\n",
      "\n",
      "ˆθjc =\n",
      "\n",
      "(3.58)\n",
      "\n",
      "It is extremely simple to implement this model ﬁtting procedure: See Algorithm 8 for some pseudo-code (and naiveBayesFit for some Matlab code). This algorithm obviously takes O(N D) time. The method is easily generalized to handle features of mixed type. This simplicity is one reason the method is so widely used.\n",
      "\n",
      "Figure 3.8 gives an example where we have 2 classes and 600 binary features, representing the presence or absence of words in a bag-of-words model. The plot visualizes the θc vectors for the two classes. The big spike at index 107 corresponds to the word “subject”, which occurs in both (In Section 3.5.4, we discuss how to “ﬁlter out” such uninformative classes with probability 1. features.)\n",
      "\n",
      "84\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "Algorithm 3.1: Fitting a naive Bayes classiﬁer to binary features 1 Nc = 0, Njc = 0; 2 for i = 1 :N do 3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "c = yi // Class label of i’th example; Nc := Nc + 1 ; for j = 1 :D do\n",
      "\n",
      "if xij = 1 then\n",
      "\n",
      "Njc := Njc + 1\n",
      "\n",
      "8 ˆπc = Nc\n",
      "\n",
      "N , ˆθjc =\n",
      "\n",
      "Njc N\n",
      "\n",
      "p(xj=1|y=1)\n",
      "\n",
      "p(xj=1|y=2)\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "600\n",
      "\n",
      "700\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "600\n",
      "\n",
      "700\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 3.8 Class conditional densities p(xj = 1|y = c) for two document classes, corresponding to “X windows” and “MS windows”. Figure generated by naiveBayesBowDemo.\n",
      "\n",
      "3.5.1.2\n",
      "\n",
      "Bayesian naive Bayes\n",
      "\n",
      "The trouble with maximum likelihood is that it can overﬁt. For example, consider the example in Figure 3.8: the feature corresponding to the word “subject” (call it feature j) always occurs in both classes, so we estimate ˆθjc = 1. What will happen if we encounter a new email which does not have this word in it? Our algorithm will crash and burn, since we will ﬁnd that p(y = c|x, ˆθ) = 0 for both classes! This is another manifestation of the black swan paradox discussed in Section 3.3.4.1.\n",
      "\n",
      "A simple solution to overﬁtting is to be Bayesian. For simplicity, we will use a factored prior:\n",
      "\n",
      "p(θ) =p( π)\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "C(cid:12)\n",
      "\n",
      "p(θjc)\n",
      "\n",
      "(3.59)\n",
      "\n",
      "j=1\n",
      "\n",
      "c=1\n",
      "\n",
      "We will use a Dir(α) prior for π and a Beta(β0, β1) prior for each θjc. Often we just take α = 1 and β = 1, corresponding to add-one or Laplace smoothing.\n",
      "\n",
      "3.5. Naive Bayes classiﬁers\n",
      "\n",
      "85\n",
      "\n",
      "Combining the factored likelihood in Equation 3.56 with the factored prior above gives the\n",
      "\n",
      "following factored posterior:\n",
      "\n",
      "p(θ|D) =p( π|D)\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "C(cid:12)\n",
      "\n",
      "p(θjc|D)\n",
      "\n",
      "(3.60)\n",
      "\n",
      "j=1\n",
      "\n",
      "c=\n",
      "\n",
      "p(π|D) = Dir(N1 + α1 . . . , NC + αC) p(θjc|D) = Beta((Nc − Njc) +β 0, Njc + β1)\n",
      "\n",
      "(3.61)\n",
      "\n",
      "(3.62)\n",
      "\n",
      "In other words, to compute the posterior, we just update the prior counts with the empirical counts from the likelihood. It is straightforward to modify algorithm 8 to handle this version of model “ﬁtting”.\n",
      "\n",
      "3.5.2\n",
      "\n",
      "Using the model for prediction\n",
      "\n",
      "At test time, the goal is to compute\n",
      "\n",
      "p(y = c|x, D) ∝ p(y = c|D)\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "p(xj|y = c, D)\n",
      "\n",
      "(3.63)\n",
      "\n",
      "j=1\n",
      "\n",
      "The correct Bayesian procedure is to integrate out the unknown parameters:\n",
      "\n",
      "(cid:2)(cid:11)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "p(y = c|x, D) ∝\n",
      "\n",
      "Cat(y = c|π)p(π|D)dπ\n",
      "\n",
      "(3.64)\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "(cid:2)(cid:11)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "Ber(xj|y = c, θjc)p(θjc|D)\n",
      "\n",
      "(3.65)\n",
      "\n",
      "j=1\n",
      "\n",
      "In particular, from Equa- Fortunately, this is easy to do, at least if the posterior is Dirichlet. tion 3.51, we know the posterior predictive density can be obtained by simply plugging in the posterior mean parameters θ. Hence\n",
      "\n",
      "p(y = c|x, D) ∝ πc\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "(θjc)I(xj =1)(1 − θjc)I(xj =0)\n",
      "\n",
      "(3.66)\n",
      "\n",
      "where α0 =\n",
      "\n",
      "If we have approximated the posterior by a single point, p(θ|D) ≈ δˆθ(θ), where ˆθ may be the ML or MAP estimate, then the posterior predictive density is obtained by simply plugging in the parameters, to yield a virtually identical rule:\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θjk =\n",
      "\n",
      "πc =\n",
      "\n",
      "c αc.\n",
      "\n",
      "j=1 Njc + β1 Nc + β0 + β1 Nc + αc N + α0\n",
      "\n",
      "(3.67)\n",
      "\n",
      "(3.68)\n",
      "\n",
      "p(y = c|x, D) ∝ ˆπc\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "(ˆθjc)I(xj =1)(1 − ˆθjc)I(xj =0)\n",
      "\n",
      "(3.69)\n",
      "\n",
      "j=1\n",
      "\n",
      "86\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "The only difference is we replaced the posterior mean θ with the posterior mode or MLE ˆθ. However, this small difference can be important in practice, since the posterior mean will result in less overﬁtting (see Section 3.4.4.1).\n",
      "\n",
      "3.5.3\n",
      "\n",
      "The log-sum-exp trick\n",
      "\n",
      "We now discuss one important practical detail that arises when using generative classiﬁers of any kind. We can compute the posterior over class labels using Equation 2.13, using the appropriate class-conditional density (and a plug-in approximation). Unfortunately a naive implementation of Equation 2.13 can fail due to numerical underﬂow. The problem is that p(x|y = c) is often a very small number, especially if x is a high-dimensional vector. This is because we require x p(x|y) = 1, so the probability of observing any particular high-dimensional vector is that small. The obvious solution is to take logs when applying Bayes rule, as follows:\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "log p(y = c|x) =b\n",
      "\n",
      "c − log\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "C(cid:6)\n",
      "\n",
      "ebc(cid:2)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(3.70)\n",
      "\n",
      "c(cid:2)=1\n",
      "\n",
      "bc (cid:2) log p(x|y = c) + log p(y = c)\n",
      "\n",
      "(3.71)\n",
      "\n",
      "However, this requires evaluating the following expression\n",
      "\n",
      "log[\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "ebc(cid:2) ] = log\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "p(y = c(cid:2), x) = log p(x)\n",
      "\n",
      "(3.72)\n",
      "\n",
      "c(cid:2)\n",
      "\n",
      "c(cid:2)\n",
      "\n",
      "and we can’t add up in the log domain. Fortunately, we can factor out the largest term, and just represent the remaining numbers relative to that. For example, (cid:18)\n",
      "\n",
      "log(e−120 + e−121) = log\n",
      "\n",
      "e−120(e0 + e−1)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "= log(e0 + e−1) − 120\n",
      "\n",
      "(3.73)\n",
      "\n",
      "In general, we have\n",
      "\n",
      "log\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "ebc = log\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "ebc−B)eB\n",
      "\n",
      "=\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "log(\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "ebc−B)\n",
      "\n",
      "+ B\n",
      "\n",
      "(3.74)\n",
      "\n",
      "c\n",
      "\n",
      "c\n",
      "\n",
      "c\n",
      "\n",
      "where B = maxc bc. This is called the log-sum-exp trick, and is widely used. (See the function logsumexp for an implementation.)\n",
      "\n",
      "This trick is used in Algorithm 1 which gives pseudo-code for using an NBC to compute p(yi|xi, ˆθ). See naiveBayesPredict for the Matlab code. Note that we do not need the log-sum-exp trick if we only want to compute ˆyi, since we can just maximize the unnormalized quantity log p(yi = c) + log p(xi|y = c).\n",
      "\n",
      "3.5.4\n",
      "\n",
      "Feature selection using mutual information\n",
      "\n",
      "Since an NBC is ﬁtting a joint distribution over potentially many features, it can suffer from overﬁtting. In addition, the run-time cost is O(D), which may be too high for some applications. One common approach to tackling both of these problems is to perform feature selection, to remove “irrelevant” features that do not help much with the classiﬁcation problem. The simplest approach to feature selection is to evaluate the relevance of each feature separately, and then\n",
      "\n",
      "3.5. Naive Bayes classiﬁers\n",
      "\n",
      "87\n",
      "\n",
      "Algorithm 3.2: Predicting with a naive bayes classiﬁer for binary features 1 for i = 1 :N do 2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "for c = 1 :C do Lic = log ˆπc; for j = 1 :D do\n",
      "\n",
      "if xij = 1 then Lic := Lic + log ˆθjc else Lic := Lic + log(1− ˆθjc)\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "pic = exp(Lic − logsumexp(Li,:)); ˆyi = argmaxc pic;\n",
      "\n",
      "take the top K, where K is chosen based on some tradeoff between accuracy and complexity. This approach is known as variable ranking, ﬁltering, orscreening.\n",
      "\n",
      "Xj and the class label Y :\n",
      "\n",
      "One way to measure relevance is to use mutual information (Section 2.8.3) between feature\n",
      "\n",
      "I(X, Y ) =\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "xj\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "y\n",
      "\n",
      "p(xj, y) log\n",
      "\n",
      "p(xj, y) p(xj)p(y)\n",
      "\n",
      "(3.75)\n",
      "\n",
      "The mutual information can be thought of as the reduction in entropy on the label distribution once we observe the value of feature j. If the features are binary, it is easy to show (Exercise 3.21) that the MI can be computed as follows\n",
      "\n",
      "where πc = p(y = c), θjc = p(xj = 1|y = c), and θj = p(xj = 1) = quantities can be computed as a by-product of ﬁtting a naive Bayes classiﬁer.)\n",
      "\n",
      "Ij =\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "c\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "θjcπc log\n",
      "\n",
      "θjc θj\n",
      "\n",
      "+ (1− θjc)πc log\n",
      "\n",
      "1 − θjc 1 − θj\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "c πcθjc.\n",
      "\n",
      "(All of these\n",
      "\n",
      "(3.76)\n",
      "\n",
      "Figure 3.1 illustrates what happens if we apply this to the binary bag of words dataset used in Figure 3.8. We see that the words with highest mutual information are much more discriminative than the words which are most probable. For example, the most probable word in both classes is “subject”, which always occurs because this is newsgroup data, which always has a subject line. But obviously this is not very discriminative. The words with highest MI with the class label are (in decreasing order) “windows”, “microsoft”, “DOS” and “motif”, which makes sense, since the classes correspond to Microsoft Windows and X Windows.\n",
      "\n",
      "3.5.5\n",
      "\n",
      "Classifying documents using bag of words\n",
      "\n",
      "Document classiﬁcation is the problem of classifying text documents into different categories. One simple approach is to represent each document as a binary vector, which records whether each word is present or not, so xij = 1 iff word j occurs in document i, otherwise xij = 0. We can then use the following class conditional density:\n",
      "\n",
      "p(xi|yi = c, θ) =\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "Ber(xij|θjc) =\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "θI(xij ) jc\n",
      "\n",
      "(1 − θjc)I(1−xij )\n",
      "\n",
      "(3.77)\n",
      "\n",
      "j=1\n",
      "\n",
      "j=1\n",
      "\n",
      "88\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "class 1 subject this with but you\n",
      "\n",
      "prob class 2 0.998 subject 0.628 windows 0.535 0.471 0.431\n",
      "\n",
      "this with but\n",
      "\n",
      "prob 0.998 0.639 0.540 0.538 0.518\n",
      "\n",
      "highest MI windows microsoft dos motif window\n",
      "\n",
      "MI 0.215 0.095 0.092 0.078 0.067\n",
      "\n",
      "Table 3.1 We list the 5 most likely words for class 1 (X windows) and class 2 (MS windows). We also show the 5 words with highest mutual information with class label. Produced by naiveBayesBowDemo\n",
      "\n",
      "This is called the Bernoulli product model, or thebinary independence model.\n",
      "\n",
      "ignoring the number of times each word occurs in a document loses some in- formation (McCallum and Nigam 1998). A more accurate representation counts the number let xi be a vector of counts for document i, so of occurrences of each word. Speciﬁcally, xij ∈ {0, 1, . . . , Ni}, where Ni is the number of terms in document i (so j=1 xij = Ni). For the class conditional densities, we can use a multinomial distribution:\n",
      "\n",
      "However,\n",
      "\n",
      "(cid:4)D\n",
      "\n",
      "p(xi|yi = c, θ) = Mu(xi|Ni, θc) =\n",
      "\n",
      "(cid:15)D\n",
      "\n",
      "Ni! j=1 xij!\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "j=1\n",
      "\n",
      "θxij jc\n",
      "\n",
      "(3.78)\n",
      "\n",
      "where we have implicitly assumed that the document length Ni is independent of the class. Here θjc is the probability of generating word j in documents of class c; these parameters satisfy the constraint that\n",
      "\n",
      "(cid:4)D\n",
      "\n",
      "j=1 θjc = 1 for each class c.3\n",
      "\n",
      "Although the multinomial classiﬁer is easy to train and easy to use at test time, it does not work particularly well for document classiﬁcation. One reason for this is that it does not take into account the burstiness of word usage. This refers to the phenomenon that most words never appear in any given document, but if they do appear once, they are likely to appear more than once, i.e., words occur in bursts.\n",
      "\n",
      "The multinomial model cannot capture the burstiness phenomenon. To see why, note that Equation 3.78 has the form θNij jc , and since θjc (cid:11) 1 for rare words, it becomes increasingly unlikely to generate many of them. For more frequent words, the decay rate is not as fast. To see why intuitively, note that the most frequent words are function words which are not speciﬁc to the class, such as “and”, “the”, and “but”; the chance of the word “and” occuring is pretty much the same no matter how many time it has previously occurred (modulo document length), so the independence assumption is more reasonable for common words. However, since rare words are the ones that matter most for classiﬁcation purposes, these are the ones we want to model the most carefully.\n",
      "\n",
      "Various ad hoc heuristics have been proposed to improve the performance of the multinomial document classiﬁer (Rennie et al. 2003). We now present an alternative class conditional density that performs as well as these ad hoc methods, yet is probabilistically sound (Madsen et al. 2005).\n",
      "\n",
      "3. Since Equation 3.78 models each word independently, this model is often called a naive Bayes classiﬁer, although technically the features xij are not independent, because of the constraint\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "j xij = Ni.\n",
      "\n",
      "3.5. Naive Bayes classiﬁers\n",
      "\n",
      "89\n",
      "\n",
      "Suppose we simply replace the multinomial class conditional density with the Dirichlet\n",
      "\n",
      "Compound Multinomial or DCM density, deﬁned as follows:\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(xi|yi = c, α) =\n",
      "\n",
      "Mu(xi|Ni, θc)Dir(θc|αc)dθc =\n",
      "\n",
      "(cid:15)D\n",
      "\n",
      "Ni! j=1 xij!\n",
      "\n",
      "B(xi + αc) B(αc)\n",
      "\n",
      "(3.79)\n",
      "\n",
      "(This equation is derived in Equation 5.24.) Surprisingly this simple change is all that is needed to capture the burstiness phenomenon. The intuitive reason for this is as follows: After seeing one occurence of a word, say word j, the posterior counts on θj gets updated, making another occurence of word j more likely. By contrast, if θj is ﬁxed, then the occurences of each word are independent. The multinomial model corresponds to drawing a ball from an urn with K colors of ball, recording its color, and then replacing it. By contrast, the DCM model corresponds to drawing a ball, recording its color, and then replacing it with one additional copy; this is called the Polya urn.\n",
      "\n",
      "Using the DCM as the class conditional density gives much better results than using the multinomial, and has performance comparable to state of the art methods, as described in (Madsen et al. 2005). The only disadvantage is that ﬁtting the DCM model is more complex; see (Minka 2000e; Elkan 2006) for the details.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 3.1 MLE for the Bernoulli/ binomial model Derive Equation 3.22 by optimizing the log of the likelihood in Equation 3.11.\n",
      "\n",
      "Exercise 3.2 Marginal likelihood for the Beta-Bernoulli model In Equation 5.23, we showed that the marginal likelihood is the ratio of the normalizing constants:\n",
      "\n",
      "p(D) =\n",
      "\n",
      "Z(α1 + N1, α0 + N0) Z(α1, α0)\n",
      "\n",
      "=\n",
      "\n",
      "Γ(α1 + N1)Γ(α0 + N0) Γ(α1 + α0 + N )\n",
      "\n",
      "Γ(α1 + α0) Γ(α1)Γ(α0)\n",
      "\n",
      "(3.80)\n",
      "\n",
      "We will now derive an alternative derivation of this fact. By the chain rule of probability,\n",
      "\n",
      "p(x1:N ) = p(x1)p(x2|x1)p(x3|x1:2) . . .\n",
      "\n",
      "(3.81)\n",
      "\n",
      "In Section 3.3.4, we showed that the posterior predictive distribution is\n",
      "\n",
      "p(X = k|D1:N ) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "Nk + αk i Ni + αi\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "Nk + αk N + α\n",
      "\n",
      "(3.82)\n",
      "\n",
      "where k ∈ {0, 1} and D1:N is the data seen so far. Now suppose D = H, T, T, H, H or D = 1, 0, 0, 1, 1. Then\n",
      "\n",
      "p(D) =\n",
      "\n",
      "=\n",
      "\n",
      "α0 + 1 α1 α α + 2 [α1(α1 + 1)(α1 + 2)] [α0(α0 + 1)] α(α + 1) · · · (α + 4)\n",
      "\n",
      "\n",
      "\n",
      "α0 α + 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "α1 + 1 α + 3\n",
      "\n",
      "\n",
      "\n",
      "α1 + 2 α + 4\n",
      "\n",
      "(3.83)\n",
      "\n",
      "(3.84)\n",
      "\n",
      "=\n",
      "\n",
      "[(α1) · · · (α1 + N1 − 1)] [(α0) · · · (α0 + N0 − 1)] (α) · · · (α + N − 1)\n",
      "\n",
      "(3.85)\n",
      "\n",
      "Show how this reduces to Equation 3.80 by using the fact that, for integers, (α − 1)! = Γ(α).\n",
      "\n",
      "90\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "Exercise 3.3 Posterior predictive for Beta-Binomial model Recall from Equation 3.32 that the posterior predictive for the Beta-Binomial is given by\n",
      "\n",
      "p(x|n, D) =Bb\n",
      "\n",
      "=\n",
      "\n",
      "(x|α(cid:2) B(x + α(cid:2)\n",
      "\n",
      "B(α(cid:2)\n",
      "\n",
      "0, α(cid:2)\n",
      "\n",
      "1, n − x + α(cid:2) 0) 1, α(cid:2) 0)\n",
      "\n",
      "1, n)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "n x\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(3.86)\n",
      "\n",
      "(3.87)\n",
      "\n",
      "Prove that this reduces to α(cid:2) 1 0 + α(cid:2) 1\n",
      "\n",
      "p(˜x = 1|D) =\n",
      "\n",
      "α(cid:2)\n",
      "\n",
      "(3.88)\n",
      "\n",
      "when n = 1 (and hence x ∈ {0, 1}). i.e., show that\n",
      "\n",
      "Bb(1|α(cid:2)\n",
      "\n",
      "1, α(cid:2)\n",
      "\n",
      "0, 1) =\n",
      "\n",
      "α(cid:2)\n",
      "\n",
      "α(cid:2) 1 1 + α(cid:2) 0\n",
      "\n",
      "(3.89)\n",
      "\n",
      "Hint: use the fact that\n",
      "\n",
      "Γ(α0 + α1 + 1) = (α0 + α1 + 1)Γ(α0 + α1)\n",
      "\n",
      "(3.90)\n",
      "\n",
      "Exercise 3.4 Beta updating from censored likelihood (Source: Gelman.) Suppose we toss a coin n = 5 times. Let X be the number of heads. We observe that there are fewer than 3 heads, but we don’t know exactly how many. Let the prior probability of heads be p(θ) = Beta(θ|1, 1). Compute the posterior p(θ|X < 3) up to normalization constants, i.e., derive an expression proportional to p(θ, X < 3). Hint: the answer is a mixture distribution.\n",
      "\n",
      "Exercise 3.5 Uninformative prior for log-odds ratio Let\n",
      "\n",
      "φ = logit(θ) = log\n",
      "\n",
      "θ 1 − θ\n",
      "\n",
      "(3.91)\n",
      "\n",
      "Show that if p(φ) ∝ 1, then p(θ) ∝ Beta(θ|0, 0). Hint: use the change of variables formula.\n",
      "\n",
      "Exercise 3.6 MLE for the Poisson distribution The Poisson pmf is deﬁned as Poi(x|λ) =e −λ λx parameter. Derive the MLE.\n",
      "\n",
      "x! , for x ∈ {0, 1, 2, . . .} where λ >0 is the rate\n",
      "\n",
      "Exercise 3.7 Bayesian analysis of the Poisson distribution In Exercise 3.6, we deﬁned the Poisson distribution with rate λ and derived its MLE. Here we perform a conjugate Bayesian analysis.\n",
      "\n",
      "a. Derive the posterior p(λ|D) assuming a conjugate prior p(λ) = Ga(λ|a, b) ∝ λa−1e−λb. Hint: the\n",
      "\n",
      "posterior is also a Gamma distribution.\n",
      "\n",
      "b. What does the posterior mean tend to as a → 0 and b → 0?\n",
      "\n",
      "distribution is a/b.)\n",
      "\n",
      "(Recall that the mean of a Ga(a, b)\n",
      "\n",
      "Exercise 3.8 MLE for the uniform distribution (Source: Kaelbling.) Consider a uniform distribution centered on 0 with width 2a. The density function is given by\n",
      "\n",
      "p(x) =\n",
      "\n",
      "1 2a\n",
      "\n",
      "I(x ∈ [−a, a])\n",
      "\n",
      "(3.92)\n",
      "\n",
      "3.5. Naive Bayes classiﬁers\n",
      "\n",
      "91\n",
      "\n",
      "a. Given a data set x1, . . . , xn, what is the maximum likelihood estimate of a (call it ˆa)? b. What probability would the model assign to a new data point xn+1 using ˆa? c. Do you see any problem with the above approach? Brieﬂy suggest (in words) a better approach.\n",
      "\n",
      "Exercise 3.9 Bayesian analysis of the uniform distribution Consider the uniform distribution Unif(0, θ). The maximum likelihood estimate is ˆθ = max(D), as we saw in Exercise 3.8, but this is unsuitable for predicting future data since it puts zero probability mass outside the training data. In this exercise, we will perform a Bayesian analysis of the uniform distribution (following (Minka 2001a)). The conjugate prior is the Pareto distribution, p(θ) = Pareto(θ|b, K), deﬁned in Section 2.4.6. Given a Pareto prior, the joint distribution of θ and D = (x1, . . . , xN ) is\n",
      "\n",
      "p(D, θ) =\n",
      "\n",
      "KbK θN +K+1\n",
      "\n",
      "I(θ ≥ max(D))\n",
      "\n",
      "(3.93)\n",
      "\n",
      "Let m = max(D). The evidence (the probability that all N samples came from the same uniform distribution) is\n",
      "\n",
      "p(D) =\n",
      "\n",
      "=\n",
      "\n",
      "(cid:5) ∞\n",
      "\n",
      "m (cid:6)\n",
      "\n",
      "K (N +K)bN KbK (N +K)mN +K\n",
      "\n",
      "KbK θN +K+1\n",
      "\n",
      "dθ\n",
      "\n",
      "if m ≤ b if m > b\n",
      "\n",
      "(3.94)\n",
      "\n",
      "(3.95)\n",
      "\n",
      "Derive the posterior p(θ|D), and show that if can be expressed as a Pareto distribution.\n",
      "\n",
      "Exercise 3.10 Taxicab (tramcar) problem Suppose you arrive in a new city and see a taxi numbered 100. How many taxis are there in this city? Let us assume taxis are numbered sequentially as integers starting from 0, up to some unknown upper bound θ. (We number taxis from 0 for simplicity; we can also count from 1 without changing the analysis.) Hence the likelihood function is p(x) =U (0, θ), the uniform distribution. The goal is to estimate θ. We will use the Bayesian analysis from Exercise 3.9.\n",
      "\n",
      "a. Suppose we see one taxi numbered 100, so D = {100}, m = 100, N = 1. Using an (improper) non-informative prior on θ of the form p(θ) = P a(θ|0, 0) ∝ 1/θ, what is the posterior p(θ|D)? b. Compute the posterior mean, mode and median number of taxis in the city, if such quantities exist.\n",
      "\n",
      "c. Rather than trying to compute a point estimate of the number of taxis, we can compute the predictive\n",
      "\n",
      "density over the next taxicab number using\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "p(D(cid:2)|D, α) =\n",
      "\n",
      "p(D(cid:2)|θ)p(θ|D, α)dθ = p(D(cid:2)|β)\n",
      "\n",
      "(3.96)\n",
      "\n",
      "where α = (b, K) are the hyper-parameters, β = (c, N + K) are the updated hyper-parameters. Now consider the case D = {m}, and D(cid:2) = {x}. Using Equation 3.95, write down an expression for\n",
      "\n",
      "p(x|D, α)\n",
      "\n",
      "(3.97)\n",
      "\n",
      "As above, use a non-informative prior b = K = 0.\n",
      "\n",
      "d. Use the predictive density formula to compute the probability that the next taxi you will see (say, the next day) has number 100, 50 or 150, i.e., compute p(x = 100|D, α), p(x = 50|D, α), p(x = 150|D, α).\n",
      "\n",
      "e. Brieﬂy describe (1-2 sentences) some ways we might make the model more accurate at prediction.\n",
      "\n",
      "92\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "Exercise 3.11 Bayesian analysis of the exponential distribution A lifetime X of a machine is modeled by an exponential distribution with unknown parameter θ. The likelihood is p(x|θ) = θe−θx for x ≥ 0, θ > 0.\n",
      "\n",
      "a. Show that the MLE is ˆθ = 1/x, where x = 1 N b. Suppose we observe X1 = 5, X2 = 6, X3 = 4 (the lifetimes (in years) of 3 different iid machines).\n",
      "\n",
      "(cid:2)N\n",
      "\n",
      "i=1 xi.\n",
      "\n",
      "What is the MLE given this data?\n",
      "\n",
      "c. Assume that an expert believes θ should have a prior distribution that is also exponential\n",
      "\n",
      "p(θ) = Expon(θ|λ)\n",
      "\n",
      "(3.98)\n",
      "\n",
      "Choose the prior parameter, call it ˆλ, such that E [θ] = 1/3. Hint: recall that the Gamma distribution has the form\n",
      "\n",
      "Ga(θ|a, b) ∝ θa−1e−θb\n",
      "\n",
      "(3.99)\n",
      "\n",
      "and its mean is a/b.\n",
      "\n",
      "d. What is the posterior, p(θ|D, ˆλ)? e.\n",
      "\n",
      "Is the exponential prior conjugate to the exponential likelihood? (cid:7)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "f. What is the posterior mean, E\n",
      "\n",
      "θ|D, ˆλ\n",
      "\n",
      "?\n",
      "\n",
      "g. Explain why the MLE and posterior mean differ. Which is more reasonable in this example?\n",
      "\n",
      "Exercise 3.12 MAP estimation for the Bernoulli with non-conjugate priors (Source: prior p(θ) = Beta(θ|α, β). We know that, with this prior, the MAP estimate is given by\n",
      "\n",
      "Jaakkola.)\n",
      "\n",
      "In the book, we discussed Bayesian inference of a Bernoulli rate parameter with the\n",
      "\n",
      "ˆθ =\n",
      "\n",
      "N1 + α − 1 N + α + β − 2\n",
      "\n",
      "(3.100)\n",
      "\n",
      "where N1 is the number of heads, N0 is the number of tails, and N = N0 + N1 is the total number of trials.\n",
      "\n",
      "a. Now consider the following prior, that believes the coin is fair, or is slightly biased towards tails:\n",
      "\n",
      "p(θ) =\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "0.5 0.5 0\n",
      "\n",
      "if θ = 0.5 if θ = 0.4 otherwise\n",
      "\n",
      "(3.101)\n",
      "\n",
      "Derive the MAP estimate under this prior as a function of N1 and N .\n",
      "\n",
      "b. Suppose the true parameter is θ = 0.41. Which prior leads to a better estimate when N is small?\n",
      "\n",
      "Which prior leads to a better estimate when N is large?\n",
      "\n",
      "Exercise 3.13 Posterior predictive distribution for a batch of data with the dirichlet-multinomial model In Equation 3.51, we gave the the posterior predictive distribution for a single multinomial trial using a dirichlet prior. Now consider predicting a batch of new data, ˜D = (X1, . . . , Xm), consisting of m single multinomial trials (think of predicting the next m words in a sentence, assuming they are drawn iid). Derive an expression for\n",
      "\n",
      "p( ˜D|D, α)\n",
      "\n",
      "(3.102)\n",
      "\n",
      "3.5. Naive Bayes classiﬁers\n",
      "\n",
      "93\n",
      "\n",
      "Your answer should be a function of α, and the old and new counts (sufficient statistics), deﬁned as\n",
      "\n",
      "N old k\n",
      "\n",
      "=\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "I(xi = k)\n",
      "\n",
      "(3.103)\n",
      "\n",
      "N new k\n",
      "\n",
      "=\n",
      "\n",
      "i∈D (cid:12)\n",
      "\n",
      "I(xi = k)\n",
      "\n",
      "(3.104)\n",
      "\n",
      "i∈ ˜D\n",
      "\n",
      "Hint: recall that, for a vector of counts, N1:K , the marginal likelihood (evidence) is given by\n",
      "\n",
      "p(D|α) =\n",
      "\n",
      "Γ(α) Γ(N + α)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "k\n",
      "\n",
      "Γ(Nk + αk) Γ(αk)\n",
      "\n",
      "(3.105)\n",
      "\n",
      "where α =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "k αk and N =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "k Nk.\n",
      "\n",
      "Exercise 3.14 Posterior predictive for Dirichlet-multinomial (Source: Koller.).\n",
      "\n",
      "a. Suppose we compute the empirical distribution over letters of the Roman alphabet plus the space character (a distribution over 27 values) from 2000 samples. Suppose we see the letter “e” 260 times. What is p(x2001 = e|D), if we assume θ ∼ Dir(α1, . . . , α27), whereα k = 10 for all k?\n",
      "\n",
      "b. Suppose,\n",
      "\n",
      "in the 2000 samples, we saw “e” 260 times, “a” 100 times, and “p” 87 times. What is p(x2001 = p, x2002 = a|D), if we assume θ ∼ Dir(α1, . . . , α27), where αk = 10 for all k? Show your work.\n",
      "\n",
      "Exercise 3.15 Setting the beta hyper-parameters Suppose θ ∼ β(α1, α2) and we believe that E [θ] = m and var [θ] = v. Using Equation 2.62, solve for α1 and α2 in terms of m and v. What values do you get if m = 0.7 and v = 0.22?\n",
      "\n",
      "Exercise 3.16 Setting the beta hyper-parameters II (Source: Draper.) Suppose θ ∼ β(α1, α2) and we believe that E [θ] =m and p((cid:7) < θ < u) = 0.95. Write a program that can solve for α1 and α2 in terms of m, (cid:7) and u. Hint: write α2 as a function of α1 and m, so the pdf only has one unknown; then write down the probability mass contained in the interval as an integral, and minimize its squared discrepancy from 0.95. What values do you get if m = 0.15, (cid:7) = 0.05 and u = 0.3? What is the equivalent sample size of this prior?\n",
      "\n",
      "Exercise 3.17 Marginal likelihood for beta-binomial under uniform prior Suppose we toss a coin N times and observe N1 heads. Let N1 ∼ Bin(N, θ) and θ ∼ Beta(1, 1). Show that the marginal likelihood is p(N1|N ) = 1/(N + 1). Hint: Γ(x + 1) = x! if x is an integer.\n",
      "\n",
      "Exercise 3.18 Bayes factor for coin tossing Suppose we toss a coin N = 10 times and observe N1 = 9 heads. Let the null hypothesis be that the coin is fair, and the alternative be that the coin can have any bias, so p(θ) = Unif(0, 1). Derive the Bayes factor BF1,0 in favor of the biased coin hypothesis. What if N = 100 and N1 = 90? Hint: see Exercise 3.17.\n",
      "\n",
      "Exercise 3.19 Irrelevant features with naive Bayes (Source: Jaakkola.) Let xiw = 1 if word w occurs in document i and xiw = 0 otherwise. Let θcw be the estimated probability that word w occurs in documents of class c. Then the log-likelihood that document\n",
      "\n",
      "94\n",
      "\n",
      "Chapter 3. Generative models for discrete data\n",
      "\n",
      "x belongs to class c is\n",
      "\n",
      "log p(xi|c, θ) = log\n",
      "\n",
      "W(cid:13)\n",
      "\n",
      "cw (1 − θcw)1−xiw θxiw\n",
      "\n",
      "(3.106)\n",
      "\n",
      "w=1\n",
      "\n",
      "=\n",
      "\n",
      "W(cid:12)\n",
      "\n",
      "xiw log θcw + (1 − xiw) log(1 − θcw)\n",
      "\n",
      "(3.107)\n",
      "\n",
      "=\n",
      "\n",
      "w=1 W(cid:12)\n",
      "\n",
      "w=1\n",
      "\n",
      "xiw log\n",
      "\n",
      "θcw 1 − θcw\n",
      "\n",
      "+\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "w\n",
      "\n",
      "log(1 − θcw)\n",
      "\n",
      "(3.108)\n",
      "\n",
      "where W is the number of words in the vocabulary. We can write this more succintly as\n",
      "\n",
      "log p(xi|c, θ) =φ (xi)T βc\n",
      "\n",
      "(3.109)\n",
      "\n",
      "where xi = (xi1, . . . , xiW ) is a bit vector, φ(xi) = (xi, 1), and\n",
      "\n",
      "βc = (log\n",
      "\n",
      "θc1 1 − θc1\n",
      "\n",
      ", . . . ,log\n",
      "\n",
      "θcW 1 − θcW\n",
      "\n",
      ",\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "w\n",
      "\n",
      "log(1 − θcw))T\n",
      "\n",
      "(3.110)\n",
      "\n",
      "We see that this is a linear classiﬁer, since the class-conditional density is a linear function (an inner product) of the parameters βc.\n",
      "\n",
      "a. Assuming p(C = 1) =p (C = 2) = 0.5, write down an expression for the log posterior odds ratio,\n",
      "\n",
      "log2\n",
      "\n",
      "p(c=1|xi) p(c=2|xi) , in terms of the features φ(xi) and the parameters β1 and β2.\n",
      "\n",
      "b.\n",
      "\n",
      "Intuitively, words that occur in both classes are not very “discriminative”, and therefore should not affect our beliefs about the class label. Consider a particular word w. State the conditions on θ1,w and θ2,w (or equivalently the conditions on β1,w, β2,w) under which the presence or absence of w in a test document will have no effect on the class posterior (such a word will be ignored by the classiﬁer). Hint: using your previous result, ﬁgure out when the posterior odds ratio is 0.5/0.5.\n",
      "\n",
      "c. The posterior mean estimate of θ, using a Beta(1,1) prior, is given by\n",
      "\n",
      "ˆθcw =\n",
      "\n",
      "1 +\n",
      "\n",
      "2 +n c\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i∈c xiw\n",
      "\n",
      "(3.111)\n",
      "\n",
      "where the sum is over the nc documents in class c. Consider a particular word w, and suppose it always occurs in every document (regardless of class). Let there be n1 documents of class 1 and n2 be the number of documents in class 2, where n1 (cid:8)= n2 (since e.g., we get much more non-spam than If we use the above estimate for θcw, will word w be spam; this is an example of class imbalance). ignored by our classiﬁer? Explain why or why not.\n",
      "\n",
      "d. What other ways can you think of which encourage “irrelevant” words to be ignored?\n",
      "\n",
      "Exercise 3.20 Class conditional densities for binary data Consider a generative classiﬁer for C classes with class conditional density p(x|y) and uniform class prior p(y). Suppose all the D features are binary, xj ∈ {0, 1}. If we assume all the features are conditionally independent (the naive Bayes assumption), we can write\n",
      "\n",
      "p(x|y = c) =\n",
      "\n",
      "D(cid:13)\n",
      "\n",
      "Ber(xj|θjc)\n",
      "\n",
      "(3.112)\n",
      "\n",
      "j=1\n",
      "\n",
      "This requires DC parameters.\n",
      "\n",
      "3.5. Naive Bayes classiﬁers\n",
      "\n",
      "95\n",
      "\n",
      "a. Now consider a different model, which we will call the “full” model, in which all the features are fully dependent (i.e., we make no factorization assumptions). How might we represent p(x|y = c) in this case? How many parameters are needed to represent p(x|y = c)?\n",
      "\n",
      "b. Assume the number of features D is ﬁxed. Let there be N training cases. If the sample size N is very\n",
      "\n",
      "c.\n",
      "\n",
      "small, which model (naive Bayes or full) is likely to give lower test set error, and why? If the sample size N is very large, which model (naive Bayes or full) is likely to give lower test set error, and why?\n",
      "\n",
      "d. What is the computational complexity of ﬁtting the full and naive Bayes models as a function of N (Fitting the model here means computing the MLE or MAP parameter\n",
      "\n",
      "and D? Use big-Oh notation. estimates. You may assume you can convert a D-bit vector to an array index in O(D) time.)\n",
      "\n",
      "e. What is the computational complexity of applying the full and naive Bayes models at test time to a\n",
      "\n",
      "single test case?\n",
      "\n",
      "f. Suppose the test case has missing data. Let xv be the visible features of size v, and xh be the hidden (missing) features of size h, where v + h = D. What is the computational complexity of computing p(y|xv, ˆθ) for the full and naive Bayes models, as a function of v and h?\n",
      "\n",
      "Exercise 3.21 Mutual information for naive Bayes classiﬁers with binary features Derive Equation 3.76.\n",
      "\n",
      "Exercise 3.22 Fitting a naive bayes spam ﬁlter by hand (Source: Daphne Koller.). Consider a Naive Bayes model (multivariate Bernoulli version) for spam classiﬁca- tion with the vocabulary V=\"secret\", \"offer\", \"low\", \"price\", \"valued\", \"customer\", \"today\", \"dollar\", \"million\", \"sports\", \"is\", \"for\", \"play\", \"healthy\", \"pizza\". We have the following example spam messages \"million dollar offer\", \"secret offer today\", \"secret is secret\" and normal messages, \"low price for valued customer\", \"play secret sports today\", \"sports is healthy\", \"low price pizza\". Give the MLEs for the following parameters: sports|non-spam, θ θspam, θ\n",
      "\n",
      "secret|spam, θ\n",
      "\n",
      "secret|non-spam, θ\n",
      "\n",
      "dollar|spam.\n",
      "\n",
      "4 Gaussian models\n",
      "\n",
      "4.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In this chapter, we discuss the multivariate Gaussian or multivariate normal (MVN), which is the most widely used joint probability density function for continuous variables. It will form the basis for many of the models we will encounter in later chapters.\n",
      "\n",
      "Unfortunately, the level of mathematics in this chapter is higher than in many other chapters. In particular, we rely heavily on linear algebra and matrix calculus. This is the price one must pay in order to deal with high-dimensional data. Beginners may choose to skip sections marked with a *. In addition, since there are so many equations in this chapter, we have put a box around those that are particularly important.\n",
      "\n",
      "4.1.1\n",
      "\n",
      "Notation\n",
      "\n",
      "Let us brieﬂy say a few words about notation. We denote vectors by boldface lower case letters, such as x. We denote matrices by boldface upper case letters, such as X. We denote entries in a matrix by non-bold upper case letters, such as Xij.\n",
      "\n",
      "All vectors are assumed to be column vectors unless noted otherwise. We use [x1, . . . , xD] to denote a column vector created by stacking D scalars. Similarly, if we write x = [x1, . . . , xD], where the left hand side is a tall column vector, we mean to stack the xi along the rows; this is D)T , but that is rather ugly. If we write X = [x1, . . . , xD], usually written as x = (xT where the left hand side is a matrix, we mean to stack the xi along the columns, creating a matrix.\n",
      "\n",
      "1 , . . . , xT\n",
      "\n",
      "4.1.2\n",
      "\n",
      "Basics\n",
      "\n",
      "Recall from Section 2.5.2 that the pdf for an MVN in D dimensions is deﬁned by the following:\n",
      "\n",
      "N (x|μ, Σ) (cid:2)\n",
      "\n",
      "(2π)D/2|Σ|1/2 exp\n",
      "\n",
      "1\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(x − μ)T Σ−1(x − μ)\n",
      "\n",
      "(4.1)\n",
      "\n",
      "98\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "u 2\n",
      "\n",
      "u 1\n",
      "\n",
      "μ\n",
      "\n",
      "x\n",
      "\n",
      "2\n",
      "\n",
      "1/2 λ 2\n",
      "\n",
      "1/2 λ 1\n",
      "\n",
      "x 1\n",
      "\n",
      "Figure 4.1 Visualization of a 2 dimensional Gaussian density. The major and minor axes of the ellipse are deﬁned by the ﬁrst two eigenvectors of the covariance matrix, namely u1 and u2. Based on Figure 2.7 of (Bishop 2006a).\n",
      "\n",
      "The expression inside the exponent is the Mahalanobis distance between a data vector x and the mean vector μ, We can gain a better understanding of this quantity by performing an eigendecomposition of Σ. That is, we write Σ = UΛUT , whereU is an orthonormal matrix of eigenvectors satsifying UT U = I, and Λ is a diagonal matrix of eigenvalues.\n",
      "\n",
      "Using the eigendecomposition, we have that\n",
      "\n",
      "Σ−1 = U−T Λ−1U−1 = UΛ−1UT =\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "i=1\n",
      "\n",
      "1 λi\n",
      "\n",
      "uiuT i\n",
      "\n",
      "(4.2)\n",
      "\n",
      "where ui is the i’th column of U, containing the i’th eigenvector. Hence we can rewrite the Mahalanobis distance as follows:\n",
      "\n",
      "(x − μ)T Σ−1(x − μ) = (x − μ)T\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "i=1\n",
      "\n",
      "1 λi\n",
      "\n",
      "uiuT i\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(x − μ)\n",
      "\n",
      "(4.3)\n",
      "\n",
      "=\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "i=1\n",
      "\n",
      "1 λi\n",
      "\n",
      "(x − μ)T uiuT\n",
      "\n",
      "i (x − μ) =\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "i=1\n",
      "\n",
      "y2 i λi\n",
      "\n",
      "(4.4)\n",
      "\n",
      "where yi (cid:2) uT\n",
      "\n",
      "i (x − μ). Recall that the equation for an ellipse in 2d is\n",
      "\n",
      "y2 1 λ1\n",
      "\n",
      "+\n",
      "\n",
      "y2 2 λ2\n",
      "\n",
      "= 1\n",
      "\n",
      "(4.5)\n",
      "\n",
      "Hence we see that the contours of equal probability density of a Gaussian lie along ellipses. This is illustrated in Figure 4.1. The eigenvectors determine the orientation of the ellipse, and the eigenvalues determine how elogonated it is.\n",
      "\n",
      "transformed coordinate system, where we shift by μ and rotate by U.\n",
      "\n",
      "In general, we see that the Mahalanobis distance corresponds to Euclidean distance in a\n",
      "\n",
      "4.1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "99\n",
      "\n",
      "4.1.3 MLE for an MVN\n",
      "\n",
      "We now describe one way to estimate the parameters of an MVN, using MLE. In later sections, we will discuss Bayesian inference for the parameters, which can mitigate overﬁtting, and can provide a measure of conﬁdence in our estimates.\n",
      "\n",
      "Theorem 4.1.1 (MLE for a Gaussian). If we have N iid samples xi ∼ N (μ, Σ), then the MLE for the parameters is given by\n",
      "\n",
      "ˆμmle =\n",
      "\n",
      "ˆΣmle =\n",
      "\n",
      "1 N\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "xi (cid:2) x\n",
      "\n",
      "i=1 N(cid:6)\n",
      "\n",
      "(xi − x)(xi − x)T =\n",
      "\n",
      "i=1\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(\n",
      "\n",
      "i=1\n",
      "\n",
      "xixT\n",
      "\n",
      "i ) − x xT\n",
      "\n",
      "(4.6)\n",
      "\n",
      "(4.7)\n",
      "\n",
      "That is, the MLE is just the empirical mean and empirical covariance. In the univariate case, we get the following familiar results:\n",
      "\n",
      "ˆμ =\n",
      "\n",
      "ˆσ2 =\n",
      "\n",
      "1 N\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "xi = x\n",
      "\n",
      "i (cid:6)\n",
      "\n",
      "(xi − x)2 =\n",
      "\n",
      "i\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "i\n",
      "\n",
      "x2 i\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "− (x)2\n",
      "\n",
      "(4.8)\n",
      "\n",
      "(4.9)\n",
      "\n",
      "4.1.3.1\n",
      "\n",
      "Proof *\n",
      "\n",
      "To prove this result, we will need several results from matrix algebra, which we summarize In the equations, a and b are vectors, and A and B are matrices. Also, the notation below. tr(A) refers to the trace of a matrix, which is the sum of its diagonals: tr(A) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i Aii.\n",
      "\n",
      "∂(bT a) ∂a ∂(aT Aa) ∂a\n",
      "\n",
      "= b\n",
      "\n",
      "= (A + AT )a\n",
      "\n",
      "∂ ∂A ∂ ∂A\n",
      "\n",
      "tr(BA) =B T\n",
      "\n",
      "log |A| = A−T (cid:2) (A−1)T\n",
      "\n",
      "(4.10)\n",
      "\n",
      "tr(ABC) = tr(CAB) = tr(BCA)\n",
      "\n",
      "The last equation is called the cyclic permutation property of the trace operator. Using this, we can derive the widely used trace trick, which reorders the scalar inner product xT Ax as follows\n",
      "\n",
      "xT Ax = tr(xT Ax) = tr(xxT A) = tr(AxxT )\n",
      "\n",
      "(4.11)\n",
      "\n",
      "100\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "Proof. We can now begin with the proof. The log-likelihood is\n",
      "\n",
      "(cid:9)(μ, Σ) = log p(D|μ, Σ) =\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log |Λ| − 1 2\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − μ)T Λ(xi − μ)\n",
      "\n",
      "i=1\n",
      "\n",
      "(4.12)\n",
      "\n",
      "where Λ = Σ−1 is the precision matrix.\n",
      "\n",
      "Using the substitution yi = xi − μ and the chain rule of calculus, we have ∂ ∂yi\n",
      "\n",
      "∂ ∂μ\n",
      "\n",
      "(xi − μ)T Σ−1(xi − μ) =\n",
      "\n",
      "i Σ−1yi yT\n",
      "\n",
      "∂yi ∂μ\n",
      "\n",
      "(4.13)\n",
      "\n",
      "= −1(Σ−1 + Σ−T )yi\n",
      "\n",
      "(4.14)\n",
      "\n",
      "Hence\n",
      "\n",
      "∂ ∂μ\n",
      "\n",
      "(cid:9)(μ, Σ) =− 1 2\n",
      "\n",
      "ˆμ =\n",
      "\n",
      "1 N\n",
      "\n",
      "i=1 N(cid:6)\n",
      "\n",
      "i=1\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "xi = x\n",
      "\n",
      "−2Σ−1(xi − μ) = Σ−1\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − μ) = 0\n",
      "\n",
      "i=1\n",
      "\n",
      "(4.15)\n",
      "\n",
      "(4.16)\n",
      "\n",
      "So the MLE of μ is just the empirical mean.\n",
      "\n",
      "Now we can use the trace-trick to rewrite the log-likelihood for Λ as follows:\n",
      "\n",
      "(cid:9)(Λ) =\n",
      "\n",
      "=\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log |Λ| − 1 2 log |Λ| − 1 2\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "tr[(xi − μ)(xi − μ)T Λ]\n",
      "\n",
      "i\n",
      "\n",
      "tr [SμΛ]\n",
      "\n",
      "(4.17)\n",
      "\n",
      "(4.18)\n",
      "\n",
      "(4.19)\n",
      "\n",
      "where\n",
      "\n",
      "Sμ (cid:2)\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − μ)(xi − μ)T\n",
      "\n",
      "(4.20)\n",
      "\n",
      "i=1\n",
      "\n",
      "is the scatter matrix centered on μ. Taking derivatives of this expression with respect to Λ yields\n",
      "\n",
      "∂(cid:9)(Λ) Λ−T − 1 ∂Λ 2 2 Λ−T = Λ−1 = Σ =\n",
      "\n",
      "=\n",
      "\n",
      "N\n",
      "\n",
      "ST\n",
      "\n",
      "μ = 0\n",
      "\n",
      "1 N\n",
      "\n",
      "Sμ\n",
      "\n",
      "(4.21)\n",
      "\n",
      "(4.22)\n",
      "\n",
      "so\n",
      "\n",
      "ˆΣ =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − μ)(xi − μ)T\n",
      "\n",
      "i=1\n",
      "\n",
      "(4.23)\n",
      "\n",
      "If we plug-in the MLE μ = x which is just the empirical covariance matrix centered on μ. (since both parameters must be simultaneously optimized), we get the standard equation for the MLE of a covariance matrix.\n",
      "\n",
      "4.2. Gaussian discriminant analysis\n",
      "\n",
      "101\n",
      "\n",
      "4.1.4 Maximum entropy derivation of the Gaussian *\n",
      "\n",
      "In this section, we show that the multivariate Gaussian is the distribution with maximum entropy subject to having a speciﬁed mean and covariance (see also Section 9.2.6). This is one reason the Gaussian is so widely used: the ﬁrst two moments are usually all that we can reliably estimate from data, so we want a distribution that captures these properties, but otherwise makes as few addtional assumptions as possible.\n",
      "\n",
      "To simplify notation, we will assume the mean is zero. The pdf has the form\n",
      "\n",
      "exp(− 1 2 If we deﬁne fij(x) =x ixj and λij = 1 2 (Σ−1)ij, for i, j ∈ {1, . . . , D}, we see that this is in the same form as Equation 9.74. The (differential) entropy of this distribution (using log base e) is given by\n",
      "\n",
      "p(x) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "xT Σ−1x)\n",
      "\n",
      "(4.24)\n",
      "\n",
      "h(N (μ, Σ)) =\n",
      "\n",
      "1 2\n",
      "\n",
      "ln\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(2πe)D|Σ|\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(4.25)\n",
      "\n",
      "We now show the MVN has maximum entropy amongst all distributions with a speciﬁed co- variance Σ.\n",
      "\n",
      "Theorem 4.1.2. Let q(x) be any density satisfying h(q) ≤ h(p).\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "q(x)xixj = Σij. Let p = N (0, Σ). Then\n",
      "\n",
      "Proof.\n",
      "\n",
      "0 ≤ KL (q||p) =\n",
      "\n",
      "(From (Cover and Thomas 1991, p234).) We have\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "q(x) log\n",
      "\n",
      "q(x) p(x)\n",
      "\n",
      "dx\n",
      "\n",
      "(4.26)\n",
      "\n",
      "= −h(q) −\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "q(x) log p(x)dx\n",
      "\n",
      "(4.27)\n",
      "\n",
      "=∗ −h(q) −\n",
      "\n",
      "p(x) log p(x)dx\n",
      "\n",
      "(4.28)\n",
      "\n",
      "(4.29) where the key step in Equation 4.28 (marked with a *) follows since q and p yield the same moments for the quadratic form encoded by log p(x).\n",
      "\n",
      "= −h(q) +h( p)\n",
      "\n",
      "4.2\n",
      "\n",
      "Gaussian discriminant analysis\n",
      "\n",
      "One important application of MVNs is to deﬁne the the class conditional densities in a generative classiﬁer, i.e.,\n",
      "\n",
      "p(x|y = c, θ) = N (x|μc, Σc)\n",
      "\n",
      "(4.30)\n",
      "\n",
      "The resulting technique is called (Gaussian) discriminant analysis or GDA (even though it is a generative, not discriminative, classiﬁer — see Section 8.6 for more on this distinction). If Σc is diagonal, this is equivalent to naive Bayes.\n",
      "\n",
      "102\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "280\n",
      "\n",
      "red = female, blue=male\n",
      "\n",
      "280\n",
      "\n",
      "red = female, blue=male\n",
      "\n",
      "260\n",
      "\n",
      "260\n",
      "\n",
      "240\n",
      "\n",
      "240\n",
      "\n",
      "220\n",
      "\n",
      "220\n",
      "\n",
      "200\n",
      "\n",
      "200\n",
      "\n",
      "t h g e w\n",
      "\n",
      "i\n",
      "\n",
      "180\n",
      "\n",
      "t h g e w\n",
      "\n",
      "i\n",
      "\n",
      "180\n",
      "\n",
      "160\n",
      "\n",
      "160\n",
      "\n",
      "140\n",
      "\n",
      "140\n",
      "\n",
      "120\n",
      "\n",
      "120\n",
      "\n",
      "100\n",
      "\n",
      "100\n",
      "\n",
      "80\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "80\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "height\n",
      "\n",
      "height\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 4.2 (a) Height/weight data. (b) Visualization of 2d Gaussians ﬁt to each class. 95% of the probability mass is inside the ellipse. Figure generated by gaussHeightWeight.\n",
      "\n",
      "We can classify a feature vector using the following decision rule, derived from Equation 2.13:\n",
      "\n",
      "ˆy(x) = argmax\n",
      "\n",
      "c\n",
      "\n",
      "[log p(y = c|π) + log p(x|θc)]\n",
      "\n",
      "(4.31)\n",
      "\n",
      "When we compute the probability of x under each class conditional density, we are measuring the distance from x to the center of each class, μc, using Mahalanobis distance. This can be thought of as a nearest centroids classiﬁer.\n",
      "\n",
      "As an example, Figure 4.2 shows two Gaussian class-conditional densities in 2d, representing the height and weight of men and women. We can see that the features are correlated, as is to be expected (tall people tend to weigh more). The ellipses for each class contain 95% of the probability mass. If we have a uniform prior over classes, we can classify a new test vector as follows:\n",
      "\n",
      "ˆy(x) = argmin\n",
      "\n",
      "c\n",
      "\n",
      "(x − μc)T Σ−1\n",
      "\n",
      "c (x − μc)\n",
      "\n",
      "(4.32)\n",
      "\n",
      "4.2.1\n",
      "\n",
      "Quadratic discriminant analysis (QDA)\n",
      "\n",
      "The posterior over class labels is given by Equation 2.13. We can gain further insight into this model by plugging in the deﬁnition of the Gaussian density, as follows:\n",
      "\n",
      "p(y = c|x, θ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "πc|2πΣc|− 1 c(cid:2) πc(cid:2) |2πΣc(cid:2) |− 1\n",
      "\n",
      "2 exp\n",
      "\n",
      "2 exp\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "2 (x − μc)T Σ−1 − 1 (cid:20) − 1\n",
      "\n",
      "2 (x − μc(cid:2) )T Σ−1\n",
      "\n",
      "c (x − μc)\n",
      "\n",
      "c(cid:2) (x − μc(cid:2) )\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(4.33)\n",
      "\n",
      "Thresholding this results in a quadratic function of x. The result is known as quadratic discriminant analysis (QDA). Figure 4.3 gives some examples of what the decision boundaries look like in 2D.\n",
      "\n",
      "4.2. Gaussian discriminant analysis\n",
      "\n",
      "103\n",
      "\n",
      "Parabolic Boundary\n",
      "\n",
      "Some Linear, Some Quadratic\n",
      "\n",
      "8\n",
      "\n",
      "2\n",
      "\n",
      "6\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 4.3 Quadratic decision boundaries in 2D for the 2 and 3 class case. discrimAnalysisDboundariesDemo.\n",
      "\n",
      "Figure generated by\n",
      "\n",
      "T=100\n",
      "\n",
      "T=1\n",
      "\n",
      "T=0.1\n",
      "\n",
      "T=0.01\n",
      "\n",
      "0.4\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "Softmax distribution S(η/T ), where η = (3, 0, 1), at different temperatures T . When the Figure 4.4 temperature is high (left), the distribution is uniform, whereas when the temperature is low (right), the distribution is “spiky”, with all its mass on the largest element. Figure generated by softmaxDemo2.\n",
      "\n",
      "4.2.2\n",
      "\n",
      "Linear discriminant analysis (LDA)\n",
      "\n",
      "We now consider a special case in which the covariance matrices are tied or shared across classes, Σc = Σ. In this case, we can simplify Equation 4.33 as follows: (cid:3)\n",
      "\n",
      "p(y = c|x, θ) ∝ πc exp (cid:2)\n",
      "\n",
      "= exp\n",
      "\n",
      "c Σ−1x − 1 μT 2\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "c Σ−1x − 1 μT 2\n",
      "\n",
      "μT\n",
      "\n",
      "c Σ−1μc + log πc\n",
      "\n",
      "xT Σ−1x − 1 2\n",
      "\n",
      "c Σ−1μc μT (cid:3)\n",
      "\n",
      "exp[− 1 2\n",
      "\n",
      "xT Σ−1x]\n",
      "\n",
      "(4.34)\n",
      "\n",
      "(4.35)\n",
      "\n",
      "Since the quadratic term xT Σ−1x is independent of c, it will cancel out in the numerator and denominator. If we deﬁne\n",
      "\n",
      "γc = − 1 2 βc = Σ−1μc\n",
      "\n",
      "μT\n",
      "\n",
      "c Σ−1μc + log πc\n",
      "\n",
      "(4.36)\n",
      "\n",
      "(4.37)\n",
      "\n",
      "104\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "then we can write\n",
      "\n",
      "p(y = c|x, θ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "eβT c(cid:2) eβT\n",
      "\n",
      "c x+γc\n",
      "\n",
      "c(cid:2) x+γc(cid:2)\n",
      "\n",
      "= S(η)c\n",
      "\n",
      "(4.38)\n",
      "\n",
      "where η = [βT\n",
      "\n",
      "1 x + γ1, . . . , βT\n",
      "\n",
      "Cx + γC], and S is the softmax function, deﬁned as follows:\n",
      "\n",
      "S(η)c =\n",
      "\n",
      "(cid:4)C\n",
      "\n",
      "eηc c(cid:2)=1 eηc(cid:2)\n",
      "\n",
      "(4.39)\n",
      "\n",
      "The softmax function is so-called since it acts a bit like the max function. To see this, let us divide each ηc by a constant T called the temperature. Then as T → 0, we ﬁnd\n",
      "\n",
      "S(η/T )c =\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "1.0 0.0 otherwise\n",
      "\n",
      "if c = argmaxc(cid:2) ηc(cid:2)\n",
      "\n",
      "(4.40)\n",
      "\n",
      "In other words, at low temperatures, the distribution spends essentially all of its time in the most probable state, whereas at high temperatures, it visits all states uniformly. See Figure 4.4 for an illustration. Note that this terminology comes from the area of statistical physics, where it is common to use the Boltzmann distribution, which has the same form as the softmax function.\n",
      "\n",
      "An interesting property of Equation 4.38 is that, if we take logs, we end up with a linear (The reason it is linear is because the xT Σ−1x cancels from the numerator function of x. and denominator.) Thus the decision boundary between any two classes, say c and c(cid:2), will be a straight line. Hence this technique is called linear discriminant analysis or LDA. 1 We can derive the form of this line as follows: y = c(cid:2)|x, θ) c(cid:2) x + γc(cid:2)\n",
      "\n",
      "c x + γc = βT βT xT (βc(cid:2) − β) =γ c(cid:2) − γc See Figure 4.5 for some examples.\n",
      "\n",
      "p(y = c|x, θ) =p(\n",
      "\n",
      "(4.42)\n",
      "\n",
      "(4.43)\n",
      "\n",
      "(4.41)\n",
      "\n",
      "An alternative to ﬁtting an LDA model and then deriving the class posterior is to directly ﬁt p(y|x, W) = Cat(y|Wx) for some C × D weight matrix W. This is called multi-class logistic regression, or multinomial logistic regression.2 We will discuss this model in detail in Section 8.2. The difference between the two approaches is explained in Section 8.6.\n",
      "\n",
      "4.2.3\n",
      "\n",
      "Two-class LDA\n",
      "\n",
      "To gain further insight into the meaning of these equations, let us consider the binary case. In this case, the posterior is given by eβT 1 x+γ1 1 x+γ1 + eβT\n",
      "\n",
      "p(y = 1|x, θ) =\n",
      "\n",
      "=\n",
      "\n",
      "eβT\n",
      "\n",
      "1 + e(β0−β1)T x+(γ0−γ1) = sigm\n",
      "\n",
      "1\n",
      "\n",
      "0 x+γ0\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(β1 − β0)T x + (γ1 − γ0)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(4.44)\n",
      "\n",
      "(4.45)\n",
      "\n",
      "1. The abbreviation “LDA”, could either stand for “linear discriminant analysis” or “latent Dirichlet allocation” (Sec- tion 27.3). We hope the meaning is clear from text. 2. In the language modeling community, this model is called a maximum entropy model, for reasons explained in Section 9.2.6.\n",
      "\n",
      "4.2. Gaussian discriminant analysis\n",
      "\n",
      "105\n",
      "\n",
      "Linear Boundary\n",
      "\n",
      "All Linear Boundaries\n",
      "\n",
      "6\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 4.5 discrimAnalysisDboundariesDemo.\n",
      "\n",
      "Linear decision boundaries in 2D for the 2 and 3 class case.\n",
      "\n",
      "Figure generated by\n",
      "\n",
      "Figure 4.6 Geometry of LDA in the 2 class case where Σ1 = Σ2 = I.\n",
      "\n",
      "where sigm(η) refers to the sigmoid function (Equation 1.10).\n",
      "\n",
      "Now γ1 − γ0 = − 1 2 = − 1 2\n",
      "\n",
      "μT\n",
      "\n",
      "1 Σ−1μ1 +\n",
      "\n",
      "1 2\n",
      "\n",
      "μT\n",
      "\n",
      "0 Σ−1μ0 + log(π1/π0)\n",
      "\n",
      "(μ1 − μ0)T Σ−1(μ1 + μ0) + log(π1/π0)\n",
      "\n",
      "(4.46)\n",
      "\n",
      "(4.47)\n",
      "\n",
      "So if we deﬁne\n",
      "\n",
      "w = β1 − β0 = Σ−1(μ1 − μ0)\n",
      "\n",
      "(4.48)\n",
      "\n",
      "x0 =\n",
      "\n",
      "1 2\n",
      "\n",
      "(μ1 + μ0) − (μ1 − μ0)\n",
      "\n",
      "log(π1/π0) (μ1 − μ0)T Σ−1(μ1 − μ0)\n",
      "\n",
      "(4.49)\n",
      "\n",
      "106\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "then we have wT x0 = −(γ1 − γ0), and hence p(y = 1|x, θ) = sigm(wT (x − x0))\n",
      "\n",
      "(4.50)\n",
      "\n",
      "(This is closely related to logistic regression, which we will discuss in Section 8.2.) So the ﬁnal decision rule is as follows: shift x by x0, project onto the line w, and see if the result is positive or negative.\n",
      "\n",
      "If Σ = σ2I, then w is in the direction of μ1 − μ0. So we classify the point based on whether its projection is closer to μ0 or μ1. This is illustrated in Figure 4.6. Furthemore, if π1 = π0, then x0 = 1 If we make π1 > π0, then x0 gets closer to μ0, so more of the line belongs to class 1 a priori. Conversely if π1 < π0, the boundary shifts right. Thus we see that the class prior, πc, just changes the decision threshold, and not the overall geometry, as we claimed above. (A similar argument applies in the multi-class case.) The magnitude of w determines the steepness of the logistic function, and depends on how well-separated the means are, relative to the variance. In psychology and signal detection theory, it is common to deﬁne the discriminability of a signal from the background noise using a quantity called d-prime: μ1 − μ0 σ\n",
      "\n",
      "d(cid:2) (cid:2)\n",
      "\n",
      "2 (μ1 + μ0), which is half way between the means.\n",
      "\n",
      "(4.51)\n",
      "\n",
      "where μ1 is the mean of the signal and μ0 is the mean of the noise, and σ is the standard deviation of the noise. If d(cid:2) is large, the signal will be easier to discriminate from the noise.\n",
      "\n",
      "4.2.4 MLE for discriminant analysis\n",
      "\n",
      "We now discuss how to ﬁt a discriminant analysis model. The simplest way is to use maximum likelihood. The log-likelihood function is as follows:\n",
      "\n",
      "log p(D|θ) =\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "C(cid:6)\n",
      "\n",
      "I(yi = c) log πc\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "+\n",
      "\n",
      "C(cid:6)\n",
      "\n",
      "⎡\n",
      "\n",
      "⎣\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "log N (x|μc, Σc)\n",
      "\n",
      "⎤\n",
      "\n",
      "⎦\n",
      "\n",
      "(4.52)\n",
      "\n",
      "i=1\n",
      "\n",
      "c=1\n",
      "\n",
      "c=1\n",
      "\n",
      "i:yi=c\n",
      "\n",
      "We see that this factorizes into a term for π, and C terms for each μc and Σc. Hence we can estimate these parameters separately. For the class prior, we have ˆπc = Nc N , as with naive Bayes. For the class-conditional densities, we just partition the data based on its class label, and compute the MLE for each Gaussian:\n",
      "\n",
      "ˆμc =\n",
      "\n",
      "1 Nc\n",
      "\n",
      "i:yi=c\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "xi,\n",
      "\n",
      "ˆΣc =\n",
      "\n",
      "1 Nc\n",
      "\n",
      "i:yi=c\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(xi − ˆμc)(xi − ˆμc)T\n",
      "\n",
      "(4.53)\n",
      "\n",
      "See discrimAnalysisFit for a Matlab implementation. Once the model has been ﬁt, you can make predictions using discrimAnalysisPredict, which uses a plug-in approximation.\n",
      "\n",
      "4.2.5\n",
      "\n",
      "Strategies for preventing overﬁtting\n",
      "\n",
      "The speed and simplicity of the MLE method is one of its greatest appeals. However, the MLE can badly overﬁt in high dimensions. In particular, the MLE for a full covariance matrix is singular if Nc < D. And even when Nc > D, the MLE can be ill-conditioned, meaning it is close to singular. There are several possible solutions to this problem:\n",
      "\n",
      "4.2. Gaussian discriminant analysis\n",
      "\n",
      "107\n",
      "\n",
      "Use a diagonal covariance matrix for each class, which assumes the features are conditionally\n",
      "\n",
      "independent; this is equivalent to using a naive Bayes classiﬁer (Section 3.5).\n",
      "\n",
      "Use a full covariance matrix, but force it to be the same for all classes, Σc = Σ. This is an example of parameter tying or parameter sharing, and is equivalent to LDA (Section 4.2.2).\n",
      "\n",
      "Use a diagonal covariance matrix and forced it to be shared. This is called diagonal covariance\n",
      "\n",
      "LDA, and is discussed in Section 4.2.7.\n",
      "\n",
      "Use a full covariance matrix, but impose a prior and then integrate it out.\n",
      "\n",
      "If we use a conjugate prior, this can be done in closed form, using the results from Section 4.6.3; this is analogous to the “Bayesian naive Bayes” method in Section 3.5.1.2. See (Minka 2000f) for details.\n",
      "\n",
      "Fit a full or diagonal covariance matrix by MAP estimation. We discuss two different kinds\n",
      "\n",
      "of prior below.\n",
      "\n",
      "Project the data into a low dimensional subspace and ﬁt the Gaussians there. See Sec-\n",
      "\n",
      "tion 8.6.3.3 for a way to ﬁnd the best (most discriminative) linear projection.\n",
      "\n",
      "We discuss some of these options below.\n",
      "\n",
      "4.2.6\n",
      "\n",
      "Regularized LDA *\n",
      "\n",
      "Suppose we tie the covariance matrices, so Σc = Σ, as in LDA, and furthermore we perform MAP estimation of Σ using an inverse Wishart prior of the form IW(diag( ˆΣmle), ν0) (see Section 4.5.1). Then we have\n",
      "\n",
      "ˆΣ = λdiag( ˆΣmle) + (1− λ) ˆΣmle\n",
      "\n",
      "(4.54)\n",
      "\n",
      "where λ controls the amount of regularization, which is related to the strength of the prior, ν0 (see Section 4.6.2.1 for details). This technique is known as regularized discriminant analysis or RDA (Hastie et al. 2009, p656).\n",
      "\n",
      "−1 mle, which is impossible to compute if D > N . However, we can use the SVD of X (Section 12.2.3) to get around this, as we show below. (Note that this trick cannot be applied to QDA, which is a nonlinear function of x.)\n",
      "\n",
      "Let X = UDVT be the SVD of the design matrix, where V is D × N , U is an N × N orthogonal matrix, and D is a diagonal matrix of size N . Furthermore, deﬁne the N × N matrix Z = UD; this is like a design matrix in a lower dimensional space (since we assume N < D). Also, deﬁne μz = VT μ as the mean of the data in this reduced space; we can recover the original mean using μ = Vμz, since VT V = VVT = I. With these deﬁnitions, we can\n",
      "\n",
      "When we evaluate the class conditional densities, we need to compute ˆΣ\n",
      "\n",
      "−1\n",
      "\n",
      ", and hence ˆΣ\n",
      "\n",
      "108\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "rewrite the MLE as follows:\n",
      "\n",
      "ˆΣmle =\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "= V(\n",
      "\n",
      "1 N 1 N 1 N\n",
      "\n",
      "XT X − μμT\n",
      "\n",
      "(ZVT )T (ZVT ) − (Vμz)(Vμz)T\n",
      "\n",
      "VZT ZVT − VμzμT 1 N\n",
      "\n",
      "ZT Z − μzμT\n",
      "\n",
      "z )VT\n",
      "\n",
      "z VT\n",
      "\n",
      "(4.55)\n",
      "\n",
      "(4.56)\n",
      "\n",
      "(4.57)\n",
      "\n",
      "(4.58)\n",
      "\n",
      "= V ˆΣzVT\n",
      "\n",
      "(4.59)\n",
      "\n",
      "where ˆΣz is the empirical covariance of Z. Hence we can rewrite the MAP estimate as\n",
      "\n",
      "ˆΣmap = V ˜ΣzVT\n",
      "\n",
      "(4.60)\n",
      "\n",
      "˜Σz = λdiag( ˆΣz) + (1− λ) ˆΣz\n",
      "\n",
      "(4.61)\n",
      "\n",
      "Note, however, that we never need to actually compute the D ×D matrix ˆΣmap. This is because Equation 4.38 tells us that to classify using LDA, all we need to compute is p(y = c|x, θ) ∝ exp(δc), where\n",
      "\n",
      "μc, γc − 1 2 We can compute the crucial βc term for RDA without inverting the D × D matrix as follows: −1 z VT μc = V ˜Σ\n",
      "\n",
      "δc = −xT βc + γc, βc = ˆΣ\n",
      "\n",
      "βc = ˆΣ\n",
      "\n",
      "−1 mapμc = (V ˜ΣzVT )−1μc = V ˜Σ\n",
      "\n",
      "−1\n",
      "\n",
      "μT\n",
      "\n",
      "c βc + log πc\n",
      "\n",
      "−1 z μz,c\n",
      "\n",
      "(4.62)\n",
      "\n",
      "(4.63)\n",
      "\n",
      "where μz,c = VT μc is the mean of the Z matrix for data belonging to class c. See rdaFit for the code.\n",
      "\n",
      "4.2.7\n",
      "\n",
      "Diagonal LDA\n",
      "\n",
      "A simple alternative to RDA is to tie the covariance matrices, so Σc = Σ as in LDA, and then to use a diagonal covariance matrix for each class. This is called the diagonal LDA model, and is equivalent to RDA with λ = 1. The corresponding discriminant function is as follows (compare to Equation 4.33):\n",
      "\n",
      "δc(x) = log p(x, y = c|θ) = −\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "j=1\n",
      "\n",
      "(xj − μcj)2 2σ2 j\n",
      "\n",
      "+ log πc\n",
      "\n",
      "(4.64)\n",
      "\n",
      "Typically we set ˆμcj = xcj and ˆσ2 (pooled across classes) deﬁned by (cid:4)\n",
      "\n",
      "s2 j =\n",
      "\n",
      "(cid:4)C\n",
      "\n",
      "c=1\n",
      "\n",
      "i:yi=c(xij − xcj)2 N − C\n",
      "\n",
      "j = s2\n",
      "\n",
      "j , which is the pooled empirical variance of feature j\n",
      "\n",
      "(4.65)\n",
      "\n",
      "In high dimensional settings, this model can work much better than LDA and RDA (Bickel and Levina 2004).\n",
      "\n",
      "4.2. Gaussian discriminant analysis\n",
      "\n",
      "109\n",
      "\n",
      "Number of Genes\n",
      "\n",
      "2308 1\n",
      "\n",
      "1355\n",
      "\n",
      "352\n",
      "\n",
      "106\n",
      "\n",
      "36\n",
      "\n",
      "12\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "Test Train CV\n",
      "\n",
      "r o r r\n",
      "\n",
      "E n o i t a c i f i s s a c s M\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4 λ\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "Figure 4.7 Error versus amount of shrinkage for nearest shrunken centroid classiﬁer applied to the Figure generated by SRBCT gene expression data. shrunkenCentroidsSRBCTdemo.\n",
      "\n",
      "Based on Figure 18.4 of\n",
      "\n",
      "(Hastie et al. 2009).\n",
      "\n",
      "4.2.8\n",
      "\n",
      "Nearest shrunken centroids classiﬁer *\n",
      "\n",
      "One drawback of diagonal LDA is that it depends on all of the features. In high dimensional problems, we might prefer a method that only depends on a subset of the features, for reasons of accuracy and interpretability. One approach is to use a screening method, perhaps based on mutual information, as in Section 3.5.4. We now discuss another approach to this problem known as the nearest shrunken centroids classiﬁer (Hastie et al. 2009, p652).\n",
      "\n",
      "The basic idea is to perform MAP estimation for diagonal LDA with a sparsity-promoting (Laplace) prior (see Section 13.3). More precisely, deﬁne the class-speciﬁc feature mean, μcj, in terms of the class-independent feature mean, mj, and a class-speciﬁc offset, Δcj. Thus we have\n",
      "\n",
      "μcj = mj + Δcj\n",
      "\n",
      "(4.66)\n",
      "\n",
      "We will then put a prior on the Δcj terms to encourage them to be strictly zero and compute a MAP estimate. If, for feature j, we ﬁnd that Δcj = 0 for all c, then feature j will play no role in the classiﬁcation decision (since μcj will be independent of c). Thus features that are not discriminative are automatically ignored. The details can be found in (Hastie et al. 2009, p652) and (Greenshtein and Park 2009). See shrunkenCentroidsFit for some code.\n",
      "\n",
      "Let us give an example of the method in action, based on (Hastie et al. 2009, p652). Consider the problem of classifying a gene expression dataset, which 2308 genes, 4 classes, 63 training samples and 20 test samples. Using a diagonal LDA classiﬁer produces 5 errors on the test set. Using the nearest shrunken centroids classiﬁer produced 0 errors on the test set, for a range of λ values: see Figure 4.7. More importantly, the model is sparse and hence more interpretable: Figure 4.8 plots an unpenalized estimate of the difference, dcj, in gray, as well as the shrunken (These estimates are computed using the value of λ estimated by CV.) estimates Δcj in blue. We see that only 39 genes are used, out of the original 2308.\n",
      "\n",
      "Now consider an even harder problem, with 16,603 genes, a training set of 144 patients, a test set of 54 patients, and 14 different types of cancer (Ramaswamy et al. 2001). Hastie et al. (Hastie et al. 2009, p656) report that nearest shrunken centroids produced 17 errors on the test\n",
      "\n",
      "110\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "Class 1\n",
      "\n",
      "Class 2\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "−8\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "6\n",
      "\n",
      "Class 3\n",
      "\n",
      "8\n",
      "\n",
      "Class 4\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "−6\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "−4\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 4.8 Proﬁle of ure 4.7). This selects 39 genes. Based on Figure 18.4 of (Hastie et al. 2009). shrunkenCentroidsSRBCTdemo.\n",
      "\n",
      "the shrunken centroids corresponding to λ = 4.4 (CV optimal\n",
      "\n",
      "in Fig- Figure generated by\n",
      "\n",
      "set, using 6,520 genes, and that RDA (Section 4.2.6) produced 12 errors on the test set, using all 16,603 genes. The PMTK function cancerHighDimClassifDemo can be used to reproduce these numbers.\n",
      "\n",
      "4.3\n",
      "\n",
      "Inference in jointly Gaussian distributions\n",
      "\n",
      "Given a joint distribution, p(x1, x2), it is useful to be able to compute marginals p(x1) and conditionals p(x1|x2). We discuss how to do this below, and then give some applications. These operations take O(D3) time in the worst case. See Section 20.4.3 for faster methods.\n",
      "\n",
      "4.3.\n",
      "\n",
      "Inference in jointly Gaussian distributions\n",
      "\n",
      "111\n",
      "\n",
      "4.3.1\n",
      "\n",
      "Statement of the result\n",
      "\n",
      "Theorem 4.3.1 (Marginals and conditionals of an MVN). Suppose x = (x1, x2) is jointly Gaussian with parameters (cid:8)\n",
      "\n",
      "μ =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "μ1 μ2\n",
      "\n",
      ", Σ =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Σ11 Σ12 Σ21 Σ22\n",
      "\n",
      "(cid:8)\n",
      "\n",
      ", Λ = Σ−1 =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Λ11 Λ12 Λ21 Λ22\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.67)\n",
      "\n",
      "Then the marginals are given by p(x1) =N (x1|μ1, Σ11) p(x2) =N (x2|μ2, Σ22)\n",
      "\n",
      "(4.68)\n",
      "\n",
      "and the posterior conditional is given by\n",
      "\n",
      "p(x1|x2) = N (x1|μ1|2, Σ1|2) μ1|2 = μ1 + Σ12Σ−1 = μ1 − Λ−1 = Σ1|2 (Λ11μ1 − Λ12(x2 − μ2))\n",
      "\n",
      "22 (x2 − μ2) 11 Λ12(x2 − μ2)\n",
      "\n",
      "(4.69)\n",
      "\n",
      "Σ1|2 = Σ11 − Σ12Σ−1\n",
      "\n",
      "22 Σ21 = Λ−1 11\n",
      "\n",
      "Equation 4.69 is of such crucial importance in this book that we have put a box around it, so\n",
      "\n",
      "you can easily ﬁnd it. For the proof, see Section 4.3.4.\n",
      "\n",
      "We see that both the marginal and conditional distributions are themselves Gaussian. For the marginals, we just extract the rows and columns corresponding to x1 or x2. For the conditional, we have to do a bit more work. However, it is not that complicated: the conditional mean is just a linear function of x2, and the conditional covariance is just a constant matrix that is independent of x2. We give three different (but equivalent) expressions for the posterior mean, and two different (but equivalent) expressions for the posterior covariance; each one is useful in different circumstances.\n",
      "\n",
      "4.3.2\n",
      "\n",
      "Examples\n",
      "\n",
      "Below we give some examples of these equations in action, which will make them seem more intuitive.\n",
      "\n",
      "4.3.2.1 Marginals and conditionals of a 2d Gaussian\n",
      "\n",
      "Let us consider a 2d example. The covariance matrix is (cid:8)\n",
      "\n",
      "Σ =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "σ2 1 ρσ1σ2\n",
      "\n",
      "ρσ1σ2 σ2 2\n",
      "\n",
      "(4.70)\n",
      "\n",
      "The marginal p(x1) is a 1D Gaussian, obtained by projecting the joint distribution onto the x1 line:\n",
      "\n",
      "p(x1) = N (x1|μ1, σ2 1)\n",
      "\n",
      "(4.71)\n",
      "\n",
      "112\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "p(x1,x2)\n",
      "\n",
      "0.08\n",
      "\n",
      "p(x1)\n",
      "\n",
      "7\n",
      "\n",
      "p(x1|x2=1)\n",
      "\n",
      "3\n",
      "\n",
      "0.07\n",
      "\n",
      "6\n",
      "\n",
      "2\n",
      "\n",
      "0.06\n",
      "\n",
      "5\n",
      "\n",
      "1\n",
      "\n",
      "0.05\n",
      "\n",
      "4\n",
      "\n",
      "2 x\n",
      "\n",
      "0\n",
      "\n",
      "2 x\n",
      "\n",
      "0.04\n",
      "\n",
      "2 x\n",
      "\n",
      "3\n",
      "\n",
      "−1\n",
      "\n",
      "0.03\n",
      "\n",
      "2\n",
      "\n",
      "−2\n",
      "\n",
      "0.02\n",
      "\n",
      "−3\n",
      "\n",
      "0.01\n",
      "\n",
      "1\n",
      "\n",
      "−5\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0 x1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0 −5\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0 x1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "0 −5\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0 x1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 4.9 (a) A joint Gaussian distribution p(x1, x2) with a correlation coefficient of 0.8. We plot the 95% contour and the principal axes. (b) The unconditional marginal p(x1). (c) The conditional p(x1|x2) = N (x1|0.8, 0.36), obtained by slicing (a) at height x2 = 1. Figure generated by gaussCondition2Ddemo2.\n",
      "\n",
      "Suppose we observe X2 = x2; the conditional p(x1|x2) is obtained by “slicing” the joint distribution through the X2 = x2 line (see Figure 4.9):\n",
      "\n",
      "p(x1|x2) =N\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "x1|μ1 +\n",
      "\n",
      "ρσ1σ2 σ2 2\n",
      "\n",
      "(x2 − μ2), σ2\n",
      "\n",
      "1 − (ρσ1σ2)2 σ2 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.72)\n",
      "\n",
      "If σ1 = σ2 = σ, we get (cid:18)\n",
      "\n",
      "p(x1|x2) =N\n",
      "\n",
      "x1|μ1 + ρ(x2 − μ2), σ2(1 − ρ2)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(4.73)\n",
      "\n",
      "In Figure 4.9 we show an example where ρ = 0.8, σ1 = σ2 = 1, μ = 0 and x2 = 1. We see that E [x1|x2 = 1] = 0.8, which makes sense, since ρ = 0.8 means that we believe that if x2 increases by 1 (beyond its mean), then x1 increases by 0.8. We also see var [x1|x2 = 1] = 1 − 0.82 = 0.36. This also makes sense: our uncertainty about x1 has gone down, since we have learned something about x1 (indirectly) by observing x2. If ρ = 0, we get p(x1|x2) = x1|μ1, σ2 , since x2 conveys no information about x1 if they are uncorrelated (and hence N 1 independent).\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "4.3.2.2\n",
      "\n",
      "Interpolating noise-free data\n",
      "\n",
      "Suppose we want to estimate a 1d function, deﬁned on the interval [0, T ], such that yi = f (ti) for N observed points ti. We assume for now that the data is noise-free, so we want to interpolate it, that is, ﬁt a function that goes exactly through the data. (See Section 4.4.2.3 for the noisy data case.) The question is: how does the function behave in between the observed data points? It is often reasonable to assume that the unknown function is smooth. In Chapter 15, we shall see how to encode priors over functions, and how to update such a prior with observed values to get a posterior over functions. But in this section, we take a simpler approach, which is adequate for MAP estimation of functions deﬁned on 1d inputs. We follow the presentation of (Calvetti and Somersalo 2007, p135).\n",
      "\n",
      "We start by discretizing the problem. First we divide the support of the function into D equal\n",
      "\n",
      "subintervals. We then deﬁne\n",
      "\n",
      "xj = f (sj), sj = jh, h =\n",
      "\n",
      "T D\n",
      "\n",
      ", 1 ≤ j ≤ D\n",
      "\n",
      "(4.74)\n",
      "\n",
      "4.3.\n",
      "\n",
      "Inference in jointly Gaussian distributions\n",
      "\n",
      "113\n",
      "\n",
      "λ=30\n",
      "\n",
      "λ=0p1\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 4.10 Interpolating noise-free data using a Gaussian with prior precision λ. (b) λ = 0.01. See also Figure 4.15. Based on Figure 7.1 of (Calvetti and Somersalo 2007). Figure generated by gaussInterpDemo.\n",
      "\n",
      "(a) λ = 30.\n",
      "\n",
      "We can encode our smoothness prior by assuming that xj is an average of its neighbors, xj−1 and xj+1, plus some Gaussian noise:\n",
      "\n",
      "xj =\n",
      "\n",
      "1 2\n",
      "\n",
      "(xj−1 + xj+1) +(cid:15) j, 2 ≤ j ≤ D − 2\n",
      "\n",
      "(4.75)\n",
      "\n",
      "where (cid:7) ∼ N (0, (1/λ)I). The precision term λ controls how much we think the function will vary: a large λ corresponds to a belief that the function is very smooth, a small λ corresponds to a belief that the function is quite “wiggly”. In vector form, the above equation can be written as follows:\n",
      "\n",
      "Lx = (cid:7)\n",
      "\n",
      "(4.76)\n",
      "\n",
      "where L is the (D − 2) × D second order ﬁnite difference matrix\n",
      "\n",
      "L =\n",
      "\n",
      "1 2\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "−1\n",
      "\n",
      "2− 1 −1\n",
      "\n",
      "2− 1 . . .\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "(4.77)\n",
      "\n",
      "−1\n",
      "\n",
      "2− 1\n",
      "\n",
      "The corresponding prior has the form\n",
      "\n",
      "p(x) = N (x|0, (λ2LT L)−1) ∝ exp\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "−\n",
      "\n",
      "λ2\n",
      "\n",
      "2\n",
      "\n",
      "||Lx||2 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.78)\n",
      "\n",
      "We will henceforth assume we have scaled L by λ so we can ignore the λ term, and just write Λ = LT L for the precision matrix.\n",
      "\n",
      "Note that although x is D-dimensional, the precision matrix Λ only has rank D − 2. Thus this is an improper prior, known as an intrinsic Gaussian random ﬁeld (see Section 19.4.4 for\n",
      "\n",
      "114\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "more information). However, providing we observe N ≥ 2 data points, the posterior will be proper.\n",
      "\n",
      "Now let x2 be the N noise-free observations of the function, and x1 be the D − N unknown function values. Without loss of generality, assume that the unknown variables are ordered ﬁrst, then the known variables. Then we can partition the L matrix as follows:\n",
      "\n",
      "L = [L1, L2], L1 ∈ R(D−2)×(D−N ), L2 ∈ R(D−2)×(N )\n",
      "\n",
      "(4.79)\n",
      "\n",
      "We can also partition the precision matrix of the joint distribution:\n",
      "\n",
      "Λ = LT L =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Λ11 Λ12 Λ21 Λ22\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "LT LT\n",
      "\n",
      "1 L1 LT 2 L1 LT\n",
      "\n",
      "1 L2 2 L2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.80)\n",
      "\n",
      "Using Equation 4.69, we can write the conditional distribution as follows:\n",
      "\n",
      "p(x1|x2) =N (μ1|2, Σ1|2)\n",
      "\n",
      "(4.81)\n",
      "\n",
      "μ1|2 = −Λ−1 Σ1|2 = Λ−1 11\n",
      "\n",
      "11 Λ12x2 = −LT\n",
      "\n",
      "1 L2x2\n",
      "\n",
      "(4.82)\n",
      "\n",
      "(4.83)\n",
      "\n",
      "Note that we can compute the mean by solving the following system of linear equations:\n",
      "\n",
      "L1μ1|2 = −L2x2\n",
      "\n",
      "(4.84)\n",
      "\n",
      "This is efficient since L1 is tridiagonal. Figure 4.10 gives an illustration of these equations. We see that the posterior mean μ1|2 equals the observed data at the speciﬁed points, and smoothly interpolates in between, as desired.\n",
      "\n",
      "It is also interesting to plot the 95% pointwise marginal credibility intervals, μj ± (cid:9) 2 Σ1|2,jj, shown in grey. We see that the variance goes up as we move away from the data. We also see that the variance goes up as we decrease the precision of the prior, λ. In- terestingly, λ has no effect on the posterior mean, since it cancels out when multiplying Λ11 and Λ12. By contrast, when we consider noisy data in Section 4.4.2.3, we will see that the prior precision affects the smoothness of posterior mean estimate.\n",
      "\n",
      "The marginal credibility intervals do not capture the fact that neighboring locations are correlated. We can represent that by drawing complete functions (i.e., vectors x) from the posterior, and plotting them. These are shown by the thin lines in Figure 4.10. These are not quite as smooth as the posterior mean itself. This is because the prior only penalizes ﬁrst-order differences. See Section 4.4.2.3 for further discussion of this point.\n",
      "\n",
      "4.3.2.3\n",
      "\n",
      "Data imputation\n",
      "\n",
      "Suppose we are missing some entries in a design matrix. If the columns are correlated, we can use the observed entries to predict the missing entries. Figure 4.11 shows a simple example. We sampled some data from a 20 dimensional Gaussian, and then deliberately “hid” 50% of the data in each row. We then inferred the missing entries given the observed entries, using the true (generating) model. More precisely, for each row i, we compute p(xhi , θ), where hi and vi are the indices of the hidden and visible entries in case i. From this, we compute the marginal , θ). We then plot the mean of this distribution, distribution of each missing variable, p(xhij ˆxij = E [xj|xvi , θ]; this represents our “best guess” about the true value of that entry, in the\n",
      "\n",
      "|xvi\n",
      "\n",
      "|xvi\n",
      "\n",
      "4.3.\n",
      "\n",
      "Inference in jointly Gaussian distributions\n",
      "\n",
      "115\n",
      "\n",
      "10\n",
      "\n",
      "observed\n",
      "\n",
      "10\n",
      "\n",
      "imputed\n",
      "\n",
      "10\n",
      "\n",
      "truth\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "−5\n",
      "\n",
      "−5\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "Figure 4.11 Illustration of data imputation. Left column: visualization of three rows of the data matrix with missing entries. Middle column: mean of the posterior predictive, based on partially observed data in that row, but the true model parameters. Right column: Figure generated by gaussImputationDemo.\n",
      "\n",
      "true values.\n",
      "\n",
      "sense that it minimizes our expected squared error (see Section 5.7 for details). Figure 4.11 shows that the estimates are quite close to the truth. (Of course, if j ∈ vi, the expected value is equal to the observed value, ˆxij = xij.) (cid:21)\n",
      "\n",
      "shown. Alternatively, we could draw multiple samples from p(xhi imputation.\n",
      "\n",
      "We can use var\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "xhij |xvi , θ\n",
      "\n",
      "as a measure of conﬁdence in this guess, although this is not , θ); this is called multiple\n",
      "\n",
      "|xvi\n",
      "\n",
      "In addition to imputing the missing entries, we may be interested in computing the like- lihood of each partially observed row in the table, p(xvi |θ), which can be computed using Equation 4.68. This is useful for detecting outliers (atypical observations).\n",
      "\n",
      "4.3.3\n",
      "\n",
      "Information form\n",
      "\n",
      "Suppose x ∼ N (μ, Σ). One can show that E [x] =μ is the mean vector, and cov [x] =Σ is the covariance matrix. These are called the moment parameters of the distribution. However, it is sometimes useful to use the canonical parameters or natural parameters, deﬁned as\n",
      "\n",
      "Λ (cid:2) Σ−1, ξ (cid:2) Σ−1μ\n",
      "\n",
      "(4.85)\n",
      "\n",
      "We can convert back to the moment parameters using\n",
      "\n",
      "μ = Λ−1ξ, Σ = Λ−1\n",
      "\n",
      "(4.86)\n",
      "\n",
      "Using the canonical parameters, we can write the MVN in information form (i.e., in exponential family form, deﬁned in Section 9.2):\n",
      "\n",
      "Nc(x|ξ, Λ) = (2π)−D/2|Λ| 1\n",
      "\n",
      "2 exp\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(xT Λx + ξT Λ−1ξ − 2xT ξ)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(4.87)\n",
      "\n",
      "where we use the notation Nc() to distinguish from the moment parameterization N ().\n",
      "\n",
      "It is also possible to derive the marginalization and conditioning formulas in information\n",
      "\n",
      "form. We ﬁnd\n",
      "\n",
      "p(x2) =N c(x2|ξ2 − Λ21Λ−1 p(x1|x2) =N c(x1|ξ1 − Λ12x2, Λ11)\n",
      "\n",
      "11 ξ1, Λ22 − Λ21Λ−1\n",
      "\n",
      "11 Λ12)\n",
      "\n",
      "(4.88)\n",
      "\n",
      "(4.89)\n",
      "\n",
      "116\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "Thus we see that marginalization is easier in moment form, and conditioning is easier in information form.\n",
      "\n",
      "Another operation that is signiﬁcantly easier in information form is multiplying two Gaussians.\n",
      "\n",
      "One can show that\n",
      "\n",
      "Nc(ξf , λf )Nc(ξg, λg) =N c(ξf + ξg, λf + λg)\n",
      "\n",
      "(4.90)\n",
      "\n",
      "However, in moment form, things are much messier:\n",
      "\n",
      "N (μf , σ2\n",
      "\n",
      "f )N (μg, σ2\n",
      "\n",
      "g) = N\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "g + μgσ2 μf σ2 f g + σ2 σ2 g\n",
      "\n",
      ",\n",
      "\n",
      "σ2 f σ2 g g + σ2 σ2 g\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(4.91)\n",
      "\n",
      "4.3.4\n",
      "\n",
      "Proof of the result *\n",
      "\n",
      "We now prove Theorem 4.3.1. Readers who are intimidated by heavy matrix algebra can safely skip this section. We ﬁrst derive some results that we will need here and elsewhere in the book. We will return to the proof at the end.\n",
      "\n",
      "4.3.4.1\n",
      "\n",
      "Inverse of a partitioned matrix using Schur complements\n",
      "\n",
      "The key tool we need is a way to invert a partitioned matrix. This can be done using the following result.\n",
      "\n",
      "Theorem 4.3.2 (Inverse of a partitioned matrix). Consider a general partitioned matrix\n",
      "\n",
      "M =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "E F G H\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.92)\n",
      "\n",
      "where we assume E and H are invertible. We have\n",
      "\n",
      "M−1 =\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "−H−1G(M/H)−1 H−1 + H−1G(M/H)−1FH−1 (cid:8) E−1 + E−1F(M/E)−1GE−1 −E−1F(M/E)−1\n",
      "\n",
      "(M/H)−1\n",
      "\n",
      "−(M/E)−1GE−1\n",
      "\n",
      "−(M/H)−1FH−1\n",
      "\n",
      "(M/E)−1\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.93)\n",
      "\n",
      "(4.94)\n",
      "\n",
      "where\n",
      "\n",
      "M/H (cid:2) E − FH−1G M/E (cid:2) H − GE−1F\n",
      "\n",
      "(4.95)\n",
      "\n",
      "(4.96)\n",
      "\n",
      "We say that M/H is the Schur complement of M wrt H. Equation 4.93 is called the partitioned inverse formula.\n",
      "\n",
      "Proof. If we could block diagonalize M, it would be easier to invert. To zero out the top right block of M we can pre-multiply as follows\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "I −FH−1 0\n",
      "\n",
      "I\n",
      "\n",
      "(cid:8) (cid:7)\n",
      "\n",
      "E F G H\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "E − FH−1G 0 H\n",
      "\n",
      "G\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.97)\n",
      "\n",
      "4.3.\n",
      "\n",
      "Inference in jointly Gaussian distributions\n",
      "\n",
      "117\n",
      "\n",
      "Similarly, to zero out the bottom left we can post-multiply as follows (cid:8)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "E − FH−1G 0 H\n",
      "\n",
      "G\n",
      "\n",
      "(cid:8) (cid:7)\n",
      "\n",
      "0 −H−1G I\n",
      "\n",
      "I\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "E − FH−1G 0 H\n",
      "\n",
      "0\n",
      "\n",
      "(4.98)\n",
      "\n",
      "Putting it all together we get\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "!\n",
      "\n",
      "I −FH−1 0\n",
      "\n",
      "\"# X\n",
      "\n",
      "I\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "$\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "!\n",
      "\n",
      "E F G H \"# M\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "$\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "!\n",
      "\n",
      "0 −H−1G I \"# Z\n",
      "\n",
      "I\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "$\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "!\n",
      "\n",
      "E − FH−1G 0 H\n",
      "\n",
      "0\n",
      "\n",
      "\"# W\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "$\n",
      "\n",
      "(4.99)\n",
      "\n",
      "Taking the inverse of both sides yields Z−1M−1X−1 = W−1\n",
      "\n",
      "(4.100)\n",
      "\n",
      "and hence\n",
      "\n",
      "M−1 = ZW−1X\n",
      "\n",
      "(4.101)\n",
      "\n",
      "Substituting in the deﬁnitions we get\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "E F G H\n",
      "\n",
      "(cid:8)−1\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "−H−1G(M/H)−1 H−1 + H−1G(M/H)−1FH−1\n",
      "\n",
      "0 −H−1G I\n",
      "\n",
      "−H−1G(M/H)−1 H−1\n",
      "\n",
      "I\n",
      "\n",
      "(M/H)−1\n",
      "\n",
      "(M/H)−1\n",
      "\n",
      "(cid:8) (cid:7)\n",
      "\n",
      "(M/H)−1 0\n",
      "\n",
      "0\n",
      "\n",
      "(cid:8) (cid:7)\n",
      "\n",
      "I −(M/H)−1FH−1\n",
      "\n",
      "0 H−1\n",
      "\n",
      "I −FH−1 0\n",
      "\n",
      "(cid:8) (cid:7)\n",
      "\n",
      "I −FH−1 0\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "I\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.102)\n",
      "\n",
      "(4.103)\n",
      "\n",
      "(4.104)\n",
      "\n",
      "Alternatively, we could have decomposed the matrix M in terms of E and M/E = (H − GE−1F), yielding\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "E F G H\n",
      "\n",
      "(cid:8)−1\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "E−1 + E−1F(M/E)−1GE−1 −E−1F(M/E)−1\n",
      "\n",
      "−(M/E)−1GE−1\n",
      "\n",
      "(M/E)−1\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.105)\n",
      "\n",
      "4.3.4.2\n",
      "\n",
      "The matrix inversion lemma\n",
      "\n",
      "We now derive some useful corollaries of the above result.\n",
      "\n",
      "Corollary 4.3.1 (Matrix inversion lemma). Consider a general partitioned matrix M = where we assume E and H are invertible. We have\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "E F G H\n",
      "\n",
      "(cid:8)\n",
      "\n",
      ",\n",
      "\n",
      "(E − FH−1G)−1 = E−1 + E−1F(H − GE−1F)−1GE−1\n",
      "\n",
      "(4.106)\n",
      "\n",
      "(E − FH−1G)−1FH−1 = E−1F(H − GE−1F)−1 |E − FH−1G| = |H − GE−1F||H−1||E|\n",
      "\n",
      "(4.107)\n",
      "\n",
      "(4.108)\n",
      "\n",
      "118\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "The ﬁrst two equations are s known as the matrix inversion lemma or the Sherman- Morrison-Woodbury formula. The third equation is known as the matrix determinant lemma. A typical application in machine learning/ statistics is the following. Let E = Σ let F = GT = X of size N × D, where N (cid:8) D, and let be a N × N diagonal matrix, H−1 = −I. Then we have\n",
      "\n",
      "(Σ + XXT )−1 = Σ−1 − Σ−1X(I + XT Σ−1X)−1XT Σ−1\n",
      "\n",
      "(4.109)\n",
      "\n",
      "The LHS takes O(N 3) time to compute, the RHS takes time O(D3) to compute.\n",
      "\n",
      "Another application concerns computing a rank one update of an inverse matrix. H = −1 (a scalar), F = u (a column vector), and G = vT (a row vector). Then we have\n",
      "\n",
      "Let\n",
      "\n",
      "(E + uvT )−1 = E−1 + E−1u(−1 − vT E−1u)−1vT E−1\n",
      "\n",
      "(4.110)\n",
      "\n",
      "= E−1 −\n",
      "\n",
      "E−1uvT E−1 1 + vT E−1u\n",
      "\n",
      "(4.111)\n",
      "\n",
      "This is useful when we incrementally add a data vector to a design matrix, and want to update our sufficient statistics. (One can derive an analogous formula for removing a data vector.)\n",
      "\n",
      "Proof. To prove Equation 4.106, we simply equate the top left block of Equation 4.93 and Equa- tion 4.94. To prove Equation 4.107, we simple equate the top right blocks of Equations 4.93 and 4.94. The proof of Equation 4.108 is left as an exercise.\n",
      "\n",
      "4.3.4.3\n",
      "\n",
      "Proof of Gaussian conditioning formulas\n",
      "\n",
      "We can now return to our original goal, which is to derive Equation 4.69. Let us factor the joint p(x1, x2) as p(x2)p(x1|x2) as follows: (cid:8)T (cid:7)\n",
      "\n",
      "E = exp\n",
      "\n",
      "%\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "x1 − μ1 x2 − μ2\n",
      "\n",
      "Σ11 Σ12 Σ21 Σ22\n",
      "\n",
      "(cid:8)−1 (cid:7)\n",
      "\n",
      "x1 − μ1 x2 − μ2\n",
      "\n",
      "(cid:8)&\n",
      "\n",
      "(4.112)\n",
      "\n",
      "Using Equation 4.102 the above exponent becomes\n",
      "\n",
      "E = exp (cid:7)\n",
      "\n",
      "= exp\n",
      "\n",
      "(x1 − μ1 − Σ12Σ−1\n",
      "\n",
      "×\n",
      "\n",
      "%\n",
      "\n",
      "x1 − μ1 x2 − μ2 I −Σ12Σ−1 22 0 (cid:5)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(x1 − μ1 − Σ12Σ−1\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "I\n",
      "\n",
      "22 (x2 − μ2))\n",
      "\n",
      "(cid:8) (cid:7)\n",
      "\n",
      "(cid:8)T (cid:7)\n",
      "\n",
      "x1 − μ1 x2 − μ2\n",
      "\n",
      "I −Σ−1 22 Σ21 (cid:8)’\n",
      "\n",
      "22 (x2 − μ2))T (Σ/Σ22)−1 (cid:5)\n",
      "\n",
      "(\n",
      "\n",
      "× exp\n",
      "\n",
      "0 I\n",
      "\n",
      "(cid:8) (cid:7)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(Σ/Σ22)−1 0\n",
      "\n",
      "(x2 − μ2)T Σ−1\n",
      "\n",
      "0 Σ−1 22\n",
      "\n",
      "22 (x2 − μ2)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "’\n",
      "\n",
      "(4.113)\n",
      "\n",
      "(4.114)\n",
      "\n",
      "(4.115)\n",
      "\n",
      "(4.116)\n",
      "\n",
      "This is of the form\n",
      "\n",
      "exp(quadratic form in x1, x2) × exp(quadratic form in x2)\n",
      "\n",
      "(4.117)\n",
      "\n",
      "4.4. Linear Gaussian systems\n",
      "\n",
      "119\n",
      "\n",
      "Hence we have successfully factorized the joint as\n",
      "\n",
      "p(x1, x2) =p( x1|x2)p(x2)\n",
      "\n",
      "(4.118)\n",
      "\n",
      "= N (x1|μ1|2, Σ1|2)N (x2|μ2, Σ22)\n",
      "\n",
      "(4.119)\n",
      "\n",
      "where the parameters of the conditional distribution can be read off from the above equations using\n",
      "\n",
      "μ1|2 = μ1 + Σ12Σ−1 22 (x2 − μ2) Σ1|2 = Σ/Σ22 = Σ11 − Σ12Σ−1 (4.121) We can also use the fact that |M| = |M/H||H| to check the normalization constants are\n",
      "\n",
      "22 Σ21\n",
      "\n",
      "(4.120)\n",
      "\n",
      "correct:\n",
      "\n",
      "(2π)(d1+d2)/2|Σ| 1\n",
      "\n",
      "2 = (2π)(d1+d2)/2(|Σ/Σ22| |Σ22|)\n",
      "\n",
      "1 2\n",
      "\n",
      "(4.122)\n",
      "\n",
      "= (2π)d1/2|Σ/Σ22| 1\n",
      "\n",
      "2 (2π)d2/2|Σ22| 1\n",
      "\n",
      "2\n",
      "\n",
      "(4.123)\n",
      "\n",
      "where d1 = dim(x1) and d2 = dim(x2).\n",
      "\n",
      "We leave the proof of the other forms of the result in Equation 4.69 as an exercise.\n",
      "\n",
      "4.4\n",
      "\n",
      "Linear Gaussian systems\n",
      "\n",
      "Suppose we have two variables, x and y. Let x ∈ RDx be a hidden variable, and y ∈ RDy be a noisy observation of x. Let us assume we have the following prior and likelihood:\n",
      "\n",
      "p(x) = N (x|μx, Σx) p(y|x) = N (y|Ax + b, Σy)\n",
      "\n",
      "(4.124)\n",
      "\n",
      "where A is a matrix of size Dy × Dx. This is an example of a linear Gaussian system. We can represent this schematically as x → y, meaning x generates y. In this section, we show how to “invert the arrow”, that is, how to infer x from y. We state the result below, then give several examples, and ﬁnally we derive the result. We will see many more applications of these results in later chapters.\n",
      "\n",
      "4.4.1\n",
      "\n",
      "Statement of the result\n",
      "\n",
      "Theorem 4.4.1 (Bayes rule for linear Gaussian systems). Given a linear Gaussian system, as in Equation 4.124, the posterior p(x|y) is given by the following:\n",
      "\n",
      "p(x|y) = N (x|μx|y, Σx|y) x + AT Σ−1 y A (y − b) +Σ −1\n",
      "\n",
      "Σ−1 x|y = Σ−1 μx|y = Σx|y[AT Σ−1 y\n",
      "\n",
      "x μx]\n",
      "\n",
      "(4.125)\n",
      "\n",
      "120\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "In addition, the normalization constant p(y) is given by\n",
      "\n",
      "p(y) = N (y|Aμx + b, Σy + AΣxAT )\n",
      "\n",
      "(4.126)\n",
      "\n",
      "For the proof, see Section 4.4.3.\n",
      "\n",
      "4.4.2\n",
      "\n",
      "Examples\n",
      "\n",
      "In this section, we give some example applications of the above result.\n",
      "\n",
      "4.4.2.1\n",
      "\n",
      "Inferring an unknown scalar from noisy measurements\n",
      "\n",
      "Suppose we make N noisy measurements yi of some underlying quantity x; let us assume the measurement noise has ﬁxed precision λy = 1/σ2, so the likelihood is\n",
      "\n",
      "p(yi|x) =N (yi|x, λ−1 y )\n",
      "\n",
      "(4.127)\n",
      "\n",
      "Now let us use a Gaussian prior for the value of the unknown source:\n",
      "\n",
      "p(x) =N (x|μ0, λ−1 0 )\n",
      "\n",
      "(4.128)\n",
      "\n",
      "We want to compute p(x|y1, . . . , yN , σ2). We can convert this to a form that lets us apply Bayes rule for Gaussians by deﬁning y = (y1, . . . , yN ), A = 1T N (an 1 × N row vector of 1’s), and Σ−1\n",
      "\n",
      "y = diag(λyI). Then we get\n",
      "\n",
      "p(x|y) =N (x|μN , λ−1 N ) λN = λ0 + N λy\n",
      "\n",
      "(4.129) (4.130)\n",
      "\n",
      "μN =\n",
      "\n",
      "N λyy + λ0μ0 λN\n",
      "\n",
      "=\n",
      "\n",
      "N λy N λy + λ0\n",
      "\n",
      "y +\n",
      "\n",
      "λ0 N λy + λ0\n",
      "\n",
      "μ0\n",
      "\n",
      "(4.131)\n",
      "\n",
      "These equations are quite intuitive: the posterior precision λN is the prior precision λ0 plus N units of measurement precision λy. Also, the posterior mean μN is a convex combination of the MLE y and the prior mean μ0. This makes it clear that the posterior mean is a compromise If the prior is weak relative to the signal strength (λ0 is between the MLE and the prior. small relative to λy), we put more weight on the MLE. If the prior is strong relative to the signal strength (λ0 is large relative to λy), we put more weight on the prior. This is illustrated in Figure 4.12, which is very similar to the analogous results for the beta-binomial model in Figure 3.6.\n",
      "\n",
      "of precision λy is like having one measurement with value y and precision N λy.\n",
      "\n",
      "Note that the posterior mean is written in terms of N λyy, so having N measurements each\n",
      "\n",
      "We can rewrite the results in terms of the posterior variance, rather than posterior precision,\n",
      "\n",
      "4.4. Linear Gaussian systems\n",
      "\n",
      "121\n",
      "\n",
      "prior variance = 1.00\n",
      "\n",
      "prior variance = 5.00\n",
      "\n",
      "0.6\n",
      "\n",
      "prior lik post\n",
      "\n",
      "0.6\n",
      "\n",
      "prior lik post\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "0 −5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "Figure 4.12 Inference about x given a noisy observation y = 3. (a) Strong prior N (0, 1). The posterior (a) Weak prior N (0, 5). The posterior mean is mean is “shrunk” towards the prior mean, which is 0. similar to the MLE. Figure generated by gaussInferParamsMean1d.\n",
      "\n",
      "as follows:\n",
      "\n",
      "p(x|D, σ2) =N (x|μN , τ 2 N )\n",
      "\n",
      "(4.132)\n",
      "\n",
      "where τ 2\n",
      "\n",
      "1 σ2 + 1 τ 2 (cid:7) 0 μ0 τ 2 0 0 = 1/λ0 is the prior variance and τ 2\n",
      "\n",
      "μN = τ 2 N\n",
      "\n",
      "τ 2 N =\n",
      "\n",
      "N\n",
      "\n",
      "+\n",
      "\n",
      "=\n",
      "\n",
      "N y σ2\n",
      "\n",
      "σ2τ 2 0 N τ 2 0 + σ2 (cid:8)\n",
      "\n",
      "=\n",
      "\n",
      "N τ 2\n",
      "\n",
      "σ2 0 + σ2 N = 1/λN is the posterior variance.\n",
      "\n",
      "μ0 +\n",
      "\n",
      "N τ 2\n",
      "\n",
      "N τ 2 0 0 + σ2\n",
      "\n",
      "y\n",
      "\n",
      "(4.133)\n",
      "\n",
      "(4.134)\n",
      "\n",
      "If We can also compute the posterior sequentially, by updating after each observation. N = 1, we can rewrite the posterior after seeing a single observation as follows (where we deﬁne Σy = σ2, Σ0 = τ 2 1 to be the variances of the likelihood, prior and posterior):\n",
      "\n",
      "0 and Σ1 = τ 2\n",
      "\n",
      "p(x|y) =N (x|μ1, Σ1) (cid:7)\n",
      "\n",
      "Σ1 =\n",
      "\n",
      "μ1 = Σ1\n",
      "\n",
      "1 Σ0 (cid:7)\n",
      "\n",
      "μ0 Σ0\n",
      "\n",
      "+\n",
      "\n",
      "1 Σy\n",
      "\n",
      "+\n",
      "\n",
      "(cid:8)−1\n",
      "\n",
      "Σy\n",
      "\n",
      "y\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "=\n",
      "\n",
      "ΣyΣ0 Σ0 + Σy\n",
      "\n",
      "(4.135)\n",
      "\n",
      "(4.136)\n",
      "\n",
      "(4.137)\n",
      "\n",
      "We can rewrite the posterior mean in 3 different ways:\n",
      "\n",
      "μ1 =\n",
      "\n",
      "Σy Σy + Σ0\n",
      "\n",
      "μ0 +\n",
      "\n",
      "= μ0 + (y − μ0)\n",
      "\n",
      "Σ0 Σy + Σ0 Σ0 Σy + Σ0\n",
      "\n",
      "y\n",
      "\n",
      "(4.138)\n",
      "\n",
      "(4.139)\n",
      "\n",
      "= y − (y − μ0)\n",
      "\n",
      "Σy Σy + Σ0\n",
      "\n",
      "(4.140)\n",
      "\n",
      "122\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "The ﬁrst equation is a convex combination of the prior and the data. The second equation is the prior mean adjusted towards the data. The third equation is the data adjusted towards the prior mean; this is called shrinkage. These are all equivalent ways of expressing the tradeoff between likelihood and prior. If Σ0 is small relative to ΣY , corresponding to a strong prior, the amount of shrinkage is large (see Figure 4.12(a)), whereas if Σ0 is large relative to Σy, corresponding to a weak prior, the amount of shrinkage is small (see Figure 4.12(b)).\n",
      "\n",
      "Another way to quantify the amount of shrinkage is in terms of the signal-to-noise ratio,\n",
      "\n",
      "which is deﬁned as follows: (cid:20)\n",
      "\n",
      "SNR (cid:2)\n",
      "\n",
      "X 2 E E [(cid:15)2]\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "=\n",
      "\n",
      "Σ0 + μ2 0 Σy\n",
      "\n",
      "(4.141)\n",
      "\n",
      "where x ∼ N (μ0, Σ0) is the true signal, y = x + (cid:15) is the observed signal, and (cid:15) ∼ N (0, Σy) is the noise term.\n",
      "\n",
      "4.4.2.2\n",
      "\n",
      "Inferring an unknown vector from noisy measurements\n",
      "\n",
      "Now consider N vector-valued observations, yi ∼ N (x, Σy), and a Gaussian prior, x ∼ N (μ0, Σ0). Setting A = I, b = 0, and using y for the effective observation with precision N Σ−1\n",
      "\n",
      "y , we have\n",
      "\n",
      "p(x|y1, . . . , yN ) =N (x|μN , ΣN ) 0 + N Σ−1 y\n",
      "\n",
      "Σ−1 N = Σ−1 μN = ΣN (Σ−1\n",
      "\n",
      "y (N y) +Σ −1\n",
      "\n",
      "0 μ0)\n",
      "\n",
      "(4.142)\n",
      "\n",
      "(4.143)\n",
      "\n",
      "(4.144)\n",
      "\n",
      "See Figure 4.13 for a 2d example. We can think of x as representing the true, but unknown, location of an object in 2d space, such as a missile or airplane, and the yi as being noisy observations, such as radar “blips”. As we receive more blips, we are better able to localize the source. In Section 18.3.1, we will see how to extend this example to track moving objects using the famous Kalman ﬁlter algorithm.\n",
      "\n",
      "Now suppose we have multiple measuring devices, and we want to combine them together; this is known as sensor fusion. If we have multiple observations with different covariances (cor- responding to sensors with different reliabilities), the posterior will be an appropriate weighted average of the data. Consider the example in Figure 4.14. We use an uninformative prior on x, namely p(x) = N (μ0, Σ0) = N (0, 1010I2). We get 2 noisy observations, y1 ∼ N (x, Σy,1) and y2 ∼ N (x, Σy,2). We then compute p(x|y1, y2).\n",
      "\n",
      "In Figure 4.14(a), we set Σy,1 = Σy,2 = 0.01I2, so both sensors are equally reliable. In this case, the posterior mean is half way between the two observations, y1 and y2. In Figure 4.14(b), we set Σy,1 = 0.05I2 and Σy,2 = 0.01I2, so sensor 2 is more reliable than sensor 1. In this case, the posterior mean is closer to y2. In Figure 4.14(c), we set\n",
      "\n",
      "Σy,1 = 0.01\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "10 1 1 1\n",
      "\n",
      "(cid:8)\n",
      "\n",
      ", Σy,2 = 0.01\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "1 1\n",
      "\n",
      "1 10\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.145)\n",
      "\n",
      "so sensor 1 is more reliable in the y2 component (vertical direction), and sensor 2 is more In this case, the posterior mean uses y1’s reliable in the y1 component (horizontal direction). vertical component and y2’s horizontal component.\n",
      "\n",
      "4.4. Linear Gaussian systems\n",
      "\n",
      "123\n",
      "\n",
      "data\n",
      "\n",
      "prior\n",
      "\n",
      "post after 10 obs\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "Figure 4.13 Illustration of Bayesian inference for the mean of a 2d Gaussian. (a) The data is generated from yi ∼ N (x, Σy), where x = [0.5, 0.5]T and Σy = 0.1[2, 1; 1, 1]). We assume the sensor noise covariance Σy is known but x is unknown. The black cross represents x. (b) The prior is p(x) = N (x|0, 0.1I2). (c) We show the posterior after 10 data points have been observed. Figure generated by gaussInferParamsMean2d.\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "0\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.8\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−1.2\n",
      "\n",
      "−1\n",
      "\n",
      "−1.2\n",
      "\n",
      "−1.4\n",
      "\n",
      "−1.4\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "1.2\n",
      "\n",
      "1.4\n",
      "\n",
      "−1.6\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "1.2\n",
      "\n",
      "1.4\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 4.14 We observe y1 = (0, −1) (red cross) and y2 = (1, 0) (green cross) and infer E(μ|y1, y2, θ) (a) Equally reliable sensors, so the posterior mean estimate is in between the two circles. (black cross). (b) Sensor 2 is more reliable, so the estimate shifts more towards the green circle. (c) Sensor 1 is more reliable in the vertical direction, Sensor 2 is more reliable in the horizontal direction. The estimate is an appropriate combination of the two measurements. Figure generated by sensorFusion2d.\n",
      "\n",
      "Note that this technique crucially relies on modeling our uncertainty of each sensor; comput- ing an unweighted average would give the wrong result. However, we have assumed the sensor precisions are known. When they are not, we should model out uncertainty about Σ1 and Σ2 as well. See Section 4.6.4 for details.\n",
      "\n",
      "4.4.2.3\n",
      "\n",
      "Interpolating noisy data\n",
      "\n",
      "We now revisit the example of Section 4.3.2.2. This time we no longer assume noise-free let us assume that we obtain N noisy observations yi; without loss observations. of generality, assume these correspond to x1, . . . , xN . We can model this setup as a linear\n",
      "\n",
      "Instead,\n",
      "\n",
      "124\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "Gaussian system: y = Ax + (cid:7)\n",
      "\n",
      "(4.146) where (cid:7) ∼ N (0, Σy), Σy = σ2I, σ2 is the observation noise, and A is a N × D projection matrix that selects out the observed elements. For example, if N = 2 and D = 4 we have\n",
      "\n",
      "A =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "1 0 0 1\n",
      "\n",
      "0 0\n",
      "\n",
      "(cid:8) 0 0\n",
      "\n",
      "(4.147)\n",
      "\n",
      "Using the same improper prior as before, Σx = (LT L)−1, we can easily compute the posterior mean and variance. In Figure 4.15, we plot the posterior mean, posterior variance, and some posterior samples. Now we see that the prior precision λ effects the posterior mean as well as the posterior variance. In particular, for a strong prior (large λ), the estimate is very smooth, and the uncertainty is low. but for a weak prior (small λ), the estimate is wiggly, and the uncertainty (away from the data) is high.\n",
      "\n",
      "The posterior mean can also be computed by solving the following optimization problem:\n",
      "\n",
      "min x\n",
      "\n",
      "1 2σ2\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − yi)2 +\n",
      "\n",
      "i=1\n",
      "\n",
      "λ\n",
      "\n",
      "2\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "j=1\n",
      "\n",
      ")\n",
      "\n",
      "(xj − xj−1)\n",
      "\n",
      "2\n",
      "\n",
      "+ (xj − xj+1)\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "(4.148)\n",
      "\n",
      "where we have deﬁned x0 = x1 and xD+1 = xD for notational simplicity. We recognize this as a discrete approximation to the following problem:\n",
      "\n",
      "min f\n",
      "\n",
      "1 2σ2\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(f (t) − y(t))2dt +\n",
      "\n",
      "λ\n",
      "\n",
      "2\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "[f (cid:2)(t)]2dt\n",
      "\n",
      "(4.149)\n",
      "\n",
      "where f (cid:2)(t) is the ﬁrst derivative of f . The ﬁrst term measures ﬁt to the data, and the second term penalizes functions that are “too wiggly”. This is an example of Tikhonov regularization, which is a popular approach to functional data analysis. See Chapter 15 for more sophisticated approaches, which enforce higher order smoothness (so the resulting samples look less “jagged”).\n",
      "\n",
      "4.4.3\n",
      "\n",
      "Proof of the result *\n",
      "\n",
      "We now derive Equation 4.125. The basic idea is to derive the joint distribution, p(x, y) = p(x)p(y|x), and then to use the results from Section 4.3.1 for computing p(x|y).\n",
      "\n",
      "In more detail, we proceed as follows. The log of the joint distribution is as follows (dropping\n",
      "\n",
      "irrelevant constants): log p(x, y) = − 1 2\n",
      "\n",
      "(x − μx)T Σ−1\n",
      "\n",
      "x (x − μx) − 1 2\n",
      "\n",
      "(y − Ax − b)T Σ−1\n",
      "\n",
      "y (y − Ax − b)\n",
      "\n",
      "(4.150)\n",
      "\n",
      "This is clearly a joint Gaussian distribution, since it is the exponential of a quadratic form.\n",
      "\n",
      "Expanding out the quadratic terms involving x and y, and ignoring linear and constant terms,\n",
      "\n",
      "we have\n",
      "\n",
      "Q = − 1 2 = − 1 2 = − 1 2\n",
      "\n",
      "xT Σ−1 (cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "x y\n",
      "\n",
      "x y\n",
      "\n",
      "x x − 1 2 (cid:8)T (cid:7) x + AT Σ−1 Σ−1 −Σ−1 (cid:8)\n",
      "\n",
      "(cid:8)T\n",
      "\n",
      "Σ−1\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "yT Σ−1\n",
      "\n",
      "x y\n",
      "\n",
      "y A\n",
      "\n",
      "y y − 1 2\n",
      "\n",
      "y A −AT Σ−1 y\n",
      "\n",
      "(Ax)T Σ−1\n",
      "\n",
      "Σ−1 y\n",
      "\n",
      "y (Ax) +y T Σ−1 (cid:8)\n",
      "\n",
      "(cid:8) (cid:7)\n",
      "\n",
      "x y\n",
      "\n",
      "y Ax\n",
      "\n",
      "(4.151)\n",
      "\n",
      "(4.152)\n",
      "\n",
      "(4.153)\n",
      "\n",
      "4.5. Digression: The Wishart distribution *\n",
      "\n",
      "125\n",
      "\n",
      "λ=30\n",
      "\n",
      "λ=0p1\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 4.15 λ = 30. generated by gaussInterpNoisyDemo. See also splineBasisDemo.\n",
      "\n",
      "Interpolating noisy data (noise variance σ2 = 1) using a Gaussian with prior precision λ. (a) (b) λ = 0.01. See also Figure 4.10. Based on Figure 7.1 of (Calvetti and Somersalo 2007). Figure\n",
      "\n",
      "where the precision matrix of the joint is deﬁned as\n",
      "\n",
      "Σ−1 =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Σ−1\n",
      "\n",
      "x + AT Σ−1 −Σ−1\n",
      "\n",
      "y A\n",
      "\n",
      "y A −AT Σ−1 y\n",
      "\n",
      "Σ−1 y\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:2) Λ =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Λxx Λxy Λyx Λyy\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.154)\n",
      "\n",
      "From Equation 4.69, and using the fact that μy = Aμx + b, we have\n",
      "\n",
      "p(x|y) =N (μx|y, Σx|y) xx = (Σ−1\n",
      "\n",
      "Σx|y = Λ−1 μx|y = Σx|y = Σx|y\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "x + AT Σ−1 y A)−1 (cid:19) Λxxμx − Λxy(y − μy) Σ−1\n",
      "\n",
      "x μ + AT Σ−1\n",
      "\n",
      "y (y − b)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(4.155)\n",
      "\n",
      "(4.156)\n",
      "\n",
      "(4.157)\n",
      "\n",
      "(4.158)\n",
      "\n",
      "4.5 Digression: The Wishart distribution *\n",
      "\n",
      "The Wishart distribution is the generalization of the Gamma distribution to positive deﬁnite matrices. Press (Press 2005, p107) has said “The Wishart distribution ranks next to the (multi- variate) normal distribution in order of importance and usefuleness in multivariate statistics”. We will mostly use it to model our uncertainty in covariance matrices, Σ, or their inverses, Λ = Σ−1.\n",
      "\n",
      "The pdf of the Wishart is deﬁned as follows: (cid:7)\n",
      "\n",
      "Wi(Λ|S, ν) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "Wi\n",
      "\n",
      "|Λ|(ν−D−1)/2 exp\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "tr(ΛS−1)\n",
      "\n",
      "(4.159)\n",
      "\n",
      "Here ν is called the “degrees of freedom” and S is the “scale matrix”. (We shall get more intuition for these parameters shortly.) The normalization constant for this distribution (which\n",
      "\n",
      "126\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "requires integrating over all symmetric pd matrices) is the following formidable expression\n",
      "\n",
      "Z\n",
      "\n",
      "Wi = 2νD/2ΓD(ν/2)|S|ν/2\n",
      "\n",
      "(4.160)\n",
      "\n",
      "where ΓD(a) is the multivariate gamma function:\n",
      "\n",
      "ΓD(x) =π D(D−1)/4\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "Γ (x + (1− i)/2)\n",
      "\n",
      "(4.161)\n",
      "\n",
      "i=1\n",
      "\n",
      "Hence Γ1(a) = Γ(a) and\n",
      "\n",
      "ΓD(ν0/2) =\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "i=1\n",
      "\n",
      "Γ(\n",
      "\n",
      "ν0 + 1− i 2\n",
      "\n",
      ")\n",
      "\n",
      "(4.162)\n",
      "\n",
      "The normalization constant only exists (and hence the pdf is only well deﬁned) if ν > D − 1.\n",
      "\n",
      "In particular, let xi ∼ N (0, Σ). Then the scatter matrix S = i has a Wishart distribution: S ∼ Wi(Σ, 1). Hence E [S] = N Σ. More generally, one can show that the mean and mode of Wi(S, ν) are given by\n",
      "\n",
      "There is a connection between the Wishart distribution and the Gaussian.\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      "i=1 xixT\n",
      "\n",
      "mean = νS, mode = (ν − D − 1)S\n",
      "\n",
      "(4.163)\n",
      "\n",
      "where the mode only exists if ν > D + 1.\n",
      "\n",
      "If D = 1, the Wishart reduces to the Gamma distribution: s\n",
      "\n",
      "Wi(λ|s−1, ν) = Ga(λ|\n",
      "\n",
      "ν\n",
      "\n",
      "2\n",
      "\n",
      ",\n",
      "\n",
      "2\n",
      "\n",
      ")\n",
      "\n",
      "(4.164)\n",
      "\n",
      "4.5.1\n",
      "\n",
      "Inverse Wishart distribution Recall that we showed (Exercise 2.10) that if λ ∼ Ga(a, b), then that 1 λ ∼ IG(a, b). Similarly, if Σ−1 ∼ Wi(S, ν) then Σ ∼ IW(S−1, ν + D + 1), where IW is the inverse Wishart, the multidimensional generalization of the inverse Gamma. It is deﬁned as follows, for ν > D − 1 and S (cid:13) 0:\n",
      "\n",
      "IW(Σ|S, ν) =\n",
      "\n",
      "1 ZIW\n",
      "\n",
      "|Σ|−(ν+D+1)/2 exp\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "tr(S−1Σ−1)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.165)\n",
      "\n",
      "ZIW = |S|−ν/22νD/2ΓD(ν/2)\n",
      "\n",
      "(4.166)\n",
      "\n",
      "One can show that the distribution has these properties\n",
      "\n",
      "mean =\n",
      "\n",
      "S−1 ν − D − 1\n",
      "\n",
      ", mode =\n",
      "\n",
      "S−1 ν + D + 1\n",
      "\n",
      "(4.167)\n",
      "\n",
      "If D = 1, this reduces to the inverse Gamma:\n",
      "\n",
      "IW(σ2|S−1, ν) = IG(σ2|ν/2, S/2)\n",
      "\n",
      "(4.168)\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "127\n",
      "\n",
      "Wi(dof=3.0, S), E=[9.5, −0.1; −0.1, 1.9], ρ=−0.0\n",
      "\n",
      "σ2 1\n",
      "\n",
      "ρ(1,2)\n",
      "\n",
      "0.08\n",
      "\n",
      "0.8\n",
      "\n",
      "2\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "0.06\n",
      "\n",
      "0.6\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−4 −2 0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "0.04\n",
      "\n",
      "0.02\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "4 2 0 −2 −4\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0 −2\n",
      "\n",
      "0.4\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "σ2 2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−4 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "0.3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "5\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "−2\n",
      "\n",
      "−4 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "−5\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 4.16 Visualization of the Wishart distribution. Left: Some samples from the Wishart distribution, Σ ∼ Wi(S, ν), where S = [3.1653, −0.0262; −0.0262, 0.6477] and ν = 3. Right: Plots of the marginals (which are Gamma), and the approximate (sample-based) marginal on the correlation coefficient. If ν = 3 there is a lot of uncertainty about the value of the correlation coefficient ρ (see the almost uniform distribution on [−1, 1]). The sampled matrices are highly variable, and some are nearly singular. As ν increases, the sampled matrices are more concentrated on the prior S. Figure generated by wiPlotDemo.\n",
      "\n",
      "4.5.2\n",
      "\n",
      "Visualizing the Wishart distribution *\n",
      "\n",
      "Since the Wishart is a distribution over matrices, it is hard to plot as a density function. However, we can easily sample from it, and in the 2d case, we can use the eigenvectors of the resulting matrix to deﬁne an ellipse, as explained in Section 4.1.2. See Figure 4.16 for some examples.\n",
      "\n",
      "For higher dimensional matrices, we can plot marginals of the distribution. The diagonals of a Wishart distributed matrix have Gamma distributions, so are easy to plot. It is hard in general to work out the distribution of the off-diagonal elements, but we can sample matrices from In particular, we can convert the distribution, and then compute the distribution empirically. each sampled matrix to a correlation matrix, and thus compute a Monte Carlo approximation (Section 2.7) to the expected correlation coefficients:\n",
      "\n",
      "E [Rij] ≈ 1 S\n",
      "\n",
      "S(cid:6)\n",
      "\n",
      "s=1\n",
      "\n",
      "R(Σ(s))ij\n",
      "\n",
      "(4.169)\n",
      "\n",
      "where Σ(s) ∼ Wi(Σ, ν) and R(Σ) converts matrix Σ into a correlation matrix:\n",
      "\n",
      "Rij =\n",
      "\n",
      "Σij(cid:9)\n",
      "\n",
      "ΣiiΣjj\n",
      "\n",
      "(4.170)\n",
      "\n",
      "We can then use kernel density estimation (Section 14.7.2) to produce a smooth approximation to the univariate density E [Rij] for plotting purposes. See Figure 4.16 for some examples.\n",
      "\n",
      "4.6\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "So far, we have discussed inference in a Gaussian assuming the parameters θ = (μ, Σ) are known. We now discuss how to infer the parameters themselves. We will assume the data has\n",
      "\n",
      "128\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "the form xi ∼ N (μ, Σ) for i = 1 :N and is fully observed, so we have no missing data (see Section 11.6.1 for how to estimate parameters of an MVN in the presence of missing values). To simplify the presentation, we derive the posterior in three parts: ﬁrst we compute p(μ|D, Σ); then we compute p(Σ|D, μ); ﬁnally we compute the joint p(μ, Σ|D).\n",
      "\n",
      "4.6.1\n",
      "\n",
      "Posterior distribution of μ We have discussed how to compute the MLE for μ; we now discuss how to compute its posterior, which is useful for modeling our uncertainty about its value.\n",
      "\n",
      "The likelihood has the form p(D|μ) = N (x|μ, 1 N\n",
      "\n",
      "Σ)\n",
      "\n",
      "(4.171)\n",
      "\n",
      "For simplicity, we will use a conjugate prior, which in this case is a Gaussian. In particular, if p(μ) = N (μ|m0, V0) then we can derive a Gaussian posterior for μ based on the results in Section 4.4.2.2. We get\n",
      "\n",
      "p(μ|D, Σ) =N (μ|mN , VN ) 0 + N Σ−1 V−1 N = V−1 mN = VN (Σ−1(N x) +V −1\n",
      "\n",
      "0 m0)\n",
      "\n",
      "(4.172)\n",
      "\n",
      "(4.173)\n",
      "\n",
      "(4.174)\n",
      "\n",
      "This is exactly the same process as inferring the location of an object based on noisy radar “blips”, except now we are inferring the mean of a distribution based on noisy samples. (To a Bayesian, there is no difference between uncertainty about parameters and uncertainty about anything else.)\n",
      "\n",
      "N (x, 1 goes down as 1/N , which is a standard result from frequentist statistics.\n",
      "\n",
      "We can model an uninformative prior by setting V0 = ∞I. In this case we have p(μ|D, Σ) = N Σ), so the posterior mean is equal to the MLE. We also see that the posterior variance\n",
      "\n",
      "4.6.2\n",
      "\n",
      "Posterior distribution of Σ * We now discuss how to compute p(Σ|D, μ). The likelihood has the form\n",
      "\n",
      "p(D|μ, Σ) ∝ |Σ|− N\n",
      "\n",
      "2 exp\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "tr(SμΣ−1)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.175)\n",
      "\n",
      "The corresponding conjugate prior is known as the inverse Wishart distribution (Section 4.5.1). Recall that this has the following pdf:\n",
      "\n",
      "IW(Σ|S−1\n",
      "\n",
      "0 , ν0) ∝ |Σ|−(ν0+D+1)/2 exp\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "tr(S0Σ−1)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.176)\n",
      "\n",
      "Here ν0 > D − 1 is the degrees of freedom (dof), and S0 is a symmetric pd matrix. We see that S−1 0 plays the role of the prior scatter matrix, and N0 (cid:2) ν0 + D + 1 controls the strength of the prior, and hence plays a role analogous to the sample size N .\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "129\n",
      "\n",
      "N=100, D=50\n",
      "\n",
      "N=50, D=50\n",
      "\n",
      "N=25, D=50\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "true, k=10.00 MLE, k= 71 MAP, k=8.62\n",
      "\n",
      "true, k=10.00 MLE, k=1.7e+17 MAP, k=8.85\n",
      "\n",
      "true, k=10.00 MLE, k=2.2e+18 MAP, k=21.09\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "e u a v n e g e\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "e u a v n e g e\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "e u a v n e g e\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "Figure 4.17 Estimating a covariance matrix in D = 50 dimensions using N ∈ {100, 50, 25} samples. We plot the eigenvalues in descending order for the true covariance matrix (solid black), the MLE (dotted blue) and the MAP estimate (dashed red), using Equation 4.184 with λ = 0.9. We also list the condition number of each matrix in the legend. Based on Figure 1 of (Schaefer and Strimmer 2005). Figure generated by shrinkcovDemo.\n",
      "\n",
      "Multiplying the likelihood and prior we ﬁnd that the posterior is also inverse Wishart:\n",
      "\n",
      "p(Σ|D, μ) ∝ |Σ|− N (cid:7)\n",
      "\n",
      "= |Σ|− N +(ν0+D+1)\n",
      "\n",
      "exp\n",
      "\n",
      "2 exp\n",
      "\n",
      "− 1 2\n",
      "\n",
      "tr(Σ−1S0)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "exp\n",
      "\n",
      "tr(Σ−1Sμ) (cid:8)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "tr\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Σ−1(Sμ + S0)\n",
      "\n",
      "|Σ|−(ν0+D+1)/2\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.177)\n",
      "\n",
      "(4.178)\n",
      "\n",
      "= IW(Σ|SN , νN )\n",
      "\n",
      "(4.179)\n",
      "\n",
      "(4.181) In words, this says that the posterior strength νN is the prior strength ν0 plus the number of observations N , and the posterior scatter matrix SN is the prior scatter matrix S0 plus the data scatter matrix Sμ.\n",
      "\n",
      "νN = ν0 + N S−1 N = S0 + Sμ\n",
      "\n",
      "(4.180)\n",
      "\n",
      "4.6.2.1 MAP estimation\n",
      "\n",
      "We see from Equation 4.7 that ˆΣmle is a rank min(N, D) matrix. If N < D, this is not full rank, and hence will be uninvertible. And even if N > D, it may be the case that ˆΣ is ill-conditioned (meaning it is nearly singular).\n",
      "\n",
      "To solve these problems, we can use the posterior mode (or mean). One can show (using\n",
      "\n",
      "techniques analogous to the derivation of the MLE) that the MAP estimate is given by\n",
      "\n",
      "ˆΣmap =\n",
      "\n",
      "SN νN + D + 1\n",
      "\n",
      "=\n",
      "\n",
      "S0 + Sμ N0 + N\n",
      "\n",
      "(4.182)\n",
      "\n",
      "If we use an improper uniform prior, corresponding to N0 = 0 and S0 = 0, we recover the MLE.\n",
      "\n",
      "130\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "Let us now consider the use of a proper informative prior, which is necessary whenever D/N is large (say bigger than 0.1). Let μ = x, so Sμ = Sx. Then we can rewrite the MAP estimate as a convex combination of the prior mode and the MLE. To see this, let Σ0 (cid:2) S0 N0 be the prior mode. Then the posterior mode can be rewritten as\n",
      "\n",
      "ˆΣmap =\n",
      "\n",
      "S0 + Sx N0 + N\n",
      "\n",
      "=\n",
      "\n",
      "N0 N0 + N\n",
      "\n",
      "S0 N0\n",
      "\n",
      "+\n",
      "\n",
      "N N0 + N\n",
      "\n",
      "S N\n",
      "\n",
      "= λΣ0 + (1− λ) ˆΣmle\n",
      "\n",
      "(4.183)\n",
      "\n",
      "where λ = N0\n",
      "\n",
      "This begs the question: where do the parameters of the prior come from? It is common to set λ by cross validation. Alternatively, we can use the closed-form formula provided in (Ledoit and Wolf 2004b,a; Schaefer and Strimmer 2005), which is the optimal frequentist estimate if we use squared loss. This is arguably not the most natural loss function for covariance matrices (because it ignores the postive deﬁnite constraint), but it results in a simple estimator, which is implemented in the PMTK function shrinkcov. We discuss Bayesian ways of estimating λ later.\n",
      "\n",
      "prior: S0 = diag( ˆΣmle). In this case, the MAP estimate is given by\n",
      "\n",
      "As for the prior covariance matrix, S0, it is common to use the following (data dependent)\n",
      "\n",
      "N0+N , controls the amount of shrinkage towards the prior.\n",
      "\n",
      "ˆΣmap(i, j) =\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "ˆΣmle(i, j) (1 − λ) ˆΣmle(i, j)\n",
      "\n",
      "if i = j otherwise\n",
      "\n",
      "(4.184)\n",
      "\n",
      "Thus we see that the diagonal entries are equal to their ML estimates, and the off diago- nal elements are “shrunk” somewhat towards 0. This technique is therefore called shrinkage estimation, orregularized estimation.\n",
      "\n",
      "The beneﬁts of MAP estimation are illustrated in Figure 4.17. We consider ﬁtting a 50 dimen- sional Gaussian to N = 100, N = 50 and N = 25 data points. We see that the MAP estimate is always well-conditioned, unlike the MLE. In particular, we see that the eigenvalue spectrum of the MAP estimate is much closer to that of the true matrix than the MLE’s. The eigenvectors, however, are unaffected.\n",
      "\n",
      "The importance of regularizing the estimate of Σ will become apparent in later chapters,\n",
      "\n",
      "when we consider ﬁtting covariance matrices to high dimensional data.\n",
      "\n",
      "4.6.2.2\n",
      "\n",
      "Univariate posterior\n",
      "\n",
      "In the 1d case, the likelihood has the form\n",
      "\n",
      "p(D|σ2) ∝ (σ2)−N/2 exp\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "− 1 2σ2\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "i=1\n",
      "\n",
      "(xi − μ)2\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(4.185)\n",
      "\n",
      "The standard conjugate prior is the inverse Gamma distribution, which is just the scalar version of the inverse Wishart:\n",
      "\n",
      "IG(σ2|a0, b0) ∝ (σ2)−(a0+1) exp(−\n",
      "\n",
      "b0 σ2 )\n",
      "\n",
      "(4.186)\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "131\n",
      "\n",
      "0.35\n",
      "\n",
      "0.3\n",
      "\n",
      "N=2 N=5 N=50 N=100\n",
      "\n",
      "prior = IW(ν=0.001, S=0.001), true σ2=10.000\n",
      "\n",
      "0.25\n",
      "\n",
      "0.2\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "σ2\n",
      "\n",
      "Figure 4.18 Sequential updating of the posterior for σ2 starting from an uninformative prior. The data was generated from a Gaussian with known mean μ = 5 and unknown variance σ2 = 10. Figure generated by gaussSeqUpdateSigma1D.\n",
      "\n",
      "Multiplying the likelihood and the prior, we see that the posterior is also IG:\n",
      "\n",
      "p(σ2|D) = IG(σ2|aN , bN ) aN = a0 + N/2 N(cid:6)\n",
      "\n",
      "bN = b0 +\n",
      "\n",
      "1 2\n",
      "\n",
      "i=1\n",
      "\n",
      "(xi − μ)2\n",
      "\n",
      "(4.187)\n",
      "\n",
      "(4.188)\n",
      "\n",
      "(4.189)\n",
      "\n",
      "See Figure 4.18 for an illustration.\n",
      "\n",
      "The form of the posterior is not quite as pretty as the multivariate case, because of the factors of 1 2 ). Another problem with using the IG(a0, b0) distribution is that the strength of the prior is encoded in both a0 and b0. To avoid both of these problems, it is common (in the statistics literature) to use an alternative parameterization of the IG distribution, known as the (scaled) inverse chi-squared distribution. This is deﬁned as follows:\n",
      "\n",
      "2 . This arises because IW(σ2|s0, ν0) = IG(σ2| s0\n",
      "\n",
      "2 , ν0\n",
      "\n",
      "χ−2(σ2|ν0, σ2\n",
      "\n",
      "0) = IG(σ2|\n",
      "\n",
      "ν0 2\n",
      "\n",
      ",\n",
      "\n",
      "ν0σ2 0 2\n",
      "\n",
      ") ∝ (σ2)−ν0/2−1 exp(−\n",
      "\n",
      "ν0σ2 0 2σ2 )\n",
      "\n",
      "(4.190)\n",
      "\n",
      "Here ν0 controls the strength of the prior, and σ2 prior, the posterior becomes\n",
      "\n",
      "0 encodes the value of the prior. With this\n",
      "\n",
      "p(σ2|D, μ) =χ −2(σ2|νN , σ2 νN = ν0 + N 0 +\n",
      "\n",
      "σ2\n",
      "\n",
      "N =\n",
      "\n",
      "ν0σ2\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      "i=1(xi − μ)2 νN\n",
      "\n",
      "N )\n",
      "\n",
      "(4.191)\n",
      "\n",
      "(4.192)\n",
      "\n",
      "(4.193)\n",
      "\n",
      "We see that the posterior dof νN is the prior dof ν0 plus N , and the posterior sum of squares νN σ2\n",
      "\n",
      "We can emulate an uninformative prior, p(σ2) ∝ σ−2, by setting ν0 = 0, which makes\n",
      "\n",
      "N is the prior sum of squares ν0σ2\n",
      "\n",
      "0 plus the data sum of squares.\n",
      "\n",
      "intuitive sense (since it corresponds to a zero virtual sample size).\n",
      "\n",
      "132\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "4.6.3\n",
      "\n",
      "Posterior distribution of μ and Σ * We now discuss how to compute p(μ, Σ|D). These results are a bit complex, but will prove useful later on in this book. Feel free to skip this section on a ﬁrst reading.\n",
      "\n",
      "4.6.3.1\n",
      "\n",
      "Likelihood\n",
      "\n",
      "The likelihood is given by\n",
      "\n",
      "p(D|μ, Σ) = (2π)−N D/2|Σ|− N\n",
      "\n",
      "2 exp\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − μ)T Σ−1(xi − μ)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "i=1\n",
      "\n",
      "(4.194)\n",
      "\n",
      "Now one can show that\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − μ)T Σ−1(xi − μ) = tr(Σ−1Sx) +N (x − μ)T Σ−1(x − μ)\n",
      "\n",
      "(4.195)\n",
      "\n",
      "i=1\n",
      "\n",
      "Hence we can rewrite the likelihood as follows: (cid:7)\n",
      "\n",
      "p(D|μ, Σ) = (2π)−N D/2|Σ|− N\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "−\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "tr(Σ−1Sx)\n",
      "\n",
      "2 exp (cid:8)\n",
      "\n",
      "−\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(μ − x)T Σ−1(μ − x)\n",
      "\n",
      "(4.196)\n",
      "\n",
      "(4.197)\n",
      "\n",
      "We will use this form below.\n",
      "\n",
      "4.6.3.2\n",
      "\n",
      "Prior\n",
      "\n",
      "The obvious prior to use is the following\n",
      "\n",
      "p(μ, Σ) =N (μ|m0, V0)IW(Σ|S0, ν0)\n",
      "\n",
      "(4.198)\n",
      "\n",
      "Unfortunately, this is not conjugate to the likelihood. To see why, note that μ and Σ appear together in a non-factorized way in the likelihood; hence they will also be coupled together in the posterior.\n",
      "\n",
      "The above prior is sometimes called semi-conjugate or conditionally conjugate, since both conditionals, p(μ|Σ) and p(Σ|μ), are individually conjugate. To create a full conjugate prior, we need to use a prior where μ and Σ are dependent on each other. We will use a joint distribution of the form\n",
      "\n",
      "p(μ, Σ) =p( Σ)p(μ|Σ)\n",
      "\n",
      "(4.199)\n",
      "\n",
      "Looking at the form of the likelihood equation, Equation 4.197, we see that a natural conjugate\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "133\n",
      "\n",
      "prior has the form of a Normal-inverse-wishart or NIW distribution, deﬁned as follows:\n",
      "\n",
      "NIW(μ, Σ|m0, κ0, ν0, S0) (cid:2) N (μ|m0, 1 κ0 |Σ|− 1\n",
      "\n",
      "=\n",
      "\n",
      "1 ZN IW ×|Σ|− ν0+D+1\n",
      "\n",
      "2\n",
      "\n",
      "2 exp\n",
      "\n",
      "Σ) × IW(Σ|S0, ν0)\n",
      "\n",
      "exp\n",
      "\n",
      "+\n",
      "\n",
      "− (cid:7)\n",
      "\n",
      "κ0 2 − 1 2\n",
      "\n",
      "tr(Σ−1S0)\n",
      "\n",
      "(μ − m0)T Σ−1(μ − m0)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      ",\n",
      "\n",
      "(4.200)\n",
      "\n",
      "(4.201)\n",
      "\n",
      "(4.202)\n",
      "\n",
      "(4.203)\n",
      "\n",
      "κ0 2 ZN IW = 2v0D/2ΓD(ν0/2)(2π/κ0)D/2|S0|−ν0/2\n",
      "\n",
      "=\n",
      "\n",
      "× exp\n",
      "\n",
      "1 ZN IW\n",
      "\n",
      "|Σ|− ν0+D+2 (cid:7)\n",
      "\n",
      "−\n",
      "\n",
      "(μ − m0)T Σ−1(μ − m0) − 1 2\n",
      "\n",
      "2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "tr(Σ−1S0)\n",
      "\n",
      "(4.204)\n",
      "\n",
      "(4.205)\n",
      "\n",
      "(4.206)\n",
      "\n",
      "where ΓD(a) is the multivariate Gamma function.\n",
      "\n",
      "The parameters of the NIW can be interpreted as follows: m0 is our prior mean for μ, and κ0 is how strongly we believe this prior; and S0 is (proportional to) our prior mean for Σ, and ν0 is how strongly we believe this prior.3\n",
      "\n",
      "One can show (Minka 2000f) that the (improper) uninformative prior has the form\n",
      "\n",
      "lim k→0\n",
      "\n",
      "N (μ|m0, Σ/k)IW(Σ|S0, k) ∝ |2πΣ|− 1\n",
      "\n",
      "2 |Σ|−(D+1)/2\n",
      "\n",
      "(4.207)\n",
      "\n",
      "∝ |Σ|−( D\n",
      "\n",
      "2 +1) ∝ NIW(μ, Σ|0, 0, 0, 0I)\n",
      "\n",
      "(4.208)\n",
      "\n",
      "In practice, it is often better to use a weakly informative data-dependent prior. A common (Fraley and Raftery 2007, p6)) is to use S0 = choice (see e.g., diag(Sx)/N , and ν0 = D + 2, to ensure E [Σ] = S0, and to set μ0 = x and κ0 to some small number, such as 0.01.\n",
      "\n",
      "(Chipman et al. 2001, p81),\n",
      "\n",
      "3. Although this prior has four parameters, there are really only three free parameters, since our uncertainty in the mean is proportional to the variance. In particular, if we believe that the variance is large, then our uncertainty in μ must be large too. This makes sense intuitively, since if the data has large spread, it may be hard to pin down its mean. See also Exercise 9.1, where we will see the three free parameters more explicitly. If we want separate “control” over our conﬁdence in μ and Σ, we must use a semi-conjugate prior.\n",
      "\n",
      "134\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "4.6.3.3\n",
      "\n",
      "Posterior\n",
      "\n",
      "The posterior can be shown (Exercise 4.11) to be NIW with updated parameters:\n",
      "\n",
      "where we have deﬁned S (cid:2) to update incrementally than the centered version).\n",
      "\n",
      "p(μ, Σ|D) = NIW(μ, Σ|mN , κN , νN , SN ) κ0m0 + N x κN κN = κ0 + N νN = ν0 + N\n",
      "\n",
      "mN =\n",
      "\n",
      "SN = S0 + Sx +\n",
      "\n",
      "κ0N κ0 + N = S0 + S + κ0m0mT i=1 xixT\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      "=\n",
      "\n",
      "κ0 κ0 + N\n",
      "\n",
      "i as the uncentered sum-of-squares matrix (this is easier\n",
      "\n",
      "0 − κN mN mT N\n",
      "\n",
      "(x − m0)(x − m0)T\n",
      "\n",
      "m0 +\n",
      "\n",
      "N κ0 + N\n",
      "\n",
      "x\n",
      "\n",
      "(4.209)\n",
      "\n",
      "(4.210)\n",
      "\n",
      "(4.212)\n",
      "\n",
      "(4.213)\n",
      "\n",
      "(4.214)\n",
      "\n",
      "(4.211)\n",
      "\n",
      "This result is actually quite intuitive: the posterior mean is a convex combination of the prior mean and the MLE, with “strength” κ0 + N ; and the posterior scatter matrix SN is the prior scatter matrix S0 plus the empirical scatter matrix Sx plus an extra term due to the uncertainty in the mean (which creates its own virtual scatter matrix).\n",
      "\n",
      "4.6.3.4\n",
      "\n",
      "Posterior mode\n",
      "\n",
      "The mode of the joint distribution has the following form:\n",
      "\n",
      "argmax p(μ, Σ|D) = (mN ,\n",
      "\n",
      "SN νN + D + 2\n",
      "\n",
      ")\n",
      "\n",
      "(4.215)\n",
      "\n",
      "If we set κ0 = 0, this reduces to\n",
      "\n",
      "argmax p(μ, Σ|D) = (x,\n",
      "\n",
      "S0 + Sx ν0 + N + D + 2\n",
      "\n",
      ")\n",
      "\n",
      "(4.216)\n",
      "\n",
      "The corresponding estimate ˆΣ is almost the same as Equation 4.183, but differs by 1 in the denominator, because this is the mode of the joint, not the mode of the marginal.\n",
      "\n",
      "4.6.3.5\n",
      "\n",
      "Posterior marginals\n",
      "\n",
      "The posterior marginal for Σ is simply (cid:11)\n",
      "\n",
      "p(Σ|D) =\n",
      "\n",
      "p(μ, Σ|D)dμ = IW(Σ|SN , νN )\n",
      "\n",
      "(4.217)\n",
      "\n",
      "The mode and mean of this marginal are given by\n",
      "\n",
      "ˆΣmap =\n",
      "\n",
      "SN νN + D + 1\n",
      "\n",
      ", E [Σ] =\n",
      "\n",
      "SN νN − D − 1\n",
      "\n",
      "(4.218)\n",
      "\n",
      "One can show that the posterior marginal for μ has a multivariate Student T distribution:\n",
      "\n",
      "p(μ|D) =\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(μ, Σ|D)dΣ = T (μ|mN ,\n",
      "\n",
      "1 κN (νN − D + 1)\n",
      "\n",
      "SN , νN − D + 1)\n",
      "\n",
      "(4.219)\n",
      "\n",
      "This follows from the fact that the Student distribution can be represented as a scaled mixture of Gaussians (see Equation 11.61).\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "135\n",
      "\n",
      "Figure 4.19 The N Iχ2(m0, κ0, ν0, σ2 believe this; σ2 1, σ2 0 = 1. Notice that the contour plot (underneath the surface) is shaped like a “squashed egg”. increase the strength of our belief in the mean, so it gets narrower: m0 = 0, κ0 = 5, ν0 = 1, σ2 We increase the strength of our belief in the variance, so it gets narrower: m0 = 0, κ0 = 1, ν0 = 5, σ2 1. Figure generated by NIXdemo2.\n",
      "\n",
      "0 is the prior variance and ν0 is how strongly we believe this.\n",
      "\n",
      "0) distribution. m0 is the prior mean and κ0 is how strongly we (a) m0 = 0, κ0 = 1, ν0 = (b) We 0 = 1. (c) 0 =\n",
      "\n",
      "4.6.3.6\n",
      "\n",
      "Posterior predictive\n",
      "\n",
      "The posterior predictive is given by p(x, D) p(D)\n",
      "\n",
      "p(x|D) =\n",
      "\n",
      "(4.220)\n",
      "\n",
      "so it can be easily evaluated in terms of a ratio of marginal likelihoods.\n",
      "\n",
      "It turns out that this ratio has the form of a multivariate Student-T distribution:\n",
      "\n",
      "(cid:11) (cid:11)\n",
      "\n",
      "p(x|D) =\n",
      "\n",
      "N (x|μ, Σ)NIW(μ, Σ|mN , κN , νN , SN )dμdΣ\n",
      "\n",
      "(4.221)\n",
      "\n",
      "= T (x|mN ,\n",
      "\n",
      "κN + 1 κN (νN − D + 1)\n",
      "\n",
      "SN , νN − D + 1)\n",
      "\n",
      "(4.222)\n",
      "\n",
      "The Student-T has wider tails than a Gaussian, which takes into account the fact that Σ is unknown. However, this rapidly becomes Gaussian-like.\n",
      "\n",
      "4.6.3.7\n",
      "\n",
      "Posterior for scalar data\n",
      "\n",
      "We now specialise the above results to the case where xi is 1d. These results are widely used in the statistics literature. As in Section 4.6.2.2, it is conventional not to use the normal inverse\n",
      "\n",
      "136\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "Wishart, but to use the normal inverse chi-squared or NIX distribution, deﬁned by\n",
      "\n",
      "1 σ2 )(ν0+3)/2 exp See Figure 4.19 for some plots. Along the μ axis, the distribution is shaped like a Gaussian, and along the σ2 axis, the distribution is shaped like a χ−2; the contours of the joint density have Interestingly, we see that the contours for μ are more peaked a “squashed egg” appearance. for small values of σ2, which makes sense, since if the data is low variance, we will be able to estimate its mean more reliably.\n",
      "\n",
      "N Iχ2(μ, σ2|m0, κ0, ν0, σ2\n",
      "\n",
      "0) (cid:2) N (μ|m0, σ2/κ0) χ−2(σ2|ν0, σ2 0)\n",
      "\n",
      "∝ (\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "−\n",
      "\n",
      "ν0σ2\n",
      "\n",
      "0 + κ0(μ − m0)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(4.223)\n",
      "\n",
      "(4.224)\n",
      "\n",
      "One can show that the posterior is given by p(μ, σ2|D) =N Iχ 2(μ, σ2|mN , κN , νN , σ2 κ0m0 + N x κN κN = κ0 + N νN = ν0 + N\n",
      "\n",
      "mN =\n",
      "\n",
      "N )\n",
      "\n",
      "(4.225)\n",
      "\n",
      "(4.226)\n",
      "\n",
      "(4.227)\n",
      "\n",
      "(4.228)\n",
      "\n",
      "νN σ2\n",
      "\n",
      "N = ν0σ2\n",
      "\n",
      "0 +\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − x)2 +\n",
      "\n",
      "i=1\n",
      "\n",
      "N κ0 κ0 + N\n",
      "\n",
      "(m0 − x)2\n",
      "\n",
      "(4.229)\n",
      "\n",
      "The posterior marginal for σ2 is just (cid:11)\n",
      "\n",
      "with the posterior mean given by E\n",
      "\n",
      "= νN The posterior marginal for μ has a Student T distribution, which follows from the scale\n",
      "\n",
      "p(σ2|D) =\n",
      "\n",
      "p(μ, σ2|D)dμ = χ−2(σ2|νN , σ2 (cid:20)\n",
      "\n",
      "σ2|D\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "νN −2 σ2 N .\n",
      "\n",
      "N )\n",
      "\n",
      "(4.230)\n",
      "\n",
      "mixture representation of the student: (cid:11)\n",
      "\n",
      "p(μ|D) =\n",
      "\n",
      "p(μ, σ2|D)dσ2 = T (μ|mN , σ2\n",
      "\n",
      "N /κN , νN )\n",
      "\n",
      "(4.231)\n",
      "\n",
      "with the posterior mean given by E [μ|D] = mN .\n",
      "\n",
      "Let us see how these results look if we use the following uninformative prior: p(μ, σ2) ∝ p(μ)p(σ2) ∝ σ−2 ∝ N Iχ2(μ, σ2|μ0 = 0, κ0 = 0, ν0 = −1, σ2\n",
      "\n",
      "0 = 0)\n",
      "\n",
      "(4.232)\n",
      "\n",
      "With this prior, the posterior has the form\n",
      "\n",
      "p(μ, σ2|D) =N Iχ 2(μ, σ2|mN = x, κN = N, νN = N − 1, σ2\n",
      "\n",
      "N = s2)\n",
      "\n",
      "(4.233)\n",
      "\n",
      "where\n",
      "\n",
      "s2 (cid:2)\n",
      "\n",
      "1 N − 1\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − x)2 =\n",
      "\n",
      "i=1\n",
      "\n",
      "N N − 1\n",
      "\n",
      "ˆσ2 mle\n",
      "\n",
      "(4.234)\n",
      "\n",
      "is the the sample standard deviation. estimate of the variance.) Hence the marginal posterior for the mean is given by\n",
      "\n",
      "(In Section 6.4.2, we show that this is an unbiased\n",
      "\n",
      "p(μ|D) = T (μ|x,\n",
      "\n",
      "s2 N\n",
      "\n",
      ", N − 1)\n",
      "\n",
      "(4.235)\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "137\n",
      "\n",
      "and the posterior variance of μ is\n",
      "\n",
      "var [μ|D] =\n",
      "\n",
      "νN νN − 2\n",
      "\n",
      "σ2\n",
      "\n",
      "N =\n",
      "\n",
      "N − 1 N − 3\n",
      "\n",
      "s2 N\n",
      "\n",
      "→\n",
      "\n",
      "s2 N\n",
      "\n",
      "(4.236)\n",
      "\n",
      "The square root of this is called the standard error of the mean:\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "var [μ|D] ≈\n",
      "\n",
      "s √ N\n",
      "\n",
      "(4.237)\n",
      "\n",
      "Thus an approximate 95% posterior credible interval for the mean is\n",
      "\n",
      "I.95(μ|D) = x ± 2\n",
      "\n",
      "s √ N\n",
      "\n",
      "(4.238)\n",
      "\n",
      "(Bayesian credible intervals are discussed in more detail in Section 5.2.2; they are contrasted with frequentist conﬁdence intervals in Section 6.6.1.)\n",
      "\n",
      "4.6.3.8\n",
      "\n",
      "Bayesian t-test\n",
      "\n",
      "Suppose we want to test the hypothesis that μ (cid:6)= μ0 for some known value μ0 (often 0), given values xi ∼ N (μ, σ2). This is called a two-sided, one-sample t-test. A simple way to perform such a test is just to check if μ0 ∈ I0.95(μ|D). If it is not, then we can be 95% sure that μ (cid:6)= μ0.4 A more common scenario is when we want to test if two paired samples have the same mean. More precisely, suppose yi ∼ N (μ1, σ2) and zi ∼ N (μ2, σ2). We want to determine if μ = μ1 − μ2 > 0, using xi = yi − zi as our data. We can evaluate this quantity as follows:\n",
      "\n",
      "(cid:11) ∞\n",
      "\n",
      "p(μ > μ0|D) =\n",
      "\n",
      "μ0\n",
      "\n",
      "p(μ|D)dμ\n",
      "\n",
      "(4.239)\n",
      "\n",
      "This is called a one-sided, paired t-test. the difference in binomial proportions, see Section 5.2.3.)\n",
      "\n",
      "(For a similar approach to unpaired tests, comparing\n",
      "\n",
      "As we showed above, we ﬁnd that the posterior marginal on μ has the form\n",
      "\n",
      "To calculate the posterior, we must specify a prior. Suppose we use an uninformative prior.\n",
      "\n",
      "p(μ|D) = T (μ|x,\n",
      "\n",
      "s2 N\n",
      "\n",
      ", N − 1)\n",
      "\n",
      "(4.240)\n",
      "\n",
      "Now let us deﬁne the following t statistic:\n",
      "\n",
      "t (cid:2)\n",
      "\n",
      "x − μ0 √ N s/\n",
      "\n",
      "(4.241)\n",
      "\n",
      "where the denominator is the standard error of the mean. We see that\n",
      "\n",
      "p(μ|D) = 1 − FN −1(t)\n",
      "\n",
      "(4.242)\n",
      "\n",
      "where Fν(t) is the cdf of the standard Student t distribution T (0, 1, ν).\n",
      "\n",
      "4. A more complex approach is to perform Bayesian model comparison. That is, we compute the Bayes factor (described in Section 5.3.3) p(D|H0)/p(D|H1), where H0 is the point null hypothesis that μ = μ0, and H1 is the alternative hypothesis that μ (cid:3)= μ0. See (Gonen et al. 2005; Rouder et al. 2009) for details.\n",
      "\n",
      "138\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "4.6.3.9\n",
      "\n",
      "Connection with frequentist statistics *\n",
      "\n",
      "If we use an uninformative prior, it turns out that the above Bayesian analysis gives the same result as derived using frequentist methods. (We discuss frequentist statistics in Chapter 6.) Speciﬁcally, from the above results, we see that\n",
      "\n",
      "μ − x (cid:9)\n",
      "\n",
      "s/N\n",
      "\n",
      "|D ∼ tN −1\n",
      "\n",
      "(4.243)\n",
      "\n",
      "This has the same form as the sampling distribution of the MLE:\n",
      "\n",
      "μ − X (cid:9)\n",
      "\n",
      "s/N\n",
      "\n",
      "|μ ∼ tN −1\n",
      "\n",
      "(4.244)\n",
      "\n",
      "The reason is that the Student distribution is symmetric in its ﬁrst two arguments, so T (x|μ, σ2, ν) = T (μ|x, σ2, ν); hence statements about the posterior for μ have the same form as statements about the sampling distribution of x. Consequently, the (one-sided) p-value (deﬁned in Sec- tion 6.6.2) returned by a frequentist test is the same as p(μ > μ0|D) returned by the Bayesian method. See bayesTtestDemo for an example.\n",
      "\n",
      "in the Bayesian approach, μ is unknown and x is ﬁxed, whereas in the frequentist approach, X is unknown and μ is ﬁxed. More equivalences between frequentist and Bayesian inference in simple models using uninformative priors can be found in (Box and Tiao 1973). See also Section 7.6.3.3.\n",
      "\n",
      "Despite the superﬁcial similarity, these two results have a different interpretation:\n",
      "\n",
      "4.6.4\n",
      "\n",
      "Sensor fusion with unknown precisions *\n",
      "\n",
      "In this section, we apply the results in Section 4.6.3 to the problem of sensor fusion in the case where the precision of each measurement device is unknown. This generalizes the results of Section 4.4.2.2, where the measurement model was assumed to be Gaussian with known precision. The unknown precision case turns out to give qualitatively different results, yielding a potentially multi-modal posterior as we will see. Our presentation is based on (Minka 2001e). Suppose we want to pool data from multiple sources to estimate some quantity μ ∈ R, but the reliability of the sources is unknown. Speciﬁcally, suppose we have two different measurement devices, x and y, with different precisions: xi|μ ∼ N (μ, λ−1 y ). We make two independent measurements with each device, which turn out to be\n",
      "\n",
      "x ) and yi|μ ∼ N (μ, λ−1\n",
      "\n",
      "x1 = 1.1, x2 = 1.9, y1 = 2.9, y2 = 4.1\n",
      "\n",
      "(4.245)\n",
      "\n",
      "We will use a non-informative prior for μ, p(μ) ∝ 1, which we can emulate using an inﬁnitely broad Gaussian, p(μ) =N (μ|m0 = 0, λ−1 0 = ∞). If the λx and λy terms were known, then the posterior would be Gaussian:\n",
      "\n",
      "p(μ|D, λx, λy) =N (μ|mN , λ−1 N )\n",
      "\n",
      "(4.246)\n",
      "\n",
      "λN = λ0 + Nxλx + Nyλy\n",
      "\n",
      "(4.247)\n",
      "\n",
      "mN =\n",
      "\n",
      "λxNxx + λyNyy Nxλx + Nyλy\n",
      "\n",
      "(4.248)\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "139\n",
      "\n",
      "where Nx = 2 is the number of x measurements, Ny = 2 is the number of y measurements, x = 1 i=1 yi = 3.5. This result follows because the posterior Nx precision is the sum of the measurement precisions, and the posterior mean is a weighted sum of the prior mean (which is 0) and the data means.\n",
      "\n",
      "(cid:4)Nx\n",
      "\n",
      "i=1 xi = 1.5 and y = 1 Ny\n",
      "\n",
      "(cid:4)Ny\n",
      "\n",
      "However, the measurement precisions are not known.\n",
      "\n",
      "Initially we will estimate them by\n",
      "\n",
      "maximum likelihood. The log-likelihood is given by\n",
      "\n",
      "(cid:9)(μ, λx, λy) = log λx −\n",
      "\n",
      "λx 2\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(xi − μ)2 + log λy −\n",
      "\n",
      "i\n",
      "\n",
      "λy 2\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(yi − μ)2\n",
      "\n",
      "i\n",
      "\n",
      "(4.249)\n",
      "\n",
      "The MLE is obtained by solving the following simultaneous equations:\n",
      "\n",
      "∂(cid:9) ∂μ\n",
      "\n",
      "= λxNx(x − μ) +λ yNy(y − μ) = 0\n",
      "\n",
      "(4.250)\n",
      "\n",
      "∂(cid:9) ∂λx\n",
      "\n",
      "∂(cid:9) ∂λy\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "1 λx\n",
      "\n",
      "1 λy\n",
      "\n",
      "− 1 Nx\n",
      "\n",
      "− 1 Ny\n",
      "\n",
      "Nx(cid:6)\n",
      "\n",
      "(xi − μ)2 = 0\n",
      "\n",
      "i=1 Ny(cid:6)\n",
      "\n",
      "(yi − μ)2 = 0\n",
      "\n",
      "i=1\n",
      "\n",
      "(4.251)\n",
      "\n",
      "(4.252)\n",
      "\n",
      "This gives\n",
      "\n",
      "ˆμ =\n",
      "\n",
      "1/ˆλx =\n",
      "\n",
      "1/ˆλy =\n",
      "\n",
      "Nxˆλxx + Ny ˆλyy Nxˆλx + Ny ˆλy 1 Nx\n",
      "\n",
      "1 Ny\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "i (cid:6)\n",
      "\n",
      "i\n",
      "\n",
      "(xi − ˆμ)2\n",
      "\n",
      "(yi − ˆμ)2\n",
      "\n",
      "(4.253)\n",
      "\n",
      "(4.254)\n",
      "\n",
      "(4.255)\n",
      "\n",
      "We notice that the MLE for μ has the same form as the posterior mean, mN . (cid:4)Ny\n",
      "\n",
      "and λy = 1/s2 Using this, we get ˆμ = 2.1154, so p(μ|D, ˆλx, ˆλy) =N (μ|2.1154, 0.0554). we converge to ˆλx = 1/0.1662, ˆλy = 1/4.0509, p(μ|D, ˆλx, ˆλy) = N (μ|1.5788, 0.0798).\n",
      "\n",
      "We can solve these equations by ﬁxed point iteration. Let us initialize by estimating λx = 1/s2 x i=1(yi − y)2 = 0.36. i=1(xi − x)2 = 0.16 and s2 If we now iterate,\n",
      "\n",
      "y, where s2\n",
      "\n",
      "x = 1 Nx\n",
      "\n",
      "(cid:4)Nx\n",
      "\n",
      "y = 1 Ny\n",
      "\n",
      "The plug-in approximation to the posterior is plotted in Figure 4.20(a). This weights each sensor according to its estimated precision. Since sensor y was estimated to be much less * reliable than sensor x, we haveE\n",
      "\n",
      ")\n",
      "\n",
      "μ|D, ˆλx, ˆλy\n",
      "\n",
      "≈ x, so we effectively ignore the y sensor.\n",
      "\n",
      "Now we will adopt a Bayesian approach and integrate out the unknown precisions, rather\n",
      "\n",
      "than trying to estimate them. That is, we compute\n",
      "\n",
      "(cid:2)(cid:11)\n",
      "\n",
      "(cid:3) (cid:2)(cid:11)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "p(μ|D) ∝ p(μ)\n",
      "\n",
      "p(Dx|μ, λx)p(λx|μ)dλx\n",
      "\n",
      "p(Dy|μ, λy)p(λy|μ)dλy\n",
      "\n",
      "(4.256)\n",
      "\n",
      "We will use uninformative Jeffrey’s priors, p(μ) ∝ 1, p(λx|μ) ∝ 1/λx and p(λy|μ) ∝ 1/λy.\n",
      "\n",
      "140\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "Since the x and y terms are symmetric, we will just focus on one of them. The key integral is\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "I =\n",
      "\n",
      "p(Dx|μ, λx)p(λx|μ)dλx ∝\n",
      "\n",
      "exp\n",
      "\n",
      "x (Nxλx)Nx/2 λ−1 (cid:7)\n",
      "\n",
      "−\n",
      "\n",
      "Nx 2\n",
      "\n",
      "λx(x − μ)2 −\n",
      "\n",
      "Nx 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "s2 xλx\n",
      "\n",
      "dλx\n",
      "\n",
      "(4.257)\n",
      "\n",
      "(4.258)\n",
      "\n",
      "Exploiting the fact that Nx = 2 this simpliﬁes to\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "I =\n",
      "\n",
      "x λ1 λ−1\n",
      "\n",
      "x exp(−λx[(x − μ)2 + s2\n",
      "\n",
      "x])dλx\n",
      "\n",
      "(4.259)\n",
      "\n",
      "We recognize this as proportional to the integral of an unnormalized Gamma density\n",
      "\n",
      "Ga(λ|a, b) ∝ λa−1e−λb\n",
      "\n",
      "(4.260)\n",
      "\n",
      "where a = 1 and b = (x − μ)2 + s2 constant of the Gamma distribution, Γ(a)b−a, so we get\n",
      "\n",
      "I ∝\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(Dx|μ, λx)p(λx|μ)dλx ∝\n",
      "\n",
      "x. Hence the integral is proportional to the normalizing\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "x − μ)2 + s2 x\n",
      "\n",
      "(cid:19)−1\n",
      "\n",
      "(4.261)\n",
      "\n",
      "and the posterior becomes\n",
      "\n",
      "p(μ|D) ∝\n",
      "\n",
      "1 (x − μ)2 + s2 x\n",
      "\n",
      "1 (y − μ)2 + s2 y\n",
      "\n",
      "(4.262)\n",
      "\n",
      "The exact posterior is plotted in Figure 4.20(b). We see that it has two modes, one near x = 1.5 and one near y = 3.5. These correspond to the beliefs that the x sensor is more reliable than the y one, and vice versa. The weight of the ﬁrst mode is larger, since the data from the x sensor agree more with each other, so it seems slightly more likely that the x sensor is the reliable one. (They obviously cannot both be reliable, since they disagree on the values that they are reporting.) However, the Bayesian solution keeps open the possibility that the y sensor is the more reliable one; from two measurements, we cannot tell, and choosing just the x sensor, as the plug-in approximation does, results in over conﬁdence (a posterior that is too narrow).\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 4.1 Uncorrelated does not imply independent Let X ∼ U (−1, 1) and Y = X 2. Clearly Y is dependent on X (in fact, Y is uniquely determined by X). However, show that ρ(X, Y ) = 0. Hint: if X ∼ U (a, b) then E[X] = (a + b)/2 and var [X] = (b − a)2/12.\n",
      "\n",
      "Exercise 4.2 Uncorrelated and Gaussian does not imply independent unless jointly Gaussian Let X ∼ N (0, 1) and Y = W X, where p(W = −1) = p(W = 1) = 0.5. It is clear that X and Y are not independent, since Y is a function of X.\n",
      "\n",
      "a. Show Y ∼ N (0, 1).\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "141\n",
      "\n",
      "0.8\n",
      "\n",
      "1.5\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.5\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "0 −2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 4.20 Posterior for μ. sensorFusionUnknownPrec.\n",
      "\n",
      "(a) Plug-in approximation.\n",
      "\n",
      "(b) Exact posterior.\n",
      "\n",
      "Figure generated by\n",
      "\n",
      "b. Show cov [X, Y ] = 0. Thus X and Y are uncorrelated but dependent, even though they are Gaussian.\n",
      "\n",
      "Hint: use the deﬁnition of covariance\n",
      "\n",
      "cov [X, Y ] = E [XY ] − E [X] E [Y ]\n",
      "\n",
      "(4.263)\n",
      "\n",
      "and the rule of iterated expectation\n",
      "\n",
      "E [XY ] = E [E [XY |W ]]\n",
      "\n",
      "(4.264)\n",
      "\n",
      "Exercise 4.3 Correlation coefficient is between -1 and +1 Prove that −1 ≤ ρ(X, Y ) ≤ 1\n",
      "\n",
      "Exercise 4.4 Correlation coefficient for linearly related variables is ±1 Show that, if Y = aX + b for some parameters a >0 and b, then ρ(X, Y ) = 1. Similarly show that if a < 0, then ρ(X, Y ) = −1.\n",
      "\n",
      "Exercise 4.5 Normalization constant for a multidimensional Gaussian Prove that the normalization constant for a d-dimensional Gaussian is given by\n",
      "\n",
      "i λi to write the joint pdf as a product of d one- Hint: diagonalize Σ and use the fact that |Σ| = dimensional Gaussians in a transformed coordinate system. (You will need the change of variables formula.) Finally, use the normalization constant for univariate Gaussians.\n",
      "\n",
      "(2π)d/2|Σ|\n",
      "\n",
      "1 2 =\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "exp(− 1 2\n",
      "\n",
      "(x − μ)T Σ−1(x − μ))dx\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(4.265)\n",
      "\n",
      "Exercise 4.6 Bivariate Gaussian Let x ∼ N (μ, Σ) where x ∈ R2 and\n",
      "\n",
      "Σ =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "σ2 1 ρσ1σ2\n",
      "\n",
      "ρσ1σ2 σ2 2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(4.266)\n",
      "\n",
      "where ρ is the correlation coefficient. Show that the pdf is given by\n",
      "\n",
      "p(x1, x2) =\n",
      "\n",
      "exp\n",
      "\n",
      "2πσ1σ2 (cid:3)\n",
      "\n",
      "−\n",
      "\n",
      "1 (cid:15)\n",
      "\n",
      "1 2(1 − ρ2)\n",
      "\n",
      "1 − ρ2\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(x1 − μ1)2 σ2 1\n",
      "\n",
      "+\n",
      "\n",
      "(x2 − μ2)2 σ2 2\n",
      "\n",
      "− 2ρ (x1 − μ1)\n",
      "\n",
      "σ1\n",
      "\n",
      "(x2 − μ2) σ2\n",
      "\n",
      "(4.267)\n",
      "\n",
      "(cid:4)(cid:4)\n",
      "\n",
      "(4.268)\n",
      "\n",
      "142\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "280\n",
      "\n",
      "raw\n",
      "\n",
      "22\n",
      "\n",
      "35\n",
      "\n",
      "4\n",
      "\n",
      "standarized\n",
      "\n",
      "260\n",
      "\n",
      "22\n",
      "\n",
      "35\n",
      "\n",
      "240\n",
      "\n",
      "34 44\n",
      "\n",
      "13\n",
      "\n",
      "57\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "whitened\n",
      "\n",
      "220\n",
      "\n",
      "200\n",
      "\n",
      "180\n",
      "\n",
      "160\n",
      "\n",
      "140\n",
      "\n",
      "120\n",
      "\n",
      "20\n",
      "\n",
      "12 46\n",
      "\n",
      "56 28\n",
      "\n",
      "30 41\n",
      "\n",
      "53\n",
      "\n",
      "29 14\n",
      "\n",
      "16\n",
      "\n",
      "43 24\n",
      "\n",
      "66\n",
      "\n",
      "61 42\n",
      "\n",
      "31\n",
      "\n",
      "73 26\n",
      "\n",
      "39 58 7\n",
      "\n",
      "23\n",
      "\n",
      "33\n",
      "\n",
      "9 68 4 37 55 72 64 10 6 3 32\n",
      "\n",
      "59\n",
      "\n",
      "60 8 54\n",
      "\n",
      "5\n",
      "\n",
      "25 19\n",
      "\n",
      "49 27\n",
      "\n",
      "65\n",
      "\n",
      "69 18 47 63 50 62\n",
      "\n",
      "15\n",
      "\n",
      "48 17 45\n",
      "\n",
      "51\n",
      "\n",
      "71\n",
      "\n",
      "40 52\n",
      "\n",
      "11\n",
      "\n",
      "67\n",
      "\n",
      "1\n",
      "\n",
      "38\n",
      "\n",
      "2\n",
      "\n",
      "21\n",
      "\n",
      "36\n",
      "\n",
      "70\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "12 46\n",
      "\n",
      "56 28\n",
      "\n",
      "30 41\n",
      "\n",
      "16\n",
      "\n",
      "53\n",
      "\n",
      "29 14\n",
      "\n",
      "31\n",
      "\n",
      "66\n",
      "\n",
      "43 24\n",
      "\n",
      "61 42\n",
      "\n",
      "23\n",
      "\n",
      "73 26\n",
      "\n",
      "33\n",
      "\n",
      "39 58 7\n",
      "\n",
      "9 68 4 37 55 72 64 10 6 3 32\n",
      "\n",
      "59\n",
      "\n",
      "1\n",
      "\n",
      "34 44\n",
      "\n",
      "13\n",
      "\n",
      "57\n",
      "\n",
      "20\n",
      "\n",
      "25 19\n",
      "\n",
      "65\n",
      "\n",
      "11\n",
      "\n",
      "2\n",
      "\n",
      "49 27\n",
      "\n",
      "60 8 54\n",
      "\n",
      "15\n",
      "\n",
      "69 18 47 63 50 62\n",
      "\n",
      "51\n",
      "\n",
      "48 17 45\n",
      "\n",
      "40 52\n",
      "\n",
      "71\n",
      "\n",
      "67\n",
      "\n",
      "21\n",
      "\n",
      "36\n",
      "\n",
      "70\n",
      "\n",
      "5\n",
      "\n",
      "38\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "2\n",
      "\n",
      "36\n",
      "\n",
      "21\n",
      "\n",
      "70\n",
      "\n",
      "−2\n",
      "\n",
      "35\n",
      "\n",
      "22\n",
      "\n",
      "57\n",
      "\n",
      "13\n",
      "\n",
      "34 44\n",
      "\n",
      "71\n",
      "\n",
      "67\n",
      "\n",
      "38\n",
      "\n",
      "40 52\n",
      "\n",
      "9 49 48 17 27 68 4 37 55 45 72 69 47 18 64 6263 50 8 60 54 39 10 6 3 58 32 5 7 59\n",
      "\n",
      "11\n",
      "\n",
      "51\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "65\n",
      "\n",
      "25 19\n",
      "\n",
      "33\n",
      "\n",
      "29 14 1\n",
      "\n",
      "61 42\n",
      "\n",
      "73 26 43 24\n",
      "\n",
      "23 66 53\n",
      "\n",
      "30 41\n",
      "\n",
      "31\n",
      "\n",
      "16\n",
      "\n",
      "56 28\n",
      "\n",
      "12 46\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "−2\n",
      "\n",
      "100\n",
      "\n",
      "80\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "Figure 4.21\n",
      "\n",
      "(a) Height/weight data for the men. (b) Standardized. (c) Whitened.\n",
      "\n",
      "Exercise 4.7 Conditioning a bivariate Gaussian Consider a bivariate Gaussian distribution p(x1, x2) =N (x|μ, Σ) where\n",
      "\n",
      "Σ =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "σ2 1 σ21\n",
      "\n",
      "σ12 σ2 2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "= σ1σ2\n",
      "\n",
      "(cid:3) σ1 σ2 ρ\n",
      "\n",
      "ρ σ2 σ1\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(4.269)\n",
      "\n",
      "where the correlation coefficient is given by\n",
      "\n",
      "ρ (cid:2)\n",
      "\n",
      "σ12 σ1σ2\n",
      "\n",
      "(4.270)\n",
      "\n",
      "a. What is P (X2|x1)? Simplify your answer by expressing it in terms of ρ, σ2, σ1, μ1,μ2 and x1. b. Assume σ1 = σ2 = 1. What is P (X2|x1) now?\n",
      "\n",
      "Exercise 4.8 Whitening vs standardizing a. Load the height/weight data using rawdata = dlmread(’heightWeightData.txt’). The ﬁrst col- umn is the class label (1=male, 2=female), the second column is height, the third weight. Extract the height/weight data corresponding to the males. Fit a 2d Gaussian to the male data, using the empirical mean and covariance. Plot your Gaussian as an ellipse (use gaussPlot2d), superimposing on your scatter plot. It should look like Figure 4.21(a), where have labeled each datapoint by its index. Turn in your ﬁgure and code.\n",
      "\n",
      "b. Standardizing the data means ensuring the empirical variance along each dimension is 1. This can be , where σj is the empirical std of dimension j. Standardize the data and\n",
      "\n",
      "done by computing replot. It should look like Figure 4.21(b). (Use axis(’equal’).) Turn in your ﬁgure and code.\n",
      "\n",
      "xij −xj σj\n",
      "\n",
      "c. Whitening or sphereing the data means ensuring its empirical covariance matrix is proportional to I, so the data is uncorrelated and of equal variance along each dimension. This can be done by computing Λ− 1 2 UT x for each data vector x, where U are the eigenvectors and Λ the eigenvalues of X. Whiten the data and replot. It should look like Figure 4.21(c). Note that whitening rotates the data, so people move to counter-intuitive locations in the new coordinate system (see e.g., person 2, who moves from the right hand side to the left).\n",
      "\n",
      "Exercise 4.9 Sensor fusion with known variances in 1d Suppose we have two sensors with known (and different) variances v1 and v2, but unknown (and the same) mean μ. Suppose we observe n1 observations y(1) i ∼ N (μ, v1) from the ﬁrst sensor and n2 observations\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "143\n",
      "\n",
      "y(2) (For example, suppose μ is the true temperature outside, i ∼ N (μ, v2) from the second sensor. and sensor 1 is a precise (low variance) digital thermosensing device, and sensor 2 is an imprecise (high variance) mercury thermometer.) Let D represent all the data from both sensors. What is the posterior p(μ|D), assuming a non-informative prior for μ (which we can simulate using a Gaussian with a precision of 0)? Give an explicit expression for the posterior mean and variance.\n",
      "\n",
      "Exercise 4.10 Derivation of information form formulae for marginalizing and conditioning Derive the information form results of Section 4.3.1.\n",
      "\n",
      "Exercise 4.11 Derivation of the NIW posterior Derive Equation 4.209. Hint: one can show that\n",
      "\n",
      "N (x − μ)(x − μ)T + κ0(μ − m0)(μ − m0)T\n",
      "\n",
      "(4.271)\n",
      "\n",
      "= κN (μ − mN )(μ − mN )T +\n",
      "\n",
      "κ0N κN\n",
      "\n",
      "(x − m0)(x − m0)T\n",
      "\n",
      "(4.272)\n",
      "\n",
      "This is a matrix generalization of an operation called completing the square.5 Derive the corresponding result for the normal-Wishart model.\n",
      "\n",
      "Exercise 4.12 BIC for Gaussians (Source: Jaakkola.) The Bayesian information criterion (BIC) is a penalized log-likelihood function that can be used for model selection (see Section 5.3.2.4). It is deﬁned as\n",
      "\n",
      "BIC = log p(D|ˆθM L) −\n",
      "\n",
      "d\n",
      "\n",
      "2\n",
      "\n",
      "log(N )\n",
      "\n",
      "(4.273)\n",
      "\n",
      "where d is the number of free parameters in the model and N is the number of samples. In this question, we will see how to use this to choose between a full covariance Gaussian and a Gaussian with a diagonal covariance. Obviously a full covariance Gaussian has higher likelihood, but it may not be “worth” the extra parameters if the improvement over a diagonal covariance matrix is too small. So we use the BIC score to choose the model. Following Section 4.1.3, we can write\n",
      "\n",
      "log p(D| ˆΣ, ˆμ) =−\n",
      "\n",
      "ˆS =\n",
      "\n",
      "1 N\n",
      "\n",
      "N\n",
      "\n",
      "2 N(cid:12)\n",
      "\n",
      "i=1\n",
      "\n",
      "tr\n",
      "\n",
      "(xi − x)(xi − x)T\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "ˆΣ\n",
      "\n",
      "−1 ˆS\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "−\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log( ˆ|Σ|)\n",
      "\n",
      "(4.274)\n",
      "\n",
      "(4.275)\n",
      "\n",
      "where ˆS is the scatter matrix (empirical covariance), the trace of a matrix is the sum of its diagonals, and we have used the trace trick.\n",
      "\n",
      "a. Derive the BIC score for a Gaussian in D dimensions with full covariance matrix. Simplify your answer as much as possible, exploiting the form of the MLE. Be sure to specify the number of free parameters d.\n",
      "\n",
      "b. Derive the BIC score for a Gaussian in D dimensions with a diagonal covariance matrix. Be sure to specify the number of free parameters d. Hint: for the digaonal case, the ML estimate of Σ is the same as ˆΣM L except the off-diagonal terms are zero:\n",
      "\n",
      "ˆΣdiag = diag( ˆΣM L(1, 1), . . . , ˆΣM L(D, D))\n",
      "\n",
      "(4.276)\n",
      "\n",
      "5. In the scalar case, completing the square means rewriting c2x2 + c1x + c0 as −a(x − b)2 + w where a = −c2, b =\n",
      "\n",
      "c1 2c2\n",
      "\n",
      "and w =\n",
      "\n",
      "c2 1 4c2\n",
      "\n",
      "+ c0.\n",
      "\n",
      "144\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "Exercise 4.13 Gaussian posterior credible interval (Source: DeGroot.) Let X ∼ N (μ, σ2 = 4) where μ is unknown but has prior μ ∼ N (μ0, σ2 seeing n samples is μ ∼ N (μn, σ2 conﬁdence interval.) How big does n have to be to ensure\n",
      "\n",
      "n).\n",
      "\n",
      "0 = 9). The posterior after (This is called a credible interval, and is the Bayesian analog of a\n",
      "\n",
      "(4.277) where ((cid:7), u) is an interval (centered on μn) of width 1 and D is the data. Hint: recall that 95% of the probability mass of a Gaussian is within ±1.96σ of the mean.\n",
      "\n",
      "p((cid:7) ≤ μn ≤ u|D) ≥ 0.95\n",
      "\n",
      "Exercise 4.14 MAP estimation for 1D Gaussians (Source: Jaakkola.) Consider samples x1, . . . , xn from a Gaussian random variable with known variance σ2 and unknown mean μ. We further assume a prior distribution (also Gaussian) over the mean, μ ∼ N (m, s2), with ﬁxed mean m and ﬁxed variance s2. Thus the only unknown is μ.\n",
      "\n",
      "a. Calculate the MAP estimate ˆμM AP . You can state the result without proof. Alternatively, with a lot\n",
      "\n",
      "more work, you can compute derivatives of the log posterior, set to zero and solve.\n",
      "\n",
      "b. Show that as the number of samples n increase, the MAP estimate converges to the maximum likelihood\n",
      "\n",
      "estimate.\n",
      "\n",
      "c. Suppose n is small and ﬁxed. What does the MAP estimator converge to if we increase the prior\n",
      "\n",
      "variance s2?\n",
      "\n",
      "d. Suppose n is small and ﬁxed. What does the MAP estimator converge to if we decrease the prior\n",
      "\n",
      "variance s2?\n",
      "\n",
      "Exercise 4.15 Sequential (recursive) updating of ˆΣ (Source: (Duda et al. 2001, Q3.35,3.36).) The unbiased estimates for the covariance of a d-dimensional Gaussian based on n samples is given by\n",
      "\n",
      "ˆΣ = Cn =\n",
      "\n",
      "1 n − 1\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "(xi − mn)(xi − mn)T\n",
      "\n",
      "i=1\n",
      "\n",
      "(4.278)\n",
      "\n",
      "It is clear that it takes O(nd2) time to compute Cn. efficient to incrementally update these estimates than to recompute from scratch.\n",
      "\n",
      "If the data points arrive one at a time, it is more\n",
      "\n",
      "a. Show that the covariance can be sequentially udpated as follows 1 n + 1\n",
      "\n",
      "Cn+1 =\n",
      "\n",
      "n − 1 n\n",
      "\n",
      "Cn +\n",
      "\n",
      "(xn+1 − mn)(xn+1 − mn)T\n",
      "\n",
      "(4.279)\n",
      "\n",
      "b. How much time does it take per sequential update? (Use big-O notation.) c. Show that we can sequentially update the precision matrix using\n",
      "\n",
      "C−1\n",
      "\n",
      "n+1 =\n",
      "\n",
      "n n − 1\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "C−1\n",
      "\n",
      "n −\n",
      "\n",
      "n2−1\n",
      "\n",
      "C−1 n + (xn+1 − mn)T C−1\n",
      "\n",
      "n (xn+1 − mn)(xn+1 − mn)T C−1 n\n",
      "\n",
      "n (xn+1 − mn)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(4.280)\n",
      "\n",
      "Hint: notice that the update to Cn+1 consists of adding a rank-one matrix, namely uuT , where u = xn+1 − mn. Use the matrix inversion lemma for rank-one updates (Equation 4.111), which we repeat here for convenience:\n",
      "\n",
      "(E + uvT )−1 = E−1 −\n",
      "\n",
      "E−1uvT E−1 1 +v T E−1u\n",
      "\n",
      "(4.281)\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "145\n",
      "\n",
      "d. What is the time complexity per update?\n",
      "\n",
      "Exercise 4.16 Likelihood ratio for Gaussians Source: Source: Alpaydin p103 ex 4. Consider a binary classiﬁer where the K class conditional densities are MVN p(x|y = j) = N (x|μj, Σj). By Bayes rule, we have\n",
      "\n",
      "log\n",
      "\n",
      "p(y = 1|x) p(y = 0|x)\n",
      "\n",
      "= log\n",
      "\n",
      "p(x|y = 1) p(x|y = 0)\n",
      "\n",
      "+ log\n",
      "\n",
      "p(y = 1) p(y = 0)\n",
      "\n",
      "(4.282)\n",
      "\n",
      "In other words, the log posterior ratio is the log likelihood ratio plus the log prior ratio. For each of the 4 cases in the table below, derive an expression for the log likelihood ratio log p(x|y=1) p(x|y=0) , simplifying as much as possible.\n",
      "\n",
      "Form of Σj Cov Σj Arbitrary Σj = Σ Shared Shared, axis-aligned Σj = Σ with Σij = 0 for i (cid:8)= j Σj = σ2I Shared, spherical\n",
      "\n",
      "Num parameters Kd(d + 1)/2 d(d + 1)/2 d 1\n",
      "\n",
      "Exercise 4.17 LDA/QDA on height/weight data The function discrimAnalysisHeightWeightDemo ﬁts an LDA and QDA model to the height/weight data. Compute the misclassiﬁcation rate of both of these models on the training set. Turn in your numbers and code.\n",
      "\n",
      "Exercise 4.18 Naive Bayes with mixed features Consider a 3 class naive Bayes classiﬁer with one binary feature and one Gaussian feature:\n",
      "\n",
      "y ∼ Mu(y|π, 1), x1|y = c ∼ Ber(x1|θc), x2|y = c ∼ N (x2|μc, σ2 c )\n",
      "\n",
      "(4.283)\n",
      "\n",
      "Let the parameter vectors be as follows:\n",
      "\n",
      "π = (0.5, 0.25, 0.25), θ = (0.5, 0.5, 0.5), μ = (−1, 0, 1), σ2 = (1, 1, 1)\n",
      "\n",
      "(4.284)\n",
      "\n",
      "a. Compute p(y|x1 = 0, x2 = 0) (the result should be a vector of 3 numbers that sums to 1). b. Compute p(y|x1 = 0). c. Compute p(y|x2 = 0). d. Explain any interesting patterns you see in your results. Hint: look at the parameter vector θ.\n",
      "\n",
      "Exercise 4.19 Decision boundary for LDA with semi tied covariances Consider a generative classiﬁer with class conditional densities of the form N (x|μc, Σc). In LDA, we assume Σc = Σ, and in QDA, each Σc is arbitrary. Here we consider the 2 class case in which Σ1 = kΣ0, for k > 1. That is, the Gaussian ellipsoids have the same “shape”, but the one for class 1 is “wider”. Derive an expression for p(y = 1|x, θ), simplifying as much as possible. Give a geometric interpretation of your result, if possible.\n",
      "\n",
      "Exercise 4.20 Logistic regression vs LDA/QDA (Source: Jaakkola.) Suppose we train the following binary classiﬁers via maximum likelihood.\n",
      "\n",
      "a. GaussI: A generative classiﬁer, where the class conditional densities are Gaussian, with both covariance\n",
      "\n",
      "matrices set to I (identity matrix), i.e., p(x|y = c) = N (x|μc, I). We assume p(y) is uniform.\n",
      "\n",
      "b. GaussX: as for GaussI, but the covariance matrices are unconstrained, i.e., p(x|y = c) = N (x|μc, Σc).\n",
      "\n",
      "146\n",
      "\n",
      "Chapter 4. Gaussian models\n",
      "\n",
      "c. LinLog: A logistic regression model with linear features.\n",
      "\n",
      "d. QuadLog: A logistic regression model, using linear and quadratic features (i.e., polynomial basis function\n",
      "\n",
      "expansion of degree 2).\n",
      "\n",
      "After training we compute the performance of each model M on the training set as follows:\n",
      "\n",
      "L(M ) =\n",
      "\n",
      "1 n\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "i=1\n",
      "\n",
      "log p(yi|xi, ˆθ, M )\n",
      "\n",
      "(4.285)\n",
      "\n",
      "(Note that this is the conditional log-likelihood p(y|x, ˆθ) and not the joint log-likelihood p(y, x|ˆθ).) We now want to compare the performance of each model. We will write L(M ) ≤ L(M (cid:2)) if model M must have lower (or equal) log likelihood (on the training set) than M (cid:2), for any training set (in other words, M is worse than M (cid:2), at least as far as training set logprob is concerned). For each of the following model pairs, state whether L(M ) ≤ L(M (cid:2)), L(M ) ≥ L(M (cid:2)), or whether no such statement can be made (i.e., M might sometimes be better than M (cid:2) and sometimes worse); also, for each question, brieﬂy (1-2 sentences) explain why.\n",
      "\n",
      "a. GaussI, LinLog. b. GaussX, QuadLog. c. LinLog, QuadLog. d. GaussI, QuadLog. e. Now suppose we measure performance in terms of the average misclassiﬁcation rate on the training\n",
      "\n",
      "set:\n",
      "\n",
      "R(M ) =\n",
      "\n",
      "1 n\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "i=1\n",
      "\n",
      "I(yi (cid:8)= ˆy(xi))\n",
      "\n",
      "(4.286)\n",
      "\n",
      "Is it true in general that L(M ) > L(M (cid:2)) implies that R(M ) < R(M (cid:2))? Explain why or why not.\n",
      "\n",
      "Exercise 4.21 Gaussian decision boundaries (Source: 1, μ2 = 1, σ2\n",
      "\n",
      "(Duda et al. 2001, Q3.7).) Let p(x|y = j) =N (x|μj, σj) where j = 1, 2 and μ1 = 0, σ2\n",
      "\n",
      "2 = 106. Let the class priors be equal, p(y = 1) =p (y = 2) = 0.5.\n",
      "\n",
      "1 =\n",
      "\n",
      "a. Find the decision region\n",
      "\n",
      "R1 = {x : p(x|μ1, σ1) ≥ p(x|μ2, σ2)}\n",
      "\n",
      "(4.287)\n",
      "\n",
      "Sketch the result. Hint: draw the curves and ﬁnd where they intersect. Find both solutions of the equation\n",
      "\n",
      "p(x|μ1, σ1) = p(x|μ2, σ2)\n",
      "\n",
      "(4.288)\n",
      "\n",
      "Hint: recall that to solve a quadratic equation ax2 + bx + c = 0, we use\n",
      "\n",
      "x =\n",
      "\n",
      "−b ±\n",
      "\n",
      "√\n",
      "\n",
      "b2 − 4ac 2a\n",
      "\n",
      "(4.289)\n",
      "\n",
      "b. Now suppose σ2 = 1 (and all other parameters remain the same). What is R1 in this case?\n",
      "\n",
      "4.6.\n",
      "\n",
      "Inferring the parameters of an MVN\n",
      "\n",
      "147\n",
      "\n",
      "Exercise 4.22 QDA with 3 classes Consider a three category classiﬁcation problem. Let the prior probabilites:\n",
      "\n",
      "P (Y = 1) =P (Y = 2) =P (Y = 3) = 1/3\n",
      "\n",
      "(4.290)\n",
      "\n",
      "The class-conditional densities are multivariate normal densities with parameters:\n",
      "\n",
      "μ1 = [0, 0]T , μ2 = [1, 1]T , μ3 = [−1, 1]T\n",
      "\n",
      "(4.291)\n",
      "\n",
      "Σ1 =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "0.7 0\n",
      "\n",
      "0 0.7\n",
      "\n",
      "(cid:21)\n",
      "\n",
      ", Σ2 =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "0.8 0.2\n",
      "\n",
      "0.2 0.8\n",
      "\n",
      "(cid:21)\n",
      "\n",
      ", Σ3 =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "0.8 0.2\n",
      "\n",
      "0.2 0.8\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(4.292)\n",
      "\n",
      "Classify the following points:\n",
      "\n",
      "a. x = [−0.5, 0.5] b. x = [0.5, 0.5]\n",
      "\n",
      "Exercise 4.23 Scalar QDA [Note: you can solve this exercise by hand or using a computer (matlab, R, whatever). In either case, show your work.] Consider the following training set of heights x (in inches) and gender y (male/female) of some US college students: x = (67, 79, 71, 68, 67, 60), y = (m, m, m, f, f, f ).\n",
      "\n",
      "a. Fit a Bayes classiﬁer to this data, using maximum likelihood estimation, i.e., estimate the parameters of\n",
      "\n",
      "the class conditional likelihoods p(x|y = c) = N (x; μc, σc)\n",
      "\n",
      "(4.293)\n",
      "\n",
      "and the class prior p(y = c) = πc\n",
      "\n",
      "(4.294) What are your values of μc, σc, πc for c = m, f ? Show your work (so you can get partial credit if you make an arithmetic error).\n",
      "\n",
      "b. Compute p(y = m|x, ˆθ), where x = 72, and ˆθ are the MLE parameters.\n",
      "\n",
      "(This is called a plug-in\n",
      "\n",
      "prediction.)\n",
      "\n",
      "c. What would be a simple way to extend this technique if you had multiple attributes per person, such\n",
      "\n",
      "as height and weight? Write down your proposed model as an equation.\n",
      "\n",
      "5 Bayesian statistics\n",
      "\n",
      "5.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "We have now seen a variety of different probability models, and we have discussed how to i.e., we have discussed how to compute MAP parameter estimates ˆθ = ﬁt them to data, argmax p(θ|D), using a variety of different priors. We have also discussed how to compute the full posterior p(θ|D), as well as the posterior predictive density, p(x|D), for certain special cases (and in later chapters, we will discuss algorithms for the general case).\n",
      "\n",
      "Using the posterior distribution to summarize everything we know about a set of unknown variables is at the core of Bayesian statistics. In this chapter, we discuss this approach to statistics in more detail. In Chapter 6, we discuss an alternative approach to statistics known as frequentist or classical statistics.\n",
      "\n",
      "5.2\n",
      "\n",
      "Summarizing posterior distributions\n",
      "\n",
      "The posterior p(θ|D) summarizes everything we know about the unknown quantities θ. In this section, we discuss some simple quantities that can be derived from a probability distribution, such as a posterior. These summary statistics are often easier to understand and visualize than the full joint.\n",
      "\n",
      "5.2.1 MAP estimation\n",
      "\n",
      "We can easily compute a point estimate of an unknown quantity by computing the posterior mean, median or mode. In Section 5.7, we discuss how to use decision theory to choose between these methods. Typically the posterior mean or median is the most appropriate choice for a real- valued quantity, and the vector of posterior marginals is the best choice for a discrete quantity. However, the posterior mode, aka the MAP estimate, is the most popular choice because it reduces to an optimization problem, for which efficient algorithms often exist. Futhermore, MAP estimation can be interpreted in non-Bayesian terms, by thinking of the log prior as a regularizer (see Section 6.5 for more details).\n",
      "\n",
      "Although this approach is computationally appealing, it is important to point out that there are various drawbacks to MAP estimation, which we brieﬂy discuss below. This will provide motivation for the more thoroughly Bayesian approach which we will study later in this chapter (and elsewhere in this book).\n",
      "\n",
      "150\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "4.5\n",
      "\n",
      "0.9\n",
      "\n",
      "4\n",
      "\n",
      "0.8\n",
      "\n",
      "3.5\n",
      "\n",
      "0.7\n",
      "\n",
      "3\n",
      "\n",
      "0.6\n",
      "\n",
      "2.5\n",
      "\n",
      "0.5\n",
      "\n",
      "2\n",
      "\n",
      "0.4\n",
      "\n",
      "1.5\n",
      "\n",
      "0.3\n",
      "\n",
      "1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.5\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 5.1 (a) A bimodal distribution in which the mode is very untypical of the distribution. The thin blue vertical line is the mean, which is arguably a better summary of the distribution, since it is near the majority of the probability mass. Figure generated by bimodalDemo. (b) A skewed distribution in which the mode is quite different from the mean. Figure generated by gammaPlotDemo.\n",
      "\n",
      "5.2.1.1\n",
      "\n",
      "No measure of uncertainty\n",
      "\n",
      "The most obvious drawback of MAP estimation, and indeed of any other point estimate such as the posterior mean or median, is that it does not provide any measure of uncertainty. In many applications, it is important to know how much one can trust a given estimate. We can derive such conﬁdence measures from the posterior, as we discuss in Section 5.2.2.\n",
      "\n",
      "5.2.1.2\n",
      "\n",
      "Plugging in the MAP estimate can result in overﬁtting\n",
      "\n",
      "In machine learning, we often care more about predictive accuracy than in interpreting the parameters of our models. However, if we don’t model the uncertainty in our parameters, then our predictive distribution will be overconﬁdent. We saw several examples of this in Chapter 3, and we will see more examples later. Overconﬁdence in predictions is particularly problematic in situations where we may be risk averse; see Section 5.7 for details.\n",
      "\n",
      "5.2.1.3\n",
      "\n",
      "The mode is an untypical point\n",
      "\n",
      "Choosing the mode as a summary of a posterior distribution is often a very poor choice, since the mode is usually quite untypical of the distribution, unlike the mean or median. This is illustrated in Figure 5.1(a) for a 1d continuous space. The basic problem is that the mode is a point of measure zero, whereas the mean and median take the volume of the space into account. Another example is shown in Figure 5.1(b): here the mode is 0, but the mean is non-zero. Such skewed distributions often arise when inferring variance parameters, especially in hierarchical models. In such cases the MAP estimate (and hence the MLE) is obviously a very bad estimate. How should we summarize a posterior if the mode is not a good choice? The answer is to use decision theory, which we discuss in Section 5.7. The basic idea is to specify a loss function, where L(θ, ˆθ) is the loss you incur if the truth is θ and your estimate is ˆθ. If we use 0-1 loss, L(θ, ˆθ) = I(θ (cid:6)= ˆθ), then the optimal estimate is the posterior mode. 0-1 loss means you only get “points” if you make no errors, otherwise you get nothing: there is no “partial credit” under\n",
      "\n",
      "5.2. Summarizing posterior distributions\n",
      "\n",
      "151\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "g\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "p\n",
      "\n",
      "Y\n",
      "\n",
      "0.1\n",
      "\n",
      "p\n",
      "\n",
      "X\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "Figure 5.2 Example of the transformation of a density under a nonlinear transform. Note how the mode of the transformed distribution is not the transform of the original mode. Based on Exercise 1.4 of (Bishop 2006b). Figure generated by bayesChangeOfVar.\n",
      "\n",
      "this loss function! For continuous-valued quantities, we often prefer to use squared error loss, L(θ, ˆθ) = (θ − ˆθ)2; the corresponding optimal estimator is then the posterior mean, as we show in Section 5.7. Or we can use a more robust loss function, L(θ, ˆθ) = |θ − ˆθ|, which gives rise to the posterior median.\n",
      "\n",
      "5.2.1.4 MAP estimation is not invariant to reparameterization *\n",
      "\n",
      "A more subtle problem with MAP estimation is that the result we get depends on how we pa- rameterize the probability distribution. Changing from one representation to another equivalent representation changes the result, which is not very desirable, since the units of measurement are arbitrary (e.g., when measuring distance, we can use centimetres or inches).\n",
      "\n",
      "the distribution for y is given by Equation 2.87, which we repeat here for convenience:\n",
      "\n",
      "To understand the problem, suppose we compute the posterior for x. If we deﬁne y = f (x),\n",
      "\n",
      "py(y) = px(x)\n",
      "\n",
      "- dx dy\n",
      "\n",
      "-\n",
      "\n",
      "(5.1)\n",
      "\n",
      "The | dx dy | term is called the Jacobian, and it measures the change in size of a unit volume passed through f . Let ˆx = argmaxx px(x) be the MAP estimate for x. In general it is not the case that ˆy = argmaxy py(y) is given by f (ˆx). For example, let x ∼ N (6, 1) and y = f (x), where\n",
      "\n",
      "f (x) =\n",
      "\n",
      "1 1 + exp(−x + 5)\n",
      "\n",
      "(5.2)\n",
      "\n",
      "We can derive the distribution of y using Monte Carlo simulation (see Section 2.7.1). The result is shown in Figure 5.2. We see that the original Gaussian has become “squashed” by the sigmoid nonlinearity. In particular, we see that the mode of the transformed distribution is not equal to the transform of the original mode.\n",
      "\n",
      "152\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "To see how this problem arises in the context of MAP estimation, consider the following example, due to Michael Jordan. The Bernoulli distribution is typically parameterized by its mean μ, so p(y = 1|μ) =μ , where y ∈ {0, 1}. Suppose we have a uniform prior on the unit interval: pμ(μ) = 1I (0 ≤ μ ≤ 1). If there is no data, the MAP estimate is just the mode of the prior, which can be anywhere between 0 and 1. We will now show that different parameterizations can pick different points in this interval arbitrarily.\n",
      "\n",
      "First let θ =\n",
      "\n",
      "pθ(θ) = pμ(μ)\n",
      "\n",
      "√\n",
      "\n",
      "μ so μ = θ2. The new prior is - - dμ dθ\n",
      "\n",
      "- = 2θ\n",
      "\n",
      "(5.3)\n",
      "\n",
      "for θ ∈ [0, 1] so the new mode is\n",
      "\n",
      "Now let φ = 1 − - - dμ dφ\n",
      "\n",
      "ˆθM AP = arg max θ∈[0,1] √\n",
      "\n",
      "pφ(φ) = pμ(μ)\n",
      "\n",
      "1 − μ. The new prior is - - = 2(1 − φ)\n",
      "\n",
      "2θ = 1\n",
      "\n",
      "(5.4)\n",
      "\n",
      "(5.5)\n",
      "\n",
      "for φ ∈ [0, 1], so the new mode is\n",
      "\n",
      "ˆφM AP = arg max φ∈[0,1]\n",
      "\n",
      "2 − 2φ = 0\n",
      "\n",
      "(5.6)\n",
      "\n",
      "Thus the MAP estimate depends on the parameterization. The MLE does not suffer from this since the likelihood is a function, not a probability density. Bayesian inference does not suffer from this problem either, since the change of measure is taken into account when integrating over the parameter space.\n",
      "\n",
      "One solution to the problem is to optimize the following objective function:\n",
      "\n",
      "ˆθ = argmax\n",
      "\n",
      "θ\n",
      "\n",
      "p(D|θ)p(θ)|I(θ)|− 1\n",
      "\n",
      "2\n",
      "\n",
      "(5.7)\n",
      "\n",
      "Here I(θ) is the Fisher information matrix associated with p(x|θ) (see Section 6.2.2). This estimate is parameterization independent, for reasons explained in (Jermyn 2005; Druilhet and Marin 2007). Unfortunately, optimizing Equation 5.7 is often difficult, which minimizes the appeal of the whole approach.\n",
      "\n",
      "5.2.2\n",
      "\n",
      "Credible intervals\n",
      "\n",
      "In addition to point estimates, we often want a measure of conﬁdence. A standard measure of conﬁdence in some (scalar) quantity θ is the “width” of its posterior distribution. This can be measured using a 100(1 − α)% credible interval, which is a (contiguous) region C = ((cid:9), u) (standing for lower and upper) which contains 1 − α of the posterior probability mass, i.e.,\n",
      "\n",
      "Cα(D) = ((cid:9), u) : P ((cid:9) ≤ θ ≤ u|D) = 1 − α\n",
      "\n",
      "(5.8)\n",
      "\n",
      "There may be many such intervals, so we choose one such that there is (1 − α)/2 mass in each tail; this is called a central interval.\n",
      "\n",
      "5.2. Summarizing posterior distributions\n",
      "\n",
      "153\n",
      "\n",
      "3.5\n",
      "\n",
      "3.5\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "2.5\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 5.3 (a) Central interval and (b) HPD region for a Beta(3,9) posterior. The CI is (0.06, 0.52) and the HPD is (0.04, 0.48). Based on Figure 3.6 of (Hoff 2009). Figure generated by betaHPD.\n",
      "\n",
      "If the posterior has a known functional form, we can compute the posterior central interval using (cid:9) = F −1(α/2) and u = F −1(1−α/2), where F is the cdf of the posterior. For example, if the posterior is Gaussian, p(θ|D) = N (0, 1), and α = 0.05, then we have (cid:9) = Φ(α/2) = −1.96, and u = Φ(1 − α/2) = 1.96, where Φ denotes the cdf of the Gaussian. This is illustrated in Figure 2.3(c). This justiﬁes the common practice of quoting a credible interval in the form of μ ± 2σ, where μ represents the posterior mean, σ represents the posterior standard deviation, and 2 is a good approximation to 1.96.\n",
      "\n",
      "Of course, the posterior is not always Gaussian. For example, in our coin example, if we use a uniform prior and we observe N1 = 47 heads out of N = 100 trials, then the posterior is a beta distribution, p(θ|D) = Beta(48, 54). We ﬁnd the 95% posterior credible interval is (0.3749, 0.5673) (see betaCredibleInt for the one line of Matlab code we used to compute this).\n",
      "\n",
      "If we don’t know the functional form, but we can draw samples from the posterior, then we can use a Monte Carlo approximation to the posterior quantiles: we simply sort the S samples, and ﬁnd the one that occurs at location α/S along the sorted list. As S → ∞, this converges to the true quantile. See mcQuantileDemo for a demo.\n",
      "\n",
      "People often confuse Bayesian credible intervals with frequentist conﬁdence intervals. How- ever, they are not the same thing, as we discuss in Section 6.6.1. In general, credible intervals are usually what people want to compute, but conﬁdence intervals are usually what they actually compute, because most people are taught frequentist statistics but not Bayesian statistics. Fortu- nately, the mechanics of computing a credible interval is just as easy as computing a conﬁdence interval (see e.g., betaCredibleInt for how to do it in Matlab).\n",
      "\n",
      "5.2.2.1\n",
      "\n",
      "Highest posterior density regions *\n",
      "\n",
      "A problem with central intervals is that there might be points outside the CI which have higher probability density. This is illustrated in Figure 5.3(a), where we see that points outside the left-most CI boundary have higher density than those just inside the right-most CI boundary.\n",
      "\n",
      "This motivates an alternative quantity known as the highest posterior density or HPD region. This is deﬁned as the (set of) most probable points that in total constitute 100(1 − α)% of the\n",
      "\n",
      "154\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "α/2\n",
      "\n",
      "α/2\n",
      "\n",
      "pMIN\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 5.4 Figure 2.2 of (Gelman et al. 2004). Figure generated by postDensityIntervals.\n",
      "\n",
      "(a) Central interval and (b) HPD region for a hypothetical multimodal posterior. Based on\n",
      "\n",
      "probability mass. More formally, we ﬁnd the threshold p∗ on the pdf such that\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "1 − α =\n",
      "\n",
      "θ:p(θ|D)>p∗\n",
      "\n",
      "p(θ|D)dθ\n",
      "\n",
      "(5.9)\n",
      "\n",
      "and then deﬁne the HPD as\n",
      "\n",
      "Cα(D) = {θ : p(θ|D) ≥ p∗}\n",
      "\n",
      "(5.10)\n",
      "\n",
      "In 1d, the HPD region is sometimes called a highest density interval or HDI. For example, Figure 5.3(b) shows the 95% HDI of a Beta(3, 9) distribution, which is (0.04, 0.48). We see that this is narrower than the CI, even though it still contains 95% of the mass; furthermore, every point inside of it has higher density than every point outside of it.\n",
      "\n",
      "For a unimodal distribution, the HDI will be the narrowest interval around the mode contain- ing 95% of the mass. To see this, imagine “water ﬁlling” in reverse, where we lower the level until 95% of the mass is revealed, and only 5% is submerged. This gives a simple algorithm for computing HDIs in the 1d case: simply search over points such that the interval contains 95% of the mass and has minimal width. This can be done by 1d numerical optimization if we know the inverse CDF of the distribution, or by search over the sorted data points if we have a bag of samples (see betaHPD for a demo).\n",
      "\n",
      "If the posterior is multimodal, the HDI may not even be a connected region: see Figure 5.4(b)\n",
      "\n",
      "for an example. However, summarizing multimodal posteriors is always difficult.\n",
      "\n",
      "5.2.3\n",
      "\n",
      "Inference for a difference in proportions\n",
      "\n",
      "Sometimes we have multiple parameters, and we are interested in computing the posterior distribution of some function of these parameters. For example, suppose you are about to buy something from Amazon.com, and there are two sellers offering it for the same price. Seller 1 has 90 positive reviews and 10 negative reviews. Seller 2 has 2 positive reviews and 0 negative reviews. Who should you buy from?1\n",
      "\n",
      "1. This example is from www.johndcook.com/blog/2011/09/27/bayesian-amazon. See also lingpipe-blog.c om/2009/10/13/bayesian-counterpart-to-fisher-exact-test-on-contingency-tables.\n",
      "\n",
      "5.3. Bayesian model selection\n",
      "\n",
      "155\n",
      "\n",
      "14\n",
      "\n",
      "p(θ |data) 1 p(θ |data) 2\n",
      "\n",
      "2.5\n",
      "\n",
      "12\n",
      "\n",
      "2\n",
      "\n",
      "10\n",
      "\n",
      "1.5\n",
      "\n",
      "8\n",
      "\n",
      "f d p\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      "4\n",
      "\n",
      "0.5\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "0 −0.4\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "δ\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(a) Exact posteriors p(θi|Di). (b) Monte Carlo approximation to p(δ|D). We use kernel density Figure 5.5 estimation to get a smooth plot. The vertical lines enclose the 95% central interval. Figure generated by amazonSellerDemo,\n",
      "\n",
      "On the face of it, you should pick seller 2, but we cannot be very conﬁdent that seller 2 is better since it has had so few reviews. In this section, we sketch a Bayesian analysis of this problem. Similar methodology can be used to compare rates or proportions across groups for a variety of other settings.\n",
      "\n",
      "Let θ1 and θ2 be the unknown reliabilities of the two sellers. Since we don’t know much about them, we’ll endow them both with uniform priors, θi ∼ Beta(1, 1). The posteriors are p(θ1|D1) = Beta(91, 11) and p(θ2|D2) = Beta(3, 1).\n",
      "\n",
      "let us deﬁne δ = θ1 − θ2 as the difference in the rates. (Alternatively we might want to work in terms of the log-odds ratio.) We can compute the desired quantity using numerical integration:\n",
      "\n",
      "We want to compute p(θ1 > θ2|D). For convenience,\n",
      "\n",
      "(cid:11) 1\n",
      "\n",
      "(cid:11) 1\n",
      "\n",
      "p(δ > 0|D) =\n",
      "\n",
      "I(θ1 > θ2)Beta(θ1|y1 + 1, N1 − y1 + 1)\n",
      "\n",
      "(5.11) We ﬁnd p(δ > 0|D) = 0.710, which means you are better off buying from seller 1! See (It is also possible to solve the integral analytically (Cook amazonSellerDemo for the code. 2005).)\n",
      "\n",
      "A simpler way to solve the problem is to approximate the posterior p(δ|D) by Monte Carlo sampling. This is easy, since θ1 and θ2 are independent in the posterior, and both have beta distributions, which can be sampled from using standard methods. The distributions p(θi|Di) are shown in Figure 5.5(a), and a MC approximation to p(δ|D), together with a 95% HPD, is shown Figure 5.5(b). An MC approximation to p(δ > 0|D) is obtained by counting the fraction of samples where θ1 > θ2; this turns out to be 0.718, which is very close to the exact value. (See amazonSellerDemo for the code.)\n",
      "\n",
      "0 Beta(θ2|y2 + 1, N2 − y2 + 1)dθ1dθ2\n",
      "\n",
      "0\n",
      "\n",
      "5.3\n",
      "\n",
      "Bayesian model selection\n",
      "\n",
      "In Figure 1.18, we saw that using too high a degree polynomial results in overﬁtting, and using too low a degree results in underﬁtting. Similarly, in Figure 7.8(a), we saw that using too small\n",
      "\n",
      "156\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "a regularization parameter results in overﬁtting, and too large a value results in underﬁtting. In general, when faced with a set of models (i.e., families of parametric distributions) of different complexity, how should we choose the best one? This is called the model selection problem.\n",
      "\n",
      "One approach is to use cross-validation to estimate the generalization error of all the candiate models, and then to pick the model that seems the best. However, this requires ﬁtting each model K times, where K is the number of CV folds. A more efficient approach is to compute the posterior over models,\n",
      "\n",
      "p(m|D) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(D|m)p(m) m∈M p(m, D)\n",
      "\n",
      "(5.12)\n",
      "\n",
      "From this, we can easily compute the MAP model, ˆm = argmax p(m|D). This is called Bayesian model selection.\n",
      "\n",
      "If we use a uniform prior over models, p(m) ∝ 1, this amounts to picking the model which\n",
      "\n",
      "maximizes\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(D|m) =\n",
      "\n",
      "p(D|θ)p(θ|m)dθ\n",
      "\n",
      "(5.13)\n",
      "\n",
      "This quantity is called the marginal likelihood, the integrated likelihood, or the evidence for model m. The details on how to perform this integral will be discussed in Section 5.3.2. But ﬁrst we give an intuitive interpretation of what this quantity means.\n",
      "\n",
      "5.3.1\n",
      "\n",
      "Bayesian Occam’s razor\n",
      "\n",
      "One might think that using p(D|m) to select models would always favor the model with the most parameters. This is true if we use p(D|ˆθm) to select models, where ˆθm is the MLE or MAP estimate of the parameters for model m, because models with more parameters will ﬁt the data better, and hence achieve higher likelihood. However, if we integrate out the parameters, rather than maximizing them, we are automatically protected from overﬁtting: models with more parameters do not necessarily have higher marginal likelihood. This is called the Bayesian Occam’s razor effect (MacKay 1995b; Murray and Ghahramani 2005), named after the principle known as Occam’s razor, which says one should pick the simplest model that adequately explains the data.\n",
      "\n",
      "One way to understand the Bayesian Occam’s razor is to notice that the marginal likelihood\n",
      "\n",
      "can be rewritten as follows, based on the chain rule of probability (Equation 2.5):\n",
      "\n",
      "p(D) = p(y1)p(y2|y1)p(y3|y1:2) . . . p(yN |y1:N −1)\n",
      "\n",
      "(5.14)\n",
      "\n",
      "where we have dropped the conditioning on x for brevity. This is similar to a leave-one-out cross-validation estimate (Section 1.4.8) of the likelihood, since we predict each future point given all the previous ones. (Of course, the order of the data does not matter in the above expression.) If a model is too complex, it will overﬁt the “early” examples and will then predict the remaining ones poorly.\n",
      "\n",
      "Another way to understand the Bayesian Occam’s razor effect is to note that probabilities must D(cid:2) p(D(cid:2)|m) = 1, where the sum is over all possible data sets. Complex sum to one. Hence models, which can predict many things, must spread their probability mass thinly, and hence will not obtain as large a probability for any given data set as simpler models. This is sometimes\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "5.3. Bayesian model selection\n",
      "\n",
      "157\n",
      "\n",
      "Figure 5.6 A schematic illustration of the Bayesian Occam’s razor. The broad (green) curve corresponds to a complex model, the narrow (blue) curve to a simple model, and the middle (red) curve is just right. Based on Figure 3.13 of (Bishop 2006a). See also (Murray and Ghahramani 2005, Figure 2) for a similar plot produced on real data.\n",
      "\n",
      "called the conservation of probability mass principle, and is illustrated in Figure 5.6. On the horizontal axis we plot all possible data sets in order of increasing complexity (measured in some abstract sense). On the vertical axis we plot the predictions of 3 possible models: a simple one, M1; a medium one, M2; and a complex one, M3. We also indicate the actually observed data D0 by a vertical line. Model 1 is too simple and assigns low probability to D0. Model 3 also assigns D0 relatively low probability, because it can predict many data sets, and hence it spreads its probability quite widely and thinly. Model 2 is “just right”: it predicts the observed data with a reasonable degree of conﬁdence, but does not predict too many other things. Hence model 2 is the most probable model.\n",
      "\n",
      "As a concrete example of the Bayesian Occam’s razor, consider the data in Figure 5.7. We plot polynomials of degrees 1, 2 and 3 ﬁt to N = 5 data points. It also shows the posterior over models, where we use a Gaussian prior (see Section 7.6 for details). There is not enough data to justify a complex model, so the MAP model is d = 1. Figure 5.8 shows what happens when N = 30. Now it is clear that d = 2 is the right model (the data was in fact generated from a quadratic).\n",
      "\n",
      "As another example, Figure 7.8(c) plots log p(D|λ) vs log(λ), for the polynomial ridge regres- sion model, where λ ranges over the same set of values used in the CV experiment. We see that the maximum evidence occurs at roughly the same point as the minimum of the test MSE, which also corresponds to the point chosen by CV.\n",
      "\n",
      "When using the Bayesian approach, we are not restricted to evaluating the evidence at a ﬁnite grid of values. Instead, we can use numerical optimization to ﬁnd λ∗ = argmaxλ p(D|λ). This technique is called empirical Bayes or type II maximum likelihood (see Section 5.6 for details). An example is shown in Figure 7.8(b): we see that the curve has a similar shape to the CV estimate, but it can be computed more efficiently.\n",
      "\n",
      "158\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "d=1, logev=−18.593, EB\n",
      "\n",
      "d=2, logev=−20.218, EB\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "60\n",
      "\n",
      "60\n",
      "\n",
      "50\n",
      "\n",
      "40\n",
      "\n",
      "40\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "−20\n",
      "\n",
      "0\n",
      "\n",
      "−40\n",
      "\n",
      "−10\n",
      "\n",
      "−60\n",
      "\n",
      "−20\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "−80\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "300\n",
      "\n",
      "d=3, logev=−21.718, EB\n",
      "\n",
      "1\n",
      "\n",
      "N=5, method=EB\n",
      "\n",
      "250\n",
      "\n",
      "200\n",
      "\n",
      "0.8\n",
      "\n",
      "150\n",
      "\n",
      "100\n",
      "\n",
      "50\n",
      "\n",
      "0\n",
      "\n",
      ")\n",
      "\n",
      "D M P\n",
      "\n",
      "|\n",
      "\n",
      "(\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "−50\n",
      "\n",
      "0.2\n",
      "\n",
      "−100\n",
      "\n",
      "−150\n",
      "\n",
      "−200\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "M\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(a-c) We plot polynomials of degrees 1, 2 and 3 ﬁt to N = 5 data points using empirical Figure 5.7 Bayes. The solid green curve is the true function, the dashed red curve is the prediction (dotted blue lines represent ±σ around the mean). (d) We plot the posterior over models, p(d|D), assuming a uniform prior p(d) ∝ 1. Based on a ﬁgure by Zoubin Ghahramani. Figure generated by linregEbModelSelVsN.\n",
      "\n",
      "5.3.2\n",
      "\n",
      "Computing the marginal likelihood (evidence)\n",
      "\n",
      "When discussing parameter inference for a ﬁxed model, we often wrote\n",
      "\n",
      "p(θ|D, m) ∝ p(θ|m)p(D|θ, m)\n",
      "\n",
      "(5.15)\n",
      "\n",
      "thus ignoring the normalization constant p(D|m). This is valid since p(D|m) is constant wrt θ. However, when comparing models, we need to know how to compute the marginal likelihood, p(D|m). In general, this can be quite hard, since we have to integrate over all possible parameter values, but when we have a conjugate prior, it is easy to compute, as we now show.\n",
      "\n",
      "Let p(θ) =q (θ)/Z0 be our prior, where q(θ) is an unnormalized distribution, and Z0 is the normalization constant of the prior. Let p(D|θ) =q (D|θ)/Z(cid:11) be the likelihood, where Z(cid:11) contains any constant factors in the likelihood. Finally let p(θ|D) = q(θ|D)/ZN be our poste-\n",
      "\n",
      "5.3. Bayesian model selection\n",
      "\n",
      "159\n",
      "\n",
      "d=1, logev=−106.110, EB\n",
      "\n",
      "d=2, logev=−103.025, EB\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−10\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "−10\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "100\n",
      "\n",
      "d=3, logev=−107.410, EB\n",
      "\n",
      "1\n",
      "\n",
      "N=30, method=EB\n",
      "\n",
      "80\n",
      "\n",
      "0.8\n",
      "\n",
      "60\n",
      "\n",
      "40\n",
      "\n",
      ")\n",
      "\n",
      "D M P\n",
      "\n",
      "|\n",
      "\n",
      "(\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "20\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "−20\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "M\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 5.8 Same as Figure 5.7 except now N = 30. Figure generated by linregEbModelSelVsN.\n",
      "\n",
      "rior, where q(θ|D) = q(D|θ)q(θ) is the unnormalized posterior, and ZN is the normalization constant of the posterior. We have\n",
      "\n",
      "p(θ|D) =\n",
      "\n",
      "q(θ|D) ZN\n",
      "\n",
      "=\n",
      "\n",
      "p(D) =\n",
      "\n",
      "p(D|θ)p(θ) p(D) q(D|θ)q(θ) Z(cid:11)Z0p(D) ZN Z0Z(cid:11)\n",
      "\n",
      "(5.16)\n",
      "\n",
      "(5.17)\n",
      "\n",
      "(5.18)\n",
      "\n",
      "So assuming the relevant normalization constants are tractable, we have an easy way to compute the marginal likelihood. We give some examples below.\n",
      "\n",
      "160\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "5.3.2.1\n",
      "\n",
      "Beta-binomial model Let us apply the above result to the Beta-binomial model. Since we know p(θ|D) = Beta(θ|a(cid:2), b(cid:2)), where a(cid:2) = a + N1 and b(cid:2) = b + N0, we know the normalization constant of the posterior is B(a(cid:2), b(cid:2)). Hence\n",
      "\n",
      "p(θ|D) =\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "p(D|θ)p(θ) p(D) (cid:2) 1 p(D) (cid:8) (cid:7) N N1\n",
      "\n",
      "1 B(a, b) 1 p(D)\n",
      "\n",
      "1 B(a, b)\n",
      "\n",
      "θa−1(1 − θ)b−1\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "θa+N1−1(1 − θ)b+N0−1\n",
      "\n",
      "(cid:3) (cid:2)(cid:7)\n",
      "\n",
      "N N1\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "θN1 (1 − θ)N0\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(5.19)\n",
      "\n",
      "(5.20)\n",
      "\n",
      "(5.21)\n",
      "\n",
      "So\n",
      "\n",
      "1 B(a + N1, b + N0)\n",
      "\n",
      "=\n",
      "\n",
      "p(D) =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "N N1 N N1\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 1 p(D) B(a, b) B(a + N1, b + N0) B(a, b)\n",
      "\n",
      "(5.22)\n",
      "\n",
      "(5.23)\n",
      "\n",
      "The marginal likelihood for the Beta-Bernoulli model is the same as above, except it is missing the\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "N N1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "term.\n",
      "\n",
      "5.3.2.2\n",
      "\n",
      "Dirichlet-multinoulli model\n",
      "\n",
      "By the same reasoning as the Beta-Bernoulli case, one can show that the marginal likelihood for the Dirichlet-multinoulli model is given by\n",
      "\n",
      "p(D) =\n",
      "\n",
      "B(N + α) B(α)\n",
      "\n",
      "(5.24)\n",
      "\n",
      "where\n",
      "\n",
      "B(α) =\n",
      "\n",
      "(cid:15)K\n",
      "\n",
      "k=1 Γ(αk) (cid:4) k αk) Γ(\n",
      "\n",
      "(5.25)\n",
      "\n",
      "Hence we can rewrite the above result in the following form, which is what is usually presented in the literature:\n",
      "\n",
      "p(D) =\n",
      "\n",
      "Γ( Γ(N +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "k αk) (cid:4)\n",
      "\n",
      "k αk)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "k\n",
      "\n",
      "Γ(Nk + αk) Γ(αk)\n",
      "\n",
      "(5.26)\n",
      "\n",
      "We will see many applications of this equation later.\n",
      "\n",
      "5.3.2.3\n",
      "\n",
      "Gaussian-Gaussian-Wishart model\n",
      "\n",
      "Consider the case of an MVN with a conjugate NIW prior. Let Z0 be the normalizer for the prior, ZN be normalizer for the posterior, and let Zl = (2π)N D/2 be the normalizer for the\n",
      "\n",
      "5.3. Bayesian model selection\n",
      "\n",
      "161\n",
      "\n",
      "likelihood. Then it is easy to see that\n",
      "\n",
      "p(D) =\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "ZN Z0Zl\n",
      "\n",
      "1 πN D/2\n",
      "\n",
      "1 πN D/2\n",
      "\n",
      "1 2N D/2\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "κ0 κN\n",
      "\n",
      "(cid:8)D/2 |S0|ν0/2 |SN |νN /2\n",
      "\n",
      "+\n",
      "\n",
      "2π κN +\n",
      "\n",
      ",D/2\n",
      "\n",
      "2π κ0\n",
      "\n",
      ",D/2\n",
      "\n",
      "|SN |−νN /22(ν0+N )D/2ΓD(νN /2)\n",
      "\n",
      "|S0|−ν0/22ν0D/2ΓD(ν0/2)\n",
      "\n",
      "ΓD(νN /2) ΓD(ν0/2)\n",
      "\n",
      "(5.27)\n",
      "\n",
      "(5.28)\n",
      "\n",
      "(5.29)\n",
      "\n",
      "This equation will prove useful later.\n",
      "\n",
      "5.3.2.4\n",
      "\n",
      "BIC approximation to log marginal likelihood\n",
      "\n",
      "In general, computing the integral in Equation 5.13 can be quite difficult. One simple but popular approximation is known as the Bayesian information criterion or BIC, which has the following form (Schwarz 1978):\n",
      "\n",
      "BIC (cid:2) log p(D|ˆθ) − dof(ˆθ)\n",
      "\n",
      "2\n",
      "\n",
      "log N ≈ log p(D)\n",
      "\n",
      "(5.30)\n",
      "\n",
      "where dof(ˆθ) is the number of degrees of freedom in the model, and ˆθ is the MLE for the model.2 We see that this has the form of a penalized log likelihood, where the penalty term depends on the model’s complexity. See Section 8.4.2 for the derivation of the BIC score.\n",
      "\n",
      "(XT X)−1XT y and ˆσ2 = RSS/N , where RSS = log likelihood is given by\n",
      "\n",
      "As an example, consider linear regression. As we show in Section 7.3, the MLE is given by ˆw = mlexi)2. The corresponding\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      "i=1(yi − ˆwT\n",
      "\n",
      "log p(D|ˆθ) = −\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log(2πˆσ2) −\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "(5.31)\n",
      "\n",
      "Hence the BIC score is as follows (dropping constant terms)\n",
      "\n",
      "BIC = −\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log(ˆσ2) −\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "log(N )\n",
      "\n",
      "(5.32)\n",
      "\n",
      "where D is the number of variables in the model. use an alternative deﬁnition of BIC, which we call the BIC cost (since we want to minimize it):\n",
      "\n",
      "In the statistics literature, it is common to\n",
      "\n",
      "BIC-cost (cid:2) −2 log p(D|ˆθ) + dof(ˆθ) log N ≈ −2 log p(D)\n",
      "\n",
      "(5.33)\n",
      "\n",
      "In the context of linear regression, this becomes\n",
      "\n",
      "BIC-cost = N log(ˆσ2) +D log(N )\n",
      "\n",
      "(5.34)\n",
      "\n",
      "2. Traditionally the BIC score is deﬁned using the ML estimate ˆθ, so it is independent of the prior. However, for models such as mixtures of Gaussians, the ML estimate can be poorly behaved, so it is better to evaluate the BIC score using the MAP estimate, as in (Fraley and Raftery 2007).\n",
      "\n",
      "162\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "The BIC method is very closely related to the minimum description length or MDL principle, which characterizes the score for a model in terms of how well it ﬁts the data, minus how complex the model is to deﬁne. See (Hansen and Yu 2001) for details.\n",
      "\n",
      "There is a very similar expression to BIC/ MDL called the Akaike information criterion or\n",
      "\n",
      "AIC, deﬁned as\n",
      "\n",
      "AIC(m, D) (cid:2) log p(D|ˆθM LE) − dof(m)\n",
      "\n",
      "(5.35)\n",
      "\n",
      "This is derived from a frequentist framework, and cannot be interpreted as an approximation to the marginal likelihood. Nevertheless, the form of this expression is very similar to BIC. We see that the penalty for AIC is less than for BIC. This causes AIC to pick more complex models. However, this can result in better predictive accuracy. See e.g., (Clarke et al. 2009, sec 10.2) for further discussion on such information criteria.\n",
      "\n",
      "5.3.2.5\n",
      "\n",
      "Effect of the prior\n",
      "\n",
      "Sometimes it is not clear how to set the prior. When we are performing posterior inference, the details of the prior may not matter too much, since the likelihood often overwhelms the prior anyway. But when computing the marginal likelihood, the prior plays a much more important role, since we are averaging the likelihood over all possible parameter settings, as weighted by the prior.\n",
      "\n",
      "In Figures 5.7 and 5.8, where we demonstrated model selection for linear regression, we used a prior of the form p(w) = N (0, α−1I). Here α is a tuning parameter that controls how strong the prior is. This parameter can have a large effect, as we discuss in Section 7.5. Intuitively, if α is large, the weights are “forced” to be small, so we need to use a complex model with many small parameters (e.g., a high degree polynomial) to ﬁt the data. Conversely, if α is small, we will favor simpler models, since each parameter is “allowed” to vary in magnitude by a lot.\n",
      "\n",
      "If the prior is unknown, the correct Bayesian procedure is to put a prior on the prior. That is, we should put a prior on the hyper-parameter α as well as the parametrs w. To compute the marginal likelihood, we should integrate out all unknowns, i.e., we should compute\n",
      "\n",
      "(cid:11) (cid:11)\n",
      "\n",
      "p(D|m) =\n",
      "\n",
      "p(D|w)p(w|α, m)p(α|m)dwdα\n",
      "\n",
      "(5.36)\n",
      "\n",
      "Of course, this requires specifying the hyper-prior. Fortunately, the higher up we go in the Bayesian hierarchy, the less sensitive are the results to the prior settings. So we can usually make the hyper-prior uninformative.\n",
      "\n",
      "A computational shortcut is to optimize α rather than integrating it out. That is, we use\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(D|m) ≈\n",
      "\n",
      "p(D|w)p(w|ˆα, m)dw\n",
      "\n",
      "(5.37)\n",
      "\n",
      "where\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ˆα = argmax\n",
      "\n",
      "α\n",
      "\n",
      "p(D|α, m) = argmax\n",
      "\n",
      "α\n",
      "\n",
      "p(D|w)p(w|α, m)dw\n",
      "\n",
      "(5.38)\n",
      "\n",
      "This approach is called empirical Bayes (EB), and is discussed in more detail in Section 5.6. This is the method used in Figures 5.7 and 5.8.\n",
      "\n",
      "5.3. Bayesian model selection\n",
      "\n",
      "163\n",
      "\n",
      "Bayes factor BF (1, 0) BF < 1 100 BF < 1 10 1 10 < BF < 1 1 3 < BF <1 1 < BF <3 3 < BF <10 BF > 10 BF > 100\n",
      "\n",
      "3\n",
      "\n",
      "Interpretation Decisive evidence for M0 Strong evidence for M0 Moderate evidence for M0 Weak evidence for M0 Weak evidence for M1 Moderate evidence for M1 Strong evidence for M1 Decisive evidence for M1\n",
      "\n",
      "Table 5.1\n",
      "\n",
      "Jeffreys’ scale of evidence for interpreting Bayes factors.\n",
      "\n",
      "5.3.3\n",
      "\n",
      "Bayes factors\n",
      "\n",
      "Suppose our prior on models is uniform, p(m) ∝ 1. Then model selection is equivalent to picking the model with the highest marginal likelihood. Now suppose we just have two models we are considering, call them the null hypothesis, M0, and the alternative hypothesis, M1. Deﬁne the Bayes factor as the ratio of marginal likelihoods: p(M1|D) p(M0|D)\n",
      "\n",
      "BF1,0 (cid:2)\n",
      "\n",
      "p(D|M1) p(D|M0)\n",
      "\n",
      "=\n",
      "\n",
      "/\n",
      "\n",
      "p(M1) p(M0)\n",
      "\n",
      "(5.39)\n",
      "\n",
      "(This is like a likelihood ratio, except we integrate out the parameters, which allows us to If BF1,0 > 1 then we prefer model 1, otherwise we compare models of different complexity.) prefer model 0.\n",
      "\n",
      "In that case, we are not very conﬁdent that model 1 is better. Jeffreys (1961) proposed a scale of evidence for interpreting the magnitude of a Bayes factor, which is shown in Table 5.1. This is a Bayesian alternative to the frequentist concept of a p-value.3 Alternatively, we can just convert the Bayes factor to a posterior over models. If p(M1) = p(M0) = 0.5, we have\n",
      "\n",
      "Of course, it might be that BF1,0 is only slightly greater than 1.\n",
      "\n",
      "p(M0|D) =\n",
      "\n",
      "BF0,1 1 + BF0,1\n",
      "\n",
      "=\n",
      "\n",
      "1 BF1,0 + 1\n",
      "\n",
      "(5.40)\n",
      "\n",
      "5.3.3.1\n",
      "\n",
      "Example: Testing if a coin is fair\n",
      "\n",
      "Suppose we observe some coin tosses, and want to decide if the data was generated by a fair coin, θ = 0.5, or a potentially biased coin, where θ could be any value in [0, 1]. Let us denote the ﬁrst model by M0 and the second model by M1. The marginal likelihood under M0 is simply\n",
      "\n",
      "p(D|M0) =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:8)N\n",
      "\n",
      "(5.41)\n",
      "\n",
      "3. A p-value, is deﬁned as the probability (under the null hypothesis) of observing some test statistic f (D) (such as the chi-squared statistic) that is as large or larger than that actually observed, i.e., pvalue(D) (cid:2) P (f ( ˜D) ≥ f (D)| ˜D ∼ H0). Note that has almost nothing to do with what we really want to know, which is p(H0|D).\n",
      "\n",
      "164\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "log\n",
      "\n",
      "10\n",
      "\n",
      "p(D|M1)\n",
      "\n",
      "BIC approximation to log\n",
      "\n",
      "10\n",
      "\n",
      "p(D|M1)\n",
      "\n",
      "−0.4\n",
      "\n",
      "−2\n",
      "\n",
      "−0.6\n",
      "\n",
      "−2.05\n",
      "\n",
      "−2.1\n",
      "\n",
      "−0.8\n",
      "\n",
      "−2.15\n",
      "\n",
      "−1\n",
      "\n",
      "−2.2\n",
      "\n",
      "−2.25\n",
      "\n",
      "−1.2\n",
      "\n",
      "−2.3\n",
      "\n",
      "−1.4\n",
      "\n",
      "−2.35\n",
      "\n",
      "−2.4\n",
      "\n",
      "−1.6\n",
      "\n",
      "−2.45\n",
      "\n",
      "−1.8\n",
      "\n",
      "0 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 5\n",
      "\n",
      "−2.5\n",
      "\n",
      "0 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 5.9 (a) Log marginal likelihood for the coins example. (b) BIC approximation. Figure generated by coinsModelSelDemo.\n",
      "\n",
      "where N is the number of coin tosses. The marginal likelihood under M1, using a Beta prior, is\n",
      "\n",
      "p(D|M1) =\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(D|θ)p(θ)dθ =\n",
      "\n",
      "B(α1 + N1, α0 + N0) B(α1, α0)\n",
      "\n",
      "(5.42)\n",
      "\n",
      "We plot log p(D|M1) vs the number of heads N1 in Figure 5.9(a), assuming N = 5 and α1 = α0 = 1. (The shape of the curve is not very sensitive to α1 and α0, as long as α0 = α1.) If we observe 2 or 3 heads, the unbiased coin hypothesis M0 is more likely than M1, since M0 is a simpler model (it has no free parameters) — it would be a suspicious coincidence if the coin were biased but happened to produce almost exactly 50/50 heads/tails. However, as the counts become more extreme, we favor the biased coin hypothesis. Note that, if we plot the log Bayes factor, log BF1,0, it will have exactly the same shape, since log p(D|M0) is a constant. See also Exercise 3.18.\n",
      "\n",
      "In Figure 5.9(b) shows the BIC approximation to log p(D|M1) for our biased coin example from Section 5.3.3.1. We see that the curve has approximately the same shape as the exact log marginal likelihood, which is all that matters for model selection purposes, since the absolute scale is irrelevant. In particular, it favors the simpler model unless the data is overwhelmingly in support of the more complex model.\n",
      "\n",
      "5.3.4\n",
      "\n",
      "Jeffreys-Lindley paradox *\n",
      "\n",
      "Problems can arise when we use improper priors (i.e., priors that do not integrate to 1) for model selection/ hypothesis testing, even though such priors may be acceptable for other purposes. For example, consider testing the hypotheses M0 : θ ∈ Θ0 vs M1 : θ ∈ Θ1. To deﬁne the marginal density on θ, we use the following mixture model\n",
      "\n",
      "p(θ) =p(\n",
      "\n",
      "θ|M0)p(M0) +p( θ|M1)p(M1)\n",
      "\n",
      "(5.43)\n",
      "\n",
      "5.4. Priors\n",
      "\n",
      "165\n",
      "\n",
      "This is only meaningful if p(θ|M0) and p(θ|M1) are proper (normalized) density functions. In this case, the posterior is given by\n",
      "\n",
      "p(M0|D) =\n",
      "\n",
      "=\n",
      "\n",
      "p(M0)p(D|M0) p(M0)p(D|M0) +p( M1)p(D|M1)\n",
      "\n",
      "p(M0)\n",
      "\n",
      "(cid:22) Θ0\n",
      "\n",
      "(cid:22) Θ0 p(D|θ)p(θ|M0)dθ + p(M1)\n",
      "\n",
      "p(M0)\n",
      "\n",
      "p(D|θ)p(θ|M0)dθ\n",
      "\n",
      "(cid:22) Θ1\n",
      "\n",
      "p(D|θ)p(θ|M1)dθ\n",
      "\n",
      "(5.44)\n",
      "\n",
      "(5.45)\n",
      "\n",
      "Now suppose we use improper priors, p(θ|M0) ∝ c0 and p(θ|M1) ∝ c1. Then\n",
      "\n",
      "where (cid:9)i = p(M1) = 1\n",
      "\n",
      "p(M0|D) =\n",
      "\n",
      "(cid:22) Θi 2 . Hence\n",
      "\n",
      "=\n",
      "\n",
      "p(D|θ)dθ is the integrated or marginal likelihood for model i. Now let p(M0) =\n",
      "\n",
      "p(M0)c0\n",
      "\n",
      "Θ0 p(M0)c0(cid:9)0 p(M0)c0(cid:9)0 + p(M1)c1(cid:9)1\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "p(M0)c0 p(D|θ)dθ Θ0 (cid:22) p(D|θ)dθ + p(M1)c1\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "Θ1\n",
      "\n",
      "p(D|θ)dθ\n",
      "\n",
      "(5.46)\n",
      "\n",
      "(5.47)\n",
      "\n",
      "p(M0|D) =\n",
      "\n",
      "c0(cid:9)0 c0(cid:9)0 + c1(cid:9)1\n",
      "\n",
      "=\n",
      "\n",
      "(cid:9)0 (cid:9)0 + (c1/c0)(cid:9)1\n",
      "\n",
      "(5.48)\n",
      "\n",
      "Thus we can change the posterior arbitrarily by choosing c1 and c0 as we please. Note that using proper, but very vague, priors can cause similar problems. In particular, the Bayes factor will always favor the simpler model, since the probability of the observed data under a complex model with a very diffuse prior will be very small. This is called the Jeffreys-Lindley paradox. Thus it is important to use proper priors when performing model selection. Note, however, that, if M0 and M1 share the same prior over a subset of the parameters, this part of the prior can be improper, since the corresponding normalization constant will cancel out.\n",
      "\n",
      "5.4\n",
      "\n",
      "Priors\n",
      "\n",
      "The most controversial aspect of Bayesian statistics is its reliance on priors. Bayesians argue this is unavoidable, since nobody is a tabula rasa or blank slate: all inference must be done conditional on certain assumptions about the world. Nevertheless, one might be interested in minimizing the impact of one’s prior assumptions. We brieﬂy discuss some ways to do this below.\n",
      "\n",
      "5.4.1\n",
      "\n",
      "Uninformative priors\n",
      "\n",
      "If we don’t have strong beliefs about what θ should be, it is common to use an uninformative or non-informative prior, and to “let the data speak for itself”.\n",
      "\n",
      "The issue of designing uninformative priors is actually somewhat tricky. As an example of the difficulty, consider a Bernoulli parameter, θ ∈ [0, 1]. One might think that the most uninformative prior would be the uniform distribution, Beta(1, 1). But the posterior mean in this case is E [θ|D] = N1+1 N1+N0 . Hence one could argue that the prior wasn’t completely uninformative after all.\n",
      "\n",
      "N1+N0+2 , whereas the MLE is\n",
      "\n",
      "N1\n",
      "\n",
      "166\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "Clearly by decreasing the magnitude of the pseudo counts, we can lessen the impact of the\n",
      "\n",
      "prior. By the above argument, the most non-informative prior is\n",
      "\n",
      "lim c→0\n",
      "\n",
      "Beta(c, c) = Beta(0, 0)\n",
      "\n",
      "(5.49)\n",
      "\n",
      "which is a mixture of two equal point masses at 0 and 1 (see (Zhu and Lu 2004)). This is also called the Haldane prior. Note that the Haldane prior is an improper prior, meaning it does not integrate to 1. However, as long as we see at least one head and at least one tail, the posterior will be proper.\n",
      "\n",
      "In Section 5.4.2.1 we will argue that the “right” uninformative prior is in fact Beta( 1 2 , 1 2 ). Clearly the difference in practice between these three priors is very likely negligible. In general, it is advisable to perform some kind of sensitivity analysis, in which one checks how much one’s conclusions or predictions change in response to change in the modeling assumptions, which includes the choice of prior, but also the choice of likelihood and any kind of data pre- processing. If the conclusions are relatively insensitive to the modeling assumptions, one can have more conﬁdence in the results.\n",
      "\n",
      "5.4.2\n",
      "\n",
      "Jeffreys priors *\n",
      "\n",
      "Harold Jeffreys4 designed a general purpose technique for creating non-informative priors. The result is known as the Jeffreys prior. The key observation is that if p(φ) is non-informative, then any re-parameterization of the prior, such as θ = h(φ) for some function h, should also be non-informative. Now, by the change of variables formula,\n",
      "\n",
      "pθ(θ) = pφ(φ)\n",
      "\n",
      "- dφ dθ\n",
      "\n",
      "-\n",
      "\n",
      "(5.50)\n",
      "\n",
      "so the prior will in general change. However, let us pick\n",
      "\n",
      "pφ(φ) ∝ (I(φ))\n",
      "\n",
      "1 2\n",
      "\n",
      "(5.51)\n",
      "\n",
      "where I(φ) is the Fisher information:\n",
      "\n",
      "I(φ) (cid:2) −E\n",
      "\n",
      "(cid:2)(cid:7)\n",
      "\n",
      "d log p(X|φ) dφ\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "2\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(5.52)\n",
      "\n",
      "This is a measure of curvature of the expected negative log likelihood and hence a measure of stability of the MLE (see Section 6.2.2). Now\n",
      "\n",
      "d log p(x|θ) dθ\n",
      "\n",
      "=\n",
      "\n",
      "d log p(x|φ) dφ\n",
      "\n",
      "dφ dθ\n",
      "\n",
      "(5.53)\n",
      "\n",
      "Squaring and taking expectations over x, we have\n",
      "\n",
      "I(θ) =−E\n",
      "\n",
      "(cid:16)(cid:7)\n",
      "\n",
      "d log p(X|θ) dθ\n",
      "\n",
      "(cid:8)2\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "= I(φ)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "dφ dθ\n",
      "\n",
      "(cid:8)2\n",
      "\n",
      "(5.54)\n",
      "\n",
      "I(θ)\n",
      "\n",
      "1\n",
      "\n",
      "2 = I(φ)\n",
      "\n",
      "1 2\n",
      "\n",
      "- dφ dθ\n",
      "\n",
      "-\n",
      "\n",
      "(5.55)\n",
      "\n",
      "4. Harold Jeffreys, 1891 – 1989, was an English mathematician, statistician, geophysicist, and astronomer.\n",
      "\n",
      "5.4. Priors\n",
      "\n",
      "167\n",
      "\n",
      "so we ﬁnd the transformed prior is\n",
      "\n",
      "pθ(θ) =p φ(φ)\n",
      "\n",
      "- dφ dθ\n",
      "\n",
      "- ∝ (I(φ))\n",
      "\n",
      "1 2\n",
      "\n",
      "- dφ dθ\n",
      "\n",
      "- = I(θ)\n",
      "\n",
      "1 2\n",
      "\n",
      "(5.56)\n",
      "\n",
      "So pθ(θ) and pφ(φ) are the same.\n",
      "\n",
      "Some examples will make this clearer.\n",
      "\n",
      "5.4.2.1\n",
      "\n",
      "Example: Jeffreys prior for the Bernoulli and multinoulli\n",
      "\n",
      "Suppose X ∼ Ber(θ). The log likelihood for a single sample is\n",
      "\n",
      "log p(X|θ) = X log θ + (1− X) log(1 − θ)\n",
      "\n",
      "(5.57)\n",
      "\n",
      "The score function is just the gradient of the log-likelihood:\n",
      "\n",
      "s(θ) (cid:2)\n",
      "\n",
      "d dθ\n",
      "\n",
      "log p(X|θ) =\n",
      "\n",
      "X θ\n",
      "\n",
      "− 1 − X 1 − θ\n",
      "\n",
      "(5.58)\n",
      "\n",
      "The observed information is the second derivative of the log-likelihood:\n",
      "\n",
      "J(θ) =−\n",
      "\n",
      "d2 dθ2 log p(X|θ) = −s(cid:2)(θ|X) =\n",
      "\n",
      "X θ2 +\n",
      "\n",
      "1 − X (1 − θ)2\n",
      "\n",
      "(5.59)\n",
      "\n",
      "The Fisher information is the expected information:\n",
      "\n",
      "I(θ) =E [J(θ|X)|X ∼ θ] =\n",
      "\n",
      "θ θ2 +\n",
      "\n",
      "1 − θ (1 − θ)2 =\n",
      "\n",
      "1 θ(1 − θ)\n",
      "\n",
      "(5.60)\n",
      "\n",
      "Hence Jeffreys’ prior is\n",
      "\n",
      "p(θ) ∝ θ− 1\n",
      "\n",
      "2 (1 − θ)− 1\n",
      "\n",
      "2 =\n",
      "\n",
      "1(cid:9)\n",
      "\n",
      "θ(1 − θ)\n",
      "\n",
      "∝ Beta(\n",
      "\n",
      "1 2\n",
      "\n",
      ", 1 2\n",
      "\n",
      ")\n",
      "\n",
      "(5.61)\n",
      "\n",
      "Now consider a multinoulli random variable with K states. One can show that the Jeffreys’\n",
      "\n",
      "prior is given by\n",
      "\n",
      "p(θ) ∝ Dir(\n",
      "\n",
      "1 2\n",
      "\n",
      ", . . . , 1 2\n",
      "\n",
      ")\n",
      "\n",
      "(5.62)\n",
      "\n",
      "Note that this is different from the more obvious choices of Dir( 1\n",
      "\n",
      "K , . . . , 1\n",
      "\n",
      "K ) or Dir(1, . . . , 1).\n",
      "\n",
      "5.4.2.2\n",
      "\n",
      "Example: Jeffreys prior for location and scale parameters\n",
      "\n",
      "One can show that the Jeffreys prior for a location parameter, such as the Gaussian mean, is p(μ) ∝ 1. Thus is an example of a translation invariant prior, which satisﬁes the property that the probability mass assigned to any interval, [A, B] is the same as that assigned to any other shifted interval of the same width, such as [A − c, B − c]. That is,\n",
      "\n",
      "(cid:11) B−c\n",
      "\n",
      "(cid:11) B\n",
      "\n",
      "A−c\n",
      "\n",
      "p(μ)dμ = (A − c) − (B − c) = (A − B) =\n",
      "\n",
      "A\n",
      "\n",
      "p(μ)dμ\n",
      "\n",
      "(5.63)\n",
      "\n",
      "168\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "This can be achieved using p(μ) ∝ 1, which we can approximate by using a Gaussian with inﬁnite variance, p(μ) =N (μ|0, ∞). Note that this is an improper prior, since it does not integrate to 1. Using improper priors is ﬁne as long as the posterior is proper, which will be the case provided we have seen N ≥ 1 data points, since we can “nail down” the location as soon as we have seen a single data point.\n",
      "\n",
      "Similarly, one can show that the Jeffreys prior for a scale parameter, such as the Gaussian variance, is p(σ2) ∝ 1/σ2. This is an example of a scale invariant prior, which satisﬁes the property that the probability mass assigned to any interval [A, B] is the same as that assigned to any other interval [A/c, B/c] which is scaled in size by some constant factor c > 0. (For example, if we change units from meters to feet we do not want that to affect our inferences.) This can be achieved by using\n",
      "\n",
      "p(s) ∝ 1/s\n",
      "\n",
      "(5.64)\n",
      "\n",
      "To see this, note that\n",
      "\n",
      "(cid:11) B/c\n",
      "\n",
      "A/c\n",
      "\n",
      "p(s)ds = [log s]\n",
      "\n",
      "B/c A/c = log(B/c) − log(A/c)\n",
      "\n",
      "(cid:11) B\n",
      "\n",
      "(5.65)\n",
      "\n",
      "= log(B) − log(A) =\n",
      "\n",
      "A\n",
      "\n",
      "p(s)ds\n",
      "\n",
      "(5.66)\n",
      "\n",
      "We can approximate this using a degenerate Gamma distribution (Section 2.4.4), p(s) = Ga(s|0, 0). The prior p(s) ∝ 1/s is also improper, but the posterior is proper as soon as we have seen N ≥ 2 data points (since we need at least two data points to estimate a variance).\n",
      "\n",
      "5.4.3\n",
      "\n",
      "Robust priors\n",
      "\n",
      "In many cases, we are not very conﬁdent in our prior, so we want to make sure it does not have an undue inﬂuence on the result. This can be done by using robust priors (Insua and Ruggeri 2000), which typically have heavy tails, which avoids forcing things to be too close to the prior mean.\n",
      "\n",
      "Let us consider an example from (Berger 1985, p7). Suppose x ∼ N (θ, 1). We observe that x = 5 and we want to estimate θ. The MLE is of course ˆθ = 5, which seems reasonable. The posterior mean under a uniform prior is also θ = 5. But now suppose we know that the prior median is 0, and the prior quantiles are at -1 and 1, so p(θ ≤ −1) = p(−1 < θ ≤ 0) = p(0 < θ ≤ 1) = p(1 < θ) = 0.25. Let us also assume the prior is smooth and unimodal.\n",
      "\n",
      "It is easy to show that a Gaussian prior of the form N (θ|0, 2.192) satisﬁes these prior constraints. But in this case the posterior mean is given by 3.43, which doesn’t seem very satisfactory.\n",
      "\n",
      "Now suppose we use as a Cauchy prior T (θ|0, 1, 1). This also satisﬁes the prior constraints of our example. But this time we ﬁnd (using numerical method integration: see robustPriorDemo for the code) that the posterior mean is about 4.6, which seems much more reasonable.\n",
      "\n",
      "5.4.4 Mixtures of conjugate priors\n",
      "\n",
      "Robust priors are useful, but can be computationally expensive to use. Conjugate priors simplify the computation, but are often not robust, and not ﬂexible enough to encode our prior knowl-\n",
      "\n",
      "5.4. Priors\n",
      "\n",
      "169\n",
      "\n",
      "edge. However, it turns out that a mixture of conjugate priors is also conjugate (Exercise 5.1), and can approximate any kind of prior (Dallal and Hall 1983; Diaconis and Ylvisaker 1985). Thus such priors provide a good compromise between computational convenience and ﬂexibility.\n",
      "\n",
      "For example, suppose we are modeling coin tosses, and we think the coin is either fair, or is biased towards heads. This cannot be represented by a beta distribution. However, we can model it using a mixture of two beta distributions. For example, we might use\n",
      "\n",
      "p(θ) = 0.5 Beta(θ|20, 20) + 0.5 Beta(θ|30, 10)\n",
      "\n",
      "(5.67)\n",
      "\n",
      "If θ comes from the ﬁrst distribution, the coin is fair, but if it comes from the second, it is biased towards heads.\n",
      "\n",
      "that θ comes from mixture component k. The prior has the form\n",
      "\n",
      "We can represent a mixture by introducing a latent indicator variable z, where z = k means\n",
      "\n",
      "p(θ) =\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "p(z = k)p(θ|z = k)\n",
      "\n",
      "(5.68)\n",
      "\n",
      "k\n",
      "\n",
      "where each p(θ|z = k) is conjugate, and p(z = k) are called the (prior) mixing weights. One can show (Exercise 5.1) that the posterior can also be written as a mixture of conjugate distributions as follows:\n",
      "\n",
      "p(θ|D) =\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "p(z = k|D)p(θ|D, z = k)\n",
      "\n",
      "(5.69)\n",
      "\n",
      "k\n",
      "\n",
      "where p(Z = k|D) are the posterior mixing weights given by\n",
      "\n",
      "p(Z = k|D) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(Z = k)p(D|Z = k) k(cid:2) p(Z = k(cid:2))p(D|Z = k(cid:2))\n",
      "\n",
      "(5.70)\n",
      "\n",
      "Here the quantity p(D|Z = k) is the marginal likelihood for mixture component k (see Sec- tion 5.3.2.1).\n",
      "\n",
      "5.4.4.1\n",
      "\n",
      "Example\n",
      "\n",
      "Suppose we use the mixture prior\n",
      "\n",
      "p(θ) = 0.5Beta(θ|a1, b1) + 0.5Beta(θ|a2, b2)\n",
      "\n",
      "(5.71)\n",
      "\n",
      "where a1 = b1 = 20 and a2 = b2 = 10. and we observe N1 heads and N0 tails. The posterior becomes\n",
      "\n",
      "p(θ|D) = p(Z = 1|D)Beta(θ|a1 + N1, b1 + N0) +p( Z = 2|D)Beta(θ|a2 + N1, b2 + N0)(5.72)\n",
      "\n",
      "If N1 = 20 heads and N0 = 10 tails, then, using Equation 5.23, the posterior becomes\n",
      "\n",
      "p(θ|D) = 0.346 Beta(θ|40, 30) + 0.654 Beta(θ|50, 20)\n",
      "\n",
      "(5.73)\n",
      "\n",
      "See Figure 5.10 for an illustration.\n",
      "\n",
      "170\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "5\n",
      "\n",
      "mixture of Beta distributions\n",
      "\n",
      "4.5\n",
      "\n",
      "prior posterior\n",
      "\n",
      "4\n",
      "\n",
      "3.5\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "Figure 5.10 A mixture of two Beta distributions. Figure generated by mixBetaDemo.\n",
      "\n",
      "5.4.4.2\n",
      "\n",
      "Application: Finding conserved regions in DNA and protein sequences\n",
      "\n",
      "We mentioned that Dirichlet-multinomial models are widely used in biosequence analysis. Let us give a simple example to illustrate some of the machinery that has developed. Speciﬁcally, consider the sequence logo discussed in Section 2.3.2.1. Now suppose we want to ﬁnd locations which represent coding regions of the genome. Such locations often have the same letter across all sequences, because of evolutionary pressure. So we need to ﬁnd columns which are “pure”, or nearly so, in the sense that they are mostly all As, mostly all Ts, mostly all Cs, or mostly all Gs. One approach is to look for low-entropy columns; these will be ones whose distribution is nearly deterministic (pure).\n",
      "\n",
      "But suppose we want to associate a conﬁdence measure with our estimates of purity. This In this case, we can let can be useful if we believe adjacent locations are conserved together. Z1 = 1 if location t is conserved, and let Zt = 0 otherwise. We can then add a dependence between adjacent Zt variables using a Markov chain; see Chapter 17 for details.\n",
      "\n",
      "In any case, we need to deﬁne a likelihood model, p(Nt|Zt), where Nt is the vector of (A,C,G,T) counts for column t. It is natural to make this be a multinomial distribution with parameter θt. Since each column has a different distribution, we will want to integrate out θt and thus compute the marginal likelihood\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(Nt|Zt) =\n",
      "\n",
      "p(Nt|θt)p(θt|Zt)dθt\n",
      "\n",
      "(5.74)\n",
      "\n",
      "But what prior should we use for θt? When Zt = 0 we can use a uniform prior, p(θ|Zt = 0) = Dir(1, 1, 1, 1), but what should we use if Zt = 1? After all, if the column is conserved, it could be a (nearly) pure column of As, Cs, Gs, or Ts. A natural approach is to use a mixture of Dirichlet priors, each one of which is “tilted” towards the appropriate corner of the 4-dimensional simplex, e.g.,\n",
      "\n",
      "p(θ|Zt = 1) =\n",
      "\n",
      "1 4\n",
      "\n",
      "Dir(θ|(10, 1, 1, 1)) + · · · +\n",
      "\n",
      "1 4\n",
      "\n",
      "Dir(θ|(1, 1, 1, 10))\n",
      "\n",
      "(5.75)\n",
      "\n",
      "Since this is conjugate, we can easily compute p(Nt|Zt).\n",
      "\n",
      "See (Brown et al. 1993) for an\n",
      "\n",
      "5.5. Hierarchical Bayes\n",
      "\n",
      "171\n",
      "\n",
      "application of these ideas to a real bio-sequence problem.\n",
      "\n",
      "5.5 Hierarchical Bayes\n",
      "\n",
      "A key requirement for computing the posterior p(θ|D) is the speciﬁcation of a prior p(θ|η), where η are the hyper-parameters. What if we don’t know how to set η? In some cases, we can use uninformative priors, we we discussed above. A more Bayesian approach is to put a prior on our priors! In terms of graphical models (Chapter 10), we can represent the situation as follows:\n",
      "\n",
      "η → θ → D\n",
      "\n",
      "(5.76)\n",
      "\n",
      "This is an example of a hierarchical Bayesian model, also called a multi-level model, since there are multiple levels of unknown quantities. We give a simple example below, and we will see many others later in the book.\n",
      "\n",
      "5.5.1\n",
      "\n",
      "Example: modeling related cancer rates\n",
      "\n",
      "Consider the problem of predicting cancer rates in various cities (this example is from (Johnson and Albert 1999, p24)). In particular, suppose we measure the number of people in various cities, Ni, and the number of people who died of cancer in these cities, xi. We assume xi ∼ Bin(Ni, θi), and we want to estimate the cancer rates θi. One approach is to estimate them all separately, but this will suffer from the sparse data problem (underestimation of the rate of cancer due to small Ni). Another approach is to assume all the θi are the same; this is called parameter tying. The resulting pooled MLE is just ˆθ = i Ni . But the assumption that all the cities have the same rate is a rather strong one. A compromise approach is to assume that the θi are similar, but that there may be city-speciﬁc variations. This can be modeled by assuming the θi are drawn from some common distribution, say θi ∼ Beta(a, b). The full joint distribution can be written as\n",
      "\n",
      "(cid:2) i xi(cid:2)\n",
      "\n",
      "p(D, θ, η|N) =p( η)\n",
      "\n",
      "N(cid:12)\n",
      "\n",
      "Bin(xi|Ni, θi)Beta(θi|η)\n",
      "\n",
      "(5.77)\n",
      "\n",
      "i=1\n",
      "\n",
      "where η = (a, b).\n",
      "\n",
      "Note that it is crucial that we infer η = (a, b) from the data; if we just clamp it to a constant, the θi will be conditionally independent, and there will be no information ﬂow between them. By contrast, by treating η as an unknown (hidden variable), we allow the data-poor cities to borrow statistical strength from data-rich ones.\n",
      "\n",
      "Suppose we compute the joint posterior p(η, θ|D). From this we can get the posterior marginals p(θi|D). In Figure 5.11(a), we plot the posterior means, E [θi|D], as blue bars, as well as the population level mean, E [a/(a + b)|D], shown as a red line (this represents the average of the θi’s). We see that the posterior mean is shrunk towards the pooled estimate more strongly for cities with small sample sizes Ni. For example, city 1 and city 20 both have a 0 observed cancer incidence rate, but city 20 has a smaller population, so its rate is shrunk more towards the population-level estimate (i.e., it is closer to the horizontal red line) than city 1.\n",
      "\n",
      "Figure 5.11(b) shows the 95% posterior credible intervals for θi. We see that city 15, which has a very large population (53,637 people), has small posterior uncertainty. Consequently this city\n",
      "\n",
      "172\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "5\n",
      "\n",
      "number of people with cancer (truncated at 5)\n",
      "\n",
      "20\n",
      "\n",
      "95% credible interval on theta, *=median\n",
      "\n",
      "18\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "2000\n",
      "\n",
      "5\n",
      "\n",
      "pop of city (truncated at 2000)\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "16\n",
      "\n",
      "14\n",
      "\n",
      "1000\n",
      "\n",
      "12\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "MLE*1000 (red line=pooled MLE)\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "10\n",
      "\n",
      "8\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "posterior mean*1000 (red line=pop mean)\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "x 10\n",
      "\n",
      "8 −3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 5.11 (a) Results of ﬁtting the model using the data from (Johnson and Albert 1999, p24). First row: Number of cancer incidents xi in 20 cities in Missouri. Second row: population size Ni. The largest city (number 15) has a population of N15 = 53637 and x15 = 54 incidents, but we truncate the vertical axes of the ﬁrst two rows so that the differences between the other cities are visible. Third row: MLE ˆθi. The red line is the pooled MLE. Fourth row: posterior mean E [θi|D]. The red line is E [a/(a + b)|D], the population-level mean. (b) Posterior 95% credible intervals on the cancer rates. Figure generated by cancerRatesEb\n",
      "\n",
      "has the largest impact on the posterior estimate of η, which in turn will impact the estimate of the cancer rates for other cities. Cities 10 and 19, which have the highest MLE, also have the highest posterior uncertainty, reﬂecting the fact that such a high estimate is in conﬂict with the prior (which is estimated from all the other cities).\n",
      "\n",
      "In the above example, we have one parameter per city, modeling the probability the response is on. By making the Bernoulli rate parameter be a function of covariates, θi = sigm(wT i x), we can model multiple correlated logistic regression tasks. This is called multi-task learning, and will be discussed in more detail in Section 9.5.\n",
      "\n",
      "5.6\n",
      "\n",
      "Empirical Bayes\n",
      "\n",
      "In hierarchical Bayesian models, we need to compute the posterior on multiple levels of latent variables. For example, in a two-level model, we need to compute\n",
      "\n",
      "p(η, θ|D) ∝ p(D|θ)p(θ|η)p(η)\n",
      "\n",
      "(5.78)\n",
      "\n",
      "In some cases, we can analytically marginalize out θ; this leaves is with the simpler problem of just computing p(η|D).\n",
      "\n",
      "As a computational shortcut, we can approximate the posterior on the hyper-parameters with a point-estimate, p(η|D) ≈ δˆη(η), where ˆη = argmax p(η|D). Since η is typically much smaller than θ in dimensionality, it is less prone to overﬁtting, so we can safely use a uniform prior on η. Then the estimate becomes (cid:2)(cid:11)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "ˆη = argmax p(D|η) = argmax\n",
      "\n",
      "p(D|θ)p(θ|η)dθ\n",
      "\n",
      "(5.79)\n",
      "\n",
      "5.6. Empirical Bayes\n",
      "\n",
      "173\n",
      "\n",
      "where the quantity inside the brackets is the marginal or integrated likelihood, sometimes called the evidence. This overall approach is called empirical Bayes (EB) ortype-II maximum likelihood. In machine learning, it is sometimes called the evidence procedure.\n",
      "\n",
      "Empirical Bayes violates the principle that the prior should be chosen independently of the data. However, we can just view it as a computationally cheap approximation to inference in a hierarchical Bayesian model, just as we viewed MAP estimation as an approximation to inference in the one level model θ → D. In fact, we can construct a hierarchy in which the more integrals one performs, the “more Bayesian” one becomes:\n",
      "\n",
      "Method Maximum likelihood MAP estimation ML-II (Empirical Bayes) MAP-II Full Bayes\n",
      "\n",
      "Deﬁnition ˆθ = argmaxθ p(D|θ) ˆθ = argmaxθ p(D|θ)p(θ|η) (cid:22) ˆη = argmaxη (cid:22) ˆη = argmaxη p(θ, η|D) ∝ p(D|θ)p(θ|η)p(η)\n",
      "\n",
      "p(D|θ)p(θ|η)dθ = argmaxη p(D|η) p(D|θ)p(θ|η)p(η)dθ = argmaxη p(D|η)p(η)\n",
      "\n",
      "Note that EB can be shown to have good frequentist properties (see e.g., (Carlin and Louis 1996; Efron 2010)), so it is widely used by non-Bayesians. For example, the popular James-Stein estimator, discussed in Section 6.3.3.2, can be derived using EB.\n",
      "\n",
      "5.6.1\n",
      "\n",
      "Example: beta-binomial model\n",
      "\n",
      "Let us return to the cancer rates model. We can analytically integrate out the θi’s, and write down the marginal likelihood directly, as follows: (cid:11)\n",
      "\n",
      "p(D|a, b) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "Bin(xi|Ni, θi)Beta(θi|a, b)dθi\n",
      "\n",
      "(5.80)\n",
      "\n",
      "=\n",
      "\n",
      "i (cid:12)\n",
      "\n",
      "i\n",
      "\n",
      "B(a + xi, b + Ni − xi) B(a, b)\n",
      "\n",
      "(5.81)\n",
      "\n",
      "Various ways of maximizing this wrt a and b are discussed in (Minka 2000e).\n",
      "\n",
      "Having estimated a and b, we can plug in the hyper-parameters to compute the posterior p(θi|ˆa, ˆb, D) in the usual way, using conjugate analysis. The net result is that the posterior mean of each θi is a weighted average of its local MLE and the prior means, which depends on η = (a, b); but since η is estimated based on all the data, each θi is inﬂuenced by all the data.\n",
      "\n",
      "5.6.2\n",
      "\n",
      "Example: Gaussian-Gaussian model\n",
      "\n",
      "We now study another example that is analogous to the cancer rates example, except the data is real-valued. We will use a Gaussian likelihood and a Gaussian prior. This will allow us to write down the solution analytically.\n",
      "\n",
      "In particular, suppose we have data from multiple related groups. For example, xij could be the test score for student i in school j, for j = 1 :D and i = 1 :N j. We want to estimate the mean score for each school, θj. However, since the sample size, Nj, may be small for\n",
      "\n",
      "174\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "some schools, we can regularize the problem by using a hierarchical Bayesian model, where we assume θj come from a common prior, N (μ, τ 2). The joint distribution has the following form:\n",
      "\n",
      "p(θ, D|η, σ2) =\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "N (θj|μ, τ 2)\n",
      "\n",
      "Nj(cid:12)\n",
      "\n",
      "N (xij|θj, σ2)\n",
      "\n",
      "(5.82)\n",
      "\n",
      "j=1\n",
      "\n",
      "i=1\n",
      "\n",
      "where we assume σ2 is known for simplicity. (We relax this assumption in Exercise 24.4.) We explain how to estimate η below. Once we have estimated η = (μ, τ ), we can compute the posteriors over the θj’s. To do that, it simpliﬁes matters to rewrite the joint distribution in the following form, exploiting the fact that Nj Gaussian measurements with values xij and variance σ2 are equivalent to one measurement of value xj (cid:2) 1 j (cid:2) σ2/Nj. Nj This yields\n",
      "\n",
      "(cid:4)Nj\n",
      "\n",
      "i=1 xij with variance σ2\n",
      "\n",
      "p(θ, D|ˆη, σ2) =\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "N (θj|ˆμ, ˆτ 2)N (xj|θj, σ2 j )\n",
      "\n",
      "(5.83)\n",
      "\n",
      "j=1\n",
      "\n",
      "From this, it follows from the results of Section 4.4.1 that the posteriors are given by\n",
      "\n",
      "p(θj|D, ˆμ, ˆτ 2) =N (θj| ˆBj ˆμ + (1− ˆBj)xj, (1 − ˆBj)σ2 j )\n",
      "\n",
      "(5.84)\n",
      "\n",
      "ˆBj (cid:2)\n",
      "\n",
      "σ2 j σ2 j + ˆτ 2\n",
      "\n",
      "(5.85)\n",
      "\n",
      "where ˆμ = x and ˆτ 2 will be deﬁned below.\n",
      "\n",
      "The quantity 0 ≤ ˆBj ≤ 1 controls the degree of shrinkage towards the overall mean, μ. If the data is reliable for group j (e.g., because the sample size Nj is large), then σ2 j will be small relative to τ 2; hence ˆBj will be small, and we will put more weight on xj when we estimate θj. However, groups with small sample sizes will get regularized (shrunk towards the overall mean μ) more heavily. We will see an example of this below.\n",
      "\n",
      "If σj = σ for all groups j, the posterior mean becomes\n",
      "\n",
      "ˆθj = ˆBx + (1− ˆB)xj = x + (1− ˆB)(xj − x)\n",
      "\n",
      "(5.86)\n",
      "\n",
      "This has exactly the same form as the James Stein estimator discussed in Section 6.3.3.2.\n",
      "\n",
      "5.6.2.1\n",
      "\n",
      "Example: predicting baseball scores\n",
      "\n",
      "We now give an example of shrinkage applied to baseball batting averages, from (Efron and Morris 1975). We observe the number of hits for D = 18 players during the ﬁrst T = 45 games. Call the number of hits bi. We assume bj ∼ Bin(T, θj), where θj is the “true” batting average for player j. The goal is to estimate the θj. The MLE is of course ˆθj = xj, where xj = bj/T is the empirical batting average. However, we can use an EB approach to do better.\n",
      "\n",
      "Gaussian, xj ∼ N (θj, σ2) for known σ2.\n",
      "\n",
      "To apply the Gaussian shrinkage approach described above, we require that the likelihood be (We drop the i subscript since we assume Nj = 1,\n",
      "\n",
      "5.6. Empirical Bayes\n",
      "\n",
      "175\n",
      "\n",
      "MLE (top) and shrinkage estimates (bottom)\n",
      "\n",
      "MSE MLE = 0.0042, MSE shrunk = 0.0013\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.4\n",
      "\n",
      "0.35\n",
      "\n",
      "0.3\n",
      "\n",
      "true shrunk MLE\n",
      "\n",
      "0.6\n",
      "\n",
      "0.25\n",
      "\n",
      "E S M\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.15\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.25\n",
      "\n",
      "0.3\n",
      "\n",
      "0.35\n",
      "\n",
      "0.4\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3 player number\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 5.12 (a) MLE parameters (top) and corresponding shrunken estimates (bottom). (b) We plot the true parameters (blue), the posterior mean estimate (green), and the MLEs (red) for 5 of the players. Figure generated by shrinkageDemoBaseball.\n",
      "\n",
      "since xj already represents the average for player j.) However, binomial likelihood. While this has the right mean, E [xj] = θj, the variance is not constant:\n",
      "\n",
      "in this example we have a\n",
      "\n",
      "var [xj] =\n",
      "\n",
      "1 T 2 var [bj] =\n",
      "\n",
      "T θj(1 − θj) T 2\n",
      "\n",
      "(5.87)\n",
      "\n",
      "So let us apply a variance stabilizing transform5 to xj to better match the Gaussian assump- tion:\n",
      "\n",
      "(5.88) Now we have approximately yj ∼ N (f (θj), 1) = N (μj, 1). We use Gaussian shrinkage to estimate the μj using Equation 5.86 with σ2 = 1, and we then transform back to get\n",
      "\n",
      "(5.89) The results are shown in Figure 5.12(a-b). In (a), we plot the MLE ˆθj and the posterior mean θj. We see that all the estimates have shrunk towards the global mean, 0.265. In (b), we plot the true value θj, the MLE ˆθj and the posterior mean θj. (The “true” values of θj are estimated from a large number of independent games.) We see that, on average, the shrunken estimate is much closer to the true parameters than the MLE is. Speciﬁcally, the mean squared error, deﬁned by MSE = 1 j=1(θj − θj)2, is over three times smaller using the shrinkage estimates N θj than using the MLEs ˆθj.\n",
      "\n",
      "yj = f (yj) =\n",
      "\n",
      "ˆθj = 0.5(sin(ˆμj/\n",
      "\n",
      "√\n",
      "\n",
      "T arcsin(2yj − 1)\n",
      "\n",
      "√\n",
      "\n",
      "(cid:4)D\n",
      "\n",
      "T ) + 1)\n",
      "\n",
      "5.6.2.2\n",
      "\n",
      "Estimating the hyper-parameters In this section, we give an algorithm for estimating η. Suppose initially that σ2 j = σ2 is the same for all groups. In this case, we can derive the EB estimate in closed form, as we now show. From Equation 4.126, we have\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(xj|μ, τ 2, σ2) =\n",
      "\n",
      "N (xj|θj, σ2)N (θj|μ, τ 2)dθj = N (xj|μ, τ 2 + σ2)\n",
      "\n",
      "(5.90)\n",
      "\n",
      "5. Suppose E [X] =μ and var [X] =σ 2(μ). Let Y = f (X). Then a Taylor series expansions gives Y ≈ f (μ) + (X − μ)f (cid:2)(μ). Hence var [Y ] ≈ f (cid:2)(μ)2var [X − μ] =f (cid:2)(μ)2σ2(μ). A variance stabilizing transformation is a function f such that f (cid:2)(μ)2σ2(μ) is independent of μ.\n",
      "\n",
      "176\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "Hence the marginal likelihood is\n",
      "\n",
      "p(D|μ, τ 2, σ2) =\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "N (xj|μ, τ 2 + σ2)\n",
      "\n",
      "(5.91)\n",
      "\n",
      "j=1\n",
      "\n",
      "Thus we can estimate the hyper-parameters using the usual MLEs for a Gaussian. For μ, we have\n",
      "\n",
      "ˆμ =\n",
      "\n",
      "1 D\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "j=1\n",
      "\n",
      "xj = x\n",
      "\n",
      "(5.92)\n",
      "\n",
      "which is the overall mean.\n",
      "\n",
      "For the variance, we can use moment matching (which is equivalent to the MLE for a\n",
      "\n",
      "Gaussian): we simply equate the model variance to the empirical variance:\n",
      "\n",
      "ˆτ 2 + σ2 =\n",
      "\n",
      "1 D\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "(xj − x)2 (cid:2) s2\n",
      "\n",
      "j=1\n",
      "\n",
      "(5.93)\n",
      "\n",
      "so ˆτ 2 = s2 − σ2. Since we know τ 2 must be positive, it is common to use the following revised estimate:\n",
      "\n",
      "ˆτ 2 = max{0, s2 − σ2} = (s2 − σ2)+\n",
      "\n",
      "(5.94)\n",
      "\n",
      "Hence the shrinkage factor is\n",
      "\n",
      "j ’s are different, we can no longer derive a solution in closed form. Exercise 11.13 discusses how to use the EM algorithm to derive an EB estimate, and Exercise 24.4 discusses how to perform full Bayesian inference in this hierarchical model.\n",
      "\n",
      "σ2 σ2 + ˆτ 2 = In the case where the σ2\n",
      "\n",
      "ˆB =\n",
      "\n",
      "σ2 σ2 + (s2 − σ2)+\n",
      "\n",
      "(5.95)\n",
      "\n",
      "5.7\n",
      "\n",
      "Bayesian decision theory\n",
      "\n",
      "We have seen how probability theory can be used to represent and updates our beliefs about the state of the world. However, ultimately our goal is to convert our beliefs into actions. In this section, we discuss the optimal way to do this.\n",
      "\n",
      "We can formalize any given statistical decision problem as a game against nature (as opposed to a game against other strategic players, which is the topic of game theory, see e.g., (Shoham and Leyton-Brown 2009) for details). In this game, nature picks a state or parameter or label, y ∈ Y, unknown to us, and then generates an observation, x ∈ X , which we get to see. We then have to make a decision, that is, we have to choose an action a from some action space A. Finally we incur some loss, L(y, a), which measures how compatible our action a is with nature’s hidden state y. For example, we might use misclassiﬁcation loss, L(y, a) =I (y (cid:6)= a), or squared loss, L(y, a) = (y − a)2. We will see some other examples below.\n",
      "\n",
      "5.7. Bayesian decision theory\n",
      "\n",
      "177\n",
      "\n",
      "Our goal is to devise a decision procedure or policy, δ : X → A, which speciﬁes the optimal action for each possible input. By optimal, we mean the action that minimizes the expected loss:\n",
      "\n",
      "δ(x) = argmin\n",
      "\n",
      "a∈A\n",
      "\n",
      "E [L(y, a)]\n",
      "\n",
      "(5.96)\n",
      "\n",
      "In economics, U (y, a) = −L(y, a). Thus the above rule becomes\n",
      "\n",
      "it is more common to talk of a utility function; this is just negative loss,\n",
      "\n",
      "δ(x) = argmax\n",
      "\n",
      "a∈A\n",
      "\n",
      "E [U (y, a)]\n",
      "\n",
      "(5.97)\n",
      "\n",
      "This is called the maximum expected utility principle, and is the essence of what we mean by rational behavior.\n",
      "\n",
      "In the Bayesian version, which we discuss below, we mean the expected value of y given the data we In the frequentist version, which we discuss in Section 6.3, we mean the have seen so far. expected value of y and x that we expect to see in the future.\n",
      "\n",
      "as the action a that minimizes the posterior expected loss:\n",
      "\n",
      "Note that there are two different interpretations of what we mean by “expected”.\n",
      "\n",
      "In the Bayesian approach to decision theory, the optimal action, having observed x, is deﬁned\n",
      "\n",
      "ρ(a|x) (cid:2) Ep(y|x) [L(y, a)] =\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "L(y, a)p(y|x)\n",
      "\n",
      "(5.98)\n",
      "\n",
      "y\n",
      "\n",
      "(If y is continuous (e.g., when we want to estimate a parameter vector), we should replace the sum with an integral.) Hence the Bayes estimator, also called the Bayes decision rule, is given by\n",
      "\n",
      "δ(x) = arg min a∈A\n",
      "\n",
      "ρ(a|x)\n",
      "\n",
      "(5.99)\n",
      "\n",
      "5.7.1\n",
      "\n",
      "Bayes estimators for common loss functions\n",
      "\n",
      "In this section we show how to construct Bayes estimators for the loss functions most commonly arising in machine learning.\n",
      "\n",
      "5.7.1.1 MAP estimate minimizes 0-1 loss\n",
      "\n",
      "The 0-1 loss is deﬁned by\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "L(y, a) = I(y (cid:6)= a) =\n",
      "\n",
      "0 1\n",
      "\n",
      "if a = y if a (cid:6)= y\n",
      "\n",
      "(5.100)\n",
      "\n",
      "This is commonly used in classiﬁcation problems where y is the true class label and a = ˆy is the estimate.\n",
      "\n",
      "For example, in the two class case, we can write the loss matrix as follows:\n",
      "\n",
      "y = 1 y = 0\n",
      "\n",
      "ˆy = 1 0 1\n",
      "\n",
      "ˆy = 0 1 0\n",
      "\n",
      "178\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "(cid:2)(cid:3)(cid:4) (cid:2)(cid:3)(cid:4)(cid:2)(cid:5)(cid:3)(cid:6)(cid:8)(cid:9)\n",
      "\n",
      "(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:17)(cid:9)(cid:17)(cid:10)(cid:11)\n",
      "\n",
      "(cid:5)(cid:6)(cid:7)(cid:17)(cid:8)(cid:17)(cid:9)(cid:17)(cid:9)(cid:17)(cid:10)(cid:11)\n",
      "\n",
      "(cid:4)(cid:3)(cid:4)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:12)(cid:13)(cid:14)(cid:13)(cid:15)(cid:16)(cid:17) (cid:12)(cid:13)(cid:17)(cid:18)(cid:19)(cid:20)\n",
      "\n",
      "Figure 5.13 For some regions of input space, where the class posteriors are uncertain, we may prefer not to choose class 1 or 2; instead we may prefer the reject option. Based on Figure 1.26 of (Bishop 2006a).\n",
      "\n",
      "(In Section 5.7.2, we generalize this loss function so it penalizes the two kinds of errors on\n",
      "\n",
      "the off-diagonal differently.)\n",
      "\n",
      "The posterior expected loss is\n",
      "\n",
      "ρ(a|x) = p(a (cid:6)= y|x) = 1 − p(y|x)\n",
      "\n",
      "(5.101)\n",
      "\n",
      "Hence the action that minimizes the expected loss is the posterior mode or MAP estimate\n",
      "\n",
      "y∗(x) = arg max y∈Y\n",
      "\n",
      "p(y|x)\n",
      "\n",
      "(5.102)\n",
      "\n",
      "5.7.1.2\n",
      "\n",
      "Reject option\n",
      "\n",
      "In classiﬁcation problems where p(y|x) is very uncertain, we may prefer to choose a reject action, in which we refuse to classify the example as any of the speciﬁed classes, and instead say “don’t know”. Such ambiguous cases can be handled by e.g., a human expert. See Figure 5.13 for an illustration. This is useful in risk averse domains such as medicine and ﬁnance.\n",
      "\n",
      "We can formalize the reject option as follows. Let choosing a = C + 1 correspond to picking the reject action, and choosing a ∈ {1, . . . , C} correspond to picking one of the classes. Suppose we deﬁne the loss function as\n",
      "\n",
      "L(y = j, a = i) =\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "0 λr λs\n",
      "\n",
      "if i = j and i, j ∈ {1, . . . , C} if i = C + 1 otherwise\n",
      "\n",
      "(5.103)\n",
      "\n",
      "where λr is the cost of the reject action, and λs is the cost of a substitution error. In Exercise 5.3, you will show that the optimal action is to pick the reject action if the most probable class has a probability below 1 − λr\n",
      "\n",
      "λs ; otherwise you should just pick the most probable class.\n",
      "\n",
      "5.7. Bayesian decision theory\n",
      "\n",
      "179\n",
      "\n",
      "|x|0.2\n",
      "\n",
      "|x|1.0\n",
      "\n",
      "|x|2.0\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0 −2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 5.14 generated by lossFunctionFig.\n",
      "\n",
      "(a-c). Plots of the L(y, a) =|y − a|q vs |y − a| for q = 0.2, q = 1 and q = 2. Figure\n",
      "\n",
      "5.7.1.3\n",
      "\n",
      "Posterior mean minimizes (cid:9)2 (quadratic) loss\n",
      "\n",
      "For continuous parameters, a more appropriate loss function is squared error, (cid:9)2 loss, or quadratic loss, deﬁned as L(y, a) = (y − a)2\n",
      "\n",
      "(5.104)\n",
      "\n",
      "The posterior expected loss is given by (cid:20) (cid:20)\n",
      "\n",
      "ρ(a|x) =E\n",
      "\n",
      "(y − a)2|x\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "= E\n",
      "\n",
      "y2|x\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "− 2aE [y|x] +a 2\n",
      "\n",
      "(5.105)\n",
      "\n",
      "Hence the optimal estimate is the posterior mean:\n",
      "\n",
      "∂ ∂a\n",
      "\n",
      "ρ(a|x) = −2E [y|x] + 2a = 0 ⇒ ˆy = E [y|x] =\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "yp(y|x)dy\n",
      "\n",
      "(5.106)\n",
      "\n",
      "This is often called the minimum mean squared error estimate or MMSE estimate.\n",
      "\n",
      "In a linear regression problem, we have p(y|x, θ) = N (y|xT w, σ2)\n",
      "\n",
      "(5.107)\n",
      "\n",
      "In this case, the optimal estimate given some training data D is given by\n",
      "\n",
      "E [y|x, D] = xT E [w|D]\n",
      "\n",
      "(5.108)\n",
      "\n",
      "That is, we just plug-in the posterior mean parameter estimate. Note that this is the optimal thing to do no matter what prior we use for w.\n",
      "\n",
      "5.7.1.4\n",
      "\n",
      "Posterior median minimizes (cid:9)1 (absolute) loss\n",
      "\n",
      "The (cid:9)2 loss penalizes deviations from the truth quadratically, and thus is sensitive to outliers. A more robust alternative is the absolute or (cid:9)1 loss, L(y, a) = |y −a| (see Figure 5.14). The optimal estimate is the posterior median, i.e., a value a such that P (y < a|x) = P (y ≥ a|x) = 0.5. See Exercise 5.9 for a proof.\n",
      "\n",
      "5.7.1.5\n",
      "\n",
      "Supervised learning Consider a prediction function δ : X → Y, and suppose we have some cost function (cid:9)(y, y(cid:2)) which gives the cost of predicting y(cid:2) when the truth is y. We can deﬁne the loss incurred by\n",
      "\n",
      "180\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "taking action δ (i.e., using this predictor) when the unknown state of nature is θ (the parameters of the data generating mechanism) as follows: (cid:6)\n",
      "\n",
      "L(θ, δ) (cid:2) E(x,y)∼p(x,y|θ) [(cid:9)(y, δ(x)] =\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "L(y, δ(x))p(x, y|θ)\n",
      "\n",
      "(5.109)\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "This is known as the generalization error. Our goal is to minimize the posterior expected loss, given by\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ρ(δ|D) =\n",
      "\n",
      "p(θ|D)L(θ, δ)dθ\n",
      "\n",
      "(5.110)\n",
      "\n",
      "This should be contrasted with the frequentist risk which is deﬁned in Equation 6.47.\n",
      "\n",
      "5.7.2\n",
      "\n",
      "The false positive vs false negative tradeoff\n",
      "\n",
      "In this section, we focus on binary decision problems, such as hypothesis testing, two-class classiﬁcation, object/ event detection, etc. There are two types of error we can make: a false positive (aka false alarm), which arises when we estimate ˆy = 1 but the truth is y = 0; or a false negative (aka missed detection), which arises when we estimate ˆy = 0 but the truth is y = 1. The 0-1 loss treats these two kinds of errors equivalently. However, we can consider the following more general loss matrix:\n",
      "\n",
      "ˆy = 1 0 LF P where LF N is the cost of a false negative, and LF P is the cost of a false positive. The\n",
      "\n",
      "y = 1 y = 0\n",
      "\n",
      "ˆy = 0 LF N 0\n",
      "\n",
      "posterior expected loss for the two possible actions is given by\n",
      "\n",
      "ρ(ˆy = 0|x) =L F N p(y = 1|x) ρ(ˆy = 1|x) =L F P p(y = 0|x)\n",
      "\n",
      "(5.111)\n",
      "\n",
      "(5.112)\n",
      "\n",
      "Hence we should pick class ˆy = 1 iff\n",
      "\n",
      "ρ(ˆy = 0|x) > ρ(ˆy = 1|x) p(y = 1|x) p(y = 0|x)\n",
      "\n",
      ">\n",
      "\n",
      "LF P LF N\n",
      "\n",
      "(5.113)\n",
      "\n",
      "(5.114)\n",
      "\n",
      "If LF N = cLF P , it is easy to show (Exercise 5.10) that we should pick ˆy = 1 iff p(y = 1|x)/p(y = 0|x) > τ , where τ = c/(1 + c) (see also (Muller et al. 2004)). For example, if a false negative costs twice as much as false positive, so c = 2, then we use a decision threshold of 2/3 before declaring a positive.\n",
      "\n",
      "Below we discuss ROC curves, which provide a way to study the FP-FN tradeoff without having\n",
      "\n",
      "to choose a speciﬁc threshold.\n",
      "\n",
      "5.7.2.1\n",
      "\n",
      "ROC curves and all that\n",
      "\n",
      "Suppose we are solving a binary decision problem, such as classiﬁcation, hypothesis testing, object detection, etc. Also, assume we have a labeled data set, D = {(xi, yi)}. Let δ(x) =\n",
      "\n",
      "5.7. Bayesian decision theory\n",
      "\n",
      "181\n",
      "\n",
      "Truth\n",
      "\n",
      "Estimate\n",
      "\n",
      "1 0 Σ N+ = T P + F N N− = F P + T N N = T P + F P + F N + T N\n",
      "\n",
      "1 TP FN\n",
      "\n",
      "0 FP TN\n",
      "\n",
      "Σ ˆN+ = T P + F P ˆN− = F N + T N\n",
      "\n",
      "Table 5.2 Quantities derivable from a confusion matrix. N+ is the true number of positives, ˆN+ is the “called” number of positives, N− is the true number of negatives, ˆN− is the “called” number of negatives.\n",
      "\n",
      "y = 1 T P/N+=TPR=sensitivity=recall ˆy = 1 ˆy = 0 F N/N+=FNR=miss rate=type II\n",
      "\n",
      "y = 0 F P/N−=FPR=type I T N/N−=TNR=speciﬁty\n",
      "\n",
      "Table 5.3 Estimating p(ˆy|y) from a confusion matrix. Abbreviations: FNR = false negative rate, FPR = false positive rate, TNR = true negative rate, TPR = true positive rate.\n",
      "\n",
      "I(f (x) > τ ) be our decision rule, where f (x) is a measure of conﬁdence that y = 1 (this should be monotonically related to p(y = 1|x), but does not need to be a probability), and τ is some threshold parameter. For each given value of τ , we can apply our decision rule and count the number of true positives, false positives, true negatives, and false negatives that occur, as shown in Table 5.2. This table of errors is called a confusion matrix.\n",
      "\n",
      "From this table, we can compute the true positive rate (TPR), also known as the sensitivity, recall or hit rate, by using T P R = T P/N+ ≈ p(ˆy = 1|y = 1). We can also compute the false positive rate (FPR), also called the false alarm rate, or the type I error rate, by using F P R = F P/N− ≈ p(ˆy = 1|y = 0). These and other deﬁnitions are summarized in Tables 5.3 and 5.4. We can combine these errors in any way we choose to compute a loss function.\n",
      "\n",
      "However, rather than than computing the TPR and FPR for a ﬁxed threshold τ , we can run our detector for a set of thresholds, and then plot the TPR vs FPR as an implicit function of τ . This is called a receiver operating characteristic or ROC curve. See Figure 5.15(a) for an example. Any system can achieve the point on the bottom left, (F P R = 0, T P R = 0), by setting τ = 1 and thus classifying everything as negative; similarly any system can achieve the point on the top right, (F P R = 1, T P R = 1), by setting τ = 0 and thus classifying everything as positive. If a system is performing at chance level, then we can achieve any point on the diagonal line T P R = F P R by choosing an appropriate threshold. A system that perfectly separates the positives from negatives has a threshold that can achieve the top left corner, (F P R = 0, T P R = 1); by varying the threshold such a system will “hug” the left axis and then the top axis, as shown in Figure 5.15(a).\n",
      "\n",
      "The quality of a ROC curve is often summarized as a single number using the area under the curve or AUC. Higher AUC scores are better; the maximum is obviously 1. Another summary statistic that is used is the equal error rate or EER, also called the cross over rate, deﬁned as the value which satisﬁes F P R = F N R. Since F N R = 1 − T P R, we can compute the EER by drawing a line from the top left to the bottom right and seeing where it intersects the ROC curve (see points A and B in Figure 5.15(a)). Lower EER scores are better; the minimum is obviously 0.\n",
      "\n",
      "182\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "r p t\n",
      "\n",
      "n o s c e r p\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "0 0\n",
      "\n",
      "fpr\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "recall\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 5.15 (a) ROC curves for two hypothetical classiﬁcation systems. A is better than B. We plot the true positive rate (TPR) vs the false positive rate (FPR) as we vary the threshold τ . We also indicate the equal error rate (EER) with the red and blue dots, and the area under the curve (AUC) for classiﬁer B. (b) A precision-recall curve for two hypothetical classiﬁcation systems. A is better than B. Figure generated by PRhand.\n",
      "\n",
      "ˆy = 1 ˆy = 0\n",
      "\n",
      "T P/ ˆN+=precision=PPV F P/ ˆN+=FDP T N/ ˆN−=NPV\n",
      "\n",
      "F N/ ˆN−\n",
      "\n",
      "y = 1\n",
      "\n",
      "y = 0\n",
      "\n",
      "Table 5.4 Estimating p(y|ˆy) from a confusion matrix. Abbreviations: FDP = false discovery probability, NPV = negative predictive value, PPV = positive predictive value,\n",
      "\n",
      "5.7.2.2\n",
      "\n",
      "Precision recall curves\n",
      "\n",
      "When trying to detect a rare event (such as retrieving a relevant document or ﬁnding a face in an image), the number of negatives is very large. Hence comparing T P R = T P/N+ to F P R = F P/N− is not very informative, since the FPR will be very small. Hence all the In such cases, it is common to plot “action” in the ROC curve will occur on the extreme left. the TPR versus the number of false positives, rather than vs the false positive rate.\n",
      "\n",
      "However, in some cases, the very notion of “negative” is not well-deﬁned. For example, when detecting objects in images (see Section 1.2.1.3), if the detector works by classifying patches, then the number of patches examined — and hence the number of true negatives — is a parameter of the algorithm, not part of the problem deﬁnition. So we would like to use a measure that only talks about positives.\n",
      "\n",
      "The precision is deﬁned as T P/ ˆN+ = p(y = 1|ˆy = 1) and the recall is deﬁned as T P/N+ = p(ˆy = 1|y = 1). Precision measures what fraction of our detections are actually positive, and recall measures what fraction of the positives we actually detected. If ˆyi ∈ {0, 1} is the predicted label, and yi ∈ {0, 1} is the true label, we can estimate precision and recall using\n",
      "\n",
      "P =\n",
      "\n",
      "(cid:4) i yi ˆyi(cid:4) i ˆyi\n",
      "\n",
      ", R =\n",
      "\n",
      "(cid:4) i yi ˆyi(cid:4) i yi\n",
      "\n",
      "(5.115)\n",
      "\n",
      "A precision recall curve is a plot of precision vs recall as we vary the threshold τ . See Figure 5.15(b). Hugging the top right is the best one can do.\n",
      "\n",
      "This curve can be summarized as a single number using the mean precision (averaging over\n",
      "\n",
      "5.7. Bayesian decision theory\n",
      "\n",
      "183\n",
      "\n",
      "ˆy = 1 ˆy = 0\n",
      "\n",
      "Class 1 y = 1 10 10\n",
      "\n",
      "y = 0 10 970\n",
      "\n",
      "ˆy = 1 ˆy = 0\n",
      "\n",
      "Class 2 y = 1 90 10\n",
      "\n",
      "y = 0 10 890\n",
      "\n",
      "ˆy = 1 ˆy = 0\n",
      "\n",
      "Pooled y = 1 100 20\n",
      "\n",
      "y = 0 20 1860\n",
      "\n",
      "Illustration of the difference between macro- and micro-averaging. y is the true label, and ˆy Table 5.5 is the called label. In this example, the macro-averaged precision is [10/(10 + 10) + 90/(10 + 90)]/2 = (0.5 + 0.9)/2 = 0.7. The micro-averaged precision is 100/(100 + 20) ≈ 0.83. Based on Table 13.7 of (Manning et al. 2008).\n",
      "\n",
      "recall values), which approximates the area under the curve. Alternatively, one can quote the precision for a ﬁxed recall level, such as the precision of the ﬁrst K = 10 entities recalled. This is called the average precision at K score. This measure is widely used when evaluating information retrieval systems.\n",
      "\n",
      "5.7.2.3\n",
      "\n",
      "F-scores *\n",
      "\n",
      "For a ﬁxed threshold, one can compute a single precision and recall value. These are often combined into a single statistic called the F score, or F1 score, which is the harmonic mean of precision and recall:\n",
      "\n",
      "F1 (cid:2)\n",
      "\n",
      "2 1/P + 1/R\n",
      "\n",
      "=\n",
      "\n",
      "2P R R + P\n",
      "\n",
      "(5.116)\n",
      "\n",
      "Using Equation 5.115, we can write this as\n",
      "\n",
      "F1 =\n",
      "\n",
      "2 (cid:4)N i=1 yi +\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      "i=1 yi ˆyi (cid:4)N\n",
      "\n",
      "i=1 ˆyi\n",
      "\n",
      "(5.117)\n",
      "\n",
      "This is a widely used measure in information retrieval systems.\n",
      "\n",
      "To understand why we use the harmonic mean instead of the arithmetic mean, (P + R)/2, consider the following scenario. Suppose we recall all entries, so R = 1. The precision will be given by the prevalence, p(y = 1). Suppose the prevalence is low, say p(y = 1) = 10−4. The arithmetic mean of P and R is given by (P + R)/2 = (10−4 + 1)/2 ≈ 50%. By contrast, the harmonic mean of this strategy is only 2×10−4×1\n",
      "\n",
      "1+10−4 ≈ 0.2%. In the multi-class case (e.g., for document classiﬁcation problems), there are two ways to generalize F1 scores. The ﬁrst is called macro-averaged F1, and is deﬁned as c=1 F1(c)/C, where F1(c) is the F1 score obtained on the task of distinguishing class c from all the others. The other is called micro-averaged F1, and is deﬁned as the F1 score where we pool all the counts from each class’s contingency table.\n",
      "\n",
      "(cid:4)C\n",
      "\n",
      "Table 5.5 gives a worked example that illustrates the difference. We see that the precision of class 1 is 0.5, and of class 2 is 0.9. The macro-averaged precision is therefore 0.7, whereas the micro-averaged precision is 0.83. The latter is much closer to the precision of class 2 than to the precision of class 1, since class 2 is ﬁve times larger than class 1. To give equal weight to each class, use macro-averaging.\n",
      "\n",
      "184\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "5.7.2.4\n",
      "\n",
      "False discovery rates *\n",
      "\n",
      "Suppose we are trying to discover a rare phenomenon using some kind of high throughput measurement device, such as a gene expression micro array, or a radio telescope. We will need to make many binary decisions of the form p(yi = 1|D) > τ , whereD = {xi}N i=1 and N may be large. This is called multiple hypothesis testing. Note that the difference from standard binary classiﬁcation is that we are classifying yi based on all the data, not just based on xi. So this is a simultaneous classiﬁcation problem, where we might hope to do better than a series of individual classiﬁcation problems.\n",
      "\n",
      "How should we set the threshold τ ? A natural approach is to try to minimize the expected\n",
      "\n",
      "number of false positives. In the Bayesian approach, this can be computed as follows:\n",
      "\n",
      "F D(τ, D) (cid:2)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "i\n",
      "\n",
      "(1 − pi) ! \"# $ pr. error\n",
      "\n",
      "I(pi > τ ) ! $ \"# discovery\n",
      "\n",
      "(5.118)\n",
      "\n",
      "where pi (cid:2) p(yi = 1|D) is your belief that this object exhibits the phenomenon in question. We then deﬁne the posterior expected false discovery rate as follows:\n",
      "\n",
      "where N (τ, D) = i I(pi > τ ) is the number of discovered items. Given a desired FDR tolerance, say α = 0.05, one can then adapt τ to achieve this; this is called the direct posterior probability approach to controlling the FDR (Newton et al. 2004; Muller et al. 2004).\n",
      "\n",
      "In order to control the FDR it is very helpful to estimate the pi’s jointly (e.g., using a hierar- chical Bayesian model, as in Section 5.5), rather than independently. This allows the pooling of statistical strength, and thus lower FDR. See e.g., (Berry and Hochberg 1999) for more information.\n",
      "\n",
      "F DR(τ, D) (cid:2) F D(τ, D)/N (τ, D) (cid:4)\n",
      "\n",
      "(5.119)\n",
      "\n",
      "5.7.3\n",
      "\n",
      "Other topics *\n",
      "\n",
      "In this section, we brieﬂy mention a few other topics related to Bayesian decision theory. We do not have space to go into detail, but we include pointers to the relevant literature.\n",
      "\n",
      "5.7.3.1\n",
      "\n",
      "Contextual bandits\n",
      "\n",
      "A one-armed bandit is a colloquial term for a slot machine, found in casinos around the world. The game is this: you insert some money, pull an arm, and wait for the machine to stop; if you’re lucky, you win some money. Now imagine there is a bank of K such machines to choose from. Which one should you use? This is called a multi-armed bandit, and can be modeled using Bayesian decision theory: there are K possible actions, and each action has an unknown reward (payoff function) rk. By maintaining a belief state, p(r1:K|D) = k p(rk|D), one can devise an optimal policy; this can be compiled into a series of Gittins Indices (Gittins 1989). This optimally solves the exploration-exploitation tradeoff, which speciﬁes how many times one should try each action before deciding to go with the winner.\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "Now consider an extension where each arm, and the player, has an associated feature vector; call all these features x. This is called a contextual bandit (see e.g., (Sarkar 1991; Scott 2010; Li et al. 2011)). For example, the “arms” could represent ads or news articles which we want to show to the user, and the features could represent properties of these ads or articles, such\n",
      "\n",
      "5.7. Bayesian decision theory\n",
      "\n",
      "185\n",
      "\n",
      "If we assume a as a bag of words, as well as properties of the user, such as demographics. linear model for reward, rk = θT k x, we can maintain a distribution over the parameters of each arm, p(θk|D), where D is a series of tuples of the form (a, x, r), which speciﬁes which arm was pulled, what its features were, and what the resulting outcome was (e.g., r = 1 if the user clicked on the ad, and r = 0 otherwise). We discuss ways to compute p(θk|D) from linear and logistic regression models in later chapters.\n",
      "\n",
      "Given the posterior, we must decide what action to take. One common heuristic, known as\n",
      "\n",
      "UCB (which stands for “upper conﬁdence bound”) is to take the action which maximizes\n",
      "\n",
      "K argmax k=1 where μk = E [rk|D], σ2 k = var [rk|D] and λ is a tuning parameter that trades off exploration and exploitation. The intuition is that we should pick actions about which we believe are good (μk is large), and/ or actions about which we are uncertain (σk is large).\n",
      "\n",
      "k∗ =\n",
      "\n",
      "μk + λσk\n",
      "\n",
      "(5.120)\n",
      "\n",
      "action k with a probability that is equal to its probability of being the optimal action:\n",
      "\n",
      "An even simpler method, known as Thompson sampling, is as follows. At each step, we pick\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "pk =\n",
      "\n",
      "I(E [r|a, x, θ] = max\n",
      "\n",
      "a(cid:2)\n",
      "\n",
      "E [r|a(cid:2), x, θ])p(θ|D)dθ\n",
      "\n",
      "(5.121)\n",
      "\n",
      "We can approximate this by drawing a single sample from the posterior, θt ∼ p(θ|D), and then choosing k∗ = argmaxk E . Despite its simplicity, this has been shown to work quite well (Chapelle and Li 2011).\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "r|x, k,θ t\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "5.7.3.2\n",
      "\n",
      "Utility theory\n",
      "\n",
      "Suppose we are a doctor trying to decide whether to operate on a patient or not. We imagine there are 3 states of nature: the patient has no cancer, the patient has lung cancer, or the patient has breast cancer. Since the action and state space is discrete, we can represent the loss function L(θ, a) as a loss matrix, such as the following:\n",
      "\n",
      "Surgery No surgery\n",
      "\n",
      "20 10 10 These numbers reﬂects the fact that not performing surgery when the patient has cancer is very bad (loss of 50 or 60, depending on the type of cancer), since the patient might die; not performing surgery when the patient does not have cancer incurs no loss (0); performing surgery when the patient does not have cancer is wasteful (loss of 20); and performing surgery when the patient does have cancer is painful but necessary (10).\n",
      "\n",
      "No cancer Lung cancer Breast cancer\n",
      "\n",
      "0 50 60\n",
      "\n",
      "It is natural to ask where these numbers come from. Ultimately they represent the personal preferences or values of a ﬁctitious doctor, and are somewhat arbitrary: just as some people prefer chocolate ice cream and others prefer vanilla, there is no such thing as the “right” loss/ utility function. However, it can be shown (see e.g., (DeGroot 1970)) that any set of consistent preferences can be converted to a scalar loss/ utility function. Note that utility can be measured on an arbitrary scale, such as dollars, since it is only relative values that matter.6\n",
      "\n",
      "6. People are often squeamish about talking about human lives in monetary terms, but all decision making requires\n",
      "\n",
      "186\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "5.7.3.3\n",
      "\n",
      "Sequential decision theory\n",
      "\n",
      "So far, we have concentrated on one-shot decision problems, where we only have to make one decision and then the game ends. In Setion 10.6, we will generalize this to multi-stage or sequential decision problems. Such problems frequently arise in many business and engineering settings. This is closely related to the problem of reinforcement learning. However, further discussion of this point is beyond the scope of this book.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 5.1 Proof that a mixture of conjugate priors is indeed conjugate Derive Equation 5.69.\n",
      "\n",
      "Exercise 5.2 Optimal threshold on classiﬁcation probability Consider a case where we have learned a conditional probability distribution P (y|x). Suppose there are only two classes, and let p0 = P (Y = 0|x) and p1 = P (Y = 1|x). Consider the loss matrix below:\n",
      "\n",
      "predicted label ˆy 0 1\n",
      "\n",
      "true label y 1 λ01 0\n",
      "\n",
      "0 0 λ10\n",
      "\n",
      "a. Show that the decision ˆy that minimizes the expected loss is equivalent to setting a probability threshold θ and predicting ˆy = 0 if p1 < θ and ˆy = 1 if p1 ≥ θ. What is θ as a function of λ01 and λ10? (Show your work.)\n",
      "\n",
      "b. Show a loss matrix where the threshold is 0.1. (Show your work.)\n",
      "\n",
      "Exercise 5.3 Reject option in classiﬁers (Source: (Duda et al. 2001, Q2.13).) In many classiﬁcation problems one has the option either of assigning x to class j or, if you are too uncertain, of choosing the reject option. If the cost for rejects is less than the cost of falsely classifying the object, it may be the optimal action. Let αi mean you choose action i, for i = 1 : C + 1, where C is the number of classes and C + 1 is the reject action. Let Y = j be the true (but unknown) state of nature. Deﬁne the loss function as follows ⎧ ⎨\n",
      "\n",
      "λ(αi|Y = j) =\n",
      "\n",
      "⎩\n",
      "\n",
      "0 λr λs\n",
      "\n",
      "if i = j and i, j ∈ {1, . . . , C} if i = C + 1 otherwise\n",
      "\n",
      "(5.122)\n",
      "\n",
      "In otherwords, you incur 0 loss if you correctly classify, you incur λr loss (cost) if you choose the reject option, and you incur λs loss (cost) if you make a substitution error (misclassiﬁcation).\n",
      "\n",
      "tradeoffs, and one needs to use some kind of “currency” to compare different courses of action. Insurance companies do this all the time. Ross Schachter, a decision theorist at Stanford University, likes to tell a story of a school board who rejected a study on absestos removal from schools because it performed a cost-beneﬁt analysis, which was considered “inhumane” because they put a dollar value on children’s health; the result of rejecting the report was that the absestos was not removed, which is surely more “inhumane”. In medical domains, one often measures utility in terms of QALY, or quality-adjusted life-years, instead of dollars, but it’s the same idea. Of course, even if you do not explicitly specify how much you value different people’s lives, your behavior will reveal your implicit values/ preferences, and these preferences can then be converted to a real-valued scale, such as dollars or QALY. Inferring a utility function from behavior is called inverse reinforcement learning.\n",
      "\n",
      "5.7. Bayesian decision theory\n",
      "\n",
      "187\n",
      "\n",
      "Decision ˆy predict 0 predict 1 reject\n",
      "\n",
      "true label y 1 10 0 3\n",
      "\n",
      "0 0 10 3\n",
      "\n",
      "a. Show that the minimum risk is obtained if we decide Y = j if p(Y = j|x) ≥ p(Y = k|x) for all k\n",
      "\n",
      "(i.e., j is the most probable class) and if p(Y = j|x) ≥ 1 − λr\n",
      "\n",
      "λs ; otherwise we decide to reject.\n",
      "\n",
      "b. Describe qualitatively what happens as λr/λs is increased from 0 to 1 (i.e., the relative cost of rejection\n",
      "\n",
      "increases).\n",
      "\n",
      "Exercise 5.4 More reject options In many applications, the classiﬁer is allowed to “reject” a test example rather than classifying it into one of the classes. Consider, for example, a case in which the cost of a misclassiﬁcation is $10 but the cost of having a human manually make the decison is only $3. We can formulate this as the following loss matrix:\n",
      "\n",
      "a. Suppose P (y = 1|x) is predicted to be 0.2. Which decision minimizes the expected loss? b. Now suppose P (y = 1|x)=0.4. Now which decision minimizes the expected loss? c. Show that in general, for this loss matrix, but for any posterior distribution, there will be two thresholds θ0 and θ1 such that the optimal decisionn is to predict 0 if p1 < θ0, reject if θ0 ≤ p1 ≤ θ1, and predict 1 if p1 > θ1 (where p1 = p(y = 1|x)). What are these thresholds?\n",
      "\n",
      "Exercise 5.5 Newsvendor problem Consider the following classic problem in decision theory/ economics. Suppose you are trying to decide how much quantity Q of some product (e.g., newspapers) to buy to maximize your proﬁts. The optimal amount will depend on how much demand D you think there is for your product, as well as its cost to you C and its selling price P . Suppose D is unknown but has pdf f (D) and cdf F (D). We can evaluate the expected proﬁt by considering two cases: if D > Q, then we sell all Q items, and make proﬁt π = (P − C)Q; but if D < Q, we only sell D items, at proﬁt (P − C)D, but have wasted C(Q − D) on the unsold items. So the expected proﬁt if we buy quantity Q is\n",
      "\n",
      "(cid:5) ∞\n",
      "\n",
      "(cid:5) Q\n",
      "\n",
      "(cid:5) Q\n",
      "\n",
      "Eπ(Q) =\n",
      "\n",
      "Q\n",
      "\n",
      "(P − C)Qf (D)dD +\n",
      "\n",
      "0\n",
      "\n",
      "(P − C)Df (D) −\n",
      "\n",
      "0\n",
      "\n",
      "C(Q − D)f (D)dD\n",
      "\n",
      "(5.123)\n",
      "\n",
      "Simplify this expression, and then take derivatives wrt Q to show that the optimal quantity Q∗ (which maximizes the expected proﬁt) satisﬁes\n",
      "\n",
      "F (Q∗) =\n",
      "\n",
      "P − C P\n",
      "\n",
      "(5.124)\n",
      "\n",
      "Exercise 5.6 Bayes factors and ROC curves Let B = p(D|H1)/p(D|H0) be the bayes factor in favor of model 1. Suppose we plot two ROC curves, one computed by thresholding B, and the other computed by thresholding p(H1|D). Will they be the same or different? Explain why.\n",
      "\n",
      "Exercise 5.7 Bayes model averaging helps predictive accuracy Let Δ be a quantity that we want to predict, let D be the observed data and M be a ﬁnite set of models. Suppose our action is to provide a probabilistic prediction p(), and the loss function is L(Δ, p()) =\n",
      "\n",
      "188\n",
      "\n",
      "Chapter 5. Bayesian statistics\n",
      "\n",
      "− log p(Δ). We can either perform Bayes model averaging and predict using\n",
      "\n",
      "pBM A(Δ) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(Δ|m, D)p(m|D)\n",
      "\n",
      "(5.125)\n",
      "\n",
      "m∈M\n",
      "\n",
      "or we could predict using any single model (a plugin approximation)\n",
      "\n",
      "pm(Δ) = p(Δ|m, D)\n",
      "\n",
      "(5.126)\n",
      "\n",
      "Show that, for all models m ∈ M, the posterior expected loss using BMA is lower, i.e.,\n",
      "\n",
      "E\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "L(Δ, pBM A)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "≤ E [L(Δ, pm)]\n",
      "\n",
      "(5.127)\n",
      "\n",
      "where the expectation over Δ is with respect to\n",
      "\n",
      "p(Δ|D) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(Δ|m, D)p(m|D)\n",
      "\n",
      "(5.128)\n",
      "\n",
      "m∈M\n",
      "\n",
      "Hint: use the non-negativity of the KL divergence.\n",
      "\n",
      "Exercise 5.8 MLE and model selection for a 2d discrete distribution (Source: Jaakkola.) Let x ∈ {0, 1} denote the result of a coin toss (x = 0 for tails, x = 1 for heads). The coin is potentially biased, so that heads occurs with probability θ1. Suppose that someone else observes the coin ﬂip and reports to you the outcome, y. But this person is unreliable and only reports the result correctly with probability θ2; i.e., p(y|x, θ2) is given by\n",
      "\n",
      "x = 0 x = 1\n",
      "\n",
      "y = 0 θ2 1 − θ2\n",
      "\n",
      "y = 1 1 − θ2 θ2\n",
      "\n",
      "Assume that θ2 is independent of x and θ1.\n",
      "\n",
      "a. Write down the joint probability distribution p(x, y|θ) as a 2 × 2 table, in terms of θ = (θ1, θ2). b. Suppose have the following dataset: x = (1, 1, 0, 1, 1, 0, 0), y = (1, 0, 0, 0, 1, 0, 1). What are the\n",
      "\n",
      "MLEs for θ1 and θ2? Justify your answer. Hint: note that the likelihood function factorizes,\n",
      "\n",
      "p(x, y|θ) =p (y|x, θ2)p(x|θ1)\n",
      "\n",
      "(5.129)\n",
      "\n",
      "What is p(D|ˆθ, M2) where M2 denotes this 2-parameter model? fractional form if you wish.)\n",
      "\n",
      "(You may leave your answer in\n",
      "\n",
      "c. Now consider a model with 4 parameters, θ = (θ0,0, θ0,1, θ1,0, θ1,1), representing p(x, y|θ) =θ x,y. (Only 3 of these parameters are free to vary, since they must sum to one.) What is the MLE of θ? What is p(D|ˆθ, M4) where M4 denotes this 4-parameter model?\n",
      "\n",
      "d. Suppose we are not sure which model is correct. We compute the leave-one-out cross validated log\n",
      "\n",
      "likelihood of the 2-parameter model and the 4-parameter model as follows:\n",
      "\n",
      "L(m) =\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "log p(xi, yi|m, ˆθ(D−i))\n",
      "\n",
      "(5.130)\n",
      "\n",
      "i=1\n",
      "\n",
      "and ˆθ(D−i)) denotes the MLE computed on D excluding row i. Which model will CV pick and why? Hint: notice how the table of counts changes when you omit each training case one at a time.\n",
      "\n",
      "5.7. Bayesian decision theory\n",
      "\n",
      "189\n",
      "\n",
      "e. Recall that an alternative to CV is to use the BIC score, deﬁned as\n",
      "\n",
      "BIC(M, D) (cid:2) log p(D|ˆθM LE) − dof(M )\n",
      "\n",
      "2\n",
      "\n",
      "log N\n",
      "\n",
      "(5.131)\n",
      "\n",
      "where dof(M ) is the number of free parameters in the model, Compute the BIC scores for both models (use log base e). Which model does BIC prefer?\n",
      "\n",
      "Exercise 5.9 Posterior median is optimal estimate under L1 loss Prove that the posterior median is optimal estimate under L1 loss.\n",
      "\n",
      "Exercise 5.10 Decision rule for trading off FPs and FNs If LF N = cLF P , show that we should pick ˆy = 1 iff p(y = 1|x)/p(y = 0|x) > τ , where τ = c/(1 + c)\n",
      "\n",
      "6 Frequentist statistics\n",
      "\n",
      "6.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "The approach to statistical inference that we described in Chapter 5 is known as Bayesian statistics. Perhaps surprisingly, this is considered controversial by some people, whereas the ap- plication of Bayes rule to non-statistical problems — such as medical diagnosis (Section 2.2.3.1), spam ﬁltering (Section 3.4.4.1), or airplane tracking (Section 18.2.1) — is not controversial. The reason for the objection has to do with a misguided distinction between parameters of a statis- tical model and other kinds of unknown quantities.1\n",
      "\n",
      "Attempts have been made to devise approaches to statistical inference that avoid treating parameters like random variables, and which thus avoid the use of priors and Bayes rule. Such approaches are known as frequentist statistics, classical statistics or orthodox statistics. Instead of being based on the posterior distribution, they are based on the concept of a sampling distribution. This is the distribution that an estimator has when applied to multiple data sets sampled from the true but unknown distribution; see Section 6.2 for details. It is this notion of variation across repeated trials that forms the basis for modeling uncertainty used by the frequentist approach.\n",
      "\n",
      "By contrast, in the Bayesian approach, we only ever condition on the actually observed data; there is no notion of repeated trials. This allows the Bayesian to compute the probability of one-off events, as we discussed in Section 2.1. Perhaps more importantly, the Bayesian approach avoids certain paradoxes that plague the frequentist approach (see Section 6.6). Nevertheless, it is important to be familiar with frequentist statistics (especially Section 6.5), since it is widely used in machine learning.\n",
      "\n",
      "6.2\n",
      "\n",
      "Sampling distribution of an estimator\n",
      "\n",
      "In frequentist statistics, a parameter estimate ˆθ is computed by applying an estimator δ to some data D, so ˆθ = δ(D). The parameter is viewed as ﬁxed and the data as random, which is the exact opposite of the Bayesian approach. The uncertainty in the parameter estimate can be measured by computing the sampling distribution of the estimator. To understand this\n",
      "\n",
      "1. Parameters are sometimes considered to represent true (but unknown) physical quantities, which are therefore not random. However, we have seen that it is perfectly reasonable to use a probability distribution to represent one’s uncertainty about an unknown constant.\n",
      "\n",
      "192\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "Boot: true = 0.70, n=10, mle = 0.90, se = 0.001\n",
      "\n",
      "Boot: true = 0.70, n=100, mle = 0.70, se = 0.000\n",
      "\n",
      "4000\n",
      "\n",
      "3500\n",
      "\n",
      "3500\n",
      "\n",
      "3000\n",
      "\n",
      "3000\n",
      "\n",
      "2500\n",
      "\n",
      "2500\n",
      "\n",
      "2000\n",
      "\n",
      "2000\n",
      "\n",
      "1500\n",
      "\n",
      "1500\n",
      "\n",
      "1000\n",
      "\n",
      "1000\n",
      "\n",
      "500\n",
      "\n",
      "500\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 6.1 A bootstrap approximation to the sampling distribution of ˆθ for a Bernoulli distribution. We use B = 10, 000 bootstrap samples. The N datacases were generated from Ber(θ = 0.7). (a) MLE with N = 10. (b) MLE with N = 100. Figure generated by bootstrapDemoBer.\n",
      "\n",
      "concept, imagine sampling many different data sets D(s) from some true model, p(·|θ∗), i.e., let D(s) = {x(s) i ∼ p(·|θ∗), and θ∗ is the true parameter. Here s = 1 :S indexes the sampled data set, and N is the size of each such dataset. Now apply the estimator ˆθ(·) to each D(s) to get a set of estimates, {ˆθ(D(s))}. As we let S → ∞, the distribution induced on ˆθ(·) is the sampling distribution of the estimator. We will discuss various ways to use the sampling distribution in later sections. But ﬁrst we sketch two approaches for computing the sampling distribution itself.\n",
      "\n",
      "i }N\n",
      "\n",
      "i=1, where xs\n",
      "\n",
      "6.2.1\n",
      "\n",
      "Bootstrap\n",
      "\n",
      "The bootstrap is a simple Monte Carlo technique to approximate the sampling distribution. This is particularly useful in cases where the estimator is a complex function of the true parameters. The idea is simple. If we knew the true parameters θ∗, we could generate many (say S) fake i ∼ p(·|θ∗), for s = 1 : S, i = 1 : N . datasets, each of size N , from the true distribution, xs We could then compute our estimator from each sample, ˆθs = f (xs 1:N ) and use the empirical distribution of the resulting samples as our estimate of the sampling distribution. Since θ is unknown, the idea of the parametric bootstrap is to generate the samples using ˆθ(D) instead. An alternative, called the non-parametric bootstrap, is to sample the xs i (with replacement) from the original data D, and then compute the induced distribution as before. Some methods for speeding up the bootstrap when applied to massive data sets are discussed in (Kleiner et al. 2011).\n",
      "\n",
      "Figure 6.1 shows an example where we compute the sampling distribution of the MLE for a Bernoulli using the parametric bootstrap. (Results using the non-parametric bootstrap are essentially the same.) We see that the sampling distribution is asymmetric, and therefore quite far from Gaussian, when N = 10; when N = 100, the distribution looks more Gaussian, as theory suggests (see below).\n",
      "\n",
      "A natural question is: what is the connection between the parameter estimates ˆθs = ˆθ(xs 1:N ) computed by the bootstrap and parameter values sampled from the posterior, θs ∼ p(·|D)?\n",
      "\n",
      "6.2. Sampling distribution of an estimator\n",
      "\n",
      "193\n",
      "\n",
      "Conceptually they are quite different. But in the common case that that the prior is not very strong, they can be quite similar. For example, Figure 6.1(c-d) shows an example where we compute the posterior using a uniform Beta(1,1) prior, and then sample from it. We see that the posterior and the sampling distribution are quite similar. So one can think of the bootstrap distribution as a “poor man’s” posterior; see (Hastie et al. 2001, p235) for details.\n",
      "\n",
      "However, perhaps surprisingly, bootstrap can be slower than posterior sampling. The reason is that the bootstrap has to ﬁt the model S times, whereas in posterior sampling, we usually only ﬁt the model once (to ﬁnd a local mode), and then perform local exploration around the mode. Such local exploration is usually much faster than ﬁtting a model from scratch.\n",
      "\n",
      "6.2.2\n",
      "\n",
      "Large sample theory for the MLE *\n",
      "\n",
      "In some cases, the sampling distribution for some estimators can be computed analytically. In particular, it can be shown that, under certain conditions, as the sample size tends to inﬁnity, Informally, the requirement for this the sampling distribution of the MLE becomes Gaussian. result to hold is that each parameter in the model gets to “see” an inﬁnite amount of data, and that the model be identiﬁable. Unfortunately this excludes many of the models of interest to machine learning. Nevertheless, let us assume we are in a simple setting where the theorem holds.\n",
      "\n",
      "The center of the Gaussian will be the MLE ˆθ. But what about the variance of this Gaussian? Intuitively the variance of the estimator will be (inversely) related to the amount of curvature of the likelihood surface at its peak. If the curvature is large, the peak will be “sharp”, and the variance low; in this case, the estimate is “well determined”. By contrast, if the curvature is small, the peak will be nearly “ﬂat”, so the variance is high.\n",
      "\n",
      "likelihood evaluated at some point ˆθ:\n",
      "\n",
      "Let us now formalize this intuition. Deﬁne the score function as the gradient of the log\n",
      "\n",
      "s(ˆθ) (cid:2) ∇ log p(D|θ)|ˆθ\n",
      "\n",
      "(6.1)\n",
      "\n",
      "Deﬁne the observed information matrix as the gradient of the negative score function, or equivalently, the Hessian of the NLL: J(ˆθ(D)) (cid:2) −∇s(ˆθ) = −∇2\n",
      "\n",
      "θ log p(D|θ)|ˆθ\n",
      "\n",
      "(6.2)\n",
      "\n",
      "In 1D, this becomes d dθ2 log p(D|θ)|ˆθ\n",
      "\n",
      "J(ˆθ(D)) = −\n",
      "\n",
      "(6.3)\n",
      "\n",
      "This is just a measure of curvature of the log-likelihood function at ˆθ.\n",
      "\n",
      "Since we are studying the sampling distribution, D = (x1, . . . , xN ) is a set of random variables. The Fisher information matrix is deﬁned to be the expected value of the observed information matrix:2 IN (ˆθ|θ∗) (cid:2) Eθ∗\n",
      "\n",
      ")\n",
      "\n",
      "J(ˆθ|D)\n",
      "\n",
      "\n",
      "\n",
      "(6.4)\n",
      "\n",
      "2. This is not the usual deﬁnition, but is equivalent to it under standard assumptions. More precisely, the standard deﬁnition is as follows (we just give the scalar case to simplify notation): I(ˆθ|θ∗) (cid:2) varθ∗ , that\n",
      "\n",
      "is, the variance of the score function.\n",
      "\n",
      "If ˆθ is the MLE, it is easy to see that Eθ∗\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:4) d dθ log p(X|θ)| ˆθ\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:4) d dθ log p(X|θ)| ˆθ\n",
      "\n",
      "= 0 (since\n",
      "\n",
      "194\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "i=1 f (xi)p(xi|θ∗) is the expected value of the function f when where Eθ∗ [f (D)] (cid:2) 1 N applied to data sampled from θ∗ , representing the “true parameter” that generated the data, is assumed known, so we just write IN (ˆθ) (cid:2) IN (ˆθ|θ∗) for short. Furthermore, it is easy to see that IN (ˆθ) =N I1(ˆθ), because the log-likelihood for a sample of size N is just N times “steeper” than the log-likelihood for a sample of size 1. So we can drop the 1 subscript and just write I(ˆθ) (cid:2) I1(ˆθ). This is the notation that is usually used.\n",
      "\n",
      "Now let ˆθ (cid:2) ˆθmle(D) be the MLE, where D ∼ θ∗\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      ". Often θ∗\n",
      "\n",
      ". It can be shown that\n",
      "\n",
      "ˆθ → N (θ∗, IN (θ∗)−1)\n",
      "\n",
      "(6.5)\n",
      "\n",
      "as N → ∞ (see e.g., (Rice 1995, p265) for a proof). We say that the sampling distribution of the MLE is asymptotically normal.\n",
      "\n",
      "in the MLE? Unfortunately, θ∗ distribution. However, we can approximate the sampling distribution by replacing θ∗ Consequently, the approximate standard errors of ˆθk are given by\n",
      "\n",
      "What about the variance of the MLE, which can be used as some measure of conﬁdence is unknown, so we can’t evaluate the variance of the sampling with ˆθ.\n",
      "\n",
      "ˆsek (cid:2) IN (ˆθ)\n",
      "\n",
      "− 1 2 kk\n",
      "\n",
      "(6.6)\n",
      "\n",
      "For example, from Equation 5.60 we know that the Fisher information for a binomial sampling\n",
      "\n",
      "model is\n",
      "\n",
      "I(θ) =\n",
      "\n",
      "1 θ(1 − θ)\n",
      "\n",
      "(6.7)\n",
      "\n",
      "So the approximate standard error of the MLE is\n",
      "\n",
      "where ˆθ = 1 N under a uniform prior.\n",
      "\n",
      "ˆse =\n",
      "\n",
      "11\n",
      "\n",
      "IN (ˆθ) (cid:4)\n",
      "\n",
      "i Xi. Compare this to Equation 3.27, which is the posterior standard deviation\n",
      "\n",
      "=\n",
      "\n",
      "11\n",
      "\n",
      "N I(ˆθ)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "ˆθ(1 − ˆθ) N\n",
      "\n",
      "(cid:14) 1\n",
      "\n",
      "2\n",
      "\n",
      "(6.8)\n",
      "\n",
      "6.3\n",
      "\n",
      "Frequentist decision theory\n",
      "\n",
      "In frequentist or classical decision theory, there is a loss function and a likelihood, but there is no prior and hence no posterior or posterior expected loss. Thus there is no automatic way of deriving an optimal estimator, unlike the Bayesian case. Instead, in the frequentist approach, we are free to choose any estimator or decision procedure δ : X → A we want.3\n",
      "\n",
      "the gradient must be zero at a maximum), so the variance reduces to the expected square of the score function: I(ˆθ|θ∗) =E θ∗ = (cid:3)\n",
      "\n",
      "−Eθ∗ is a much more intuitive quantity than the variance of the score. 3. In practice, the frequentist approach is usually only applied to one-shot statistical decision problems — such as classiﬁcation, regression and parameter estimation — since its non-constructive nature makes it difficult to apply to sequential decision problems, which adapt to data online.\n",
      "\n",
      "d2 dθ2 log p(X|θ)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "( d dθ log p(X|θ))2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      ", so now the Fisher information reduces to the expected second derivative of the NLL, which\n",
      "\n",
      "(cid:4)\n",
      "\n",
      ".\n",
      "\n",
      "It can be shown (e.g.,\n",
      "\n",
      "(Rice 1995, p263)) that Eθ∗\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "( d dθ log p(X|θ))2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "6.3. Frequentist decision theory\n",
      "\n",
      "195\n",
      "\n",
      "Having chosen an estimator, we deﬁne its expected loss or risk as follows:\n",
      "\n",
      "R(θ∗, δ) (cid:2) Ep( ˜D|θ∗)\n",
      "\n",
      ")\n",
      "\n",
      "L(θ∗, δ( ˜D))\n",
      "\n",
      "\n",
      "\n",
      "=\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "L(θ∗, δ( ˜D))p( ˜D|θ∗)d ˜D\n",
      "\n",
      "(6.9)\n",
      "\n",
      "where ˜D is data sampled from “nature’s distribution”, which is represented by parameter θ∗. In other words, the expectation is wrt the sampling distribution of the estimator. Compare this to the Bayesian posterior expected loss:\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ρ(a|D, π) (cid:2) Ep(θ|D,π) [L(θ, a)] =\n",
      "\n",
      "Θ\n",
      "\n",
      "L(θ, a)p(θ|D, π)dθ\n",
      "\n",
      "(6.10)\n",
      "\n",
      "We see that the Bayesian approach averages over θ (which is unknown) and conditions on D (which is known), whereas the frequentist approach averages over ˜D (thus ignoring the observed data), and conditions on θ∗ (which is unknown).\n",
      "\n",
      "Not only is the frequentist deﬁnition unnatural, it cannot even be computed, because θ∗ is unknown. Consequently, we cannot compare different estimators in terms of their frequentist risk. We discuss various solutions to this below.\n",
      "\n",
      "6.3.1\n",
      "\n",
      "Bayes risk How do we choose amongst estimators? We need some way to convert R(θ∗, δ) into a single measure of quality, R(δ), which does not depend on knowing θ∗ . One approach is to put a prior on θ∗, and then to deﬁne Bayes risk or integrated risk of an estimator as follows:\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "RB(δ) (cid:2) Ep(θ∗) [R(θ∗, δ)] =\n",
      "\n",
      "R(θ∗, δ)p(θ∗)dθ∗\n",
      "\n",
      "(6.11)\n",
      "\n",
      "A Bayes estimator or Bayes decision rule is one which minimizes the expected risk:\n",
      "\n",
      "δB (cid:2) argmin\n",
      "\n",
      "δ\n",
      "\n",
      "RB(δ)\n",
      "\n",
      "(6.12)\n",
      "\n",
      "Note that the integrated risk is also called the preposterior risk, since it is before we have seen the data. Minimizing this can be useful for experiment design.\n",
      "\n",
      "We will now prove a very important theorem, that connects the Bayesian and frequentist\n",
      "\n",
      "approaches to decision theory.\n",
      "\n",
      "Theorem 6.3.1. A Bayes estimator can be obtained by minimizing the posterior expected loss for each x.\n",
      "\n",
      "Proof. By switching the order of integration, we have (cid:17)\n",
      "\n",
      "RB(δ) =\n",
      "\n",
      "(cid:11) (cid:16)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "L(y, δ(x))p(x, y|θ∗)\n",
      "\n",
      "p(θ∗)dθ∗\n",
      "\n",
      "(6.13)\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "x (cid:6)\n",
      "\n",
      "x (cid:6)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "y\n",
      "\n",
      "L(y, δ(x))p(x, y,θ ∗)dθ∗\n",
      "\n",
      "y (cid:16) (cid:6)\n",
      "\n",
      "Θ\n",
      "\n",
      "L(y, δ(x))p(y|x)dy\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "p(x)\n",
      "\n",
      "(6.14)\n",
      "\n",
      "(6.15)\n",
      "\n",
      "=\n",
      "\n",
      "x (cid:6)\n",
      "\n",
      "y\n",
      "\n",
      "ρ(δ(x)|x) p(x)\n",
      "\n",
      "(6.16)\n",
      "\n",
      "x\n",
      "\n",
      "196\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "R\n",
      "\n",
      "R(θ, δ1)\n",
      "\n",
      "R(θ, δ2)\n",
      "\n",
      "θ\n",
      "\n",
      "Figure 6.2 Risk functions for two decision procedures, δ1 and δ2. Since δ1 has lower worst case risk, it is the minimax estimator, even though δ2 has lower risk for most values of θ. Thus minimax estimators are overly conservative.\n",
      "\n",
      "To minimize the overall expectation, we just minimize the term inside for each x, so our decision rule is to pick\n",
      "\n",
      "δB(x) = argmin\n",
      "\n",
      "a∈A\n",
      "\n",
      "ρ(a|x)\n",
      "\n",
      "(6.17)\n",
      "\n",
      "Hence we see that the picking the optimal action on a case-by-case basis (as in the Bayesian approach) is optimal on average (as in the frequentist approach). In other words, the Bayesian approach provides a good way of achieving frequentist goals. In fact, one can go further and prove the following.\n",
      "\n",
      "Theorem 6.3.2 (Wald, 1950). Every admissable decision rule is a Bayes decision rule with respect to some, possibly improper, prior distribution.\n",
      "\n",
      "This theorem shows that the best way to minimize frequentist risk is to be Bayesian! See\n",
      "\n",
      "(Bernardo and Smith 1994, p448) for further discussion of this point.\n",
      "\n",
      "6.3.2 Minimax risk\n",
      "\n",
      "Obviously some frequentists dislike using Bayes risk since it requires the choice of a prior (al- though this is only in the evaluation of the estimator, not necessarily as part of its construction). An alternative approach is as follows. Deﬁne the maximum risk of an estimator as\n",
      "\n",
      "Rmax(δ) (cid:2) max θ∗\n",
      "\n",
      "R(θ∗, δ)\n",
      "\n",
      "(6.18)\n",
      "\n",
      "A minimax rule is one which minimizes the maximum risk:\n",
      "\n",
      "δMM (cid:2) argmin\n",
      "\n",
      "δ\n",
      "\n",
      "Rmax(δ)\n",
      "\n",
      "(6.19)\n",
      "\n",
      "6.3. Frequentist decision theory\n",
      "\n",
      "197\n",
      "\n",
      "For example, in Figure 6.2, we see that δ1 has lower worst-case risk than δ2, ranging over all possible values of θ∗, so it is the minimax estimator (see Section 6.3.3.1 for an explanation of how to compute a risk function for an actual model).\n",
      "\n",
      "Minimax estimators have a certain appeal. However, computing them can be hard. And furthermore, they are very pessimistic. In fact, one can show that all minimax estimators are equivalent to Bayes estimators under a least favorable prior. In most statistical situations (excluding game theoretic ones), assuming nature is an adversary is not a reasonable assumption.\n",
      "\n",
      "6.3.3\n",
      "\n",
      "Admissible estimators\n",
      "\n",
      "The basic problem with frequentist decision theory is that it relies on knowing the true distri- bution p(·|θ∗) in order to evaluate the risk. However, It might be the case that some estimators are worse than others regardless of the value of θ∗. In particular, if R(θ, δ1) ≤ R(θ, δ2) for all θ ∈ Θ, then we say that δ1 dominates δ2. The domination is said to be strict if the inequality is strict for some θ. An estimator is said to be admissible if it is not strictly dominated by any other estimator.\n",
      "\n",
      "6.3.3.1\n",
      "\n",
      "Example\n",
      "\n",
      "Let us give an example, based on (Bernardo and Smith 1994). Consider the problem of estimating the mean of a Gaussian. We assume the data is sampled from xi ∼ N (θ∗, σ2 = 1) and use quadratic loss, L(θ, ˆθ) = (θ − ˆθ)2. The corresponding risk function is the MSE. Some possible decision rules or estimators ˆθ(x) = δ(x) are as follows:\n",
      "\n",
      "\n",
      "\n",
      "δ1(x) = x, the sample mean\n",
      "\n",
      "\n",
      "\n",
      "δ2(x) = ˜x, the sample median\n",
      "\n",
      "\n",
      "\n",
      "δ3(x) = θ0, a ﬁxed value\n",
      "\n",
      "\n",
      "\n",
      "δκ(x), the posterior mean under a N (θ|θ0, σ2/κ) prior:\n",
      "\n",
      "δκ(x) =\n",
      "\n",
      "N N + κ\n",
      "\n",
      "x +\n",
      "\n",
      "κ N + κ\n",
      "\n",
      "θ0 = wx + (1− w)θ0\n",
      "\n",
      "(6.20)\n",
      "\n",
      "For δκ, we consider a weak prior, κ = 1, and a stronger prior, κ = 5. The prior mean is θ0, some ﬁxed value. We assume σ2 is known. (Thus δ3(x) is the same as δκ(x) with an inﬁnitely strong prior, κ = ∞.)\n",
      "\n",
      "we know the true parameter θ∗.) into squared bias plus variance: )\n",
      "\n",
      "Let us now derive the risk functions analytically.\n",
      "\n",
      "M SE(ˆθ(·)|θ∗) = var\n",
      "\n",
      "ˆθ\n",
      "\n",
      "+ bias\n",
      "\n",
      "(We can do this since in this toy example, In Section 6.4.4, we show that the MSE can be decomposed\n",
      "\n",
      "2(ˆθ)\n",
      "\n",
      "(6.21)\n",
      "\n",
      "The sample mean is unbiased, so its risk is\n",
      "\n",
      "M SE(δ1|θ∗) = var [x] =\n",
      "\n",
      "σ2 N\n",
      "\n",
      "(6.22)\n",
      "\n",
      "198\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "risk functions for n=5\n",
      "\n",
      "risk functions for n=20\n",
      "\n",
      "0.5\n",
      "\n",
      "0.18\n",
      "\n",
      "0.45\n",
      "\n",
      "0.4\n",
      "\n",
      "0.35\n",
      "\n",
      "mle median fixed postmean1 postmean5\n",
      "\n",
      "0.16\n",
      "\n",
      "0.14\n",
      "\n",
      "0.12\n",
      "\n",
      "mle median fixed postmean1 postmean5\n",
      "\n",
      "0.3\n",
      "\n",
      ") δ ,\n",
      "\n",
      "θ ( R\n",
      "\n",
      "\n",
      "\n",
      "0.25\n",
      "\n",
      ") δ ,\n",
      "\n",
      "θ ( R\n",
      "\n",
      "\n",
      "\n",
      "0.1\n",
      "\n",
      "0.08\n",
      "\n",
      "0.2\n",
      "\n",
      "0.15\n",
      "\n",
      "0.06\n",
      "\n",
      "0.1\n",
      "\n",
      "0.04\n",
      "\n",
      "0.05\n",
      "\n",
      "0.02\n",
      "\n",
      "0 −2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0 θ *\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0 θ *\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 6.3 Risk functions for estimating the mean of a Gaussian using data sampled N (θ∗, σ2 = 1). The solid dark blue horizontal line is the MLE, the solid light blue curved line is the posterior mean when κ = 5. Left: N = 5 samples. Right: N = 20 samples. Based on Figure B.1 of (Bernardo and Smith 1994). Figure generated by riskFnGauss.\n",
      "\n",
      "The sample median is also unbiased. One can show that the variance is approximately π/(2N ), so\n",
      "\n",
      "M SE(δ2|θ∗) =\n",
      "\n",
      "π 2N\n",
      "\n",
      "(6.23)\n",
      "\n",
      "For δ3(x) = θ0, the variance is zero, so\n",
      "\n",
      "M SE(δ3|θ∗) = (θ∗ − θ0)2\n",
      "\n",
      "(6.24)\n",
      "\n",
      "Finally, for the posterior mean, we have\n",
      "\n",
      "M SE(δκ|θ∗) =E\n",
      "\n",
      "= E = w2 σ2 N 1 (N + κ)2\n",
      "\n",
      "=\n",
      "\n",
      ")\n",
      "\n",
      "(wx + (1− w)θ0 − θ∗) )\n",
      "\n",
      "(w(x − θ∗) + (1− w)(θ0 − θ∗))\n",
      "\n",
      "+ (1− w)2(θ0 − θ∗)2\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "N σ2 + κ2(θ0 − θ∗)2\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "(6.25)\n",
      "\n",
      "(6.26)\n",
      "\n",
      "(6.27)\n",
      "\n",
      "(6.28)\n",
      "\n",
      "These functions are plotted in Figure 6.3 for N ∈ {5, 20}. We see that in general, the best If θ∗ is very close to θ0, then δ3 estimator depends on the value of θ∗, which is unknown. If θ∗ is within some reasonable range around θ0, then the (which just predicts θ0) is best. posterior mean, which combines the prior guess of θ0 with the actual data, is best. If θ∗ is far from θ0, the MLE is best. None of this should be suprising: a small amount of shrinkage (using the posterior mean with a weak prior) is usually desirable, assuming our prior mean is sensible. What is more surprising is that the risk of decision rule δ2 (sample median) is always higher than that of δ1 (sample mean) for every value of θ∗. Consequently the sample median is an\n",
      "\n",
      "6.3. Frequentist decision theory\n",
      "\n",
      "199\n",
      "\n",
      "inadmissible estimator for this particular problem (where the data is assumed to come from a Gaussian).\n",
      "\n",
      "In practice, the sample median is often better than the sample mean, because it is more robust to outliers. One can show (Minka 2000d) that the median is the Bayes estimator (under squared loss) if we assume the data comes from a Laplace distribution, which has heavier tails than a Gaussian. More generally, we can construct robust estimators by using ﬂexible models of our data, such as mixture models or non-parametric density estimators (Section 14.7.2), and then computing the posterior mean or median.\n",
      "\n",
      "6.3.3.2\n",
      "\n",
      "Stein’s paradox *\n",
      "\n",
      "Suppose we have N iid random variables Xi ∼ N (θi, 1), and we want to estimate the θi. The obvious estimator is the MLE, which in this case sets ˆθi = xi. It turns out that this is an inadmissible estimator under quadratic loss, when N ≥ 4.\n",
      "\n",
      "To show this, it suffices to construct an estimator that is better. The James-Stein estimator is\n",
      "\n",
      "one such estimator, and is deﬁned as follows:\n",
      "\n",
      "where x = 1 N θi towards the overall mean. Section 5.6.2.)\n",
      "\n",
      "ˆθi = ˆBx + (1− ˆB)xi = x + (1− ˆB)(xi − x)\n",
      "\n",
      "(cid:4)N\n",
      "\n",
      "i=1 xi and 0 < B <1 is some tuning constant. This estimate “shrinks” the (We derive this estimator using an empirical Bayes approach in\n",
      "\n",
      "(6.29)\n",
      "\n",
      "It can be shown that this shrinkage estimator has lower frequentist risk (MSE) than the MLE (sample mean) for N ≥ 4. This is known as Stein’s paradox. The reason it is called a paradox is illustrated by the following example. Suppose θi is the “true” IQ of student i and Xi is his test score. Why should my estimate of θi depend on the global mean x, and hence on some other student’s scores? One can create even more paradoxical examples by making the different dimensions be qualitatively different, e.g., θ1 is my IQ, θ2 is the average rainfall in Vancouver, etc.\n",
      "\n",
      "The solution to the paradox is the following. If your goal is to estimate just θi, you cannot do better than using xi, but if the goal is to estimate the whole vector θ, and you use squared error as your loss function, then shrinkage helps. To see this, suppose we want to estimate ||θ||2 2 from a single sample x ∼ N (θ, I). A simple estimate is ||x||2 2, but this will overestimate the result, since\n",
      "\n",
      "E\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:17)x(cid:17)2 2\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "= E\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "x2 i\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "=\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "1 + θ2 i\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "= N + ||θ||2 2\n",
      "\n",
      "(6.30)\n",
      "\n",
      "i\n",
      "\n",
      "i=1\n",
      "\n",
      "Consequently we can reduce our risk by pooling information, even from unrelated sources, and shrinking towards the overall mean. In Section 5.6.2, we give a Bayesian explanation for this. See also (Efron and Morris 1975).\n",
      "\n",
      "6.3.3.3\n",
      "\n",
      "Admissibility is not enough\n",
      "\n",
      "It seems clear that we can restrict our search for good estimators to the class of admissible estimators. But in fact it is easy to construct admissible estimators, as we show in the following example.\n",
      "\n",
      "200\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "Theorem 6.3.3. Let X ∼ N (θ, 1), and consider estimating θ under squared loss. Let δ1(x) =θ 0, a constant independent of the data. This is an admissible estimator. Proof. Suppose not. Then there is some other estimator δ2 with smaller risk, so R(θ∗, δ2) ≤ R(θ∗, δ1), where the inequality must be strict for some θ∗. Suppose the true parameter is θ∗ = θ0. Then R(θ∗, δ1) = 0, and (cid:11)\n",
      "\n",
      "R(θ∗, δ2) =\n",
      "\n",
      "(δ2(x) − θ0)2p(x|θ0)dx\n",
      "\n",
      "(6.31)\n",
      "\n",
      "Since 0 ≤ R(θ∗, δ2) ≤ R(θ∗, δ1) for all θ∗, and R(θ0, δ1) = 0, we have R(θ0, δ2) = 0 and hence δ2(x) =θ 0 = δ1(x). Thus the only way δ2 can avoid having higher risk than δ1 at some speciﬁc point θ0 is by being equal to δ1. Hence there is no other estimator δ2 with strictly lower risk, so δ2 is admissible.\n",
      "\n",
      "6.4 Desirable properties of estimators\n",
      "\n",
      "Since frequentist decision theory does not provide an automatic way to choose the best estimator, we need to come up with other heuristics for choosing amongst them. In this section, we discuss some properties we would like estimators to have. Unfortunately, we will see that we cannot achieve all of these properties at the same time.\n",
      "\n",
      "6.4.1\n",
      "\n",
      "Consistent estimators\n",
      "\n",
      "An estimator is said to be consistent if it eventually recovers the true parameters that generated the data as the sample size goes to inﬁnity, i.e., ˆθ(D) → θ∗ as |D| → ∞ (where the arrow denotes convergence in probability). Of course, this concept only makes sense if the data actually comes from the speciﬁed model with parameters θ∗, which is not usually the case with real data. Nevertheless, it can be a useful theoretical property.\n",
      "\n",
      "mizing likelihood is equivalent to minimizing KL distribution and p(·|ˆθ) is our estimate. We can achieve 0 KL divergence iff ˆθ = θ∗\n",
      "\n",
      "It can be shown that the MLE is a consistent estimator. The intuitive reason is that maxi- , where p(·|θ∗) is the true\n",
      "\n",
      "+\n",
      "\n",
      "p(·|θ∗)||p(·|ˆθ)\n",
      "\n",
      ",\n",
      "\n",
      ".4\n",
      "\n",
      "6.4.2\n",
      "\n",
      "Unbiased estimators\n",
      "\n",
      "The bias of an estimator is deﬁned as * ˆθ(D) − θ∗\n",
      "\n",
      "bias(ˆθ(·)) = Ep(D|θ∗)\n",
      "\n",
      ")\n",
      "\n",
      "(6.32)\n",
      "\n",
      "where θ∗ is the true parameter value. If the bias is zero, the estimator is called unbiased. This means the sampling distribution is centered on the true parameter. For example, the MLE for a Gaussian mean is unbiased:\n",
      "\n",
      "bias(ˆμ) = E [x] − μ = E\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "i=1\n",
      "\n",
      "xi\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "− μ =\n",
      "\n",
      "N μ N\n",
      "\n",
      "− μ = 0\n",
      "\n",
      "(6.33)\n",
      "\n",
      "4. If the model is unidentiﬁable, the MLE may select a set of parameters that is different from the true parameters but for which the induced distribution, p(·|ˆθ), is the same as the exact distribution. Such parameters are said to be likelihood equivalent.\n",
      "\n",
      "6.4. Desirable properties of estimators\n",
      "\n",
      "201\n",
      "\n",
      "However, the MLE for a Gaussian variance, ˆσ2, is not an unbiased estimator of σ2. In fact, one can show (Exercise 6.3) that\n",
      "\n",
      "E\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ˆσ2\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "=\n",
      "\n",
      "N − 1 N\n",
      "\n",
      "σ2\n",
      "\n",
      "(6.34)\n",
      "\n",
      "However, the following estimator\n",
      "\n",
      "ˆσ2 N −1 =\n",
      "\n",
      "N N − 1\n",
      "\n",
      "ˆσ2 =\n",
      "\n",
      "1 N − 1\n",
      "\n",
      "N(cid:6)\n",
      "\n",
      "(xi − x)2\n",
      "\n",
      "i=1\n",
      "\n",
      "(6.35)\n",
      "\n",
      "is an unbiased estimator, which we can easily prove as follows: (cid:3)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ˆσ2 N −1\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "= E\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "N N − 1\n",
      "\n",
      "ˆσ2\n",
      "\n",
      "=\n",
      "\n",
      "N N − 1\n",
      "\n",
      "N − 1 N\n",
      "\n",
      "σ2 = σ2\n",
      "\n",
      "(6.36)\n",
      "\n",
      "In Matlab, var(X) returns ˆσ2 N , the difference will be negligible.\n",
      "\n",
      "N −1, whereas var(X,1) returns ˆσ2 (the MLE). For large enough\n",
      "\n",
      "Although the MLE may sometimes be a biased estimator, one can show that asymptotically, it\n",
      "\n",
      "is always unbiased. (This is necessary for the MLE to be a consistent estimator.)\n",
      "\n",
      "Although being unbiased sounds like a desirable property, this is not always true. See Sec-\n",
      "\n",
      "tion 6.4.4 and (Lindley 1972) for discussion of this point.\n",
      "\n",
      "6.4.3 Minimum variance estimators\n",
      "\n",
      "It seems intuitively reasonable that we want our estimator to be unbiased (although we shall give some arguments against this claim below). However, being unbiased is not enough. For example, suppose we want to estimate the mean of a Gaussian from D = {x1, . . . , xN }. The estimator that just looks at the ﬁrst data point, ˆθ(D) =x 1, is an unbiased estimator, but will generally be further from θ∗ than the empirical mean x (which is also unbiased). So the variance of an estimator is also important.\n",
      "\n",
      "A natural question is: how long can the variance go? A famous result, called the Cramer- Rao lower bound, provides a lower bound on the variance of any unbiased estimator. More precisely,\n",
      "\n",
      "Theorem 6.4.1 (Cramer-Rao inequality). Let X1, . . . , Xn ∼ p(X|θ0) and ˆθ = ˆθ(x1, . . . , xn) be an unbiased estimator of θ0. Then, under various smoothness assumptions on p(X|θ0), we have\n",
      "\n",
      "var\n",
      "\n",
      ")\n",
      "\n",
      "ˆθ\n",
      "\n",
      "≥\n",
      "\n",
      "1 nI(θ0)\n",
      "\n",
      "(6.37)\n",
      "\n",
      "where I(θ0) is the Fisher information matrix (see Section 6.2.2).\n",
      "\n",
      "A proof can be found e.g., in (Rice 1995, p275). It can be shown that the MLE achieves the Cramer Rao lower bound, and hence has the smallest asymptotic variance of any unbiased estimator. Thus MLE is said to be asymptotically optimal.\n",
      "\n",
      "202\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "6.4.4\n",
      "\n",
      "The bias-variance tradeoff\n",
      "\n",
      "Although using an unbiased estimator seems like a good idea, this is not always the case. To see why, suppose we use quadratic loss. As we showed above, the corresponding risk is the MSE. We now derive a very useful decomposition of the MSE. (All expectations and variances are wrt the true distribution p(D|θ∗), but we drop the explicit conditioning for notational brevity.) Let ˆθ = ˆθ(D) denote the estimate, and θ = E denote the expected value of the estimate (as we vary D). Then we have\n",
      "\n",
      "E\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(ˆθ − θ∗)2\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "= E\n",
      "\n",
      "= E\n",
      "\n",
      "(cid:4)(cid:2)\n",
      "\n",
      "(cid:4)(cid:6)\n",
      "\n",
      "(ˆθ − θ) + (θ − θ∗) (cid:5)\n",
      "\n",
      "ˆθ − θ\n",
      "\n",
      "(cid:7)2\n",
      "\n",
      "+ 2(θ − θ∗)E\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:3) ˆθ\n",
      "\n",
      "(cid:3)2\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "ˆθ − θ\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "+ (θ − θ∗)2\n",
      "\n",
      "(6.38)\n",
      "\n",
      "(6.39)\n",
      "\n",
      "= E\n",
      "\n",
      "= var\n",
      "\n",
      "(cid:4)(cid:6)\n",
      "\n",
      "ˆθ − θ (cid:3) (cid:2) ˆθ\n",
      "\n",
      "+ bias\n",
      "\n",
      "(cid:7)2\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "2(ˆθ)\n",
      "\n",
      "+ (θ − θ∗)2\n",
      "\n",
      "(6.40)\n",
      "\n",
      "(6.41)\n",
      "\n",
      "In words,\n",
      "\n",
      "2 MSE = variance + bias\n",
      "\n",
      "(6.42)\n",
      "\n",
      "This is called the bias-variance tradeoff (see e.g., (Geman et al. 1992)). What it means is that it might be wise to use a biased estimator, so long as it reduces our variance, assuming our goal is to minimize squared error.\n",
      "\n",
      "6.4.4.1\n",
      "\n",
      "Example: estimating a Gaussian mean\n",
      "\n",
      "Let us give an example, based on (Hoff 2009, p79). Suppose we want to estimate the mean of a Gaussian from x = (x1, . . . , xN ). We assume the data is sampled from xi ∼ N (θ∗ = 1, σ2). An obvious estimate is the MLE. This has a bias of 0 and a variance of\n",
      "\n",
      "var [x|θ∗] =\n",
      "\n",
      "σ2 N\n",
      "\n",
      "(6.43)\n",
      "\n",
      "But we could also use a MAP estimate. In Section 4.6.1, we show that the MAP estimate under a Gaussian prior of the form N (θ0, σ2/κ0) is given by\n",
      "\n",
      "˜x (cid:2)\n",
      "\n",
      "N N + κ0\n",
      "\n",
      "x +\n",
      "\n",
      "κ0 N + κ0\n",
      "\n",
      "θ0 = wx + (1− w)θ0\n",
      "\n",
      "(6.44)\n",
      "\n",
      "where 0 ≤ w ≤ 1 controls how much we trust the MLE compared to our prior. (This is also the posterior mean, since the mean and mode of a Gaussian are the same.) The bias and variance are given by\n",
      "\n",
      "E [˜x] − θ∗ = wθ0 + (1− w)θ0 − θ∗ = (1 − w)(θ0 − θ∗)\n",
      "\n",
      "(6.45)\n",
      "\n",
      "var [˜x] =w 2 σ2 N\n",
      "\n",
      "(6.46)\n",
      "\n",
      "6.4. Desirable properties of estimators\n",
      "\n",
      "203\n",
      "\n",
      "1.5\n",
      "\n",
      "sampling distribution, truth = 1.0, prior = 0.0, n = 5\n",
      "\n",
      "postMean0 postMean1 postMean2 postMean3\n",
      "\n",
      "1.3\n",
      "\n",
      "1.2\n",
      "\n",
      "1.1\n",
      "\n",
      "MSE of postmean / MSE of MLE\n",
      "\n",
      "postMean0 postMean1 postMean2 postMean3\n",
      "\n",
      "1\n",
      "\n",
      "E S M e v i t\n",
      "\n",
      "a e r\n",
      "\n",
      "l\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.5\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0 −1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "2.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "sample size\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Left: Sampling distribution of the MAP estimate with different prior strengths κ0. Figure 6.4 (The MLE corresponds to κ0 = 0.) Right: MSE relative to that of the MLE versus sample size. Based on Figure 5.6 of (Hoff 2009). Figure generated by samplingDistGaussShrinkage.\n",
      "\n",
      "So although the MAP estimate is biased (assuming w < 1), it has lower variance.\n",
      "\n",
      "Let us assume that our prior is slightly misspeciﬁed, so we use θ0 = 0, whereas the truth is θ∗ = 1. In Figure 6.4(a), we see that the sampling distribution of the MAP estimate for κ0 > 0 is biased away from the truth, but has lower variance (is narrower) than that of the MLE.\n",
      "\n",
      "In Figure 6.4(b), we plot mse(˜x)/mse(x) vs N . We see that the MAP estimate has lower MSE than the MLE, especially for small sample size, for κ0 ∈ {1, 2}. The case κ0 = 0 corresponds to the MLE, and the case κ0 = 3 corresponds to a strong prior, which hurts performance because the prior mean is wrong. It is clearly important to “tune” the strength of the prior, a topic we discuss later.\n",
      "\n",
      "6.4.4.2\n",
      "\n",
      "Example: ridge regression\n",
      "\n",
      "Another important example of the bias variance tradeoff arises in ridge regression, which we discuss in Section 7.5. In brief, this corresponds to MAP estimation for linear regression under a Gaussian prior, p(w) = N (w|0, λ−1I) The zero-mean prior encourages the weights to be small, which reduces overﬁtting; the precision term, λ, controls the strength of this prior. Setting λ = 0 results in the MLE; using λ >0 results in a biased estimate. To illustrate the effect on the variance, consider a simple example. Figure 6.5 on the left plots each individual ﬁtted curve, and on the right plots the average ﬁtted curve. We see that as we increase the strength of the regularizer, the variance decreases, but the bias increases.\n",
      "\n",
      "6.4.4.3\n",
      "\n",
      "Bias-variance tradeoff for classiﬁcation\n",
      "\n",
      "If we use 0-1 loss instead of squared error, the above analysis breaks down, since the frequentist risk is no longer expressible as squared bias plus variance. In fact, one can show (Exercise 7.2 of (Hastie et al. 2009)) that the bias and variance combine multiplicatively. If the estimate is on\n",
      "\n",
      "204\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "ln(λ) = 5\n",
      "\n",
      "ln(λ) = 5\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "ln(λ) = −5\n",
      "\n",
      "ln(λ) = −5\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "Figure 6.5 Illustration of bias-variance tradeoff for ridge regression. We generate 100 data sets from the true function, shown in solid green. Left: we plot the regularized ﬁt for 20 different data sets. We use linear regression with a Gaussian RBF expansion, with 25 centers evenly spread over the [0, 1] interval. Right: we plot the average of the ﬁts, averaged over all 100 datasets. Top row: strongly regularized: we see that the individual ﬁts are similar to each other (low variance), but the average is far from the truth (high bias). Bottom row: lightly regularized: we see that the individual ﬁts are quite different from each other (high variance), but the average is close to the truth (low bias). Based on (Bishop 2006a) Figure 3.5. Figure generated by biasVarModelComplexity3.\n",
      "\n",
      "the correct side of the decision boundary, then the bias is negative, and decreasing the variance will decrease the misclassiﬁcation rate. But if the estimate is on the wrong side of the decision boundary, then the bias is positive, so it pays to increase the variance (Friedman 1997a). This little known fact illustrates that the bias-variance tradeoff is not very useful for classiﬁcation. It is better to focus on expected loss (see below), not directly on bias and variance. We can approximate the expected loss using cross validatinon, as we discuss in Section 6.5.3.\n",
      "\n",
      "6.5\n",
      "\n",
      "Empirical risk minimization\n",
      "\n",
      "Frequentist decision theory suffers from the fundamental problem that one cannot actually compute the risk function, since it relies on knowing the true data distribution. (By contrast, the Bayesian posterior expected loss can always be computed, since it conditions on the the data rather than conditioning on θ∗.) However, there is one setting which avoids this problem, and that is where the task is to predict observable quantities, as opposed to estimating hidden variables or parameters. That is, instead of looking at loss functions of the form L(θ, δ(D)), where θ is the true but unknown parameter, and δ(D) is our estimator, let us look at loss\n",
      "\n",
      "6.5. Empirical risk minimization\n",
      "\n",
      "205\n",
      "\n",
      "functions of the form L(y, δ(x)), where y is the true but unknown response, and δ(x) is our prediction given the input x. In this case, the frequentist risk becomes\n",
      "\n",
      "R(p∗, δ) (cid:2) E(x,y)∼p∗ [L(y, δ(x)] =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "L(y, δ(x))p∗(x, y)\n",
      "\n",
      "(6.47)\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "where p∗ represents “nature’s distribution”. Of course, this distribution is unknown, but a simple approach is to use the empirical distribution, derived from some training data, to approximate p∗, i.e.,\n",
      "\n",
      "p∗(x, y) ≈ pemp(x, y) (cid:2) 1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "δxi (x)δyi (y)\n",
      "\n",
      "(6.48)\n",
      "\n",
      "We then deﬁne the empirical risk as follows:\n",
      "\n",
      "Remp(D, D) (cid:2) R(pemp, δ) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(yi, δ(xi))\n",
      "\n",
      "(6.49)\n",
      "\n",
      "In the case of 0-1 loss, L(y, δ(x)) = I(y (cid:4)= δ(x)), this becomes the misclassiﬁcation rate. In the case of squared error loss, L(y, δ(x)) = (y −δ(x))2, this becomes the mean squared error. We deﬁne the task of empirical risk minimization or ERM as ﬁnding a decision procedure (typically a classiﬁcation rule) to minimize the empirical risk:\n",
      "\n",
      "δERM (D) = argmin\n",
      "\n",
      "δ\n",
      "\n",
      "Remp(D, δ)\n",
      "\n",
      "(6.50)\n",
      "\n",
      "In the unsupervised case, we eliminate all references to y, and replace L(y, δ(x)) with L(x, δ(x)), where, for example, L(x, δ(x)) = ||x − δ(x)||2 2, which measures the reconstruc- tion error. We can deﬁne the decision rule using δ(x) =decode( encode(x)), as in vector quantization (Section 11.4.2.6) or PCA (section 12.2). Finally, we deﬁne the empirical risk as\n",
      "\n",
      "Remp(D, δ) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(xi, δ(xi))\n",
      "\n",
      "(6.51)\n",
      "\n",
      "Of course, we can always trivially minimize this risk by setting δ(x) = x, so it is critical that the encoder-decoder go via some kind of bottleneck.\n",
      "\n",
      "6.5.1\n",
      "\n",
      "Regularized risk minimization\n",
      "\n",
      "Note that the empirical risk is equal to the Bayes risk if our prior about “nature’s distribution” is that it is exactly equal to the empirical distribution (Minka 2001b):\n",
      "\n",
      "E [R(p∗, δ)|p∗ = pemp] = Remp(D, δ)\n",
      "\n",
      "(6.52)\n",
      "\n",
      "Therefore minimizing the empirical risk will typically result in overﬁtting. necessary to add a complexity penalty to the objective function:\n",
      "\n",
      "It is therefore often\n",
      "\n",
      "R(cid:4)(D, δ) = Remp(D, δ) +λC (δ)\n",
      "\n",
      "(6.53)\n",
      "\n",
      "206\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "where C(δ) measures the complexity of the prediction function δ(x) and λ controls the strength of the complexity penalty. This approach is known as regularized risk minimization (RRM). Note that if the loss function is negative log likelihood, and the regularizer is a negative log prior, this is equivalent to MAP estimation.\n",
      "\n",
      "The two key issues in RRM are: how do we measure complexity, and how do we pick λ. For a linear model, we can deﬁne the complexity of in terms of its degrees of freedom, discussed in Section 7.5.3. For more general models, we can use the VC dimension, discussed in Section 6.5.4. To pick λ, we can use the methods discussed in Section 6.5.2.\n",
      "\n",
      "6.5.2\n",
      "\n",
      "Structural risk minimization\n",
      "\n",
      "The regularized risk minimization principle says that we should ﬁt the model, for a given complexity penalty, by using\n",
      "\n",
      "ˆδλ = argmin\n",
      "\n",
      "δ\n",
      "\n",
      "[Remp(D, δ) +λC (δ)]\n",
      "\n",
      "(6.54)\n",
      "\n",
      "But how should we pick λ? We cannot using the training set, since this will underestimate the true risk, a problem known as optimism of the training error. As an alternative, we can use the following rule, known as the structural risk minimization principle: (Vapnik 1998):\n",
      "\n",
      "ˆλ = argmin\n",
      "\n",
      "λ\n",
      "\n",
      "ˆR(ˆδλ)\n",
      "\n",
      "(6.55)\n",
      "\n",
      "where ˆR(δ) is an estimate of the risk. There are two widely used estimates: cross validation and theoretical upper bounds on the risk. We discuss both of these below.\n",
      "\n",
      "6.5.3\n",
      "\n",
      "Estimating the risk using cross validation\n",
      "\n",
      "We can estimate the risk of some estimator using a validation set. If we don’t have a separate validation set, we can use cross validation (CV), as we brieﬂy discussed in Section 1.4.8. More precisely, CV is deﬁned as follows. Let there be N = |D| data cases in the training set. Denote the data in the k’th test fold by Dk and all the other data by D−k. (In stratiﬁed CV, these folds are chosen so the class proportions (if discrete labels are present) are roughly equal in each fold.) Let F be a learning algorithm or ﬁtting function that takes a dataset and a model index m (this could a discrete index, such as the degree of a polynomial, or a continuous index, such as the strength of a regularizer) and returns a parameter vector:\n",
      "\n",
      "ˆθm = F(D, m)\n",
      "\n",
      "(6.56)\n",
      "\n",
      "Finally, let P be a prediction function that takes an input and a parameter vector and returns a prediction:\n",
      "\n",
      "ˆy = P(x, ˆθ) = f (x, ˆθ)\n",
      "\n",
      "(6.57)\n",
      "\n",
      "Thus the combined ﬁt-predict cycle is denoted as\n",
      "\n",
      "fm(x, D) = P(x, F(D, m))\n",
      "\n",
      "(6.58)\n",
      "\n",
      "6.5. Empirical risk minimization\n",
      "\n",
      "207\n",
      "\n",
      "The K-fold CV estimate of the risk of fm is deﬁned by (cid:2)\n",
      "\n",
      "R(m, D, K) (cid:2) 1 N\n",
      "\n",
      "K(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "i∈Dk\n",
      "\n",
      "L (yi, P(xi, F(D−k, m)))\n",
      "\n",
      "(6.59)\n",
      "\n",
      "Note that we can call the ﬁtting algorithm once per fold. Let f k m(x) =P (x, F(D−k, m)) be the function that was trained on all the data except for the test data in fold k. Then we can rewrite the CV estimate as K(cid:2)\n",
      "\n",
      "i=1 i∈Dk where k(i) is the fold in which i is used as test data. model that was trained on data that does not contain xi.\n",
      "\n",
      "Of K = N , the method is known as leave one out cross validation or LOOCV. In this case,\n",
      "\n",
      "R(m, D, K) =\n",
      "\n",
      "1 N\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "L\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "yi, f k\n",
      "\n",
      "m(xi)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "=\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "In other words, we predict yi using a\n",
      "\n",
      "L\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "yi, f k(i)\n",
      "\n",
      "m (xi)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(6.60)\n",
      "\n",
      "n the estimated risk becomes (cid:3)\n",
      "\n",
      "R(m, D, N ) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "L\n",
      "\n",
      "yi, f −i\n",
      "\n",
      "m (xi)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(6.61)\n",
      "\n",
      "where f i m we omit the i’th training case. Fortunately, for some model classes and loss functions (namely linear models and quadratic loss), we can ﬁt the model once, and analytically “remove” the effect of the i’th training case. This is known as generalized cross validation or GCV.\n",
      "\n",
      "m(x) = P(x, F(D−i, m)). This requires ﬁtting the model N times, where for f −i\n",
      "\n",
      "6.5.3.1\n",
      "\n",
      "Example: using CV to pick λ for ridge regression\n",
      "\n",
      "As a concrete example, consider picking the strength of the (cid:6)2 regularizer in penalized linear regression. We use the following rule:\n",
      "\n",
      "ˆλ = arg\n",
      "\n",
      "min λ∈[λmin,λmax]\n",
      "\n",
      "R(λ, Dtrain, K)\n",
      "\n",
      "(6.62)\n",
      "\n",
      "where [λmin, λmax] is a ﬁnite range of λ values that we search over, and R(λ, Dtrain, K) is the K-fold CV estimate of the risk of using λ, given by\n",
      "\n",
      "R(λ, Dtrain, K) =\n",
      "\n",
      "1 |Dtrain|\n",
      "\n",
      "K(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i∈Dk\n",
      "\n",
      "L(yi, f k\n",
      "\n",
      "λ (xi))\n",
      "\n",
      "(6.63)\n",
      "\n",
      "where f k ˆwλ(D) = arg minw N LL(w, D) +λ||w ||2 of a CV estimate of the risk vs log(λ), where the loss function is squared error.\n",
      "\n",
      "λ (x) = xT ˆwλ(D−k) is the prediction function trained on data excluding fold k, and 2 is the MAP estimate. Figure 6.6(b) gives an example\n",
      "\n",
      "In this case, we optimize a convex upper bound on the empirical risk to estimate wλm but we optimize (the CV estimate of) the risk itself to estimate λ. We can handle the non-smooth 0-1 loss function when estimating λ because we are using brute-force search over the entire (one-dimensional) space.\n",
      "\n",
      "When performing classiﬁcation, we usually use 0-1 loss.\n",
      "\n",
      "When we have more than one or two tuning parameters, this approach becomes infeasible. In such cases, one can use empirical Bayes, which allows one to optimize large numbers of hyper-parameters using gradient-based optimizers instead of brute-force search. See Section 5.6 for details.\n",
      "\n",
      "208\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "mean squared error\n",
      "\n",
      "5−fold cross validation, ntrain = 50\n",
      "\n",
      "14\n",
      "\n",
      "20\n",
      "\n",
      "12\n",
      "\n",
      "train mse test mse\n",
      "\n",
      "18\n",
      "\n",
      "16\n",
      "\n",
      "10\n",
      "\n",
      "14\n",
      "\n",
      "8\n",
      "\n",
      "12\n",
      "\n",
      "e s m\n",
      "\n",
      "10\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0 −25\n",
      "\n",
      "−20\n",
      "\n",
      "−15\n",
      "\n",
      "−10 log lambda\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "0 −15\n",
      "\n",
      "−10\n",
      "\n",
      "−5 log lambda\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 6.6 (a) Mean squared error for (cid:7)2 penalized degree 14 polynomial regression vs log regularizer. Same as in Figures 7.8, except now we have N = 50 training points instead of 21. The stars correspond to the values used to plot the functions in Figure 7.7. (b) CV estimate. The vertical scale is truncated for clarity. The blue line corresponds to the value chosen by the one standard error rule. Figure generated by linregPolyVsRegDemo.\n",
      "\n",
      "6.5.3.2\n",
      "\n",
      "The one standard error rule\n",
      "\n",
      "The above procedure estimates the risk, but does not give any measure of uncertainty. A standard frequentist measure of uncertainty of an estimate is the standard error of the mean, deﬁned by\n",
      "\n",
      "se =\n",
      "\n",
      "ˆσ √ N\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ˆσ2 N\n",
      "\n",
      "(6.64)\n",
      "\n",
      "where ˆσ2 is an estimate of the variance of the loss:\n",
      "\n",
      "ˆσ2 =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(Li − L)2, Li = L(yi, f k(i)\n",
      "\n",
      "m (xi)) L =\n",
      "\n",
      "i=1\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "Li\n",
      "\n",
      "(6.65)\n",
      "\n",
      "Note that σ measures the intrinsic variability of Li across samples, whereas se measures our uncertainty about the mean L.\n",
      "\n",
      "Suppose we apply CV to a set of models and compute the mean and se of their estimated risks. A common heuristic for picking a model from these noisy estimates is to pick the value which corresponds to the simplest model whose risk is no more than one standard error above the risk of the best model; this is called the one-standard error rule (Hastie et al. 2001, p216). For example, in Figure 6.6, we see that this heuristic does not choose the lowest point on the curve, but one that is slightly to its right, since that corresponds to a more heavily regularized model with essentially the same empirical performance.\n",
      "\n",
      "6.5. Empirical risk minimization\n",
      "\n",
      "209\n",
      "\n",
      "6.5.3.3\n",
      "\n",
      "CV for model selection in non-probabilistic unsupervised learning\n",
      "\n",
      "If we are performing unsupervised learning, we must use a loss function such as L(x, δ(x)) = ||x − δ(x)||2, which measures reconstruction error. Here δ(x) is some encode-decode scheme. However, as we discussed in Section 11.5.2, we cannot use CV to determine the complexity of δ, since we will always get lower loss with a more complex model, even if evaluated on the test set. This is because more complex models will compress the data less, and induce less distortion. Consequently, we must either use probabilistic models, or invent other heuristics.\n",
      "\n",
      "6.5.4\n",
      "\n",
      "Upper bounding the risk using statistical learning theory *\n",
      "\n",
      "The principle problem with cross validation is that it is slow, since we have to ﬁt the model multiple times. This motivates the desire to compute analytic approximations or bounds to the generalization error. This is the studied in the ﬁeld of statistical learning theory (SLT). More precisely, SLT tries to bound the risk R(p∗, h) for any data distribution p∗ and hypothesis h ∈ H in terms of the empirical risk Remp(D, h), the sample size N = |D|, and the size of the hypothesis space H.\n",
      "\n",
      "Let us initially consider the case where the hypothesis space is ﬁnite, with size dim(H) = |H|. In other words, we are selecting a model/ hypothesis from a ﬁnite list, rather than optimizing real-valued parameters, Then we can prove the following.\n",
      "\n",
      "Theorem 6.5.1. For any data distribution p∗, and any dataset D of size N drawn from p∗, the probability that our estimate of the error rate will be more than (cid:8) wrong, in the worst case, is upper bounded as follows:\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "P\n",
      "\n",
      "max h∈H\n",
      "\n",
      "|Remp(D, h) − R(p∗, h)| > (cid:8)\n",
      "\n",
      "≤ 2 dim(H)e−2N (cid:4)2\n",
      "\n",
      "(6.66)\n",
      "\n",
      "Proof. To prove this, we need two useful results. First, Hoeffding’s inequality, which states that if X1, . . . , XN ∼ Ber(θ), then, for any (cid:8) > 0,\n",
      "\n",
      "where x = 1 N events, then P (∪d\n",
      "\n",
      "Finally, for notational brevity, let R(h) = R(h, p∗) be the true risk, and ˆRN (h) = Remp(D, h)\n",
      "\n",
      "P (|x − θ| > (cid:8)) ≤ 2e−2N (cid:4)2 (cid:10)N\n",
      "\n",
      "i=1 xi. Second, the union bound, which says that if A1, . . . , Ad are a set of i=1Ai) ≤\n",
      "\n",
      "(cid:10)d\n",
      "\n",
      "i=1 P (Ai).\n",
      "\n",
      "(6.67)\n",
      "\n",
      "be the empirical risk.\n",
      "\n",
      "Using these results we have\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "P\n",
      "\n",
      "max h∈H\n",
      "\n",
      "| ˆRN (h) − R(h)| > (cid:8)\n",
      "\n",
      "= P\n",
      "\n",
      "≤\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "h∈H (cid:5)\n",
      "\n",
      "P\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "| ˆRN (h) − R(h)| > (cid:8)\n",
      "\n",
      "| ˆRN (h) − R(h)| > (cid:8) (cid:6)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(6.68)\n",
      "\n",
      "(6.69)\n",
      "\n",
      "≤\n",
      "\n",
      "h∈H (cid:2)\n",
      "\n",
      "2e−2N (cid:4)2\n",
      "\n",
      "= 2 dim(H)e−2N (cid:4)2\n",
      "\n",
      "(6.70)\n",
      "\n",
      "h∈H\n",
      "\n",
      "210\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "creases with N = |D|, as is to be expected.\n",
      "\n",
      "If the hypothesis space H is inﬁnite (e.g., we have real-valued parameters), we cannot use dim(H) =|H|. Instead, we can use a quantity called the Vapnik-Chervonenkis or VC dimen- sion of the hypothesis class. See (Vapnik 1998) for details.\n",
      "\n",
      "Ths bound tells us that the optimism of the training error increases with dim(H) but de-\n",
      "\n",
      "Stepping back from all the theory, the key intuition behind statistical learning theory is quite If the hypothesis space H is very simple. Suppose we ﬁnd a model with low empirical risk. big, relative to the data size, then it is quite likely that we just got “lucky” and were given a data set that is well-modeled by our chosen function by chance. However, this does not mean that such a function will have low generalization error. But if the hypothesis class is sufficiently constrained in size, and/or the training set is sufficiently large, then we are unlikely to get lucky in this way, so a low empirical risk is evidence of a low true risk.\n",
      "\n",
      "Note that optimism of the training error does not necessarily increase with model complexity,\n",
      "\n",
      "but it does increase with the number of different models that are being searched over.\n",
      "\n",
      "The advantage of statistical learning theory compared to CV is that the bounds on the risk are quicker to compute than using CV. The disadvantage is that it is hard to compute the VC dimension for many interesting models, and the upper bounds are usually very loose (although see (Kaariainen and Langford 2005)).\n",
      "\n",
      "One can extend statistical learning theory by taking computational complexity of the learner into account. This ﬁeld is called computational learning theory or COLT. Most of this work focuses on the case where h is a binary classiﬁer, and the loss function is 0-1 loss. If we observe a low empirical risk, and the hypothesis space is suitably “small”, then we can say that our estimated function is probably approximately correct or PAC. A hypothesis space is said to be efficiently PAC-learnable if there is a polynomial time algorithm that can identify a function that is PAC. See (Kearns and Vazirani 1994) for details.\n",
      "\n",
      "6.5.5\n",
      "\n",
      "Surrogate loss functions\n",
      "\n",
      "Minimizing the loss in the ERM/ RRM framework is not always easy. For example, we might want to optimize the AUC or F1 scores. Or more simply, we might just want to minimize the 0-1 loss, as is common in classiﬁcation. Unfortunately, the 0-1 risk is a very non-smooth objective and hence is hard to optimize. One alternative is to use maximum likelihood estimation instead, since log-likelihood is a smooth convex upper bound on the 0-1 risk, as we show below.\n",
      "\n",
      "To see this, consider binary logistic regression, and let yi ∈ {−1, +1}. Suppose our decision\n",
      "\n",
      "function computes the log-odds ratio,\n",
      "\n",
      "f (xi) = log\n",
      "\n",
      "p(y = 1|xi, w) p(y = −1|xi, w)\n",
      "\n",
      "= wT xi = ηi\n",
      "\n",
      "(6.71)\n",
      "\n",
      "Then the corresponding probability distribution on the output label is\n",
      "\n",
      "p(yi|xi, w) = sigm(yiηi)\n",
      "\n",
      "(6.72)\n",
      "\n",
      "Let us deﬁne the log-loss as as\n",
      "\n",
      "L\n",
      "\n",
      "nll(y, η) = − log p(y|x, w) = log(1 + e−yη)\n",
      "\n",
      "(6.73)\n",
      "\n",
      "6.6. Pathologies of frequentist statistics *\n",
      "\n",
      "211\n",
      "\n",
      "3\n",
      "\n",
      "0−1 hinge logloss\n",
      "\n",
      "2.5\n",
      "\n",
      "2\n",
      "\n",
      "s s o\n",
      "\n",
      "l\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0 η\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "Figure 6.7 yη, the vertical axis is the loss. The log loss uses log base 2. Figure generated by hingeLossPlot.\n",
      "\n",
      "Illustration of various loss functions for binary classiﬁcation. The horizontal axis is the margin\n",
      "\n",
      "It is clear that minimizing the average log-loss is equivalent to maximizing the likelihood.\n",
      "\n",
      "ηi < 0 and ˆy = +1 if ηi ≥ 0. The 0-1 loss of our function becomes\n",
      "\n",
      "Now consider computing the most probable label, which is equivalent to using ˆy = −1 if\n",
      "\n",
      "L01(y, η) = I(y (cid:4)= ˆy) = I(yη < 0)\n",
      "\n",
      "(6.74)\n",
      "\n",
      "Figure 6.7 plots these two loss functions. We see that the NLL is indeed an upper bound on the 0-1 loss.\n",
      "\n",
      "Log-loss is an example of a surrogate loss function. Another example is the hinge loss:\n",
      "\n",
      "L\n",
      "\n",
      "hinge(y, η) = max(0, 1 − yη)\n",
      "\n",
      "(6.75)\n",
      "\n",
      "See Figure 6.7 for a plot. We see that the function looks like a door hinge, hence its name. This loss function forms the basis of a popular classiﬁcation method known as support vector machines (SVM), which we will discuss in Section 14.5.\n",
      "\n",
      "The surrogate is usually chosen to be a convex upper bound, since convex functions are easy\n",
      "\n",
      "to minimize. See e.g., (Bartlett et al. 2006) for more information.\n",
      "\n",
      "6.6\n",
      "\n",
      "Pathologies of frequentist statistics *\n",
      "\n",
      "I believe that it would be very difficult to persuade an intelligent person that current [frequentist] statistical practice was sensible, but that there would be much less difficulty with an approach via likelihood and Bayes’ theorem. — George Box, 1962.\n",
      "\n",
      "Frequentist statistics exhibits various forms of weird and undesirable behaviors, known as pathologies. We give a few examples below, in order to caution the reader; these and other examples are explained in more detail in (Lindley 1972; Lindley and Phillips 1976; Lindley 1982; Berger 1985; Jaynes 2003; Minka 1999).\n",
      "\n",
      "212\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "6.6.1\n",
      "\n",
      "Counter-intuitive behavior of conﬁdence intervals\n",
      "\n",
      "A conﬁdence interval is an interval derived from the sampling distribution of an estimator (whereas a Bayesian credible interval is derived from the posterior of a parameter, as we dis- cussed in Section 5.2.2). More precisely, a frequentist conﬁdence interval for some parameter θ is deﬁned by the following (rather un-natural) expression:\n",
      "\n",
      "C (cid:4)\n",
      "\n",
      "α(θ) = ((cid:6), u) :P ((cid:6)( ˜D) ≤ θ ≤ u( ˜D)| ˜D ∼ θ) = 1 − α\n",
      "\n",
      "(6.76)\n",
      "\n",
      "That is, if we sample hypothetical future data ˜D from θ, then ((cid:6)( ˜D), u( ˜D)), is a conﬁdence interval if the parameter θ lies inside this interval 1 − α percent of the time. Let us step back for a moment and think about what is going on.\n",
      "\n",
      "In Bayesian statistics, we condition on what is known — namely the observed data, D — and average over what is not known, namely the parameter θ. In frequentist statistics, we do exactly the opposite: we condition on what is unknown — namely the true parameter value θ — and average over hypothetical future data sets ˜D.\n",
      "\n",
      "This counter-intuitive deﬁnition of conﬁdence intervals can lead to bizarre results. Consider the following example from (Berger 1985, p11). Suppose we draw two integers D = (x1, x2) from\n",
      "\n",
      "p(x|θ) =\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "0.5 if x = θ 0.5 if x = θ + 1 0 otherwise\n",
      "\n",
      "(6.77)\n",
      "\n",
      "If θ = 39, we would expect the following outcomes each with probability 0.25:\n",
      "\n",
      "(39, 39), (39, 40), (40, 39), (40, 40)\n",
      "\n",
      "(6.78)\n",
      "\n",
      "Let m = min(x1, x2) and deﬁne the following conﬁdence interval:\n",
      "\n",
      "[(cid:6)(D), u(D)] = [m, m]\n",
      "\n",
      "(6.79)\n",
      "\n",
      "For the above samples this yields\n",
      "\n",
      "[39, 39],\n",
      "\n",
      "[39, 39],\n",
      "\n",
      "[39, 39],\n",
      "\n",
      "[40, 40]\n",
      "\n",
      "(6.80)\n",
      "\n",
      "Hence Equation 6.79 is clearly a 75% CI, since 39 is contained in 3/4 of these intervals. However, if D = (39, 40) then p(θ = 39|D) = 1.0, so we know that θ must be 39, yet we only have 75% “conﬁdence” in this fact.\n",
      "\n",
      "Another, less contrived example, is as follows. Suppose we want to estimate the parameter θ of a Bernoulli distribution. Let x = 1 i=1 xi be the sample mean. The MLE is ˆθ = x. An N approximate 95% conﬁdence interval for a Bernoulli parameter is x ± 1.96 x(1 − x)/N (this is called a Wald interval and is based on a Gaussian approximation to the Binomial distribution; compare to Equation 3.27). Now consider a single trial, where N = 1 and x1 = 0. The MLE is 0, which overﬁts, as we saw in Section 3.3.4.1. But our 95% conﬁdence interval is also (0, 0), which seems even worse. It can be argued that the above ﬂaw is because we approximated the true sampling distribution with a Gaussian, or because the sample size was to small, or the parameter “too extreme”. However, the Wald interval can behave badly even for large N , and non-extreme parameters (Brown et al. 2001).\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "6.6. Pathologies of frequentist statistics *\n",
      "\n",
      "213\n",
      "\n",
      "6.6.2\n",
      "\n",
      "p-values considered harmful\n",
      "\n",
      "Suppose we want to decide whether to accept or reject some baseline model, which we will call the null hypothesis. We need to deﬁne some decision rule. In frequentist statistics, it is standard to ﬁrst compute a quantity called the p-value, which is deﬁned as the probability (under the null) of observing some test statistic f (D) (such as the chi-squared statistic) that is as large or larger than that actually observed:5 pvalue(D) (cid:2) P (f ( ˜D) ≥ f (D)| ˜D ∼ H0)\n",
      "\n",
      "(6.81)\n",
      "\n",
      "This quantity relies on computing a tail area probability of the sampling distribution; we give an example of how to do this below.\n",
      "\n",
      "Given the p-value, we deﬁne our decision rule as follows: we reject the null hypothesis iff the p-value is less than some threshold, such as α = 0.05. If we do reject it, we say the difference between the observed test statistic and the expected test statistic is statistically signiﬁcant at level α. This approach is known as null hypothesis signiﬁcance testing, orNHST.\n",
      "\n",
      "This procedure guarantees that our expected type I (false positive) error rate is at most α. This is sometimes interpreted as saying that frequentist hypothesis testing is very conservative, since it is unlikely to accidently reject the null hypothesis. But in fact the opposite is the case: because this method only worries about trying to reject the null, it can never gather evidence in favor of the null, no matter how large the sample size. Because of this, p-values tend to overstate the evidence against the null, and are thus very “trigger happy”.\n",
      "\n",
      "In general there can be huge differences between p-values and the quantity that we really care about, which is the posterior probability of the null hypothesis given the data, p(H0|D). In particular, Sellke et al. (2001) show that even if the p-value is as slow as 0.05, the posterior probability of H0 is at least 30%, and often much higher. So frequentists often claim to have “signiﬁcant” evidence of an effect that cannot be explained by the null hypothesis, whereas Bayesians are usually more conservative in their claims. For example, p-values have been used to “prove” that ESP (extra-sensory perception) is real (Wagenmakers et al. 2011), even though ESP is clearly very improbable. For this reason, p-values have been banned from certain medical journals (Matthews 1998).\n",
      "\n",
      "Another problem with p-values is that their computation depends on decisions you make about when to stop collecting data, even if these decisions don’t change the data you actually observed. For example, suppose I toss a coin n = 12 times and observe s = 9 successes (heads) and f = 3 failures (tails), so n = s + f . In this case, n is ﬁxed and s (and hence f ) is random. The relevant sampling model is the binomial (cid:9)\n",
      "\n",
      "Bin(s|n, θ) =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "n s\n",
      "\n",
      "θs(1 − θ)n−s\n",
      "\n",
      "(6.82)\n",
      "\n",
      "Let the null hypothesis be that the coin is fair, θ = 0.5, where θ is the probability of success (heads). The one-sided p-value, using test statistic t(s) = s, is\n",
      "\n",
      "p1 = P (S ≥ 9|H0) =\n",
      "\n",
      "12(cid:2)\n",
      "\n",
      "s=9\n",
      "\n",
      "Bin(s|12, 0.5) =\n",
      "\n",
      "12(cid:2)\n",
      "\n",
      "s=9\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "12 s\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "0.512 = 0.073\n",
      "\n",
      "(6.83)\n",
      "\n",
      "5. The reason we cannot just compute the probability of the observed value of the test statistic is that this will have probability zero under a pdf. The p-value is deﬁned in terms of the cdf, so is always a number between 0 and 1.\n",
      "\n",
      "214\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "The two-sided p-value is\n",
      "\n",
      "p2 =\n",
      "\n",
      "12(cid:2)\n",
      "\n",
      "Bin(s|12, 0.5) +\n",
      "\n",
      "3(cid:2)\n",
      "\n",
      "Bin(s|12, 0.5) = 0.073 + 0.073 = 0.146\n",
      "\n",
      "(6.84)\n",
      "\n",
      "s=9\n",
      "\n",
      "s=0\n",
      "\n",
      "In either case, the p-value is larger than the magical 5% threshold, so a frequentist would not reject the null hypothesis.\n",
      "\n",
      "Now suppose I told you that I actually kept tossing the coin until I observed f = 3 tails. In this case, f is ﬁxed and n (and hence s = n − f ) is random. The probability model becomes the negative binomial distribution, given by\n",
      "\n",
      "NegBinom(s|f, θ) =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "s + f − 1 f − 1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "θs(1 − θ)f\n",
      "\n",
      "(6.85)\n",
      "\n",
      "where f = n − s.\n",
      "\n",
      "Note that the term which depends on θ is the same in Equations 6.82 and 6.85, so the posterior over θ would be the same in both cases. However, these two interpretations of the same data give different p-values. In particular, under the negative binomial model we get\n",
      "\n",
      "p3 = P (S ≥ 9|H0) =\n",
      "\n",
      "∞(cid:2)\n",
      "\n",
      "s=9\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "3 + s − 1 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(1/2)s(1/2)3 = 0.0327\n",
      "\n",
      "(6.86)\n",
      "\n",
      "So the p-value is 3%, and suddenly there seems to be signiﬁcant evidence of bias in the coin! Obviously this is ridiculous: the data is the same, so our inferences about the coin should be the same. After all, I could have chosen the experimental protocol at random. It is the outcome of the experiment that matters, not the details of how I decided which one to run.\n",
      "\n",
      "Although this might seem like just a mathematical curiosity, this also has signiﬁcant practical In particular, the fact that the stopping rule affects the computation of the p- implications. value means that frequentists often do not terminate experiments early, even when it is obvious what the conclusions are, lest it adversely affect their statistical analysis. If the experiments are costly or harmful to people, this is obviously a bad idea. Perhaps it is not surprising, then, that the US Food and Drug Administration (FDA), which regulates clinical trials of new drugs, has recently become supportive of Bayesian methods6, since Bayesian methods are not affected by the stopping rule.\n",
      "\n",
      "6.6.3\n",
      "\n",
      "The likelihood principle\n",
      "\n",
      "The fundamental reason for many of these pathologies is that frequentist inference violates the likelihood principle, which says that inference should be based on the likelihood of the observed data, not based on hypothetical future data that you have not observed. Bayes obviously satisﬁes the likelihood principle, and consequently does not suffer from these pathologies.\n",
      "\n",
      "A compelling argument in favor of the likelihood principle was presented in (Birnbaum 1962), who showed that it followed automatically from two simpler principles. The ﬁrst of these is the sufficiency principle, which says that a sufficient statistic contains all the relevant information\n",
      "\n",
      "6. See http://yamlb.wordpress.com/2006/06/19/the-us-fda-is-becoming-progressively-more-bayes ian/.\n",
      "\n",
      "6.6. Pathologies of frequentist statistics *\n",
      "\n",
      "215\n",
      "\n",
      "about an unknown parameter (arguably this is true by deﬁnition). The second principle is known as weak conditionality, which says that inferences should be based on the events that happened, not which might have happened. To motivate this, consider an example from (Berger 1985). Suppose we need to analyse a substance, and can send it either to a laboratory in New York or in California. The two labs seem equally good, so a fair coin is used to decide between them. The coin comes up heads, so the California lab is chosen. When the results come back, should it be taken into account that the coin could have come up tails and thus the New York lab could have been used? Most people would argue that the New York lab is irrelevant, since the tails event didn’t happen. This is an example of weak conditionality. Given this principle, one can show that all inferences should only be based on what was observed, which is in contrast to standard frequentist procedures. See (Berger and Wolpert 1988) for further details on the likelihood principle.\n",
      "\n",
      "6.6.4 Why isn’t everyone a Bayesian?\n",
      "\n",
      "Given these fundamental ﬂaws of frequentist statistics, and the fact that Bayesian methods do not have such ﬂaws, an obvious question to ask is: “Why isn’t everyone a Bayesian?” The (frequentist) statistician Bradley Efron wrote a paper with exactly this title (Efron 1986). His short paper is well worth reading for anyone interested in this topic. Below we quote his opening section:\n",
      "\n",
      "The title is a reasonable question to ask on at least two counts. First of all, everone used to be a Bayesian. Laplace wholeheatedly endorsed Bayes’s formulation of the inference problem, and most 19th-century scientists followed suit. This included Gauss, whose statistical work is usually presented in frequentist terms.\n",
      "\n",
      "A second and more important point is the cogency of the Bayesian argument. Modern statisticians, following the lead of Savage and de Finetti, have advanced powerful theoret- ical arguments for preferring Bayesian inference. A byproduct of this work is a disturbing catalogue of inconsistencies in the frequentist point of view.\n",
      "\n",
      "Nevertheless, everyone is not a Bayesian. The current era (1986) is the ﬁrst century in which statistics has been widely used for scientiﬁc reporting, and in fact, 20th-century statistics is mainly non-Bayesian. However, Lindley (1975) predicts a change for the 21st century.\n",
      "\n",
      "Time will tell whether Lindley was right....\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 6.1 Pessimism of LOOCV (Source: Witten05, p152.). Suppose we have a completely random labeled dataset (i.e., the features x tell us nothing about the class labels y) with N1 examples of class 1, and N2 examples of class 2, where N1 = N2. What is the best misclassiﬁcation rate any method can achieve? What is the estimated misclassiﬁcation rate of the same method using LOOCV?\n",
      "\n",
      "Exercise 6.2 James Stein estimator for Gaussian means Consider the 2 stage model Yi|θi ∼ N (θi, σ2) and θi|μ ∼ N (m0, τ 2 we observe the following 6 data points, i = 1 : 6:\n",
      "\n",
      "0 ). Suppose σ2 = 500 is known and\n",
      "\n",
      "216\n",
      "\n",
      "Chapter 6. Frequentist statistics\n",
      "\n",
      "1505, 1528, 1564, 1498, 1600, 1470\n",
      "\n",
      "a. Find the ML-II estimates of m0 and τ 2 0 . b. Find the posterior estimates E [θi|yi, m0, τ0] and var [θi|yi, m0, τ0] for i = 1.\n",
      "\n",
      "i = 2 : 6, are computed similarly.)\n",
      "\n",
      "(The other terms,\n",
      "\n",
      "c. Give a 95% credible interval for p(θi|yi, m0, τ0) for i = 1. Do you trust this interval (assuming the\n",
      "\n",
      "Gaussian assumption is reasonable)? i.e. is it likely to be too large or too small, or just right?\n",
      "\n",
      "d. What do you expect would happen to your estimates if σ2 were much smaller (say σ2 = 1)? You do not need to compute the numerical answer; just brieﬂy explain what would happen qualitatively, and why.\n",
      "\n",
      "Exercise 6.3 ˆσ2 Show that ˆσ2\n",
      "\n",
      "M LE = 1 N\n",
      "\n",
      "M LE is biased\n",
      "\n",
      "(cid:2)N\n",
      "\n",
      "n=1(xn − ˆμ)2 is a biased estimator of σ2, i.e., show\n",
      "\n",
      "EX1,...,Xn∼N (μ,σ)[ˆσ2(X1, . . . ,X n) (cid:8)= σ2\n",
      "\n",
      "Hint: note that X1, . . . , XN are independent, and use the fact that the expectation of a product of independent random variables is the product of the expectations.\n",
      "\n",
      "Exercise 6.4 Estimation of σ2 when μ is known Suppose we sample x1, . . . , xN ∼ N (μ, σ2) where μ is a known constant. Derive an expression for the MLE for σ2 in this case. Is it unbiased?\n",
      "\n",
      "7 Linear regression\n",
      "\n",
      "7.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Linear regression is the “work horse” of statistics and (supervised) machine learning. When augmented with kernels or other forms of basis function expansion, it can model also non- linear relationships. And when the Gaussian output is replaced with a Bernoulli or multinoulli distribution, it can be used for classiﬁcation, as we will see below. So it pays to study this model in detail.\n",
      "\n",
      "7.2 Model speciﬁcation\n",
      "\n",
      "As we discussed in Section 1.4.5, linear regression is a model of the form\n",
      "\n",
      "non-linear function of the inputs, φ(x). That is, we use\n",
      "\n",
      "p(y|x, θ) = N (y|wT x, σ2) (7.1) Linear regression can be made to model non-linear relationships by replacing x with some\n",
      "\n",
      "p(y|x, θ) = N (y|wT φ(x), σ2)\n",
      "\n",
      "(7.2)\n",
      "\n",
      "This is known as basis function expansion. (Note that the model is still linear in the parameters w, so it is still called linear regression; the importance of this will become clear below.) A simple example are polynomial basis functions, where the model has the form\n",
      "\n",
      "φ(x) = [1, x, x2, . . . , xd]\n",
      "\n",
      "Figure 1.18 illustrates the effect of changing d: increasingly complex functions.\n",
      "\n",
      "(7.3) increasing the degree d allows us to create\n",
      "\n",
      "We can also apply linear regression to more than 1 input. For example, consider modeling temperature as a function of location. Figure 7.1(a) plots E [y|x] =w 0 + w1x1 + w2x2, and Figure 7.1(b) plots E [y|x] = w0 + w1x1 + w2x2 + w3x2\n",
      "\n",
      "1 + w4x2 2.\n",
      "\n",
      "7.3 Maximum likelihood estimation (least squares)\n",
      "\n",
      "A common way to esitmate the parameters of a statistical model is to compute the MLE, which is deﬁned as\n",
      "\n",
      "ˆθ (cid:2) arg max\n",
      "\n",
      "θ\n",
      "\n",
      "log p(D|θ)\n",
      "\n",
      "(7.4)\n",
      "\n",
      "218\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "18\n",
      "\n",
      "18\n",
      "\n",
      "17.5\n",
      "\n",
      "17.5\n",
      "\n",
      "17\n",
      "\n",
      "17\n",
      "\n",
      "16.5\n",
      "\n",
      "16.5\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "15.5\n",
      "\n",
      "15.5\n",
      "\n",
      "15\n",
      "\n",
      "30\n",
      "\n",
      "25\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7.1 Linear regression applied to 2d data. Vertical axis is temperature, horizontal axes are location within a room. Data was collected by some remote sensing motes at Intel’s lab in Berkeley, CA (data courtesy of Romain Thibaux). (b) 1 + w4x2 Temperature data is ﬁtted with a quadratic of the form ˆf (x) =w 0 + w1x1 + w2x2 + w3x2 2. Produced by surfaceFitDemo.\n",
      "\n",
      "(a) The ﬁtted plane has the form ˆf (x) =w 0 + w1x1 + w2x2.\n",
      "\n",
      "It is common to assume the training examples are independent and identically distributed, commonly abbreviated to iid. This means we can write the log-likelihood as follows:\n",
      "\n",
      "(cid:6)(θ) (cid:2) log p(D|θ) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "log p(yi|xi, θ)\n",
      "\n",
      "(7.5)\n",
      "\n",
      "i=1\n",
      "\n",
      "Instead of maximizing the log-likelihood, we can equivalently minimize the negative log likeli- hood or NLL:\n",
      "\n",
      "NLL(θ) (cid:2) −\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "log p(yi|xi, θ)\n",
      "\n",
      "(7.6)\n",
      "\n",
      "i=1\n",
      "\n",
      "The NLL formulation is sometimes more convenient, since many optimization software packages are designed to ﬁnd the minima of functions, rather than maxima.\n",
      "\n",
      "Now let us apply the method of MLE to the linear regression setting. Inserting the deﬁnition\n",
      "\n",
      "of the Gaussian into the above, we ﬁnd that the log likelihood is given by\n",
      "\n",
      "(cid:6)(θ) =\n",
      "\n",
      "=\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1 −1 2σ2\n",
      "\n",
      "log\n",
      "\n",
      "RSS(w) −\n",
      "\n",
      "(cid:18)(cid:8)\n",
      "\n",
      "1 2πσ2\n",
      "\n",
      "(cid:9) 1 2\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log(2πσ2)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "− 1 2σ2 (yi − wT xi)2\n",
      "\n",
      "(cid:9)(cid:19)\n",
      "\n",
      "(7.7)\n",
      "\n",
      "(7.8)\n",
      "\n",
      "RSS stands for residual sum of squares and is deﬁned by\n",
      "\n",
      "RSS(w) (cid:2)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(yi − wT xi)2\n",
      "\n",
      "(7.9)\n",
      "\n",
      "i=1\n",
      "\n",
      "The RSS is also called the sum of squared errors, or SSE, and SSE/N is called the mean squared error or MSE. It can also be written as the square of the (cid:6)2 norm of the vector of\n",
      "\n",
      "7.3. Maximum likelihood estimation (least squares)\n",
      "\n",
      "219\n",
      "\n",
      "Sum of squares error contours for linear regression\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "prediction truth\n",
      "\n",
      "2.5\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1 w\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−0.5\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1 w0\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7.2 (a) In linear least squares, we try to minimize the sum of squared distances from each training point (denoted by a red circle) to its approximation (denoted by a blue cross), that is, we minimize the sum of the lengths of the little vertical blue lines. The red diagonal line represents ˆy(x) =w 0 + w1x, which is the least squares regression line. Note that these residual lines are not perpendicular to the least squares line, in contrast to Figure 12.5. Figure generated by residualsDemo. (b) Contours of the RSS error surface for the same example. The red cross represents the MLE, w = (1.45, 0.93). Figure generated by contoursSSEdemo.\n",
      "\n",
      "residual errors:\n",
      "\n",
      "RSS(w) = ||(cid:7)||2\n",
      "\n",
      "2 =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(cid:8)2 i\n",
      "\n",
      "(7.10)\n",
      "\n",
      "i=1\n",
      "\n",
      "where (cid:8)i = (yi − wT xi).\n",
      "\n",
      "We see that the MLE for w is the one that minimizes the RSS, so this method is known as least squares. This method is illustrated in Figure 7.2(a). The training data (xi, yi) are shown as red circles, the estimated values (xi, ˆyi) are shown as blue crosses, and the residuals (cid:8)i = yi − ˆyi are shown as vertical blue lines. The goal is to ﬁnd the setting of the parameters (the slope w1 and intercept w0) such that the resulting red line minimizes the sum of squared residuals (the lengths of the vertical blue lines).\n",
      "\n",
      "In Figure 7.2(b), we plot the NLL surface for our linear regression example. We see that it is a quadratic “bowl” with a unique minimum, which we now derive. (Importantly, this is true even if we use basis function expansion, such as polynomials, because the NLL is still linear in the parameters w, even if it is not linear in the inputs x.)\n",
      "\n",
      "7.3.1\n",
      "\n",
      "Derivation of the MLE\n",
      "\n",
      "First, we rewrite the objective in a form that is more amenable to differentiation:\n",
      "\n",
      "NLL(w) =\n",
      "\n",
      "1 2\n",
      "\n",
      "(y − Xw)T (y − Xw) =\n",
      "\n",
      "1 2\n",
      "\n",
      "wT (XT X)w − wT (XT y)\n",
      "\n",
      "(7.11)\n",
      "\n",
      "220\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "where\n",
      "\n",
      "XT X =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "xixT\n",
      "\n",
      "i =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎝\n",
      "\n",
      "x2 i,1\n",
      "\n",
      "xi,Dxi,1\n",
      "\n",
      "· · . . . · · ·\n",
      "\n",
      "xi,1xi,D\n",
      "\n",
      "x2\n",
      "\n",
      "i,D\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎠\n",
      "\n",
      "(7.12)\n",
      "\n",
      "is the sum of squares matrix and\n",
      "\n",
      "XT y =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "xiyi.\n",
      "\n",
      "(7.13)\n",
      "\n",
      "i=1\n",
      "\n",
      "Using results from Equation 4.10, we see that the gradient of this is given by\n",
      "\n",
      "g(w) = [XT Xw − XT y] =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "xi(wT xi − yi)\n",
      "\n",
      "(7.14)\n",
      "\n",
      "i=1\n",
      "\n",
      "Equating to zero we get XT Xw = XT y\n",
      "\n",
      "(7.15)\n",
      "\n",
      "This is known as the normal equation. The corresponding solution ˆw to this linear system of equations is called the ordinary least squares or OLS solution, which is given by\n",
      "\n",
      "ˆwOLS = (XT X)−1XT y\n",
      "\n",
      "(7.16)\n",
      "\n",
      "7.3.2\n",
      "\n",
      "Geometric interpretation\n",
      "\n",
      "This equation has an elegant geometrical intrepretation, as we now explain. We assume N > D, so we have more examples than features. The columns of X deﬁne a linear subspace of dimensionality D which is embedded in N dimensions. Let the j’th column be ˜xj, which is a vector in RN . (This should not be confused with xi ∈ RD, which represents the i’th data case.) Similarly, y is a vector in RN . For example, suppose we have N = 3 examples in D = 2 dimensions: ⎛\n",
      "\n",
      "X =\n",
      "\n",
      "⎝\n",
      "\n",
      "1 2 1 −2 2 1\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠ , y =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "8.8957 0.6130 1.7761\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "(7.17)\n",
      "\n",
      "These vectors are illustrated in Figure 7.3.\n",
      "\n",
      "We seek a vector ˆy ∈ RN that lies in this linear subspace and is as close as possible to y,\n",
      "\n",
      "i.e., we want to ﬁnd\n",
      "\n",
      "argmin ˆy∈span({˜x1,...,˜xD})\n",
      "\n",
      "(cid:10)y − ˆy(cid:10)2.\n",
      "\n",
      "(7.18)\n",
      "\n",
      "Since ˆy ∈ span(X), there exists some weight vector w such that\n",
      "\n",
      "ˆy = w1˜x1 + · · · + wD˜xD = Xw\n",
      "\n",
      "(7.19)\n",
      "\n",
      "7.3. Maximum likelihood estimation (least squares)\n",
      "\n",
      "221\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "x1\n",
      "\n",
      "ˆy\n",
      "\n",
      "3 x\n",
      "\n",
      "x2\n",
      "\n",
      "0.4\n",
      "\n",
      "y\n",
      "\n",
      "0.2\n",
      "\n",
      "0 1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "x2\n",
      "\n",
      "(0,0,0)\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "x1\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "Figure 7.3 Graphical interpretation of least squares for N = 3 examples and D = 2 features. ˜x1 and ˜x2 are vectors in R3; together they deﬁne a 2D plane. y is also a vector in R3 but does not lie on this 2D plane. The orthogonal projection of y onto this plane is denoted ˆy. The red line from y to ˆy is the residual, whose norm we want to minimize. For visual clarity, all vectors have been converted to unit norm. Figure generated by leastSquaresProjection.\n",
      "\n",
      "To minimize the norm of the residual, y − ˆy, we want the residual vector to be orthogonal to every column of X, so ˜xT\n",
      "\n",
      "j (y − ˆy) = 0 for j = 1 :D . Hence\n",
      "\n",
      "j (y − ˆy) = 0 ⇒ XT (y − Xw) = 0 ⇒ w = (XT X)−1XT y ˜xT\n",
      "\n",
      "(7.20)\n",
      "\n",
      "Hence our projected value of y is given by\n",
      "\n",
      "ˆy = X ˆw = X(XT X)−1XT y\n",
      "\n",
      "(7.21)\n",
      "\n",
      "This corresponds to an orthogonal projection of y onto the column space of X. The projection matrix P (cid:2) X(XT X)−1XT is called the hat matrix, since it “puts the hat on y”.\n",
      "\n",
      "7.3.3\n",
      "\n",
      "Convexity\n",
      "\n",
      "When discussing least squares, we noted that the NLL had a bowl shape with a unique minimum. The technical term for functions like this is convex. Convex functions play a very important role in machine learning.\n",
      "\n",
      "Let us deﬁne this concept more precisely. We say a set S is convex if for any θ, θ(cid:4) ∈ S, we\n",
      "\n",
      "have\n",
      "\n",
      "λθ + (1− λ)θ(cid:4) ∈ S, ∀ λ ∈ [0, 1]\n",
      "\n",
      "(7.22)\n",
      "\n",
      "222\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7.4\n",
      "\n",
      "(a) Illustration of a convex set. (b) Illustration of a nonconvex set.\n",
      "\n",
      "λ\n",
      "\n",
      "1 − λ\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(a) Illustration of a convex function. We see that the chord joining (x, f (x)) to (y, f (y)) lies Figure 7.5 above the function. (b) A function that is neither convex nor concave. A is a local minimum, B is a global minimum. Figure generated by convexFnHand.\n",
      "\n",
      "That is, if we draw a line from θ to θ(cid:4) for an illustration of a convex set, and Figure 7.4(b) for an illustration of a non-convex set.\n",
      "\n",
      "A function f (θ) is called convex if its epigraph (the set of points above the function) deﬁnes a convex set. Equivalently, a function f (θ) is called convex if it is deﬁned on a convex set and if, for any θ, θ(cid:4) ∈ S, and for any 0 ≤ λ ≤ 1, we have f (λθ + (1 − λ)θ(cid:4)) ≤ λf (θ) + (1 − λ)f (θ(cid:4))\n",
      "\n",
      ", all points on the line lie inside the set. See Figure 7.4(a)\n",
      "\n",
      "(7.23)\n",
      "\n",
      "See Figure 7.5 for a 1d example. A function is called strictly convex if the inequality is strict. A function f (θ) is concave if −f (θ) is convex. Examples of scalar convex functions include θ2, eθ, and θ log θ (for θ > 0). Examples of scalar concave functions include log(θ) and\n",
      "\n",
      "√\n",
      "\n",
      "θ.\n",
      "\n",
      "Intuitively, a (strictly) convex function has a “bowl shape”, and hence has a unique global minimum θ∗ corresponding to the bottom of the bowl. Hence its second derivative must be positive everywhere, d dθ f (θ) > 0. A twice-continuously differentiable, multivariate function f is convex iff its Hessian is positive deﬁnite for all θ.1 In the machine learning context, the function f often corresponds to the NLL.\n",
      "\n",
      "1. Recall that the Hessian is the matrix of second partial derivatives, deﬁned by Hjk = matrix H is positive deﬁnite iff vT Hv > 0 for any non-zero vector v.\n",
      "\n",
      "∂f 2(θ) ∂θj ∂θk\n",
      "\n",
      ". Also, recall that a\n",
      "\n",
      "7.4. Robust linear regression *\n",
      "\n",
      "223\n",
      "\n",
      "Linear data with noise and outliers\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "least squares laplace\n",
      "\n",
      "4.5\n",
      "\n",
      "4\n",
      "\n",
      "L2 L1 huber\n",
      "\n",
      "2\n",
      "\n",
      "3.5\n",
      "\n",
      "1\n",
      "\n",
      "3\n",
      "\n",
      "0\n",
      "\n",
      "2.5\n",
      "\n",
      "−1\n",
      "\n",
      "2\n",
      "\n",
      "−2\n",
      "\n",
      "1.5\n",
      "\n",
      "−3\n",
      "\n",
      "1\n",
      "\n",
      "−4\n",
      "\n",
      "0.5\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "−6\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "−0.5\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7.6 (a) Illustration of robust linear regression. Figure generated by linregRobustDemoCombined. (b) Illustration of (cid:7)2, (cid:7)1, and Huber loss functions. Figure generated by huberLossDemo.\n",
      "\n",
      "Models where the NLL is convex are desirable, since this means we can always ﬁnd the globally optimal MLE. We will see many examples of this later in the book. However, many models of interest will not have concave likelihoods. In such cases, we will discuss ways to derive locally optimal parameter estimates.\n",
      "\n",
      "7.4\n",
      "\n",
      "Robust linear regression *\n",
      "\n",
      "It is very common to model the noise in regression models using a Gaussian distribution with zero mean and constant variance, (cid:8)i ∼ N (0, σ2), where (cid:8)i = yi − wT xi. In this case, maximizing likelihood is equivalent to minimizing the sum of squared residuals, as we have seen. However, if we have outliers in our data, this can result in a poor ﬁt, as illustrated in Figure 7.6(a). (The outliers are the points on the bottom of the ﬁgure.) This is because squared error penalizes deviations quadratically, so points far from the line have more affect on the ﬁt than points near to the line.\n",
      "\n",
      "One way to achieve robustness to outliers is to replace the Gaussian distribution for the response variable with a distribution that has heavy tails. Such a distribution will assign higher likelihood to outliers, without having to perturb the straight line to “explain” them.\n",
      "\n",
      "One possibility is to use the Laplace distribution, introduced in Section 2.4.3. If we use this\n",
      "\n",
      "as our observation model for regression, we get the following likelihood:\n",
      "\n",
      "p(y|x, w, b) = Lap(y|wT x, b) ∝ exp(− 1 b\n",
      "\n",
      "|y − wT x|)\n",
      "\n",
      "(7.24)\n",
      "\n",
      "The robustness arises from the use of |y − wT x| instead of (y − wT x)2. For simplicity, we will assume b is ﬁxed. Let ri (cid:2) yi − wT xi be the i’th residual. The NLL has the form\n",
      "\n",
      "(cid:6)(w) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "|ri(w)|\n",
      "\n",
      "(7.25)\n",
      "\n",
      "i\n",
      "\n",
      "224\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "Likelihood Gaussian Gaussian Gaussian Laplace Student\n",
      "\n",
      "Prior Uniform Least squares Ridge Gaussian Laplace Lasso Uniform Robust regression Uniform Robust regression\n",
      "\n",
      "Name\n",
      "\n",
      "Section 7.3 7.5 13.3 7.4 Exercise 11.12\n",
      "\n",
      "Table 7.1 Summary of various likelihoods and priors used for linear regression. The likelihood refers to the distributional form of p(y|x, w, σ2), and the prior refers to the distributional form of p(w). MAP estimation with a uniform distribution corresponds to MLE.\n",
      "\n",
      "Unfortunately, this is a non-linear objective function, which is hard to optimize. Fortunately, we can convert the NLL to a linear objective, subject to linear constraints, using the following split variable trick. First we deﬁne i − r−\n",
      "\n",
      "ri (cid:2) r+\n",
      "\n",
      "i\n",
      "\n",
      "(7.26)\n",
      "\n",
      "and then we impose the linear inequality constraints that r+ constrained objective becomes\n",
      "\n",
      "min w,r+,r−\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "(r+\n",
      "\n",
      "i − r− i )\n",
      "\n",
      "s.t.\n",
      "\n",
      "i ≥ 0, r− r+\n",
      "\n",
      "i ≥ 0, wT xi + r+\n",
      "\n",
      "i ≥ 0 and r−\n",
      "\n",
      "i + r−\n",
      "\n",
      "i = yi\n",
      "\n",
      "i ≥ 0. Now the\n",
      "\n",
      "(7.27)\n",
      "\n",
      "This is an example of a linear program with D + 2N unknowns and 3N constraints.\n",
      "\n",
      "Since this is a convex optimization problem, it has a unique solution. To solve an LP, we must\n",
      "\n",
      "ﬁrst write it in standard form, which as follows:\n",
      "\n",
      "min θ\n",
      "\n",
      "f T θ s.t. Aθ ≤ b, Aeqθ = beq, l ≤ θ ≤ u\n",
      "\n",
      "(7.28)\n",
      "\n",
      "In our current example, θ = (w, r+, r−), f = [0, 1, 1], A = [], b = [], Aeq = [X, I, −I], beq = y, l = [−∞1, 0, 0], u = []. This can be solved by any LP solver (see e.g., (Boyd and Vandenberghe 2004)). See Figure 7.6(a) for an example of the method in action.\n",
      "\n",
      "An alternative to using NLL under a Laplace likelihood is to minimize the Huber loss function\n",
      "\n",
      "(Huber 1964), deﬁned as follows: (cid:26)\n",
      "\n",
      "LH (r, δ) =\n",
      "\n",
      "if |r| ≤ δ δ|r| −δ 2/2 if |r| > δ\n",
      "\n",
      "r2/2\n",
      "\n",
      "(7.29)\n",
      "\n",
      "This is equivalent to (cid:6)2 for errors that are smaller than δ, and is equivalent to (cid:6)1 for larger errors. See Figure 7.6(b). The advantage of this loss function is that it is everywhere differentiable, using the fact that d dr |r| = sign(r) if r (cid:4)= 0. We can also check that the function is C1 continuous, since the gradients of the two parts of the function match at r = ±δ, namely d dr LH (r, δ)|r=δ = δ. Consequently optimizing the Huber loss is much faster than using the Laplace likelihood, since we can use standard smooth optimization methods (such as quasi- Newton) instead of linear programming.\n",
      "\n",
      "similiar to the probabilistic methods. probabilistic interpretation, although it is rather unnatural (Pontil et al. 1998).)\n",
      "\n",
      "Figure 7.6(a) gives an illustration of the Huber loss function. The results are qualitatively (In fact, it turns out that the Huber method also has a\n",
      "\n",
      "7.5. Ridge regression\n",
      "\n",
      "225\n",
      "\n",
      "20\n",
      "\n",
      "ln lambda −20.135\n",
      "\n",
      "20\n",
      "\n",
      "ln lambda −8.571\n",
      "\n",
      "15\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "−5\n",
      "\n",
      "−10\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "−15\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7.7 Degree 14 Polynomial ﬁt to N = 21 data points with increasing amounts of (cid:7)2 regularization. Data was generated from noise with variance σ2 = 4. The error bars, representing the noise variance σ2, get wider as the ﬁt gets smoother, since we are ascribing more of the data variation to the noise. Figure generated by linregPolyVsRegDemo.\n",
      "\n",
      "7.5\n",
      "\n",
      "Ridge regression\n",
      "\n",
      "One problem with ML estimation is that it can result in overﬁtting. In this section, we discuss a way to ameliorate this problem by using MAP estimation with a Gaussian prior. For simplicity, we assume a Gaussian likelihood, rather than a robust likelihood.\n",
      "\n",
      "7.5.1\n",
      "\n",
      "Basic idea\n",
      "\n",
      "The reason that the MLE can overﬁt is that it is picking the parameter values that are the best for modeling the training data; but if the data is noisy, such parameters often result in complex functions. As a simple example, suppose we ﬁt a degree 14 polynomial to N = 21 data points using least squares. The resulting curve is very “wiggly”, as shown in Figure 7.7(a). The corresponding least squares coefficients (excluding w0) are as follows:\n",
      "\n",
      "6.560, -36.934, -109.255, 543.452, 1022.561, -3046.224, -3768.013, 8524.540, 6607.897, -12640.058, -5530.188, 9479.730, 1774.639, -2821.526\n",
      "\n",
      "We see that there are many large positive and negative numbers. These balance out exactly to make the curve “wiggle” in just the right way so that it almost perfectly interpolates the data. But this situation is unstable: if we changed the data a little, the coefficients would change a lot. We can encourage the parameters to be small, thus resulting in a smoother curve, by using a\n",
      "\n",
      "zero-mean Gaussian prior: (cid:27)\n",
      "\n",
      "p(w) =\n",
      "\n",
      "N (wj|0, τ 2)\n",
      "\n",
      "(7.30)\n",
      "\n",
      "j\n",
      "\n",
      "where 1/τ 2 controls the strength of the prior. The corresponding MAP estimation problem becomes\n",
      "\n",
      "argmax w\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "log N (yi|w0 + wT xi, σ2) +\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "log N (wj|0, τ 2)\n",
      "\n",
      "(7.31)\n",
      "\n",
      "226\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "25\n",
      "\n",
      "train mse test mse\n",
      "\n",
      "mean squared error\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "negative log marg. likelihood CV estimate of MSE\n",
      "\n",
      "20\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "15\n",
      "\n",
      "0.5\n",
      "\n",
      "10\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "5\n",
      "\n",
      "0.2\n",
      "\n",
      "0 −25\n",
      "\n",
      "−20\n",
      "\n",
      "−15\n",
      "\n",
      "−10 log lambda\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "0.1\n",
      "\n",
      "−20\n",
      "\n",
      "−15\n",
      "\n",
      "−10\n",
      "\n",
      "log lambda\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7.8 (a) Training error (dotted blue) and test error (solid red) for a degree 14 polynomial ﬁt by ridge regression, plotted vs log(λ). Data was generated from noise with variance σ2 = 4 (training set has size N = 21). Note: Models are ordered from complex (small regularizer) on the left to simple (large regularizer) on the right. The stars correspond to the values used to plot the functions in Figure 7.7. (b) Estimate of performance using training set. Dotted blue: 5-fold cross-validation estimate of future MSE. Solid black: negative log marginal likelihood, − log p(D|λ). Both curves have been vertically rescaled to [0,1] to make them comparable. Figure generated by linregPolyVsRegDemo.\n",
      "\n",
      "It is a simple exercise to show that this is equivalent to minimizing the following:\n",
      "\n",
      "J(w) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(yi − (w0 + wT xi))2 + λ||w||2 2\n",
      "\n",
      "i=1\n",
      "\n",
      "(7.32)\n",
      "\n",
      "where λ (cid:2) σ2/τ 2 and ||w||2 j = wT w is the squared two-norm. Here the ﬁrst term is the MSE/ NLL as usual, and the second term, λ ≥ 0, is a complexity penalty. The corresponding solution is given by\n",
      "\n",
      "2 =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "j w2\n",
      "\n",
      "ˆwridge = (λID + XT X)−1XT y\n",
      "\n",
      "(7.33)\n",
      "\n",
      "This technique is known as ridge regression, or penalized least squares. In general, adding a Gaussian prior to the parameters of a model to encourage them to be small is called (cid:6)2 regularization or weight decay. Note that the offset term w0 is not regularized, since this just affects the height of the function, not its complexity. By penalizing the sum of the magnitudes of the weights, we ensure the function is simple (since w = 0 corresponds to a straight line, which is the simplest possible function, corresponding to a constant.)\n",
      "\n",
      "We illustrate this idea in Figure 7.7, where we see that increasing λ results in smoother functions. The resulting coefficients also become smaller. For example, using λ = 10−3, we have\n",
      "\n",
      "7.5. Ridge regression\n",
      "\n",
      "227\n",
      "\n",
      "2.128, 0.807, 16.457, 3.704, -24.948, -10.472, -2.625, 4.360, 13.711, 10.063, 8.716, 3.966, -9.349, -9.232\n",
      "\n",
      "In Figure 7.8(a), we plot the MSE on the training and test sets vs log(λ). We see that, as we increase λ (so the model becomes more constrained), the error on the training set increases. For the test set, we see the characteristic U-shaped curve, where the model overﬁts and then In underﬁts. Section 1.4.8, we will discuss a more probabilistic approach.\n",
      "\n",
      "It is common to use cross validation to pick λ, as shown in Figure 7.8(b).\n",
      "\n",
      "We will consider a variety of different priors in this book. Each of these corresponds to a\n",
      "\n",
      "different form of regularization. This technique is very widely used to prevent overﬁtting.\n",
      "\n",
      "7.5.2\n",
      "\n",
      "Numerically stable computation *\n",
      "\n",
      "Interestingly, ridge regression, which works better statistically, is also easier to ﬁt numerically, since (λID + XT X) is much better conditioned (and hence more likely to be invertible) than XT X, at least for suitable largy λ.\n",
      "\n",
      "Nevertheless, inverting matrices is still best avoided, for reasons of numerical stability. (Indeed, if you write w=inv(X’ * X)*X’*y in Matlab, it will give you a warning.) We now describe a useful trick for ﬁtting ridge regression models (and hence by extension, computing vanilla OLS estimates) that is more numerically robust. We assume the prior has the form p(w) = N (0, Λ−1), where Λ is the precision matrix. In the case of ridge regression, Λ = (1/τ 2)I. To avoid penalizing the w0 term, we should center the data ﬁrst, as explained in Exercise 7.5. First let us augment the original data with some “virtual data” coming from the prior:\n",
      "\n",
      "where Λ = where the extra rows represent pseudo-data from the prior.\n",
      "\n",
      "˜X =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "X/σ√ Λ √\n",
      "\n",
      "Λ\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "√\n",
      "\n",
      ",\n",
      "\n",
      "Λ\n",
      "\n",
      "T\n",
      "\n",
      "˜y =\n",
      "\n",
      "is a Cholesky decomposition of Λ. We see that ˜X is (N + D) × D,\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "y/σ 0D×1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(7.34)\n",
      "\n",
      "We now show that the NLL on this expanded data is equivalent to penalized NLL on the\n",
      "\n",
      "original data:\n",
      "\n",
      "f (w) = (˜y − ˜Xw)T (˜y − ˜Xw) y/σ 0\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "(cid:8)(cid:8)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 σ (y − Xw) √\n",
      "\n",
      "−\n",
      "\n",
      "Λw\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "−\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "X/σ√ Λ (cid:9)T (cid:8)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "1 σ (y − Xw) √\n",
      "\n",
      "w\n",
      "\n",
      "−\n",
      "\n",
      "(cid:9)T (cid:8)(cid:8)\n",
      "\n",
      "Λw\n",
      "\n",
      "y/σ 0 (cid:9)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "−\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "X/σ√ Λ\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "w\n",
      "\n",
      "(7.35)\n",
      "\n",
      "(7.36)\n",
      "\n",
      "(7.37)\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "1 σ2 (y − Xw)T (y − Xw) + ( 1 σ2 (y − Xw)T (y − Xw) +w T Λw\n",
      "\n",
      "√\n",
      "\n",
      "Λw)T (\n",
      "\n",
      "√\n",
      "\n",
      "Λw)\n",
      "\n",
      "(7.38)\n",
      "\n",
      "(7.39)\n",
      "\n",
      "Hence the MAP estimate is given by\n",
      "\n",
      "ˆwridge = ( ˜XT ˜X)−1 ˜XT ˜y\n",
      "\n",
      "(7.40)\n",
      "\n",
      "as we claimed.\n",
      "\n",
      "228\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "Now let\n",
      "\n",
      "˜X = QR\n",
      "\n",
      "(7.41)\n",
      "\n",
      "be the QR decomposition of X, where Q is orthonormal (meaning QT Q = QQT = I), and R is upper triangular. Then\n",
      "\n",
      "( ˜XT ˜X)−1 = (RT QT QR)−1 = (RT R)−1 = R−1R−T\n",
      "\n",
      "(7.42)\n",
      "\n",
      "Hence\n",
      "\n",
      "ˆwridge = R−1R−T RT QT ˜y = R−1Q˜y\n",
      "\n",
      "(7.43)\n",
      "\n",
      "Note that R is easy to invert since it is upper triangular. This gives us a way to compute the ridge estimate while avoiding having to invert (Λ + XT X).\n",
      "\n",
      "We can use this technique to ﬁnd the MLE, by simply computing the QR decomposition of the unaugmented matrix X, and using the original y. This is the method of choice for solving least squares problems. (In fact, it is so sommon that it can be implemented in one line of Matlab, using the backslash operator: w=X\\y.) Note that computing the QR decomposition of an N × D matrix takes O(N D2) time, and is numerically very stable.\n",
      "\n",
      "If D (cid:15) N , we should ﬁrst perform an SVD decomposition. In particular, let X = USVT be the SVD of X, where VT V = IN , UUT = UT U = IN , and S is a diagonal N × N matrix. Now let Z = UD be an N × N matrix. Then we can rewrite the ridge estimate thus:\n",
      "\n",
      "ˆwridge = V(ZT Z + λIN )−1ZT y\n",
      "\n",
      "(7.44)\n",
      "\n",
      "In other words, we can replace the D-dimensional vectors xi with the N -dimensional vectors zi and perform our penalized ﬁt as before. We then transform the N -dimensional solution to the D-dimensional solution by multiplying by V. Geometrically, we are rotating to a new coordinate system in which all but the ﬁrst N coordinates are zero. This does not affect the solution since the spherical Gaussian prior is rotationally invariant. The overall time is now O(DN 2) operations.\n",
      "\n",
      "7.5.3\n",
      "\n",
      "Connection with PCA *\n",
      "\n",
      "In this section, we discuss an interesting connection between ridge regression and PCA (Sec- tion 12.2), which gives further insight into why ridge regression works well. Our discussion is based on (Hastie et al. 2009, p66).\n",
      "\n",
      "Let X = USVT be the SVD of X. From Equation 7.44, we have\n",
      "\n",
      "ˆwridge = V(S2 + λI)−1SUT y\n",
      "\n",
      "(7.45)\n",
      "\n",
      "Hence the ridge predictions on the training set are given by\n",
      "\n",
      "ˆy = X ˆwridge = USVT V(S2 + λI)−1SUT y\n",
      "\n",
      "(7.46)\n",
      "\n",
      "= U˜SUT y =\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "uj ˜SjjuT\n",
      "\n",
      "j y\n",
      "\n",
      "(7.47)\n",
      "\n",
      "j=1\n",
      "\n",
      "7.5. Ridge regression\n",
      "\n",
      "229\n",
      "\n",
      "u 2\n",
      "\n",
      "ML Estimate\n",
      "\n",
      "u 1\n",
      "\n",
      "MAP Estimate\n",
      "\n",
      "prior mean\n",
      "\n",
      "Figure 7.9 Geometry of ridge regression. The likelihood is shown as an ellipse, and the prior is shown as a circle centered on the origin. Based on Figure 3.15 of (Bishop 2006b). Figure generated by geomRidge\n",
      "\n",
      "where\n",
      "\n",
      "˜Sjj (cid:2) [S(S2 + λI)−1S]jj =\n",
      "\n",
      "σ2 j σ2 j + λ\n",
      "\n",
      "(7.48)\n",
      "\n",
      "and σj are the singular values of X. Hence\n",
      "\n",
      "ˆy = X ˆwridge =\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "uj\n",
      "\n",
      "σ2 j σ2 j + λ\n",
      "\n",
      "uT\n",
      "\n",
      "j y\n",
      "\n",
      "(7.49)\n",
      "\n",
      "In contrast, the least squares prediction is\n",
      "\n",
      "ˆy = X ˆwls = (USVT )(VS−1UT y) = UUT y =\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "ujuT\n",
      "\n",
      "j y\n",
      "\n",
      "(7.50)\n",
      "\n",
      "j=1\n",
      "\n",
      "If σ2 view of this, we deﬁne the effective number of degrees of freedom of the model as follows:\n",
      "\n",
      "j is small compared to λ, then direction uj will not have much effect on the prediction. In\n",
      "\n",
      "dof(λ) =\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "σ2 j σ2 j + λ\n",
      "\n",
      "(7.51)\n",
      "\n",
      "When λ = 0, dof(λ) =D , and as λ → ∞, dof(λ) → 0.\n",
      "\n",
      "In Section 7.6, we show that cov [w|D] =σ 2(XT X)−1, if we use a uniform prior for w. Thus the directions in which we are most uncertain about w are determined by the eigenvectors of this matrix with the smallest eigenvalues, as shown in Figure 4.1. Furthermore, in Section 12.2.3, we show that the squared singular values σ2 j are equal to the eigenvalues of XT X. Hence small singular values σj correspond to directions with high posterior variance. It is these directions which ridge shrinks the most.\n",
      "\n",
      "Let us try to understand why this behavior is desirable.\n",
      "\n",
      "230\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "This process is illustrated in Figure 7.9. The horizontal w1 parameter is not-well determined by the data (has high posterior variance), but the vertical w2 parameter is well-determined. Hence wmap is shifted strongly towards the prior mean, which is 0. (Compare to Figure 4.14(c), which illustrated sensor fusion with sensors of different reliabilities.) In this way, ill-determined parameters are reduced in size towards 0. This is called shrinkage.\n",
      "\n",
      "2\n",
      "\n",
      "is close to ˆwmle\n",
      "\n",
      "2\n",
      "\n",
      ", but wmap\n",
      "\n",
      "1\n",
      "\n",
      "There is a related, but different, technique called principal components regression. The idea is this: ﬁrst use PCA to reduce the dimensionality to K dimensions, and then use these low dimensional features as input to regression. However, this technique does not work as well as ridge in terms of predictive accuracy (Hastie et al. 2001, p70). The reason is that in PC regression, only the ﬁrst K (derived) dimensions are retained, and the remaining D − K dimensions are entirely ignored. By contrast, ridge regression uses a “soft” weighting of all the dimensions.\n",
      "\n",
      "7.5.4\n",
      "\n",
      "Regularization effects of big data\n",
      "\n",
      "Regularization is the most common way to avoid overﬁtting. However, another effective approach — which is not always available — is to use lots of data. It should be intuitively obvious that the more training data we have, the better we will be able to learn.2 So we expect the test set error to decrease to some plateau as N increases.\n",
      "\n",
      "This is illustrated in Figure 7.10, where we plot the mean squared error incurred on the test set achieved by polynomial regression models of different degrees vs N (a plot of error vs training set size is known as a learning curve). The level of the plateau for the test error consists of two terms: an irreducible component that all models incur, due to the intrinsic variability of the generating process (this is called the noise ﬂoor); and a component that depends on the discrepancy between the generating process (the “truth”) and the model: this is called structural error.\n",
      "\n",
      "In Figure 7.10, the truth is a degree 2 polynomial, and we try ﬁtting polynomials of degrees 1, 2 and 25 to this data. Call the 3 models M1, M2 and M25. We see that the structural error for models M2 and M25 is zero, since both are able to capture the true generating process. However, the structural error for M1 is substantial, which is evident from the fact that the plateau occurs high above the noise ﬂoor.\n",
      "\n",
      "For any model that is expressive enough to capture the truth (i.e., one with small structural error), the test error will go to the noise ﬂoor as N → ∞. However, it will typically go to zero faster for simpler models, since there are fewer parameters to estimate. In particular, for ﬁnite training sets, there will be some discrepancy between the parameters that we estimate and the best parameters that we could estimate given the particular model class. This is called approximation error, and goes to zero as N → ∞, but it goes to zero faster for simpler models. This is illustrated in Figure 7.10. See also Exercise 7.1.\n",
      "\n",
      "In domains with lots of data, simple methods can work surprisingly well (Halevy et al. 2009). However, there are still reasons to study more sophisticated learning methods, because there will always be problems for which we have little data. For example, even in such a data-rich domain as web search, as soon as we want to start personalizing the results, the amount of data available for any given user starts to look small again (relative to the complexity of the problem).\n",
      "\n",
      "2. This assumes the training data is randomly sampled, and we don’t just get repetitions of the same examples. Having informatively sampled data can help even more; this is the motivation for an approach known as active learning, where you get to choose your training data.\n",
      "\n",
      "7.6. Bayesian linear regression\n",
      "\n",
      "231\n",
      "\n",
      "22\n",
      "\n",
      "truth=degree 2, model = degree 1\n",
      "\n",
      "22\n",
      "\n",
      "truth=degree 2, model = degree 2\n",
      "\n",
      "20\n",
      "\n",
      "train test\n",
      "\n",
      "20\n",
      "\n",
      "train test\n",
      "\n",
      "18\n",
      "\n",
      "18\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "14\n",
      "\n",
      "14\n",
      "\n",
      "e s m\n",
      "\n",
      "12\n",
      "\n",
      "10\n",
      "\n",
      "e s m\n",
      "\n",
      "12\n",
      "\n",
      "10\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80 120 100 size of training set\n",
      "\n",
      "140\n",
      "\n",
      "160\n",
      "\n",
      "180\n",
      "\n",
      "200\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80 120 100 size of training set\n",
      "\n",
      "140\n",
      "\n",
      "160\n",
      "\n",
      "180\n",
      "\n",
      "200\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "truth=degree 2, model = degree 10\n",
      "\n",
      "truth=degree 2, model = degree 25\n",
      "\n",
      "22\n",
      "\n",
      "22\n",
      "\n",
      "20\n",
      "\n",
      "train test\n",
      "\n",
      "20\n",
      "\n",
      "train test\n",
      "\n",
      "18\n",
      "\n",
      "18\n",
      "\n",
      "16\n",
      "\n",
      "16\n",
      "\n",
      "14\n",
      "\n",
      "14\n",
      "\n",
      "e s m\n",
      "\n",
      "12\n",
      "\n",
      "10\n",
      "\n",
      "e s m\n",
      "\n",
      "12\n",
      "\n",
      "10\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80 120 100 size of training set\n",
      "\n",
      "140\n",
      "\n",
      "160\n",
      "\n",
      "180\n",
      "\n",
      "200\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80 120 100 size of training set\n",
      "\n",
      "140\n",
      "\n",
      "160\n",
      "\n",
      "180\n",
      "\n",
      "200\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 7.10 MSE on training and test sets vs size of training set, for data generated from a degree 2 polynomial with Gaussian noise of variance σ2 = 4. We ﬁt polynomial models of varying degree to this data. (a) Degree 1. (b) Degree 2. (c) Degree 10. (d) Degree 25. Note that for small training set sizes, the test error of the degree 25 polynomial is higher than that of the degree 2 polynomial, due to overﬁtting, but this difference vanishes once we have enough data. Note also that the degree 1 polynomial is too simple and has high test error even given large amounts of training data. Figure generated by linregPolyVsN.\n",
      "\n",
      "In such cases, we may want to learn multiple related models at the same time, which is known as multi-task learning. This will allow us to “borrow statistical strength” from tasks with lots of data and to share it with tasks with little data. We will discuss ways to do later in the book.\n",
      "\n",
      "7.6\n",
      "\n",
      "Bayesian linear regression\n",
      "\n",
      "Although ridge regression is a useful way to compute a point estimate, sometimes we want to compute the full posterior over w and σ2. For simplicity, we will initially assume the noise variance σ2 is known, so we focus on computing p(w|D, σ2). Then in Section 7.6.3 we consider\n",
      "\n",
      "232\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "the general case, where we compute p(w, σ2|D). We assume throughout a Gaussian likelihood model. Performing Bayesian inference with a robust likelihood is also possible, but requires more advanced techniques (see Exercise 24.5).\n",
      "\n",
      "7.6.1\n",
      "\n",
      "Computing the posterior\n",
      "\n",
      "In linear regression, the likelihood is given by\n",
      "\n",
      "where μ is an offset term. If the inputs are centered, so i xij = 0 for each j, the mean of the output is equally likely to be positive or negative. So let us put an improper prior on μ of the form p(μ) ∝ 1, and then integrate it out to get\n",
      "\n",
      "p(y|X, w, μ, σ2) =N (y|μ + Xw, σ2IN )\n",
      "\n",
      "p(y|X, w, σ2) ∝ exp\n",
      "\n",
      "∝ exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "− 1 2σ2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "− 1 2σ2 (y − μ1N − Xw)T (y − μ1N − Xw) (cid:10)\n",
      "\n",
      "||y − y1N − Xw||2 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(7.52)\n",
      "\n",
      "(7.53)\n",
      "\n",
      "(7.54)\n",
      "\n",
      "where y = 1 N assume the output has been centered, and write y for y − y1N .\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 yi is the empirical mean of the output. For notational simplicity, we shall\n",
      "\n",
      "The conjugate prior to the above Gaussian likelihood is also a Gaussian, which we will denote by p(w) = N (w|w0, V0). Using Bayes rule for Gaussians, Equation 4.125, the posterior is given by\n",
      "\n",
      "p(w|X, y, σ2) ∝ N (w|w0, V0)N (y|Xw, σ2IN ) = N (w|wN , VN ) 1 σ2\n",
      "\n",
      "wN = VN V−1\n",
      "\n",
      "0 w0 +\n",
      "\n",
      "VN XT y\n",
      "\n",
      "(7.55)\n",
      "\n",
      "(7.56)\n",
      "\n",
      "V−1\n",
      "\n",
      "1 N = V−1 σ2 VN = σ2(σ2V−1 0 + XT X)−1\n",
      "\n",
      "0 +\n",
      "\n",
      "XT X\n",
      "\n",
      "(7.57)\n",
      "\n",
      "(7.58)\n",
      "\n",
      "λ = σ2\n",
      "\n",
      "If w0 = 0 and V0 = τ 2I, then the posterior mean reduces to the ridge estimate, if we deﬁne\n",
      "\n",
      "τ 2 . This is because the mean and mode of a Gaussian are the same.\n",
      "\n",
      "To gain insight into the posterior distribution (and not just its mode), let us consider a 1D\n",
      "\n",
      "example:\n",
      "\n",
      "y(x, w) = w0 + w1x + (cid:8)\n",
      "\n",
      "(7.59)\n",
      "\n",
      "where the “true” parameters are w0 = −0.3 and w1 = 0.5. In Figure 7.11 we plot the prior, the likelihood, the posterior, and some samples from the posterior predictive. In particular, the right hand column plots the function y(x, w(s)) where x ranges over [−1, 1], and w(s) ∼ N (w|wN , VN ) is a sample from the parameter posterior. Initially, when we sample from the prior (ﬁrst row), our predictions are “all over the place”, since our prior is uniform. After we see one data point (second row), our posterior becomes constrained by the corresponding likelihood, and our predictions pass close to the observed data. However, we see that the posterior has a ridge-like shape, reﬂecting the fact that there are many possible solutions, with different\n",
      "\n",
      "7.6. Bayesian linear regression\n",
      "\n",
      "233\n",
      "\n",
      "likelihood\n",
      "\n",
      "prior/posterior 1\n",
      "\n",
      "1\n",
      "\n",
      "data space\n",
      "\n",
      "W1\n",
      "\n",
      "0\n",
      "\n",
      "y\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 W0\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 x\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "W1\n",
      "\n",
      "0\n",
      "\n",
      "W1\n",
      "\n",
      "0\n",
      "\n",
      "y\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 W0\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 W0\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 x\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "W1\n",
      "\n",
      "0\n",
      "\n",
      "W1\n",
      "\n",
      "0\n",
      "\n",
      "y\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 W0\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 W0\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 x\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "W1\n",
      "\n",
      "0\n",
      "\n",
      "W1\n",
      "\n",
      "0\n",
      "\n",
      "y\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 W0\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 W0\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0 x\n",
      "\n",
      "1\n",
      "\n",
      "Sequential Bayesian updating of a linear regression model p(y|x) = N (y|w0x0 + w1x1, σ2). Figure 7.11 Row 0 represents the prior, row 1 represents the ﬁrst data point (x1, y1), row 2 represents the second data point (x2, y2), row 3 represents the 20th data point (x20, y20). Left column: likelihood function for current data point. Middle column: posterior given data so far, p(w|x1:n, y1:n) (so the ﬁrst line is the prior). Right column: samples from the current prior/posterior predictive distribution. The white cross in columns 1 and 2 represents the true parameter value; we see that the mode of the posterior rapidly (after 20 samples) converges to this point. The blue circles in column 3 are the observed data points. Based on Figure 3.7 of (Bishop 2006a). Figure generated by bayesLinRegDemo2d.\n",
      "\n",
      "slopes/intercepts. This makes sense since we cannot uniquely infer two parameters from one observation. After we see two data points (third row), the posterior becomes much narrower, and our predictions all have similar slopes and intercepts. After we observe 20 data points (last row), the posterior is essentially a delta function centered on the true value, indicated by a white cross. (The estimate converges to the truth since the data was generated from this model, and because Bayes is a consistent estimator; see Section 6.4.1 for discussion of this point.)\n",
      "\n",
      "7.6.2\n",
      "\n",
      "Computing the posterior predictive\n",
      "\n",
      "It’s tough to make predictions, especially about the future. — Yogi Berra\n",
      "\n",
      "234\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "In machine learning, we often care more about predictions than about interpreting the parame- ters. Using Equation 4.126, we can easily show that the posterior predictive distribution at a test point x is also Gaussian: (cid:28)\n",
      "\n",
      "p(y|x, D, σ2) =\n",
      "\n",
      "N (y|xT w, σ2)N (w|wN , VN )dw\n",
      "\n",
      "(7.60)\n",
      "\n",
      "N x, σ2 = N (y|wT N (x) =σ 2 + xT VN x σ2\n",
      "\n",
      "N (x))\n",
      "\n",
      "(7.61)\n",
      "\n",
      "(7.62)\n",
      "\n",
      "The variance in this prediction, σ2 N (x), depends on two terms: the variance of the observation noise, σ2, and the variance in the parameters, VN . The latter translates into variance about observations in a way which depends on how close x is to the training data D. This is illustrated in Figure 7.12, where we see that the error bars get larger as we move away from the training points, representing increased uncertainty. This is important for applications such as active learning, where we want to model what we don’t know as well as what we do. By contrast, the plugin approximation has constant sized error bars, since\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(y|x, D, σ2) ≈\n",
      "\n",
      "N (y|xT w, σ2)δ ˆw(w)dw = p(y|x, ˆw, σ2)\n",
      "\n",
      "(7.63)\n",
      "\n",
      "See Figure 7.12(a).\n",
      "\n",
      "7.6.3\n",
      "\n",
      "Bayesian inference when σ2 is unknown * In this section, we apply the results in Section 4.6.3 to the problem of computing p(w, σ2|D) for a linear regression model. This generalizes the results from Section 7.6.1 where we assumed σ2 was known. In the case where we use an uninformative prior, we will see some interesting connections to frequentist statistics.\n",
      "\n",
      "7.6.3.1\n",
      "\n",
      "Conjugate prior\n",
      "\n",
      "As usual, the likelihood has the form\n",
      "\n",
      "p(y|X, w, σ2) = N (y|Xw, σ2IN )\n",
      "\n",
      "(7.64)\n",
      "\n",
      "By analogy to Section 4.6.3, one can show that the natural conjugate prior has the following form:\n",
      "\n",
      "p(w, σ2) = NIG(w, σ2|w0, V0, a0, b0)\n",
      "\n",
      "(7.65)\n",
      "\n",
      "(cid:2) N (w|w0, σ2V0)IG(σ2|a0, b0)\n",
      "\n",
      "(7.66)\n",
      "\n",
      "=\n",
      "\n",
      "× exp\n",
      "\n",
      "(2π)D/2|V0| 1 (cid:29)\n",
      "\n",
      "2 Γ(a0) − (w − w0)T V−1 0 (w − w0) + 2b0 2σ2\n",
      "\n",
      "ba0 0\n",
      "\n",
      "(σ2)−(a0+(D/2)+1)\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "(7.67)\n",
      "\n",
      "(7.68)\n",
      "\n",
      "7.6. Bayesian linear regression\n",
      "\n",
      "235\n",
      "\n",
      "60\n",
      "\n",
      "plugin approximation (MLE)\n",
      "\n",
      "80\n",
      "\n",
      "Posterior predictive (known variance)\n",
      "\n",
      "prediction training data\n",
      "\n",
      "70\n",
      "\n",
      "prediction training data\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0 −8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "−10\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "50\n",
      "\n",
      "functions sampled from plugin approximation to posterior\n",
      "\n",
      "100\n",
      "\n",
      "functions sampled from posterior\n",
      "\n",
      "45\n",
      "\n",
      "80\n",
      "\n",
      "40\n",
      "\n",
      "35\n",
      "\n",
      "60\n",
      "\n",
      "30\n",
      "\n",
      "25\n",
      "\n",
      "40\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "0 −8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "−20\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 7.12 (a) Plug-in approximation to predictive density (we plug in the MLE of the parameters). (b) Posterior predictive density, obtained by integrating out the parameters. Black curve is posterior mean, error bars are 2 standard deviations of the posterior predictive density. (c) 10 samples from the plugin approximation to posterior predictive. (d) 10 samples from the posterior predictive. Figure generated by linregPostPredDemo.\n",
      "\n",
      "With this prior and likelihood, one can show that the posterior has the following form: p(w, σ2|D) = NIG(w, σ2|wN , VN , aN , bN )\n",
      "\n",
      "(7.69)\n",
      "\n",
      "wN = VN (V−1 VN = (V−1 aN = a0 + n/2 (cid:3)\n",
      "\n",
      "bN = b0 +\n",
      "\n",
      "0 + XT X)−1\n",
      "\n",
      "1 2\n",
      "\n",
      "0 w0 + XT y)\n",
      "\n",
      "wT\n",
      "\n",
      "0 V−1\n",
      "\n",
      "0 w0 + yT y − wT\n",
      "\n",
      "N V−1\n",
      "\n",
      "N wN\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(7.70)\n",
      "\n",
      "(7.71)\n",
      "\n",
      "(7.72)\n",
      "\n",
      "(7.73)\n",
      "\n",
      "The expressions for wN and VN are similar to the case where σ2 is known. The expression for aN is also intuitive, since it just updates the counts. The expression for bN can be interpreted\n",
      "\n",
      "236\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "as follows: term due to the error in the prior on w. The posterior marginals are as follows: p(σ2|D) = IG(aN , bN ) bN aN\n",
      "\n",
      "p(w|D) =T\n",
      "\n",
      "it is the prior sum of squares, b0, plus the empirical sum of squares, yT y, plus a\n",
      "\n",
      "(wN ,\n",
      "\n",
      "VN , 2aN )\n",
      "\n",
      "(7.74)\n",
      "\n",
      "(7.75)\n",
      "\n",
      "We give a worked example of using these equations in Section 7.6.3.3.\n",
      "\n",
      "In particular, given m new test inputs ˜X, we have\n",
      "\n",
      "By analogy to Section 4.6.3.6, the posterior predictive distribution is a Student T distribution.\n",
      "\n",
      "p(˜y| ˜X, D) =T\n",
      "\n",
      "(˜y| ˜XwN ,\n",
      "\n",
      "bN aN\n",
      "\n",
      "(Im + ˜XVN ˜XT ), 2aN )\n",
      "\n",
      "(7.76)\n",
      "\n",
      "The predictive variance has two components: (bN /aN )Im due to the measurement noise, and (bN /aN ) ˜XVN ˜XT due to the uncertainty in w. This latter terms varies depending on how close the test inputs are to the training data.\n",
      "\n",
      "It is common to set a0 = b0 = 0, corresponding to an uninformative prior for σ2, and to set w0 = 0 and V0 = g(XT X)−1 for any positive value g. This is called Zellner’s g-prior (Zellner 1986). Here g plays a role analogous to 1/λ in ridge regression. However, the prior covariance is proportional to (XT X)−1 rather than I. This ensures that the posterior is invariant to scaling of the inputs (Minka 2000b). See also Exercise 7.10.\n",
      "\n",
      "We will see below that if we use an uninformative prior, the posterior precision given N measurements is V−1 N = XT X. The unit information prior is deﬁned to contain as much information as one sample (Kass and Wasserman 1995). To create a unit information prior for linear regression, we need to use V−1 N XT X, which is equivalent to the g-prior with g = N .\n",
      "\n",
      "0 = 1\n",
      "\n",
      "7.6.3.2\n",
      "\n",
      "Uninformative prior\n",
      "\n",
      "An uninformative prior can be obtained by considering the uninformative limit of the conjugate g-prior, which corresponds to setting g = ∞. This is equivalent to an improper NIG prior with w0 = 0, V0 = ∞I, a0 = 0 and b0 = 0, which gives p(w, σ2) ∝ σ−(D+2).\n",
      "\n",
      "Alternatively, we can start with the semi-conjugate prior p(w, σ2) =p( w)p(σ2), and take each term to its uninformative limit individually, which gives p(w, σ2) ∝ σ−2. This is equivalent to an improper NIG prior with w0 = 0,V = ∞I, a0 = −D/2 and b0 = 0. The corresponding posterior is given by\n",
      "\n",
      "p(w, σ2|D) = NIG(w, σ2|wN , VN , aN , bN ) wN = ˆwmle = (XT X)−1XT y VN = (XT X)−1\n",
      "\n",
      "(7.77)\n",
      "\n",
      "(7.78)\n",
      "\n",
      "(7.79)\n",
      "\n",
      "aN =\n",
      "\n",
      "bN =\n",
      "\n",
      "s2 (cid:2) (y − X ˆwmle)T (y − X ˆwmle\n",
      "\n",
      "N − D\n",
      "\n",
      "s2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "(7.80)\n",
      "\n",
      "(7.81)\n",
      "\n",
      "(7.82)\n",
      "\n",
      "7.6. Bayesian linear regression\n",
      "\n",
      "237\n",
      "\n",
      "wj w0 w1 w2 w3 w4 w5 w6 w7 w8 w9 w10\n",
      "\n",
      "E [wj|D] 10.998 -0.004 -0.054 0.068 -1.294 0.232 -0.357 -0.237 0.181 -1.285 -0.433\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "var [wj|D]\n",
      "\n",
      "3.06027 0.00156 0.02190 0.09947 0.56381 0.10438 1.56646 1.00601 0.23672 0.86485 0.73487\n",
      "\n",
      "95% CI [4.652, 17.345] [-0.008, -0.001] [-0.099, -0.008] [-0.138, 0.274] [-2.463, -0.124] [0.015, 0.448] [-3.605, 2.892] [-2.324, 1.849] [-0.310, 0.672] [-3.079, 0.508] [-1.957, 1.091]\n",
      "\n",
      "sig * * *\n",
      "\n",
      "*\n",
      "\n",
      "Table 7.2 Posterior mean, standard deviation and credible intervals for a linear regression model with an uninformative prior ﬁt to the caterpillar data. Produced by linregBayesCaterpillar.\n",
      "\n",
      "The marginal distribution of the weights is given by\n",
      "\n",
      "p(w|D) = T (w| ˆw,\n",
      "\n",
      "s2 N − D\n",
      "\n",
      "C, N − D)\n",
      "\n",
      "(7.83)\n",
      "\n",
      "where C = (XT X)−1 and ˆw is the MLE. We discuss the implications of these equations below.\n",
      "\n",
      "7.6.3.3\n",
      "\n",
      "An example where Bayesian and frequentist inference coincide *\n",
      "\n",
      "The use of a (semi-conjugate) uninformative prior is interesting because the resulting posterior turns out to be equivalent to the results from frequentist statistics (see also Section 4.6.3.9). In particular, from Equation 7.83 we have\n",
      "\n",
      "p(wj|D) = T (wj| ˆwj,\n",
      "\n",
      "Cjjs2 N − D\n",
      "\n",
      ", N − D)\n",
      "\n",
      "(7.84)\n",
      "\n",
      "This is equivalent to the sampling distribution of the MLE which is given by the following (see e.g., (Rice 1995, p542), (Casella and Berger 2002, p554)):\n",
      "\n",
      "wj − ˆwj sj\n",
      "\n",
      "∼ tN −D\n",
      "\n",
      "(7.85)\n",
      "\n",
      "where\n",
      "\n",
      "sj =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "s2Cjj N − D\n",
      "\n",
      "(7.86)\n",
      "\n",
      "is the standard error of the estimated parameter. (See Section 6.2 for a discussion of sampling distributions.) Consequently, the frequentist conﬁdence interval and the Bayesian marginal credible interval for the parameters are the same in this case.\n",
      "\n",
      "As a worked example of this, consider the caterpillar dataset from (Marin and Robert 2007). (The details of what the data mean don’t matter for our present purposes.) We can compute\n",
      "\n",
      "238\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "the posterior mean and standard deviation, and the 95% credible intervals (CI) for the regression coefficients using Equation 7.84. The results are shown in Table 7.2. It is easy to check that these 95% credible intervals are identical to the 95% conﬁdence intervals computed using standard frequentist methods (see linregBayesCaterpillar for the code).\n",
      "\n",
      "We can also use these marginal posteriors to compute if the coefficients are “signiﬁcantly” different from 0. An informal way to do this (without using decision theory) is to check if its 95% CI excludes 0. From Table 7.2, we see that the CIs for coefficients 0, 1, 2, 4, 5 are all signiﬁcant by this measure, so we put a little star by them. It is easy to check that these results are the same as those produced by standard frequentist software packages which compute p-values at the 5% level.\n",
      "\n",
      "Although the correspondence between the Bayesian and frequentist results might seem ap- pealing to some readers, recall from Section 6.6 that frequentist inference is riddled with patholo- gies. Also, note that the MLE does not even exist when N < D, so standard frequentist inference theory breaks down in this setting. Bayesian inference theory still works, although it requires the use of proper priors. (See (Maruyama and George 2008) for one extension of the g-prior to the case where D > N .)\n",
      "\n",
      "7.6.4\n",
      "\n",
      "EB for linear regression (evidence procedure)\n",
      "\n",
      "In this section, we describe an empirical Bayes So far, we have assumed the prior is known. procedure for picking the hyper-parameters. More precisely, we choose η = (α, λ) to maximize the marignal likelihood, where λ = 1/σ2 be the precision of the observation noise and α is the precision of the prior, p(w) =N (w|0, α−1I). This is known as the evidence procedure (MacKay 1995b).3 See Section 13.7.4 for the algorithmic details.\n",
      "\n",
      "The evidence procedure provides an alternative to using cross validation. For example, in likelihood for different values of α, as well as the Figure 7.13(b), we plot the log marginal maximum value found by the optimizer. We see that, in this example, we get the same result (We kept λ = 1/σ2 ﬁxed in both methods, to make them as 5-CV, shown in Figure 7.13(a). comparable.)\n",
      "\n",
      "The principle practical advantage of the evidence procedure over CV will become apparent in Section 13.7, where we generalize the prior by allowing a different αj for every feature. This can be used to perform feature selection, using a technique known as automatic relevancy determination or ARD. By contrast, it would not be possible to use CV to tune D different hyper-parameters.\n",
      "\n",
      "The evidence procedure is also useful when comparing different kinds of models, since it\n",
      "\n",
      "provides a good approximation to the evidence:\n",
      "\n",
      "(cid:28) (cid:28)\n",
      "\n",
      "p(D|m) =\n",
      "\n",
      "p(D|w, m)p(w|m, η)p(η|m)dwdη (cid:28)\n",
      "\n",
      "(7.87)\n",
      "\n",
      "≈ max\n",
      "\n",
      "η\n",
      "\n",
      "p(D|w, m)p(w|m, η)p(η|m)dw\n",
      "\n",
      "(7.88)\n",
      "\n",
      "It is important to (at least approximately) integrate over η rather than setting it arbitrarily, for reasons discussed in Section 5.3.2.5. Indeed, this is the method we used to evaluate the marginal\n",
      "\n",
      "3. Alternatively, we could integrate out λ analytically, as shown in Section 7.6.3, and just optimize α (Buntine and Weigend 1991). However, it turns out that this is less accurate than optimizing both α and λ (MacKay 1999).\n",
      "\n",
      "7.6. Bayesian linear regression\n",
      "\n",
      "239\n",
      "\n",
      "7 10\n",
      "\n",
      "5−fold cross validation, ntrain = 21\n",
      "\n",
      "−50\n",
      "\n",
      "log evidence\n",
      "\n",
      "6 10\n",
      "\n",
      "−60\n",
      "\n",
      "−70\n",
      "\n",
      "5 10\n",
      "\n",
      "−80\n",
      "\n",
      "4 10\n",
      "\n",
      "−90\n",
      "\n",
      "e s m\n",
      "\n",
      "3 10\n",
      "\n",
      "−100\n",
      "\n",
      "−110\n",
      "\n",
      "2 10\n",
      "\n",
      "−120\n",
      "\n",
      "1 10\n",
      "\n",
      "−130\n",
      "\n",
      "−140\n",
      "\n",
      "0 10\n",
      "\n",
      "−25\n",
      "\n",
      "−20\n",
      "\n",
      "−15\n",
      "\n",
      "−10 log lambda\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−150\n",
      "\n",
      "−25\n",
      "\n",
      "−20\n",
      "\n",
      "−15\n",
      "\n",
      "−10 log alpha\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 7.13 (a) Estimate of test MSE produced by 5-fold cross-validation vs log(λ). The smallest value is indicated by the vertical line. Note the vertical scale is in log units. (c) Log marginal likelihood vs log(α). The largest value is indicated by the vertical line. Figure generated by linregPolyVsRegDemo.\n",
      "\n",
      "likelihood for the polynomial regression models in Figures 5.7 and 5.8. For a “more Bayesian” approach, in which we model our uncertainty about η rather than computing point estimates, see Section 21.5.2.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 7.1 Behavior of training set error with increasing sample size The error on the test will always decrease as we get more training data, since the model will be better estimated. However, as shown in Figure 7.10, for sufficiently complex models, the error on the training set can increase we we get more training data, until we reach some plateau. Explain why.\n",
      "\n",
      "Exercise 7.2 Multi-output linear regression (Source: Jaakkola.)\n",
      "\n",
      "When we have multiple independent outputs in linear regression, the model becomes\n",
      "\n",
      "p(y|x, W) =\n",
      "\n",
      "M(cid:13)\n",
      "\n",
      "N (yj|wT\n",
      "\n",
      "j xi, σ2 j )\n",
      "\n",
      "(7.89)\n",
      "\n",
      "j=1\n",
      "\n",
      "Since the likelihood factorizes across dimensions, so does the MLE. Thus\n",
      "\n",
      "ˆW = [ ˆw1, . . . , ˆwM ]\n",
      "\n",
      "(7.90)\n",
      "\n",
      "where ˆwj = (XT X)−1Y:,j. In this exercise we apply this result to a model with 2 dimensional response vector yi ∈ R2. Suppose we have some binary input data, xi ∈ {0, 1}. The training data is as follows:\n",
      "\n",
      "240\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "x 0 0 0 1 1 1\n",
      "\n",
      "y (−1, −1)T (−1, −2)T (−2, −1)T (1, 1)T (1, 2)T (2, 1)T\n",
      "\n",
      "Let us embed each xi into 2d using the following basis function:\n",
      "\n",
      "φ(0) = (1, 0)T , φ(1) = (0, 1)T\n",
      "\n",
      "(7.91)\n",
      "\n",
      "The model becomes ˆy = WT φ(x)\n",
      "\n",
      "(7.92)\n",
      "\n",
      "where W is a 2 × 2 matrix. Compute the MLE for W from the above data.\n",
      "\n",
      "Exercise 7.3 Centering and ridge regression Assume that x = 0, so the input data has been centered. Show that the optimizer of\n",
      "\n",
      "J(w, w0) = (y − Xw − w01)T (y − Xw − w01) +λ wT w\n",
      "\n",
      "(7.93)\n",
      "\n",
      "is\n",
      "\n",
      "ˆw0 = y w = (XT X + λI)−1XT y\n",
      "\n",
      "(7.94)\n",
      "\n",
      "(7.95)\n",
      "\n",
      "Exercise 7.4 MLE for σ2 for linear regression Show that the MLE for the error variance in linear regression is given by\n",
      "\n",
      "ˆσ2 =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:12)\n",
      "\n",
      "(yi − xT\n",
      "\n",
      "i ˆw)2\n",
      "\n",
      "i=1\n",
      "\n",
      "(7.96)\n",
      "\n",
      "This is just the empirical variance of the residual errors when we plug in our estimate of ˆw.\n",
      "\n",
      "Exercise 7.5 MLE for the offset term in linear regression Linear regression has the form E [y|x] =w 0 + wT x. It is common to include a column of 1’s in the design matrix, so we can solve for the offset term w0 term and the other parameters w at the same time using the normal equations. However, it is also possible to solve for w and w0 separately. Show that\n",
      "\n",
      "ˆw0 =\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "i\n",
      "\n",
      "yi − 1 N\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "i\n",
      "\n",
      "i w = y − xT w xT\n",
      "\n",
      "(7.97)\n",
      "\n",
      "So ˆw0 models the difference in the average output from the average predicted output. Also, show that\n",
      "\n",
      "ˆw = (XT\n",
      "\n",
      "c Xc)−1XT\n",
      "\n",
      "c yc =\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "N(cid:12)\n",
      "\n",
      "(xi − x)(xi − x)T\n",
      "\n",
      "(cid:19)−1 (cid:18)\n",
      "\n",
      "N(cid:12)\n",
      "\n",
      "(yi − y)(xi − x)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(7.98)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "where Xc is the centered input matrix containing xc i = xi − x along its rows, and yc = y − y is the centered output vector. Thus we can ﬁrst compute ˆw on centered data, and then estimate w0 using y − xT ˆw.\n",
      "\n",
      "7.6. Bayesian linear regression\n",
      "\n",
      "241\n",
      "\n",
      "Exercise 7.6 MLE for simple linear regression Simple linear regression refers to the case where the input is scalar, so D = 1. Show that the MLE in this case is given by the following equations, which may be familiar from basic statistics classes:\n",
      "\n",
      "w1 =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i(xi − x)(yi − ¯y) (cid:2) i(xi − ¯x)2\n",
      "\n",
      "=\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i xiyi − N x y i − N x2 ≈ cov [X, Y ] (cid:2) i x2\n",
      "\n",
      "var [X]\n",
      "\n",
      "(7.99)\n",
      "\n",
      "w0 = ¯y − w1 ¯x ≈ E [Y ] − w1E [X]\n",
      "\n",
      "(7.100)\n",
      "\n",
      "See linregDemo1 for a demo.\n",
      "\n",
      "Exercise 7.7 Sufficient statistics for online linear regression (Source: not keep the original data, xi, yi, but we do have the following functions (statistics) of the data:\n",
      "\n",
      "Jaakkola.) Consider ﬁtting the model ˆy = w0 + w1x using least squares. Unfortunately we did\n",
      "\n",
      "x(n) =\n",
      "\n",
      "C (n)\n",
      "\n",
      "xx =\n",
      "\n",
      "1 n\n",
      "\n",
      "1 n\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "i=1 n(cid:12)\n",
      "\n",
      "i=1\n",
      "\n",
      "(xi − x)2, C (n)\n",
      "\n",
      "xi, y(n) =\n",
      "\n",
      "1 n\n",
      "\n",
      "xy =\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "i=1\n",
      "\n",
      "yi\n",
      "\n",
      "1 n\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "(xi − x)(yi − y), C (n)\n",
      "\n",
      "yy =\n",
      "\n",
      "i=1\n",
      "\n",
      "1 n\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "(yi − y)2\n",
      "\n",
      "i=1\n",
      "\n",
      "(7.101)\n",
      "\n",
      "(7.102)\n",
      "\n",
      "a. What are the minimal set of statistics that we need to estimate w1? (Hint: see Equation 7.99.) b. What are the minimal set of statistics that we need to estimate w0? (Hint: see Equation 7.97.) c. Suppose a new data point, xn+1, yn+1 arrives, and we want to update our sufficient statistics without (This is useful for online learning.) Show that we\n",
      "\n",
      "looking at the old data, which we have not stored. can this for x as follows.\n",
      "\n",
      "x(n+1) (cid:2)\n",
      "\n",
      "= x(n) +\n",
      "\n",
      "1 n + 1\n",
      "\n",
      "n+1(cid:12)\n",
      "\n",
      "i=1 1 n + 1\n",
      "\n",
      "xi =\n",
      "\n",
      "(xn+1 − x(n))\n",
      "\n",
      "1 n + 1\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "nx(n) + xn+1\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(7.103)\n",
      "\n",
      "(7.104)\n",
      "\n",
      "This has the form: new estimate is old estimate plus correction. We see that the size of the correction diminishes over time (i.e., as we get more samples). Derive a similar expression to update y\n",
      "\n",
      "d. Show that one can update C (n+1)\n",
      "\n",
      "e.\n",
      "\n",
      "1 n + 1 Derive a similar expression to update Cxx. Implement the online learning algorithm, i.e., write a function of the form [w,ss] = linregUpdateSS(ss, x, y), where x and y are scalars and ss is a structure containing the sufficient statistics.\n",
      "\n",
      "C (n+1) xy\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "xn+1yn+1 + nC (n)\n",
      "\n",
      "xy\n",
      "\n",
      "recursively using\n",
      "\n",
      "xy + nx(n)y(n) − (n + 1)x(n+1)y(n+1)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(7.105)\n",
      "\n",
      "f. Plot the coefficients over “time”, using the dataset in linregDemo1.\n",
      "\n",
      "(Speciﬁcally, use [x,y] = polyDataMake(’sampling’,’thibaux’).) Check that they converge to the solution given by the batch (offline) learner (i.e, ordinary least squares). Your result should look like Figure 7.14.\n",
      "\n",
      "Turn in your derivation, code and plot.\n",
      "\n",
      "Exercise 7.8 Bayesian linear regression in 1d with known σ2 (Source: Bolstad.) Consider ﬁtting a model of the form\n",
      "\n",
      "p(y|x, θ) = N (y|w0 + w1x, σ2)\n",
      "\n",
      "(7.106)\n",
      "\n",
      "to the data shown below:\n",
      "\n",
      "242\n",
      "\n",
      "Chapter 7. Linear regression\n",
      "\n",
      "online linear regression\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "w0 w1 w0 batch w1 batch\n",
      "\n",
      "0\n",
      "\n",
      "s t h g e w\n",
      "\n",
      "i\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−5\n",
      "\n",
      "−6\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "time\n",
      "\n",
      "Figure 7.14 Regression coefficients over time. Produced by Exercise 7.7.\n",
      "\n",
      "x = [94,96,94,95,104,106,108,113,115,121,131]; y = [0.47, 0.75, 0.83, 0.98, 1.18, 1.29, 1.40, 1.60, 1.75, 1.90, 2.23];\n",
      "\n",
      "a. Compute an unbiased estimate of σ2 using\n",
      "\n",
      "ˆσ2 =\n",
      "\n",
      "1 N − 2\n",
      "\n",
      "N(cid:12)\n",
      "\n",
      "(yi − ˆyi)2\n",
      "\n",
      "i=1\n",
      "\n",
      "(7.107)\n",
      "\n",
      "(The denominator is N −2 since we have 2 inputs, namely the offset term and x.) Here ˆyi = ˆw0+ ˆw1xi, and ˆw = ( ˆw0, ˆw1) is the MLE.\n",
      "\n",
      "b. Now assume the following prior on w:\n",
      "\n",
      "(7.108) Use an (improper) uniform prior on w0 and a N (0, 1) prior on w1. Show that this can be written as a Gaussian prior of the form p(w) = N (w|w0, V0). What are w0 and V0?\n",
      "\n",
      "p(w) =p (w0)p(w1)\n",
      "\n",
      "c. Compute the marginal posterior of the slope, p(w1|D, σ2), where D is the data above, and σ2 is the Show your work. (You\n",
      "\n",
      "w1|D, σ2 unbiased estimate computed above. What is E can use Matlab if you like.) Hint: the posterior variance is a very small number!\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "w1|D, σ2\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "and var\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "d. What is a 95% credible interval for w1?\n",
      "\n",
      "Exercise 7.9 Generative model for linear regression Linear regression is the problem of estimating E[Y |x] using a linear function of the form w0 + wT x. Typically we assume that the conditional distribution of Y given X is Gaussian. We can either estimate this conditional Gaussian directly (a discriminative approach), or we can ﬁt a Gaussian to the joint distribution of X, Y and then derive E[Y |X = x]. In Exercise 7.5 we showed that the discriminative approach leads to these equations\n",
      "\n",
      "E[Y |x] =w 0 + wT x w0 = y − xT w w = (XT\n",
      "\n",
      "c Xc)−1XT\n",
      "\n",
      "c yc\n",
      "\n",
      "(7.109)\n",
      "\n",
      "(7.110)\n",
      "\n",
      "(7.111)\n",
      "\n",
      "7.6. Bayesian linear regression\n",
      "\n",
      "243\n",
      "\n",
      "where Xc = X − ¯X is the centered input matrix, and ¯X = 1nxT replicates x across the rows. Similarly, yc = y − y is the centered output vector, and y = 1ny replicates y across the rows.\n",
      "\n",
      "a. By ﬁnding the maximum likelihood estimates of ΣXX , ΣXY , μX and μY , derive the above equations by ﬁtting a joint Gaussian to X, Y and using the formula for conditioning a Gaussian (see Section 4.3.1). Show your work.\n",
      "\n",
      "b. What are the advantages and disadvantages of this approach compared to the standard discriminative\n",
      "\n",
      "approach?\n",
      "\n",
      "Exercise 7.10 Bayesian linear regression using the g-prior Show that when we use the g-prior, p(w, σ2) = NIG(w, σ2|0, g(XT X)−1, 0, 0), the posterior has the following form:\n",
      "\n",
      "p(w, σ2|D) = NIG(w, σ2|wN , VN , aN , bN )\n",
      "\n",
      "VN =\n",
      "\n",
      "wN =\n",
      "\n",
      "g g + 1 g g + 1 aN = N/2\n",
      "\n",
      "(XT X)−1\n",
      "\n",
      "ˆwmle\n",
      "\n",
      "(7.112)\n",
      "\n",
      "(7.113)\n",
      "\n",
      "(7.114)\n",
      "\n",
      "(7.115)\n",
      "\n",
      "bN =\n",
      "\n",
      "s2 2\n",
      "\n",
      "+\n",
      "\n",
      "1 2(g + 1)\n",
      "\n",
      "ˆwT\n",
      "\n",
      "mleXT X ˆwmle\n",
      "\n",
      "(7.116)\n",
      "\n",
      "(7.117)\n",
      "\n",
      "8 Logistic regression\n",
      "\n",
      "8.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "One way to build a probabilistic classiﬁer is to create a joint model of the form p(y, x) and then to condition on x, thereby deriving p(y|x). This is called the generative approach. An alternative approach is to ﬁt a model of the form p(y|x) directly. This is called the discrimi- native approach, and is the approach we adopt in this chapter. In particular, we will assume discriminative models which are linear in the parameters. This will turn out to signiﬁcantly sim- plify model ﬁtting, as we will see. In Section 8.6, we compare the generative and discriminative approaches, and in later chapters, we will consider non-linear and non-parametric discriminative models.\n",
      "\n",
      "8.2 Model speciﬁcation\n",
      "\n",
      "As we discussed in Section 1.4.6, logistic regression corresponds to the following binary classiﬁ- cation model:\n",
      "\n",
      "p(y|x, w) = Ber(y|sigm(wT x))\n",
      "\n",
      "(8.1)\n",
      "\n",
      "A 1d example is shown in Figure 1.19(b). Logistic regression can easily be extended to higher- dimensional inputs. For example, Figure 8.1 shows plots of p(y = 1|x, w) = sigm(wT x) for 2d input and different weight vectors w. If we threshold these probabilities at 0.5, we induce a linear decision boundary, whose normal (perpendicular) is given by w.\n",
      "\n",
      "8.3 Model ﬁtting\n",
      "\n",
      "In this section, we discuss algorithms for estimating the parameters of a logistic regression model.\n",
      "\n",
      "246\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "W = ( 1 , 4 )\n",
      "\n",
      "W = ( 5 , 4 )\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0 −10\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0 −10\n",
      "\n",
      "W = ( −2 , 3 )\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2\n",
      "\n",
      "W = ( −2 , −1 )\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "w 2\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0 −10\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0 −10\n",
      "\n",
      "W = ( 0 , 2 )\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0 −10\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2\n",
      "\n",
      "10\n",
      "\n",
      "W = ( 2 , 2 )\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "10\n",
      "\n",
      "0 −10\n",
      "\n",
      "W = ( 1 , 0 )\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2\n",
      "\n",
      "10\n",
      "\n",
      "W = ( 3 , 0 )\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2 W = ( 2 , −2 )\n",
      "\n",
      "10\n",
      "\n",
      "0 −10\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "10\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0 −10\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0 −10\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2\n",
      "\n",
      "W = ( 5 , 1 )\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2\n",
      "\n",
      "w 1\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "−2\n",
      "\n",
      "0 −10\n",
      "\n",
      "0 x 1\n",
      "\n",
      "10 −10\n",
      "\n",
      "0 x 2\n",
      "\n",
      "10\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "Plots of sigm(w1x1 + w2x2). Here w = (w1, w2) deﬁnes the normal to the decision Figure 8.1 boundary. Points to the right of this have sigm(wT x) > 0.5, and points to the left have sigm(wT x) < 0.5. Based on Figure 39.3 of (MacKay 2003). Figure generated by sigmoidplot2D.\n",
      "\n",
      "8.3.1 MLE\n",
      "\n",
      "The negative log-likelihood for logistic regression is given by\n",
      "\n",
      "NLL(w) = −\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "log[μI(yi=1) i\n",
      "\n",
      "× (1 − μi)I(yi=0)]\n",
      "\n",
      "(8.2)\n",
      "\n",
      "= −\n",
      "\n",
      "i=1 N(cid:2)\n",
      "\n",
      "[yi log μi + (1 − yi) log(1 − μi)]\n",
      "\n",
      "(8.3)\n",
      "\n",
      "i=1\n",
      "\n",
      "This is also called the cross-entropy error function (see Section 2.8.2).\n",
      "\n",
      "have p(y = 1) =\n",
      "\n",
      "Another way of writing this is as follows. Suppose ˜yi ∈ {−1, +1} instead of yi ∈ {0, 1}. We\n",
      "\n",
      "1+exp(−wT x) and p(y = 1) =\n",
      "\n",
      "1\n",
      "\n",
      "1+exp(+wT x) . Hence\n",
      "\n",
      "1\n",
      "\n",
      "N LL(w) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "log(1 + exp(−˜yiwT xi))\n",
      "\n",
      "(8.4)\n",
      "\n",
      "i=1\n",
      "\n",
      "Unlike linear regression, we can no longer write down the MLE in closed form. Instead, we need to use an optimization algorithm to compute it. For this, we need to derive the gradient and Hessian.\n",
      "\n",
      "In the case of logistic regression, one can show (Exercise 8.3) that the gradient and Hessian\n",
      "\n",
      "8.3. Model ﬁtting\n",
      "\n",
      "247\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "2.5\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 8.2 Gradient descent on a simple function, starting from (0, 0), for 20 steps, using a ﬁxed learning rate (step size) η. The global minimum is at (1, 1). (a) η = 0.1. (b) η = 0.6. Figure generated by steepestDescentDemo.\n",
      "\n",
      "of this are given by the following\n",
      "\n",
      "g =\n",
      "\n",
      "H =\n",
      "\n",
      "d dw\n",
      "\n",
      "d dw\n",
      "\n",
      "f (w) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(μi − yi)xi = XT (μ − y)\n",
      "\n",
      "g(w)T =\n",
      "\n",
      "i (cid:2)\n",
      "\n",
      "(∇wμi)xT\n",
      "\n",
      "i =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "μi(1 − μi)xixT i\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "(8.5)\n",
      "\n",
      "(8.6)\n",
      "\n",
      "= XT SX\n",
      "\n",
      "(8.7)\n",
      "\n",
      "where S (cid:2) diag(μi(1 − μi)). One can also show (Exercise 8.3) that H is positive deﬁnite. Hence the NLL is convex and has a unique global minimum. Below we discuss some methods for ﬁnding this minimum.\n",
      "\n",
      "8.3.2\n",
      "\n",
      "Steepest descent\n",
      "\n",
      "Perhaps the simplest algorithm for unconstrained optimization is gradient descent, also known as steepest descent. This can be written as follows:\n",
      "\n",
      "θk+1 = θk − ηkgk\n",
      "\n",
      "(8.8)\n",
      "\n",
      "where ηk is the step size or learning rate. The main issue in gradient descent is: how should we set the step size? This turns out to be quite tricky. If we use a constant learning rate, but make it too small, convergence will be very slow, but if we make it too large, the method can fail to converge at all. This is illustrated in Figure 8.2. where we plot the following (convex) function\n",
      "\n",
      "f (θ) = 0.5(θ2\n",
      "\n",
      "1 − θ2)2 + 0.5(θ1 − 1)2,\n",
      "\n",
      "(8.9)\n",
      "\n",
      "We arbitrarily decide to start from (0, 0). In Figure 8.2(a), we use a ﬁxed step size of η = 0.1; we see that it moves slowly along the valley. In Figure 8.2(b), we use a ﬁxed step size of η = 0.6; we see that the algorithm starts oscillating up and down the sides of the valley and never converges to the optimum.\n",
      "\n",
      "248\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "exact line searching 1\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 8.3 (a) Steepest descent on the same function as Figure 8.2, starting from (0, 0), using line search. Figure generated by steepestDescentDemo. (b) Illustration of the fact that at the end of a line search (top of picture), the local gradient of the function will be perpendicular to the search direction. Based on Figure 10.6.1 of (Press et al. 1988).\n",
      "\n",
      "Let us develop a more stable method for picking the step size, so that the method is guaran- (This property is called global teed to converge to a local optimum no matter where we start. convergence, which should not be confused with convergence to the global optimum!) By Taylor’s theorem, we have\n",
      "\n",
      "f (θ + ηd) ≈ f (θ) +η gT d\n",
      "\n",
      "(8.10)\n",
      "\n",
      "where d is our descent direction. So if η is chosen small enough, then f (θ + ηd) < f (θ), since the gradient will be negative. But we don’t want to choose the step size η too small, or we will move very slowly and may not reach the minimum. So let us pick η to minimize\n",
      "\n",
      "φ(η) = f (θk + ηdk)\n",
      "\n",
      "(8.11)\n",
      "\n",
      "This is called line minimization or line search. There are various methods for solving this 1d optimization problem; see (Nocedal and Wright 2006) for details.\n",
      "\n",
      "Figure 8.3(a) demonstrates that line search does indeed work for our simple problem. However, we see that the steepest descent path with exact line searches exhibits a characteristic zig-zag behavior. To see why, note that an exact line search satisﬁes ηk = arg minη>0 φ(η). A necessary condition for the optimum is φ(cid:4)(η) = 0. By the chain rule, φ(cid:4)(η) =d T g, where g = f (cid:4)(θ + ηd) is the gradient at the end of the step. So we either have g = 0, which means we have found a stationary point, or g ⊥ d, which means that exact search stops at a point where the local gradient is perpendicular to the search direction. Hence consecutive directions will be orthogonal (see Figure 8.3(b)). This explains the zig-zag behavior.\n",
      "\n",
      "θk−1), as follows:\n",
      "\n",
      "One simple heuristic to reduce the effect of zig-zagging is to add a momentum term, (θk −\n",
      "\n",
      "θk+1 = θk − ηkgk + μk(θk − θk−1)\n",
      "\n",
      "(8.12)\n",
      "\n",
      "8.3. Model ﬁtting\n",
      "\n",
      "249\n",
      "\n",
      "where 0 ≤ μk ≤ 1 controls the importance of the momentum term. community, this is known as the heavy ball method (see e.g., (Bertsekas 1999)).\n",
      "\n",
      "In the optimization\n",
      "\n",
      "An alternative way to minimize “zig-zagging” is to use the method of conjugate gradients (see e.g., (Nocedal and Wright 2006, ch 5) or (Golub and van Loan 1996, Sec 10.2)). This is the method of choice for quadratic objectives of the form f (θ) = θT Aθ, which arise when solving linear systems. However, non-linear CG is less popular.\n",
      "\n",
      "8.3.3\n",
      "\n",
      "Newton’s method\n",
      "\n",
      "Algorithm 8.1: Newton’s method for minimizing a strictly convex function 1 Initialize θ0; 2 for k = 1, 2, . . . until convergence do 3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "Evaluate gk = ∇f (θk); Evaluate Hk = ∇2f (θk); Solve Hkdk = −gk for dk; Use line search to ﬁnd stepsize ηk along dk; θk+1 = θk + ηkdk;\n",
      "\n",
      "One can derive faster optimization methods by taking the curvature of the space (i.e., the into account. These are called second order optimization metods. The primary Hessian) example is Newton’s algorithm. This is an iterative algorithm which consists of updates of the form\n",
      "\n",
      "θk+1 = θk − ηkH−1\n",
      "\n",
      "k gk\n",
      "\n",
      "(8.13)\n",
      "\n",
      "The full pseudo-code is given in Algorithm 2.\n",
      "\n",
      "approximation of f (θ) around θk:\n",
      "\n",
      "This algorithm can be derived as follows. Consider making a second-order Taylor series\n",
      "\n",
      "fquad(θ) = fk + gT\n",
      "\n",
      "k (θ − θk) +\n",
      "\n",
      "1 2\n",
      "\n",
      "(θ − θk)T Hk(θ − θk)\n",
      "\n",
      "(8.14)\n",
      "\n",
      "Let us rewrite this as\n",
      "\n",
      "fquad(θ) = θT Aθ + bT θ + c\n",
      "\n",
      "(8.15)\n",
      "\n",
      "where\n",
      "\n",
      "A =\n",
      "\n",
      "1 2\n",
      "\n",
      "Hk, b = gk − Hkθk, c = fk − gT\n",
      "\n",
      "k θk +\n",
      "\n",
      "1 2\n",
      "\n",
      "θT k Hkθk\n",
      "\n",
      "(8.16)\n",
      "\n",
      "The minimum of fquad is at\n",
      "\n",
      "θ = − 1 2\n",
      "\n",
      "A−1b = θk − H−1\n",
      "\n",
      "k gk\n",
      "\n",
      "(8.17)\n",
      "\n",
      "Thus the Newton step dk = −H−1 order approximation of f around θk. See Figure 8.4(a) for an illustration.\n",
      "\n",
      "k gk is what should be added to θk to minimize the second\n",
      "\n",
      "250\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "f(x) f quad\n",
      "\n",
      "(x)\n",
      "\n",
      "f(x) f quad\n",
      "\n",
      "(x)\n",
      "\n",
      "x\n",
      "\n",
      "k\n",
      "\n",
      "x\n",
      "\n",
      "k\n",
      "\n",
      "+d k\n",
      "\n",
      "x\n",
      "\n",
      "k\n",
      "\n",
      "x\n",
      "\n",
      "k\n",
      "\n",
      "+d k\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 8.4 (a) The solid curve is the function f (x). The dotted line fquad(x) is its second order approximation at xk. The Newton step dk is what must be added to xk to get to the minimum of fquad(x). Based on Figure 13.4 of (Vandenberghe 2006). Figure generated by newtonsMethodMinQuad. (b) Illustration of Newton’s method applied to a nonconvex function. We ﬁt a quadratic around the current point xk and move to its stationary point, xk+1 = xk + dk. Unfortunately, this is a local maximum, not minimum. This means we need to be careful about the extent of our quadratic approximation. Based on Figure 13.11 of (Vandenberghe 2006). Figure generated by newtonsMethodNonConvex.\n",
      "\n",
      "Illustration of Newton’s method for minimizing a 1d function.\n",
      "\n",
      "In its simplest form (as listed), Newton’s method requires that Hk be positive deﬁnite, which will hold if the function is strictly convex. If not, the objective function is not convex, then Hk may not be positive deﬁnite, so dk = −H−1 k gk may not be a descent direction (see Figure 8.4(b) for an example). In this case, one simple strategy is to revert to steepest descent, dk = −gk. The Levenberg Marquardt algorithm is an adaptive way to blend between Newton steps and steepest descent steps. This method is widely used when solving nonlinear least squares problems. An alternative approach is this: Rather than computing dk = −H−1 k gk directly, we can solve the linear system of equations Hkdk = −gk for dk using conjugate gradient (CG). If Hk is not positive deﬁnite, we can simply truncate the CG iterations as soon as negative curvature is detected; this is called truncated Newton.\n",
      "\n",
      "8.3.4\n",
      "\n",
      "Iteratively reweighted least squares (IRLS)\n",
      "\n",
      "Let us now apply Newton’s algorithm to ﬁnd the MLE for binary logistic regression. The Newton update at iteration k + 1 for this model is as follows (using ηk = 1, since the Hessian is exact):\n",
      "\n",
      "wk+1 = wk − H−1gk\n",
      "\n",
      "= wk + (XT SkX)−1XT (y − μk) (cid:31) = (XT SkX)−1 = (XT SkX)−1XT [SkXwk + y − μk] = (XT SkX)−1XT Skzk\n",
      "\n",
      "(XT SkX)wk + XT (y − μk)\n",
      "\n",
      "(8.18) (8.19)\n",
      "\n",
      "(8.20)\n",
      "\n",
      "(8.21)\n",
      "\n",
      "(8.22)\n",
      "\n",
      "where we have deﬁned the working response as\n",
      "\n",
      "zk (cid:2) Xwk + S−1\n",
      "\n",
      "k (y − μk)\n",
      "\n",
      "(8.23)\n",
      "\n",
      "8.3. Model ﬁtting\n",
      "\n",
      "251\n",
      "\n",
      "Equation 8.22 is an example of a weighted least squares problem, which is a minimizer of\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "Ski(zki − wT xi)2\n",
      "\n",
      "(8.24)\n",
      "\n",
      "i=1\n",
      "\n",
      "Since Sk is a diagonal matrix, we can rewrite the targets in component form (for each case i = 1 :N ) as\n",
      "\n",
      "zki = wT\n",
      "\n",
      "k xi +\n",
      "\n",
      "yi − μki μki(1 − μki)\n",
      "\n",
      "(8.25)\n",
      "\n",
      "This algorithm is known as iteratively reweighted least squares or IRLS for short, since at each iteration, we solve a weighted least squares problem, where the weight matrix Sk changes at each iteration. See Algorithm 10 for some pseudocode.\n",
      "\n",
      "9 10 until converged;\n",
      "\n",
      "Algorithm 8.2: Iteratively reweighted least squares (IRLS) 1 w = 0D; 2 w0 = log(y/(1 − y)); 3 repeat 4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "ηi = w0 + wT xi; μi = sigm(ηi); si = μi(1 − μi) ; zi = ηi + yi−μi ; S = diag(s1:N ) ; w = (XT SX)−1XT Sz;\n",
      "\n",
      "si\n",
      "\n",
      "8.3.5\n",
      "\n",
      "Quasi-Newton (variable metric) methods\n",
      "\n",
      "The mother of all second-order optimization algorithm is Newton’s algorithm, which we dis- cussed in Section 8.3.3. Unfortunately, it may be too expensive to compute H explicitly. Quasi- Newton methods iteratively build up an approximation to the Hessian using information gleaned from the gradient vector at each step. The most common method is called BFGS (named after its inventors, Broyden, Fletcher, Goldfarb and Shanno), which updates the approximation to the Hessian Bk ≈ Hk as follows:\n",
      "\n",
      "Bk+1 = Bk +\n",
      "\n",
      "ykyT k yT k sk sk = θk − θk−1 yk = gk − gk−1\n",
      "\n",
      "− (Bksk)(Bksk)T sT k Bksk\n",
      "\n",
      "(8.26)\n",
      "\n",
      "(8.27)\n",
      "\n",
      "(8.28)\n",
      "\n",
      "This is a rank-two update to the matrix, and ensures that the matrix remains positive deﬁnite (under certain restrictions on the step size). We typically start with a diagonal approximation, B0 = I. Thus BFGS can be thought of as a “diagonal plus low-rank” approximation to the Hessian.\n",
      "\n",
      "252\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "Alternatively, BFGS can iteratively update an approximation to the inverse Hessian, Ck ≈ H−1 k ,\n",
      "\n",
      "as follows:\n",
      "\n",
      "Ck+1 =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "I −\n",
      "\n",
      "skyT k yT k sk\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "Ck\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "I −\n",
      "\n",
      "yksT k yT k sk\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "+\n",
      "\n",
      "sksT k yT k sk\n",
      "\n",
      "(8.29)\n",
      "\n",
      "Since storing the Hessian takes O(D2) space, for very large problems, one can use limited memory BFGS, orL-BFGS, where Hk or H−1 is approximated by a diagonal plus low rank k matrix. In particular, the product H−1 k gk can be obtained by performing a sequence of inner products with sk and yk, using only the m most recent (sk, yk) pairs, and ignoring older information. The storage requirements are therefore O(mD). Typically m ∼ 20 suffices for See (Nocedal and Wright 2006, p177) for more information. L-BFGS is good performance. often the method of choice for most unconstrained smooth optimization problems that arise in machine learning (although see Section 8.5).\n",
      "\n",
      "8.3.6\n",
      "\n",
      "(cid:9)2 regularization\n",
      "\n",
      "Just as we prefer ridge regression to linear regression, so we should prefer MAP estimation for logistic regression to computing the MLE. In fact, regularization is important in the classiﬁcation In setting even if we have lots of data. To see why, suppose the data is linearly separable. this case, the MLE is obtained when ||w|| → ∞, corresponding to an inﬁnitely steep sigmoid function, I(wT x > w0), also known as a linear threshold unit. This assigns the maximal amount of probability mass to the training data. However, such a solution is very brittle and will not generalize well.\n",
      "\n",
      "To prevent this, we can use (cid:6)2 regularization, just as we did with ridge regression. We note\n",
      "\n",
      "that the new objective, gradient and Hessian have the following forms:\n",
      "\n",
      "f (cid:4)(w) = NLL(w) +λw T w g(cid:4)(w) =g (w) +λw H(cid:4)(w) =H( w) +λI\n",
      "\n",
      "(8.30)\n",
      "\n",
      "(8.31)\n",
      "\n",
      "(8.32)\n",
      "\n",
      "It is a simple matter to pass these modiﬁed equations into any gradient-based optimizer.\n",
      "\n",
      "8.3.7 Multi-class logistic regression\n",
      "\n",
      "Now we consider multinomial logistic regression, sometimes called a maximum entropy classiﬁer. This is a model of the form\n",
      "\n",
      "p(y = c|x, W) =\n",
      "\n",
      "(cid:10)C\n",
      "\n",
      "exp(wT c x) c(cid:2)=1 exp(wT\n",
      "\n",
      "c(cid:2) x)\n",
      "\n",
      "(8.33)\n",
      "\n",
      "A slight variant, known as a conditional logit model, normalizes over a different set of classes for each data case; this can be useful for modeling choices that users make between different sets of items that are offered to them.\n",
      "\n",
      "Let us now introduce some notation. Let μic = p(yi = c|xi, W) = S(ηi)c, where ηi = WT xi is a C × 1 vector. Also, let yic = I(yi = c) be the one-of-C encoding of yi; thus yi is a bit vector, in which the c’th bit turns on iff yi = c. Following (Krishnapuram et al. 2005), let us\n",
      "\n",
      "8.3. Model ﬁtting\n",
      "\n",
      "253\n",
      "\n",
      "set wC = 0, to ensure identiﬁability, and deﬁne w = vec(W(:, 1 :C − 1)) to be a D × (C − 1) column vector.\n",
      "\n",
      "With this, the log-likelihood can be written as\n",
      "\n",
      "(cid:6)(W) = log\n",
      "\n",
      "N(cid:27)\n",
      "\n",
      "C(cid:27)\n",
      "\n",
      "μyic\n",
      "\n",
      "ic =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "C(cid:2)\n",
      "\n",
      "yic log μic\n",
      "\n",
      "(8.34)\n",
      "\n",
      "=\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1 (cid:18)(cid:11)\n",
      "\n",
      "c=1 C(cid:2)\n",
      "\n",
      "i=1 (cid:13)\n",
      "\n",
      "yicwT\n",
      "\n",
      "c xi\n",
      "\n",
      "c=1\n",
      "\n",
      "− log\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "C(cid:2)\n",
      "\n",
      "(cid:13)(cid:19)\n",
      "\n",
      "exp(wT\n",
      "\n",
      "c(cid:2) xi)\n",
      "\n",
      "(8.35)\n",
      "\n",
      "i=1\n",
      "\n",
      "c=1\n",
      "\n",
      "c(cid:2)=1\n",
      "\n",
      "Deﬁne the NLL as\n",
      "\n",
      "f (w) = −(cid:6)(w)\n",
      "\n",
      "(8.36)\n",
      "\n",
      "We now proceed to compute the gradient and Hessian of this expression. Since w is block- It helps to deﬁne A ⊗ B structured, the notation gets a bit heavy, but the ideas are simple. be the kronecker product of matrices A and B. If A is an m × n matrix and B is a p × q matrix, then A × B is the mp × nq block matrix\n",
      "\n",
      "A ⊗ B =\n",
      "\n",
      "⎡\n",
      "\n",
      "⎢ ⎣\n",
      "\n",
      "a11B · · · . . . am1B · · ·\n",
      "\n",
      "...\n",
      "\n",
      "a1nB ... amnB\n",
      "\n",
      "⎤\n",
      "\n",
      "⎥ ⎦\n",
      "\n",
      "(8.37)\n",
      "\n",
      "Returning to the task at hand, one can show (Exercise 8.4) that the gradient is given by\n",
      "\n",
      "g(W) = ∇f (w) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(μi − yi) ⊗ xi\n",
      "\n",
      "(8.38)\n",
      "\n",
      "i=1\n",
      "\n",
      "where yi = (I(yi = 1), . . . , I(yi = C − 1)) and μi(W) = [p(yi = 1|xi, W), . . . , p(yi = C − 1|xi, W)] are column vectors of length C − 1, For example, if we have D = 3 feature dimensions and C = 3 classes, this becomes\n",
      "\n",
      "g(W) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "(μi1 − yi1)xi1 (μi1 − yi1)xi2 (μi1 − yi1)xi3 (μi2 − yi2)xi1 (μi2 − yi2)xi2 (μi2 − yi2)xi3\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "(8.39)\n",
      "\n",
      "In other words, for each class c, the derivative for the weights in the c’th column is\n",
      "\n",
      "∇wc\n",
      "\n",
      "f (W) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(μic − yic)xi\n",
      "\n",
      "(8.40)\n",
      "\n",
      "i\n",
      "\n",
      "This has the same form as in the binary logistic regression case, namely an error term times xi. (This turns out to be a general property of distributions in the exponential family, as we will see in Section 9.3.2.)\n",
      "\n",
      "254\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "1) × D(C − 1) matrix:\n",
      "\n",
      "One can also show (Exercise 8.4) that the Hessian is the following block structured D(C −\n",
      "\n",
      "H(W) =∇ 2f (w) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(diag(μi) − μiμT\n",
      "\n",
      "i ) ⊗ (xixT i )\n",
      "\n",
      "(8.41)\n",
      "\n",
      "i=1\n",
      "\n",
      "For example, if we have 3 features and 3 classes, this becomes\n",
      "\n",
      "H(W) =\n",
      "\n",
      "=\n",
      "\n",
      "i (cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "μi1 − μ2 −μi1μi2\n",
      "\n",
      "(μi1 − μ2 i1)Xi −μi1μi2Xi\n",
      "\n",
      "i1 −μi1μi2 μi2 − μ2 i2\n",
      "\n",
      "−μi1μi2Xi (μi2 − μ2 i2)Xi\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "⊗\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "xi1xi1 xi1xi2 xi1xi3 xi2xi1 xi2xi2 xi2xi3 xi3xi1 xi3xi2 xi3xi3\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "(8.42)\n",
      "\n",
      "(8.43)\n",
      "\n",
      "where Xi = xixT\n",
      "\n",
      "Hc,c(cid:2) (W) =\n",
      "\n",
      "i . In other words, the block c, c(cid:4) submatrix is given by\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "μic(δc,c(cid:2) − μi,c(cid:2) )xixT i\n",
      "\n",
      "(8.44)\n",
      "\n",
      "i\n",
      "\n",
      "This is also a positive deﬁnite matrix, so there is a unique MLE.\n",
      "\n",
      "where p(W) =\n",
      "\n",
      "Now consider minimizing f (cid:4)(W) (cid:2) − log p(D|w) − log p(W)\n",
      "\n",
      "’\n",
      "\n",
      "c N (wc|0, V0). The new objective, its gradient and Hessian are given by\n",
      "\n",
      "(8.45)\n",
      "\n",
      "f (cid:4)(W) =f\n",
      "\n",
      "c g(cid:4)(W) =g (W) +V −1 0 (\n",
      "\n",
      "(W) +\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "wcV−1 (cid:2)\n",
      "\n",
      "wc)\n",
      "\n",
      "0 wc\n",
      "\n",
      "(8.46)\n",
      "\n",
      "(8.47)\n",
      "\n",
      "c\n",
      "\n",
      "H(cid:4)(W) =H( W) +I C ⊗ V−1 0\n",
      "\n",
      "(8.48)\n",
      "\n",
      "This can be passed to any gradient-based optimizer to ﬁnd the MAP estimate. Note, however, that the Hessian has size O ((CD) × (CD)), which is C times more row and columns than in the binary case, so limited memory BFGS is more appropriate than Newton’s method. See logregFit for some Matlab code.\n",
      "\n",
      "8.4\n",
      "\n",
      "Bayesian logistic regression\n",
      "\n",
      "It is natural to want to compute the full posterior over the parameters, p(w|D), for logistic regression models. This can be useful for any situation where we want to associate conﬁdence intervals with our predictions (e.g., this is necessary when solving contextual bandit problems, discussed in Section 5.7.3.1).\n",
      "\n",
      "Unfortunately, unlike the linear regression case, this cannot be done exactly, since there is no convenient conjugate prior for logistic regression. We discuss one simple approximation below; some other approaches include MCMC (Section 24.3.3.1), variational inference (Section 21.8.1.1), expectation propagation (Kuss and Rasmussen 2005), etc. For notational simplicity, we stick to binary logistic regression.\n",
      "\n",
      "8.4. Bayesian logistic regression\n",
      "\n",
      "255\n",
      "\n",
      "8.4.1\n",
      "\n",
      "Laplace approximation\n",
      "\n",
      "In this section, we discuss how to make a Gaussian approximation to a posterior distribution. The approximation works as follows. Suppose θ ∈ RD. Let\n",
      "\n",
      "p(θ|D) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "e−E(θ)\n",
      "\n",
      "(8.49)\n",
      "\n",
      "where E(θ) is called an energy function, and is equal to the negative log of the unnormal- ized log posterior, E(θ) = − log p(θ, D), with Z = p(D) being the normalization constant. Performing a Taylor series expansion around the mode θ∗ (i.e., the lowest energy state) we get\n",
      "\n",
      "E(θ) ≈ E(θ∗) + (θ − θ∗)T g +\n",
      "\n",
      "1 2\n",
      "\n",
      "(θ − θ∗)T H(θ − θ∗)\n",
      "\n",
      "(8.50)\n",
      "\n",
      "where g is the gradient and H is the Hessian of the energy function evaluated at the mode:\n",
      "\n",
      "g (cid:2) ∇E(θ)\n",
      "\n",
      "( ( θ∗ , H (cid:2)\n",
      "\n",
      "∂2E(θ) ∂θ∂θT\n",
      "\n",
      "|θ∗\n",
      "\n",
      "(8.51)\n",
      "\n",
      "Since θ∗\n",
      "\n",
      "ˆp(θ|D) ≈ 1 Z\n",
      "\n",
      "is the mode, the gradient term is zero. Hence\n",
      "\n",
      "e−E(θ∗) exp\n",
      "\n",
      "(cid:29)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(θ − θ∗)T H(θ − θ∗)\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "(8.52)\n",
      "\n",
      "= N (θ|θ∗, H−1)\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(8.53)\n",
      "\n",
      "Z = p(D) ≈\n",
      "\n",
      "ˆp(θ|D)dθ = e−E(θ∗)(2π)D/2|H|− 1\n",
      "\n",
      "2\n",
      "\n",
      "(8.54)\n",
      "\n",
      "The last line follows from normalization constant of the multivariate Gaussian.\n",
      "\n",
      "Equation 8.54 is known as the Laplace approximation to the marginal likelihood. Therefore Equation 8.52 is sometimes called the the Laplace approximation to the posterior. However, in the statistics community, the term “Laplace approximation” refers to a more sophisticated method (see e.g. It may therefore be better to use the term “Gaussian approximation” to refer to Equation 8.52. A Gaussian approximation is often a reasonable approximation, since posteriors often become more “Gaussian-like” as the sample size increases, for reasons analogous to the central (In physics, there is an analogous technique known as a saddle point approximation.)\n",
      "\n",
      "(Rue et al. 2009) for details).\n",
      "\n",
      "limit theorem.\n",
      "\n",
      "8.4.2\n",
      "\n",
      "Derivation of the BIC\n",
      "\n",
      "We can use the Gaussian approximation to write the log marginal likelihood as follows, dropping irrelevant constants:\n",
      "\n",
      "log p(D) ≈ log p(D|θ∗) + log p(θ∗) − 1 2\n",
      "\n",
      "log |H|\n",
      "\n",
      "(8.55)\n",
      "\n",
      "The penalization terms which are added to the log p(D|θ∗) are sometimes called the Occam factor, and are a measure of model complexity. If we have a uniform prior, p(θ) ∝ 1, we can drop the second term, and replace θ∗ with the MLE, ˆθ.\n",
      "\n",
      "256\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "∇∇ log p(Di|θ). Let us approximate each Hi by a ﬁxed matrix ˆH. Then we have\n",
      "\n",
      "We now focus on approximating the third term. We have H =\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 Hi, where Hi =\n",
      "\n",
      "(8.56) where D = dim(θ) and we have assumed H is full rank. We can drop the log | ˆH| term, since it is independent of N , and thus will get overwhelmed by the likelihood. Putting all the pieces together, we recover the BIC score (Section 5.3.2.4):\n",
      "\n",
      "log |H| = log |N ˆH| = log(N d| ˆH|) = D log N + log | ˆH|\n",
      "\n",
      "log p(D) ≈ log p(D|ˆθ) −\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "log N\n",
      "\n",
      "(8.57)\n",
      "\n",
      "8.4.3\n",
      "\n",
      "Gaussian approximation for logistic regression\n",
      "\n",
      "Now let us apply the Gaussian approximation to logistic regression. We will use a a Gaussian prior of the form p(w) = N (w|0, V0), just as we did in MAP estimation. The approximate posterior is given by\n",
      "\n",
      "p(w|D) ≈ N (w| ˆw, H−1)\n",
      "\n",
      "(8.58)\n",
      "\n",
      "where ˆw = arg minw E(w), E(w) = −(log p(D|w) + log p(w)), and H = ∇2E(w)| ˆw.\n",
      "\n",
      "As an example, consider the linearly separable 2D data in Figure 8.5(a). There are many parameter settings that correspond to lines that perfectly separate the training data; we show 4 examples. The likelihood surface is shown in Figure 8.5(b), where we see that the likelihood is unbounded as we move up and to the right in parameter space, along a ridge where w2/w1 = 2.35 (this is indicated by the diagonal line). The reasons for this is that we can maximize the likelihood by driving ||w|| to inﬁnity (subject to being on this line), since large regression weights make the sigmoid function very steep, turning it into a step function. Consequently the MLE is not well deﬁned when the data is linearly separable.\n",
      "\n",
      "To regularize the problem, let us use a vague spherical prior centered at the origin, N (w|0, 100I).\n",
      "\n",
      "Multiplying this spherical prior by the likelihood surface results in a highly skewed posterior, shown in Figure 8.5(c). (The posterior is skewed because the likelihood function “chops off” regions of parameter space (in a “soft” fashion) which disagree with the data.) The MAP estimate is shown by the blue dot. Unlike the MLE, this is not at inﬁnity.\n",
      "\n",
      "The Gaussian approximation to this posterior is shown in Figure 8.5(d). We see that this is a symmetric distribution, and therefore not a great approximation. Of course, it gets the mode correct (by construction), and it at least represents the fact that there is more uncertainty along the southwest-northeast direction (which corresponds to uncertainty about the orientation of separating lines) than perpendicular to this. Although a crude approximation, this is surely better than approximating the posterior by a delta function, which is what MAP estimation does.\n",
      "\n",
      "8.4.4\n",
      "\n",
      "Approximating the posterior predictive\n",
      "\n",
      "Given the posterior, we can compute credible intervals, perform hypothesis tests, etc., just as we did in Section 7.6.3.3 in the case of linear regression. But in machine learning, interest usually focusses on prediction. The posterior predictive distribution has the form\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(y|x, D) =\n",
      "\n",
      "p(y|x, w)p(w|D)dw\n",
      "\n",
      "(8.59)\n",
      "\n",
      "8.4. Bayesian logistic regression\n",
      "\n",
      "257\n",
      "\n",
      "data\n",
      "\n",
      "Log−Likelihood\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−8 −10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−8\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "8\n",
      "\n",
      "Log−Unnormalised Posterior\n",
      "\n",
      "8\n",
      "\n",
      "Laplace Approximation to Posterior\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−8\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "−8\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 8.5 (a) Two-class data in 2d. (b) Log-likelihood for a logistic regression model. The line is drawn from the origin in the direction of the MLE (which is at inﬁnity). The numbers correspond to 4 points in parameter space, corresponding to the lines in (a). (c) Unnormalized log posterior (assuming vague spherical prior). (d) Laplace approximation to posterior. Based on a ﬁgure by Mark Girolami. Figure generated by logregLaplaceGirolamiDemo.\n",
      "\n",
      "Unfortunately this integral is intractable.\n",
      "\n",
      "The simplest approximation is the plug-in approximation, which, in the binary case, takes the\n",
      "\n",
      "form\n",
      "\n",
      "p(y = 1|x, D) ≈ p(y = 1|x, E [w])\n",
      "\n",
      "(8.60)\n",
      "\n",
      "where E [w] is the posterior mean. In this context, E [w] is called the Bayes point. Of course, such a plug-in estimate underestimates the uncertainty. We discuss some better approximations below.\n",
      "\n",
      "258\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "p(y=1|x, wMAP)\n",
      "\n",
      "decision boundary for sampled w\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−8\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "−8 −10\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "8\n",
      "\n",
      "MC approx of p(y=1|x)\n",
      "\n",
      "8\n",
      "\n",
      "numerical approx of p(y=1|x)\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−8\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "−8\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 8.6 Posterior predictive distribution for a logistic regression model in 2d. Top left: contours of p(y = 1|x, ˆwmap). Top right: samples from the posterior predictive distribution. Bottom left: Averaging over these samples. Bottom right: moderated output (probit approximation). Based on a ﬁgure by Mark Girolami. Figure generated by logregLaplaceGirolamiDemo.\n",
      "\n",
      "8.4.4.1 Monte Carlo approximation\n",
      "\n",
      "A better approach is to use a Monte Carlo approximation, as follows:\n",
      "\n",
      "p(y = 1|x, D) ≈ 1 S\n",
      "\n",
      "S(cid:2)\n",
      "\n",
      "s=1\n",
      "\n",
      "sigm((ws)T x)\n",
      "\n",
      "(8.61)\n",
      "\n",
      "where ws ∼ p(w|D) are samples from the posterior. (This technique can be trivially extended to the multi-class case.) If we have approximated the posterior using Monte Carlo, we can reuse these samples for prediction. If we made a Gaussian approximation to the posterior, we can draw independent samples from the Gaussian using standard methods.\n",
      "\n",
      "Figure 8.6(b) shows samples from the posteiror predictive for our 2d example. Figure 8.6(c)\n",
      "\n",
      "8.4. Bayesian logistic regression\n",
      "\n",
      "259\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "sigmoid probit\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "460\n",
      "\n",
      "480\n",
      "\n",
      "500\n",
      "\n",
      "520\n",
      "\n",
      "540\n",
      "\n",
      "560\n",
      "\n",
      "580\n",
      "\n",
      "600\n",
      "\n",
      "620\n",
      "\n",
      "640\n",
      "\n",
      "0 −6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 8.7 (a) Posterior predictive density for SAT data. The red circle denotes the posterior mean, the blue cross the posterior median, and the blue lines denote the 5th and 95th percentiles of the predictive (b) The logistic (sigmoid) function sigm(x) in distribution. Figure generated by logregSATdemoBayes. solid red, with the rescaled probit function Φ(λx) in dotted blue superimposed. Here λ = π/8, which was chosen so that the derivatives of the two curves match at x = 0. Based on Figure 4.9 of (Bishop 2006b). Figure generated by probitPlot. Figure generated by probitRegDemo.\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "shows the average of these samples. By averaging over multiple predictions, we see that the uncertainty in the decision boundary “splays out” as we move further from the training data. So although the decision boundary is linear, the posterior predictive density is not linear. Note also that the posterior mean decision boundary is roughly equally far from both classes; this is the Bayesian analog of the large margin principle discussed in Section 14.5.2.2.\n",
      "\n",
      "Figure 8.7(a) shows an example in 1d. The red dots denote the mean of the posterior predictive evaluated at the training data. The vertical blue lines denote 95% credible intervals for the posterior predictive; the small blue star is the median. We see that, with the Bayesian approach, we are able to model our uncertainty about the probability a student will pass the exam based on his SAT score, rather than just getting a point estimate.\n",
      "\n",
      "8.4.4.2\n",
      "\n",
      "Probit approximation (moderated output) *\n",
      "\n",
      "If we have a Gaussian approximation to the posterior p(w|D) ≈ N (w|mN , VN ), we can also compute a deterministic approximation to the posterior predictive distribution, at least in the binary case. We proceed as follows:\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(y = 1|x, D) ≈\n",
      "\n",
      "sigm(wT x)p(w|D)dw =\n",
      "\n",
      "sigm(a)N (a|μa, σ2\n",
      "\n",
      "a)da\n",
      "\n",
      "(8.62)\n",
      "\n",
      "a (cid:2) wT x μa (cid:2) E [a] = mT N x (cid:28)\n",
      "\n",
      "σ2 a (cid:2) var [a] = (cid:28)\n",
      "\n",
      "p(a|D)[a2 − E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "a2\n",
      "\n",
      "]da\n",
      "\n",
      "(8.63)\n",
      "\n",
      "(8.64)\n",
      "\n",
      "(8.65)\n",
      "\n",
      "=\n",
      "\n",
      "p(w|D)[(wT x)2 − (mT\n",
      "\n",
      "N x)2]dw = xT VN x\n",
      "\n",
      "(8.66)\n",
      "\n",
      "260\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "Thus we see that we need to evaluate the expectation of a sigmoid with respect to a Gaussian. This can be approximated by exploiting the fact that the sigmoid function is similar to the probit function, which is given by the cdf of the standard normal:\n",
      "\n",
      "(cid:28) a\n",
      "\n",
      "Φ(a) (cid:2)\n",
      "\n",
      "−∞\n",
      "\n",
      "N (x|0, 1)dx\n",
      "\n",
      "(8.67)\n",
      "\n",
      "Figure 8.7(b) plots the sigmoid and probit functions. We have rescaled the axes so that sigm(a) has the same slope as Φ(λa) at the origin, where λ2 = π/8.\n",
      "\n",
      "The advantage of using the probit is that one can convolve it with a Gaussian analytically: (cid:28)\n",
      "\n",
      "Φ(λa)N (a|μ, σ2)da = Φ\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "a (λ−2 + σ2)\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(8.68)\n",
      "\n",
      "We now plug in the approximation sigm(a) ≈ Φ(λa) to both sides of this equation to get\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "sigm(a)N (a|μ, σ2)da ≈ sigm(κ(σ2)μ)\n",
      "\n",
      "(8.69)\n",
      "\n",
      "κ(σ2) (cid:2) (1 + πσ2/8)− 1\n",
      "\n",
      "2\n",
      "\n",
      "(8.70)\n",
      "\n",
      "Applying this to the logistic regression model we get the following expression (ﬁrst suggested in (Spiegelhalter and Lauritzen 1990)):\n",
      "\n",
      "p(y = 1|x, D) ≈ sigm(κ(σ2\n",
      "\n",
      "a)μa)\n",
      "\n",
      "(8.71)\n",
      "\n",
      "Figure 8.6(d) indicates that this gives very similar results to the Monte Carlo approximation.\n",
      "\n",
      "the plug-in estimate. To see this, note that 0 ≤ κ(σ2) ≤ 1 and hence\n",
      "\n",
      "Using Equation 8.71 is sometimes called a moderated output, since it is less extreme than\n",
      "\n",
      "sigm(κ(σ2)μ) ≤ sigm(μ) = p(y = 1|x, ˆw)\n",
      "\n",
      "(8.72)\n",
      "\n",
      "where the inequality is strict if μ (cid:4)= 0. If μ >0, we have p(y = 1|x, ˆw) > 0.5, but the moderated prediction is always closer to 0.5, so it is less conﬁdent. However, the decision boundary occurs whenever p(y = 1|x, D) = sigm(κ(σ2)μ) = 0.5, which implies μ = ˆwT x = 0. Hence the decision boundary for the moderated approximation is the same as for the plug-in approximation. So the number of misclassiﬁcations will be the same for the two methods, but the log-likelihood will not. (Note that in the multiclass case, taking into account posterior covariance gives different answers than the plug-in approach: see Exercise 3.10.3 of (Rasmussen and Williams 2006).)\n",
      "\n",
      "8.4.5\n",
      "\n",
      "Residual analysis (outlier detection) *\n",
      "\n",
      "It is sometimes useful to detect data cases which are “outliers”. This is called residual analysis or case analysis. In a regression setting, this can be performed by computing ri = yi−ˆyi, where ˆyi = ˆwT xi. These values should follow a N (0, σ2) distribution, if the modelling assumptions are correct. This can be assessed by creating a qq-plot, where we plot the N theoretical quantiles of a Gaussian distribution against the N empirical quantiles of the ri. Points that deviate from the straightline are potential outliers.\n",
      "\n",
      "8.5. Online learning and stochastic optimization\n",
      "\n",
      "261\n",
      "\n",
      "Classical methods, based on residuals, do not work well for binary data, because they rely on asymptotic normality of the test statistics. However, adopting a Bayesian approach, we can just deﬁne outliers to be points which which p(yi|ˆyi) is small, where we typically use ˆyi = sigm( ˆwT xi). Note that ˆw was estimated from all the data. A better method is to exclude (xi, yi) from the estimate of w when predicting yi. That is, we deﬁne outliers to be points which have low probability under the cross-validated posterior predictive distribution, deﬁned by\n",
      "\n",
      "p(yi|xi, x−i, y−i) =\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(yi|xi, w)\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "p(yi(cid:2) |xi(cid:2) , w)p(w)dw\n",
      "\n",
      "(8.73)\n",
      "\n",
      "i(cid:2)(cid:7)=i\n",
      "\n",
      "This can be efficiently approximated by sampling methods (Gelfand 1996). For further discussion of residual analysis in logistic regression models, see e.g.,(Johnson and Albert 1999, Sec 3.4).\n",
      "\n",
      "8.5 Online learning and stochastic optimization\n",
      "\n",
      "Traditionally machine learning is performed offline, which means we have a batch of data, and we optimize an equation of the following form\n",
      "\n",
      "f (θ) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "f (θ, zi)\n",
      "\n",
      "(8.74)\n",
      "\n",
      "where zi = (xi, yi) in the supervised case, or just xi in the unsupervised case, and f (θ, zi) is some kind of loss function. For example, we might use\n",
      "\n",
      "f (θ, zi) = − log p(yi|xi, θ)\n",
      "\n",
      "(8.75)\n",
      "\n",
      "in which case we are trying to maximize the likelihood. Alternatively, we might use\n",
      "\n",
      "f (θ, zi) = L(yi, h(xi, θ))\n",
      "\n",
      "(8.76)\n",
      "\n",
      "where h(xi, θ) is a prediction function, and L(y, ˆy) is some other loss function such as squared error or the Huber loss. In frequentist decision theory, the average loss is called the risk (see Section 6.3), so this overall approach is called empirical risk minimization or ERM (see Section 6.5 for details).\n",
      "\n",
      "However, if we have streaming data, we need to perform online learning, so we can update our estimates as each new data point arrives rather than waiting until “the end” (which may never occur). And even if we have a batch of data, we might want to treat it like a stream if it is too large to hold in main memory. Below we discuss learning methods for this kind of scenario.1\n",
      "\n",
      "1. A simple implementation trick can be used to speed up batch learning algorithms when applied to data sets that are too large to hold in memory. First note that the naive implementation makes a pass over the data ﬁle, from the beginning to end, accumulating the sufficient statistics and gradients as it goes; then an update is performed and the process repeats. Unfortunately, at the end of each pass, the data from the beginning of the ﬁle will have been evicted from the cache (since are are assuming it cannot all ﬁt into memory). Rather than going back to the beginning of the ﬁle and reloading it, we can simply work backwards from the end of the ﬁle, which is already in memory. We then repeat this forwards-backwards pattern over the data. This simple trick is known as rocking.\n",
      "\n",
      "262\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "8.5.1\n",
      "\n",
      "Online learning and regret minimization\n",
      "\n",
      "Suppose that at each step, “nature” presents a sample zk and the “learner” must respond with a parameter estimate θk. In the theoretical machine learning community, the objective used in online learning is the regret, which is the averaged loss incurred relative to the best we could have gotten in hindsight using a single ﬁxed parameter value:\n",
      "\n",
      "regretk (cid:2) 1 k\n",
      "\n",
      "k(cid:2)\n",
      "\n",
      "t=1\n",
      "\n",
      "f (θt, zt) − min θ∗∈Θ\n",
      "\n",
      "1 k\n",
      "\n",
      "k(cid:2)\n",
      "\n",
      "t=1\n",
      "\n",
      "f (θ∗, zt)\n",
      "\n",
      "(8.77)\n",
      "\n",
      "For example, imagine we are investing in the stock-market. Let θj be the amount we invest in stock j, and let zj be the return on this stock. Our loss function is f (θ, z) = −θT z. The regret is how much better (or worse) we did by trading at each step, rather than adopting a “buy and hold” strategy using an oracle to choose which stocks to buy.\n",
      "\n",
      "is as follows: at each step k, update the parameters using\n",
      "\n",
      "One simple algorithm for online learning is online gradient descent (Zinkevich 2003), which\n",
      "\n",
      "(8.78) where projV (v) = argminw∈V ||w − v||2 is the projection of vector v onto space V, gk = ∇f (θk, zk) is the gradient, and ηk is the step size. (The projection step is only needed if the parameter must be constrained to live in a certain subset of RD. See Section 13.4.3 for details.) Below we will see how this approach to regret minimization relates to more traditional objectives, such as MLE.\n",
      "\n",
      "θk+1 = projΘ(θk − ηkgk)\n",
      "\n",
      "There are a variety of other approaches to regret minimization which are beyond the scope\n",
      "\n",
      "of this book (see e.g., Cesa-Bianchi and Lugosi (2006) for details).\n",
      "\n",
      "8.5.2\n",
      "\n",
      "Stochastic optimization and risk minimization\n",
      "\n",
      "Now suppose that instead of minimizing regret with respect to the past, we want to minimize expected loss in the future, as is more common in (frequentist) statistical learning theory. That is, we want to minimize f (θ) = E [f (θ, z)]\n",
      "\n",
      "(8.79)\n",
      "\n",
      "where the expectation is taken over future data. Optimizing functions where some of the variables in the objective are random is called stochastic optimization.2\n",
      "\n",
      "Suppose we receive an inﬁnite stream of samples from the distribution. One way to optimize stochastic objectives such as Equation 8.79 is to perform the update in Equation 8.78 at each step. This is called stochastic gradient descent or SGD (Nemirovski and Yudin 1978). Since we typically want a single parameter estimate, we can use a running average:\n",
      "\n",
      "θk =\n",
      "\n",
      "1 k\n",
      "\n",
      "k(cid:2)\n",
      "\n",
      "t=1\n",
      "\n",
      "θt\n",
      "\n",
      "(8.80)\n",
      "\n",
      "2. Note that in stochastic optimization, the objective is stochastic, and therefore the algorithms will be, too. However, it is also possible to apply stochastic optimization algorithms to deterministic objectives. Examples include simulated annealing (Section 24.6.1) and stochastic gradient descent applied to the empirical risk minimization problem. There are some interesting theoretical connections between online learning and stochastic optimization (Cesa-Bianchi and Lugosi 2006), but this is beyond the scope of this book.\n",
      "\n",
      "8.5. Online learning and stochastic optimization\n",
      "\n",
      "263\n",
      "\n",
      "This is called Polyak-Ruppert averaging, and can be implemented recursively as follows:\n",
      "\n",
      "θk = θk−1 − 1 k\n",
      "\n",
      "(θk−1 − θk)\n",
      "\n",
      "(8.81)\n",
      "\n",
      "See e.g., (Spall 2003; Kushner and Yin 2003) for details.\n",
      "\n",
      "8.5.2.1\n",
      "\n",
      "Setting the step size\n",
      "\n",
      "We now discuss some sufficient conditions on the learning rate to guarantee convergence of SGD. These are known as the Robbins-Monro conditions:\n",
      "\n",
      "∞(cid:2)\n",
      "\n",
      "ηk = ∞,\n",
      "\n",
      "∞(cid:2)\n",
      "\n",
      "η2 k < ∞.\n",
      "\n",
      "(8.82)\n",
      "\n",
      "k=1\n",
      "\n",
      "k=1\n",
      "\n",
      "The set of values of ηk over time is called the learning rate schedule. Various formulas are used, such as ηk = 1/k, or the following (Bottou 1998; Bach and Moulines 2011):\n",
      "\n",
      "ηk = (τ0 + k)−κ\n",
      "\n",
      "(8.83)\n",
      "\n",
      "where τ0 ≥ 0 slows down early iterations of the algorithm, and κ ∈ (0.5, 1] controls the rate at which old values of are forgotten.\n",
      "\n",
      "The need to adjust these tuning parameters is one of the main drawback of stochastic optimization. One simple heuristic (Bottou 2007) is as follows: store an initial subset of the data, and try a range of η values on this subset; then choose the one that results in the fastest decrease in the objective and apply it to all the rest of the data. Note that this may not result in convergence, but the algorithm can be terminated when the performance improvement on a hold-out set plateaus (this is called early stopping).\n",
      "\n",
      "8.5.2.2\n",
      "\n",
      "Per-parameter step sizes\n",
      "\n",
      "One drawback of SGD is that it uses the same step size for all parameters. We now brieﬂy present a method known as adagrad (short for adaptive gradient) (Duchi et al. 2010), which is similar in spirit to a diagonal Hessian approximation. (See also (Schaul et al. 2012) for a similar In particular, if θi(k) is parameter i at time k, and gi(k) is its gradient, then we approach.) make an update as follows:\n",
      "\n",
      "θi(k + 1) = θi(k) − η\n",
      "\n",
      "τ0 +\n",
      "\n",
      "gi(k) (cid:17)\n",
      "\n",
      "si(k)\n",
      "\n",
      "(8.84)\n",
      "\n",
      "where the diagonal step size vector is the gradient vector squared, summed over all time steps. This can be recursively updated as follows:\n",
      "\n",
      "si(k) = si(k − 1) + gi(k)2\n",
      "\n",
      "(8.85)\n",
      "\n",
      "The result is a per-parameter step size that adapts to the curvature of the loss function. This method was original derived for the regret minimization case, but it can be applied more generally.\n",
      "\n",
      "264\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "8.5.2.3\n",
      "\n",
      "SGD compared to batch learning\n",
      "\n",
      "If we don’t have an inﬁnite data stream, we can “simulate” one by sampling data points at random from our training set. Essentially we are optimizing Equation 8.74 by treating it as an expectation with respect to the empirical distribution.\n",
      "\n",
      "Algorithm 8.3: Stochastic gradient descent 1 Initialize θ, η; 2 repeat 3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "Randomly permute data; for i = 1 : N do\n",
      "\n",
      "g = ∇f (θ, zi); θ ← projΘ(θ − ηg); Update η;\n",
      "\n",
      "8 until converged;\n",
      "\n",
      "In theory, we should sample with replacement, although in practice it is usually better to randomly permute the data and sample without replacement, and then to repeat. A single such pass over the entire data set is called an epoch. See Algorithm 8 for some pseudocode.\n",
      "\n",
      "In this offline case, it is often better to compute the gradient of a mini-batch of B data cases. If B = 1, this is standard SGD, and if B = N , this is standard steepest descent. Typically B ∼ 100 is used.\n",
      "\n",
      "Although a simple ﬁrst-order method, SGD performs surprisingly well on some problems, especially ones with large data sets (Bottou 2007). The intuitive reason for this is that one can get a fairly good estimate of the gradient by looking at just a few examples. Carefully evaluating precise gradients using large datasets is often a waste of time, since the algorithm will have It is often a better use of computer to recompute the gradient again anyway at the next step. time to have a noisy estimate and to move rapidly through parameter space. As an extreme example, suppose we double the training set by duplicating every example. Batch methods will take twice as long, but online methods will be unaffected, since the direction of the gradient has not changed (doubling the size of the data changes the magnitude of the gradient, but that is irrelevant, since the gradient is being scaled by the step size anyway).\n",
      "\n",
      "In addition to enhanced speed, SGD is often less prone to getting stuck in shallow local minima, because it adds a certain amount of “noise”. Consequently it is quite popular in the machine learning community for ﬁtting models with non-convex objectives, such as neural networks (Section 16.5) and deep belief networks (Section 28.1).\n",
      "\n",
      "8.5.3\n",
      "\n",
      "The LMS algorithm\n",
      "\n",
      "As an example of SGD, let us consider how to compute the MLE for linear regression in an online fashion. We derived the batch gradient in Equation 7.14. The online gradient at iteration k is given by\n",
      "\n",
      "gk = xi(θT\n",
      "\n",
      "k xi − yi)\n",
      "\n",
      "(8.86)\n",
      "\n",
      "8.5. Online learning and stochastic optimization\n",
      "\n",
      "265\n",
      "\n",
      "3\n",
      "\n",
      "black line = LMS trajectory towards LS soln (red cross)\n",
      "\n",
      "10\n",
      "\n",
      "RSS vs iteration\n",
      "\n",
      "2.5\n",
      "\n",
      "9\n",
      "\n",
      "2\n",
      "\n",
      "8\n",
      "\n",
      "1.5\n",
      "\n",
      "1 w\n",
      "\n",
      "1\n",
      "\n",
      "7\n",
      "\n",
      "0.5\n",
      "\n",
      "6\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−0.5\n",
      "\n",
      "4\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1 w0\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 8.8 Illustration of the LMS algorithm. Left: we start from θ = (−0.5, 2) and slowly converging to the least squares solution of ˆθ = (1.45, 0.92) (red cross). Right: plot of objective function over time. Note that it does not decrease monotonically. Figure generated by LMSdemo.\n",
      "\n",
      "where i = i(k) is the training example to use at iteration k. If the data set is streaming, we use i(k) = k; we shall assume this from now on, for notational simplicity. Equation 8.86 is easy to interpret: it is the feature vector xk weighted by the difference between what we predicted, ˆyk = θT\n",
      "\n",
      "k xk, and the true response, yk; hence the gradient acts like an error signal.\n",
      "\n",
      "After computing the gradient, we take a step along it as follows: θk+1 = θk − ηk(ˆyk − yk)xk\n",
      "\n",
      "(8.87)\n",
      "\n",
      "(There is no need for a projection step, since this is an unconstrained optimization problem.) This algorithm is called the least mean squares or LMS algorithm, and is also known as the delta rule, or theWidrow-Hoff rule.\n",
      "\n",
      "start at θ = (−0.5, 2) and converge (in the sense that ||θk − θk−1||2 of 10−2) in about 26 iterations.\n",
      "\n",
      "Figure 8.8 shows the results of applying this algorithm to the data shown in Figure 7.2. We 2 drops below a threshold\n",
      "\n",
      "Note that LMS may require multiple passes through the data to ﬁnd the optimum. By contrast, the recursive least squares algorithm, which is based on the Kalman ﬁlter and which uses second-order information, ﬁnds the optimum in a single pass (see Section 18.2.3). See also Exercise 7.7.\n",
      "\n",
      "8.5.4\n",
      "\n",
      "The perceptron algorithm\n",
      "\n",
      "Now let us consider how to ﬁt a binary logistic regression model in an online manner. The batch gradient was given in Equation 8.5. In the online case, the weight update has the simple form\n",
      "\n",
      "(8.88) where μi = p(yi = 1|xi, θk) = E [yi|xi, θk]. We see that this has exactly the same form as the LMS algorithm. Indeed, this property holds for all generalized linear models (Section 9.3).\n",
      "\n",
      "θk = θk−1 − ηkgi = θk−1 − ηk(μi − yi)xi\n",
      "\n",
      "266\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "We now consider an approximation to this algorithm. Speciﬁcally, let\n",
      "\n",
      "ˆyi = arg max y∈{0,1}\n",
      "\n",
      "p(y|xi, θ)\n",
      "\n",
      "(8.89)\n",
      "\n",
      "represent the most probable class label. We replace μi = p(y = 1|xi, θ) = sigm(θT xi) in the gradient expression with ˆyi. Thus the approximate gradient becomes\n",
      "\n",
      "gi ≈ (ˆyi − yi)xi\n",
      "\n",
      "(8.90)\n",
      "\n",
      "It will make the algebra simpler if we assume y ∈ {−1, +1} rather than y ∈ {0, 1}. case, our prediction becomes\n",
      "\n",
      "In this\n",
      "\n",
      "ˆyi = sign(θT xi)\n",
      "\n",
      "(8.91)\n",
      "\n",
      "Then if ˆyiyi = −1, we have made an error, but if ˆyiyi = +1, we guessed the right label.\n",
      "\n",
      "At each step, we update the weight vector by adding on the gradient. The key observation is that, if we predicted correctly, then ˆyi = yi, so the (approximate) gradient is zero and we do not change the weight vector. But if xi is misclassiﬁed, we update the weights as follows: If ˆyi = 1 but yi = −1, then the negative gradient is −(ˆyi − yi)xi = −2xi; and if ˆyi = −1 but yi = 1, then the negative gradient is −(ˆyi − yi)xi = 2xi. We can absorb the factor of 2 into the learning rate η and just write the update, in the case of a misclassiﬁcation, as\n",
      "\n",
      "θk = θk−1 + ηkyixi\n",
      "\n",
      "(8.92)\n",
      "\n",
      "Since it is only the sign of the weights that matter, not the magnitude, we will set ηk = 1. See Algorithm 11 for the pseudocode.\n",
      "\n",
      "One can show that this method, known as the perceptron algorithm (Rosenblatt 1958), will converge, provided the data is linearly separable, i.e., that there exist parameters θ such that predicting with sign(θT x) achieves 0 error on the training set. However, if the data is not linearly separable, the algorithm will not converge, and even if it does converge, it may take a long time. There are much better ways to train logistic regression models (such as using proper SGD, without the gradient approximation, or IRLS, discussed in Section 8.3.4). However, the perceptron algorithm is historically important: it was one of the ﬁrst machine learning algorithms ever derived (by Frank Rosenblatt in 1957), and was even implemented in analog hardware. In addition, the algorithm can be used to ﬁt models where computing marginals p(yi|x, θ) is more expensive than computing the MAP output, arg maxy p(y|x, θ); this arises in some structured-output classiﬁcation problems. See Section 19.7 for details.\n",
      "\n",
      "8.5.5\n",
      "\n",
      "A Bayesian view\n",
      "\n",
      "Another approach to online learning is to adopt a Bayesian view. This is conceptually quite simple: we just apply Bayes rule recursively:\n",
      "\n",
      "p(θ|D1:k) ∝ p(Dk|θ)p(θ|D1:k−1)\n",
      "\n",
      "(8.93)\n",
      "\n",
      "This has the obvious advantage of returning a posterior instead of just a point estimate. It also allows for the online adaptation of hyper-parameters, which is important since cross-validation cannot be used in an online setting. Finally, it has the (less obvious) advantage that it can be\n",
      "\n",
      "8.6. Generative vs discriminative classiﬁers\n",
      "\n",
      "267\n",
      "\n",
      "Algorithm 8.4: Perceptron algorithm 1 Input: linearly separable data set xi ∈ RD, yi ∈ {−1, +1} for i = 1 : N ; 2 Initialize θ0; 3 k ← 0; 4 repeat 5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "k ← k + 1; i ← k mod N ; if ˆyi (cid:4)= yi then\n",
      "\n",
      "θk+1 ← θk + yixi\n",
      "\n",
      "9\n",
      "\n",
      "else\n",
      "\n",
      "10\n",
      "\n",
      "no-op\n",
      "\n",
      "11 until converged;\n",
      "\n",
      "quicker than SGD. To see why, note that by modeling the posterior variance of each parameter in addition to its mean, we effectively associate a different learning rate for each parameter (de Freitas et al. 2000), which is a simple way to model the curvature of the space. These variances can then be adapted using the usual rules of probability theory. By contrast, getting second-order optimization methods to work online is more tricky (see e.g., (Schraudolph et al. 2007; Sunehag et al. 2009; Bordes et al. 2009, 2010)).\n",
      "\n",
      "As a simple example, in Section 18.2.3 we show how to use the Kalman ﬁlter to ﬁt a linear regression model online. Unlike the LMS algorithm, this converges to the optimal (offline) answer in a single pass over the data. An extension which can learn a robust non-linear regression model in an online fashion is described in (Ting et al. 2010). For the GLM case, we can use an assumed density ﬁlter (Section 18.5.3), where we approximate the posterior by a Gaussian with a diagonal covariance; the variance terms serve as a per-parameter step-size. See Section 18.5.3.2 for details. Another approach is to use particle ﬁltering (Section 23.5); this was used in (Andrieu et al. 2000) for sequentially learning a kernelized linear/logistic regression model.\n",
      "\n",
      "8.6\n",
      "\n",
      "Generative vs discriminative classiﬁers\n",
      "\n",
      "In Section 4.2.2, we showed that the posterior over class labels induced by Gaussian discrim- inant analysis (GDA) has exactly the same form as logistic regression, namely p(y = 1|x) = sigm(wT x). The decision boundary is therefore a linear function of x in both cases. Note, however, that many generative models can give rise to a logistic regression posterior, e.g., if each class-conditional density is Poisson, p(x|y = c) = Poi(x|λc). So the assumptions made by GDA are much stronger than the assumptions made by logistic regression. (cid:10)N\n",
      "\n",
      "inative model, we usually maximize the conditional log likelihood when ﬁtting a generative model, we usually maximize the joint log likelihood, It is clear that these can, in general, give different results (see Exercise 4.20).\n",
      "\n",
      "A further difference between these models is the way they are trained. When ﬁtting a discrim- i=1 log p(yi|xi, θ), whereas\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 log p(yi, xi|θ).\n",
      "\n",
      "When the Gaussian assumptions made by GDA are correct, the model will need less training data than logistic regression to achieve a certain level of performance, but if the Gaussian\n",
      "\n",
      "268\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "assumptions are incorrect, logistic regression will do better (Ng and Jordan 2002). This is because discriminative models do not need to model the distribution of the features. This is in illustrated in Figure 8.10. We see that the class conditional densities are rather complex; particular, p(x|y = 1) is a multimodal distribution, which might be hard to estimate. However, the class posterior, p(y = c|x), is a simple sigmoidal function, centered on the threshold value of 0.55. This suggests that, in general, discriminative methods will be more accurate, since their “job” is in some sense easier. However, accuracy is not the only important factor when choosing a method. Below we discuss some other advantages and disadvantages of each approach.\n",
      "\n",
      "8.6.1\n",
      "\n",
      "Pros and cons of each approach\n",
      "\n",
      "Easy to ﬁt? As we have seen, it is usually very easy to ﬁt generative classiﬁers. For example, in Sections 3.5.1.1 and 4.2.4, we show that we can ﬁt a naive Bayes model and an LDA model by simple counting and averaging. By contrast, logistic regression requires solving a convex optimization problem (see Section 8.3.4 for the details), which is much slower.\n",
      "\n",
      "Fit classes separately? In a generative classiﬁer, we estimate the parameters of each class conditional density independently, so we do not have to retrain the model when we add more classes. In contrast, in discriminative models, all the parameters interact, so the whole model must be retrained if we add a new class. (This is also the case if we train a generative model to maximize a discriminative objective Salojarvi et al. (2005).)\n",
      "\n",
      "Handle missing features easily? Sometimes some of the inputs (components of x) are not observed. In a generative classiﬁer, there is a simple method for dealing with this, as we discuss in Section 8.6.2. However, in a discriminative classiﬁer, there is no principled solution to this problem, since the model assumes that x is always available to be conditioned on (although see (Marlin 2008) for some heuristic approaches).\n",
      "\n",
      "Can handle unlabeled training data? There is much interest in semi-supervised learning, which uses unlabeled data to help solve a supervised task. This is fairly easy to do using generative models (see e.g., (Lasserre et al. 2006; Liang et al. 2007)), but is much harder to do with discriminative models.\n",
      "\n",
      "Symmetric in inputs and outputs? We can run a generative model “backwards”, and infer probable inputs given the output by computing p(x|y). This is not possible with a discriminative model. The reason is that a generative model deﬁnes a joint distribution on x and y, and hence treats both inputs and outputs symmetrically.\n",
      "\n",
      "Can handle feature preprocessing? A big advantage of discriminative methods is that they allow us to preprocess the input in arbitrary ways, e.g., we can replace x with φ(x), which It is often hard to could be some basis function expansion, as illustrated in Figure 8.9. deﬁne a generative model on such pre-processed data, since the new features are correlated in complex ways.\n",
      "\n",
      "Well-calibrated probabilities? Some generative models, such as naive Bayes, make strong independence assumptions which are often not valid. This can result in very extreme poste- rior class probabilities (very near 0 or 1). Discriminative models, such as logistic regression, are usually better calibrated in terms of their probability estimates.\n",
      "\n",
      "We see that there are arguments for and against both kinds of models. It is therefore useful to have both kinds in your “toolbox”. See Table 8.1 for a summary of the classiﬁcation and\n",
      "\n",
      "8.6. Generative vs discriminative classiﬁers\n",
      "\n",
      "269\n",
      "\n",
      "Linear Multinomial Logistic Regression\n",
      "\n",
      "Kernel−RBF Multinomial Logistic Regression\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 8.9 (a) Multinomial logistic regression for 5 classes in the original feature space. (b) After basis function expansion, using RBF kernels with a bandwidth of 1, and using all the data points as centers. Figure generated by logregMultinomKernelDemo.\n",
      "\n",
      "5\n",
      "\n",
      "1.2\n",
      "\n",
      "p(x|y=2)\n",
      "\n",
      "p(y=1|x)\n",
      "\n",
      "p(y=2|x)\n",
      "\n",
      "4\n",
      "\n",
      "1\n",
      "\n",
      "s e i t i s n e d\n",
      "\n",
      "3\n",
      "\n",
      "0.8\n",
      "\n",
      "l\n",
      "\n",
      "a n o i t i d n o c\n",
      "\n",
      "s s a c\n",
      "\n",
      "l\n",
      "\n",
      "2\n",
      "\n",
      "p(x|y=1)\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "1\n",
      "\n",
      "0.2\n",
      "\n",
      "0 0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "x\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "0 0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "x\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 8.10 The class-conditional densities p(x|y = c) (left) may be more complex than the class posteriors p(y = c|x) (right). Figure generated by generativeVsDiscrim.\n",
      "\n",
      "Based on Figure 1.27 of (Bishop 2006a).\n",
      "\n",
      "regression techniques we cover in this book.\n",
      "\n",
      "8.6.2\n",
      "\n",
      "Dealing with missing data\n",
      "\n",
      "Sometimes some of the inputs (components of x) are not observed; this could be due to a sensor failure, or a failure to complete an entry in a survey, etc. This is called the missing data problem (Little. and Rubin 1987). The ability to handle missing data in a principled way is one of the biggest advantages of generative models.\n",
      "\n",
      "To formalize our assumptions, we can associate a binary response variable ri ∈ {0, 1}, that speciﬁes whether each value xi The joint model has the form is observed or not. p(xi, ri|θ, φ) = p(ri|xi, φ)p(xi|θ), where φ are the parameters controlling whether the item\n",
      "\n",
      "270\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "Model Discriminant analysis Naive Bayes classiﬁer Tree-augmented Naive Bayes classiﬁer Linear regression Logistic regression Sparse linear/ logistic regression Mixture of experts Multilayer perceptron (MLP)/ Neural network Conditional random ﬁeld (CRF) K nearest neighbor classiﬁer (Inﬁnite) Mixture Discriminant analysis Classiﬁcation and regression trees (CART) Boosted model Sparse kernelized lin/logreg (SKLR) Relevance vector machine (RVM) Support vector machine (SVM) Gaussian processes (GP) Smoothing splines\n",
      "\n",
      "Classif/regr Classif Classif Classif Regr Classif Both Both Both Classif Classif Classif Both Both Both Both Both Both Regr\n",
      "\n",
      "Gen/Discr Gen Gen Gen Discrim Discrim Discrim Discrim Discrim Discrim Gen Gen Discrim Discrim Discrim Discrim Discrim Discrim Discrim\n",
      "\n",
      "Param/Non Param Param Param Param Param Param Param Param Param Non Non Non Non Non Non Non Non Non\n",
      "\n",
      "Section Sec. 4.2.2, 4.2.4 Sec. 3.5, 3.5.1.2 Sec. 10.2.1 Sec. 1.4.5, 7.3, 7.6, Sec. 1.4.6, 8.3.4, 8.4.3, 21.8.1.1 Ch. 13 Sec. 11.2.4 Ch. 16 Sec. 19.6 Sec. 1.4.2, 14.7.3 Sec. 14.7.3 Sec. 16.2 Sec. 16.4 Sec. 14.3.2 Sec. 14.3.2 Sec. 14.5 Ch. 15 Section 15.4.6\n",
      "\n",
      "Table 8.1 List of various models for classiﬁcation and regression which we discuss in this book. Columns are as follows: Model name; is the model generative or discriminative; is the model parametric or non-parametric; list of sections in book which discuss the model. See also http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tu tSupervised.html for the PMTK equivalents of these models. Any generative probabilistic model (e.g., HMMs, Boltzmann machines, Bayesian networks, etc.) can be turned into a classiﬁer by using it as a class conditional density.\n",
      "\n",
      "is the model suitable for classiﬁcation, regression, or both;\n",
      "\n",
      "is observed or not. If we assume p(ri|xi, φ) = p(ri|φ), we say the data is missing completely at random or MCAR. If we assume p(ri|xi, φ) = p(ri|xo i is the observed part of xi, we say the data is missing at random or MAR. If neither of these assumptions hold, we say the data is not missing at random or NMAR. In this case, we have to model the missing data mechanism, since the pattern of missingness is informative about the values of the missing data and the corresponding parameters. This is the case in most collaborative ﬁltering problems, for example. See e.g., (Marlin 2008) for further discussion. We will henceforth assume the data is MAR.\n",
      "\n",
      "i , φ), where xo\n",
      "\n",
      "When dealing with missing data, it is helpful to distinguish the cases when there is missing- ness only at test time (so the training data is complete data), from the harder case when there is missingness also at training time. We will discuss these two cases below. Note that the class label is always missing at test time, by deﬁnition; if the class label is also sometimes missing at training time, the problem is called semi-supervised learning.\n",
      "\n",
      "8.6. Generative vs discriminative classiﬁers\n",
      "\n",
      "271\n",
      "\n",
      "8.6.2.1 Missing data at test time\n",
      "\n",
      "In a generative classiﬁer, we can handle features that are MAR by marginalizing them out. For example, if we are missing the value of x1, we can compute\n",
      "\n",
      "p(y = c|x2:D, θ) ∝ p(y = c|θ)p(x2:D|y = c, θ) = p(y = c|θ)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(x1, x2:D|y = c, θ)\n",
      "\n",
      "(8.94)\n",
      "\n",
      "(8.95)\n",
      "\n",
      "x1\n",
      "\n",
      "If we make the naive Bayes assumption, the marginalization can be performed as follows: (cid:19)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(x1, x2:D|y = c, θ) =\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(x1|θ1c)\n",
      "\n",
      "D(cid:27)\n",
      "\n",
      "p(xj|θjc) =\n",
      "\n",
      "D(cid:27)\n",
      "\n",
      "p(xj|θjc)\n",
      "\n",
      "(8.96)\n",
      "\n",
      "p(x1|y = c, θ) = 1. Hence in a naive Bayes classiﬁer, we where we exploited the fact that can simply ignore missing features at test time. Similarly, in discriminant analysis, no matter what regularization method was used to estimate the parameters, we can always analytically marginalize out the missing variables (see Section 4.3):\n",
      "\n",
      "x1\n",
      "\n",
      "x1 (cid:10)\n",
      "\n",
      "x1\n",
      "\n",
      "j=2\n",
      "\n",
      "j=2\n",
      "\n",
      "p(x2:D|y = c, θ) = N (x2:D|μc,2:D, Σc,2:D,2:D)\n",
      "\n",
      "(8.97)\n",
      "\n",
      "8.6.2.2 Missing data at training time\n",
      "\n",
      "Missing data at training time is harder to deal with. In particular, computing the MLE or MAP estimate is no longer a simple optimization problem, for reasons discussed in Section 11.3.2. However, soon we will study are a variety of more sophisticated algorithms (such as EM algo- rithm, in Section 11.4) for ﬁnding approximate ML or MAP estimates in such cases.\n",
      "\n",
      "8.6.3\n",
      "\n",
      "Fisher’s linear discriminant analysis (FLDA) *\n",
      "\n",
      "Discriminant analysis is a generative approach to classiﬁcation, which requires ﬁtting an MVN to the features. As we have discussed, this can be problematic in high dimensions. An alternative approach is to reduce the dimensionality of the features x ∈ RD and then ﬁt an MVN to the resulting low-dimensional features z ∈ RL. The simplest approach is to use a linear projection matrix, z = Wx, where W is a L × D matrix. One approach to ﬁnding W would be to use PCA (Section 12.2); the result would be very similar to RDA (Section 4.2.6), since SVD and PCA are essentially equivalent. However, PCA is an unsupervised technique that does not take class labels into account. Thus the resulting low dimensional features are not necessarily optimal for classiﬁcation, as illustrated in Figure 8.11. An alternative approach is to ﬁnd the matrix W such that the low-dimensional data can be classiﬁed as well as possible using a Gaussian class-conditional density model. The assumption of Gaussianity is reasonable since we are computing linear combinations of (potentially non-Gaussian) features. This approach is called Fisher’s linear discriminant analysis, orFLDA.\n",
      "\n",
      "FLDA is an interesting hybrid of discriminative and generative techniques. The drawback of this technique is that it is restricted to using L ≤ C − 1 dimensions, regardless of D, for reasons that we will explain below. In the two-class case, this means we are seeking a single vector w onto which we can project the data. Below we derive the optimal w in the two-class case. We\n",
      "\n",
      "272\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "4\n",
      "\n",
      "means fisher pca\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0 −4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(a)\n",
      "\n",
      "fisher\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0 −45\n",
      "\n",
      "−40\n",
      "\n",
      "−35\n",
      "\n",
      "−30\n",
      "\n",
      "−25\n",
      "\n",
      "−20\n",
      "\n",
      "−15\n",
      "\n",
      "−10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "(b)\n",
      "\n",
      "pca\n",
      "\n",
      "25\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0 −8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 8.11 Example of Fisher’s linear discriminant. (a) Two class data in 2D. Dashed green line = ﬁrst principal basis vector. Dotted red line = Fisher’s linear discriminant vector. Solid black line joins the (c) class-conditional means. Projection of points onto PCA vector shows poor class separation. Figure generated by fisherLDAdemo.\n",
      "\n",
      "(b) Projection of points onto Fisher’s vector shows good class separation.\n",
      "\n",
      "8.6. Generative vs discriminative classiﬁers\n",
      "\n",
      "273\n",
      "\n",
      "then generalize to the multi-class case, and ﬁnally we give a probabilistic interpretation of this technique.\n",
      "\n",
      "8.6.3.1\n",
      "\n",
      "Derivation of the optimal 1d projection\n",
      "\n",
      "We now derive this optimal direction w, for the two-class case, following the presentation of (Bishop 2006b, Sec 4.1.4). Deﬁne the class-conditional means as\n",
      "\n",
      "μ1 =\n",
      "\n",
      "1 N1\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i:yi=1\n",
      "\n",
      "xi, μ2 =\n",
      "\n",
      "1 N2\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i:yi=2\n",
      "\n",
      "xi\n",
      "\n",
      "(8.98)\n",
      "\n",
      "Let mk = wT μk be the projection of each mean onto the line w. Also, let zi = wT xi be the projection of the data onto the line. The variance of the projected points is proportional to\n",
      "\n",
      "s2 k =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(zi − mk)2\n",
      "\n",
      "(8.99)\n",
      "\n",
      "i:yi=k\n",
      "\n",
      "The goal is to ﬁnd w such that we maximize the distance between the means, m2 − m1, while also ensuring the projected clusters are “tight”:\n",
      "\n",
      "J(w) =\n",
      "\n",
      "(m2 − m1)2 1 + s2 s2 2\n",
      "\n",
      "(8.100)\n",
      "\n",
      "We can rewrite the right hand side of the above in terms of w as follows\n",
      "\n",
      "J(w) =\n",
      "\n",
      "wT SBw wT SW w\n",
      "\n",
      "(8.101)\n",
      "\n",
      "where SB is the between-class scatter matrix given by\n",
      "\n",
      "SB = (μ2 − μ1)(μ2 − μ1)T\n",
      "\n",
      "(8.102)\n",
      "\n",
      "and SW is the within-class scatter matrix, given by\n",
      "\n",
      "SW =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(xi − μ1)(xi − μ1)T +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(xi − μ2)(xi − μ2)T\n",
      "\n",
      "(8.103)\n",
      "\n",
      "i:yi=1\n",
      "\n",
      "i:yi=2\n",
      "\n",
      "To see this, note that\n",
      "\n",
      "wT SBw = wT (μ2 − μ1)(μ2 − μ1)T w = (m2 − m1)(m2 − m1)\n",
      "\n",
      "(8.104)\n",
      "\n",
      "and\n",
      "\n",
      "wT SW w =\n",
      "\n",
      "=\n",
      "\n",
      "i:yi=1 (cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(zi − m1)2 +\n",
      "\n",
      "wT (xi − μ1)(xi − μ1)T w + (cid:2)\n",
      "\n",
      "(zi − m2)2\n",
      "\n",
      "i:yi=2\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "wT (xi − μ2)(xi − μ2)T w(8.105)\n",
      "\n",
      "(8.106)\n",
      "\n",
      "i:yi=1\n",
      "\n",
      "i:yi=2\n",
      "\n",
      "Equation 8.101 is a ratio of two scalars; we can take its derivative with respect to w and equate to zero. One can show (Exercise 12.6) that that J(w) is maximized when\n",
      "\n",
      "SBw = λSW w\n",
      "\n",
      "(8.107)\n",
      "\n",
      "274\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "3\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "−0.5\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "−2.5\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3.5\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "−4.5\n",
      "\n",
      "−4\n",
      "\n",
      "−3.5\n",
      "\n",
      "−3\n",
      "\n",
      "−2.5\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 8.12 (a) PCA projection of vowel data to 2d. (b) FLDA projection of vowel data to 2d. We see there is better class separation in the FLDA case. Based on Figure 4.11 of (Hastie et al. 2009). Figure generated by fisherDiscrimVowelDemo, by Hannes Bretschneider.\n",
      "\n",
      "where\n",
      "\n",
      "λ =\n",
      "\n",
      "wT SBw wT SW w\n",
      "\n",
      "(8.108)\n",
      "\n",
      "Equation 8.107 is called a generalized eigenvalue problem. If SW is invertible, we can convert it to a regular eigenvalue problem:\n",
      "\n",
      "S−1 W SBw = λw\n",
      "\n",
      "(8.109)\n",
      "\n",
      "However, in the two class case, there is a simpler solution. In particular, since SBw = (μ2 − μ1)(μ2 − μ1)T w = (μ2 − μ1)(m2 − m1)\n",
      "\n",
      "(8.110)\n",
      "\n",
      "then, from Equation 8.109 we have\n",
      "\n",
      "λ w = S−1 w ∝ S−1\n",
      "\n",
      "W (μ2 − μ1)(m2 − m1) W (μ2 − μ1)\n",
      "\n",
      "(8.111)\n",
      "\n",
      "(8.112)\n",
      "\n",
      "Since we only care about the directionality, and not the scale factor, we can just set\n",
      "\n",
      "w = S−1\n",
      "\n",
      "W (μ2 − μ1)\n",
      "\n",
      "(8.113)\n",
      "\n",
      "This is the optimal solution in the two-class case. If SW ∝ I, meaning the pooled covariance matrix is isotropic, then w is proportional to the vector that joins the class means. This is an intuitively reasonable direction to project onto, as shown in Figure 8.11.\n",
      "\n",
      "8.6.3.2\n",
      "\n",
      "Extension to higher dimensions and multiple classes\n",
      "\n",
      "We can extend the above idea to multiple classes, and to higher dimensional subspaces, by ﬁnding a projection matrix W which maps from D to L so as to maximize\n",
      "\n",
      "J(W) =\n",
      "\n",
      "|WΣBWT | |WΣW WT |\n",
      "\n",
      "(8.114)\n",
      "\n",
      "8.6. Generative vs discriminative classiﬁers\n",
      "\n",
      "275\n",
      "\n",
      "where\n",
      "\n",
      "ΣB (cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "ΣW (cid:2)\n",
      "\n",
      "c (cid:2)\n",
      "\n",
      "c\n",
      "\n",
      "Σc (cid:2) 1 Nc\n",
      "\n",
      "Nc N\n",
      "\n",
      "(μc − μ)(μc − μ)T\n",
      "\n",
      "Nc N (cid:2)\n",
      "\n",
      "Σc\n",
      "\n",
      "(xi − μc)(xi − μc)T\n",
      "\n",
      "i:yi=c\n",
      "\n",
      "(8.115)\n",
      "\n",
      "(8.116)\n",
      "\n",
      "(8.117)\n",
      "\n",
      "The solution can be shown to be\n",
      "\n",
      "W = Σ− 1\n",
      "\n",
      "W U\n",
      "\n",
      "2\n",
      "\n",
      "(8.118)\n",
      "\n",
      "where U are the L leading eigenvectors of Σ− 1 is singular, we can ﬁrst perform PCA on all the data.)\n",
      "\n",
      "Figure 8.12 gives an example of this method applied to some D = 10 dimensional speech data, representing C = 11 different vowel sounds. We see that FLDA gives better class separation than PCA.\n",
      "\n",
      "Note that FLDA is restricted to ﬁnding at most a L ≤ C − 1 dimensional linear subspace, no matter how large D, because the rank of the between class covariance matrix ΣB is C − 1. (The -1 term arises because of the μ term, which is a linear function of the μc.) This is a rather severe restriction which limits the usefulness of FLDA.\n",
      "\n",
      "W ΣBΣ− 1\n",
      "\n",
      "2\n",
      "\n",
      "W , assuming ΣW is non-singular. (If it\n",
      "\n",
      "2\n",
      "\n",
      "8.6.3.3\n",
      "\n",
      "Probabilistic interpretation of FLDA *\n",
      "\n",
      "To ﬁnd a valid probabilistic interpretation of FLDA, we follow the approach of (Kumar and Andreo 1998; Zhou et al. 2009). They proposed a model known as heteroscedastic LDA (HLDA), which works as follows. Let W be a D × D invertible matrix, and let zi = Wxi be a transformed version of the data. We now ﬁt full covariance Gaussians to the transformed data, one per class, but with the constraint that only the ﬁrst L components will be class-speciﬁc; the remaining H = D − L components will be shared across classes, and will thus not be discriminative. That is, we use\n",
      "\n",
      "p(zi|θ, yi = c) =N (zi|μc, Σc)\n",
      "\n",
      "(8.119)\n",
      "\n",
      "Σc (cid:2)\n",
      "\n",
      "μc (cid:2) (mc; m0) (cid:9) (cid:8) Sc 0 0 S0\n",
      "\n",
      "(8.120)\n",
      "\n",
      "(8.121)\n",
      "\n",
      "where m0 is the shared H dimensional mean and S0 is the shared H × H covariace. The pdf of the original (untransformed) data is given by\n",
      "\n",
      "p(xi|yi = c, W, θ) =|W | N (Wxi|μc, Σc)\n",
      "\n",
      "(8.122)\n",
      "\n",
      "where W = W using gradient methods.\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "WL WH\n",
      "\n",
      "(cid:9)\n",
      "\n",
      ". For ﬁxed W, it is easy to derive the MLE for θ. One can then optimize\n",
      "\n",
      "= |W| N (WLxi|mc, Sc) N (WH xi|m0, S0)\n",
      "\n",
      "(8.123)\n",
      "\n",
      "276\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "In the special case that the Σc are diagonal, there is a closed-form solution for W (Gales 1999). And in the special case the Σc are all equal, we recover classical LDA (Zhou et al. 2009). In view of this this result, it should be clear that HLDA will outperform LDA if the class covariances are not equal within the discriminative subspace (i.e., if the assumption that Σc is independent of c is a poor assumption). This is easy to demonstrate on synthetic data, and is also the case on more challenging tasks such as speech recognition (Kumar and Andreo 1998). Furthermore, we can extend the model by allowing each class to use its own projection matrix; this is known as multiple LDA (Gales 2002).\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 8.1 Spam classiﬁcation using logistic regression Consider the email spam data set discussed on p300 of (Hastie et al. 2009). This consists of 4601 email messages, from which 57 features have been extracted. These are as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "48 features, in [0, 100], giving the percentage of words in a given message which match a given word on the list. The list contains words such as “business”, “free”, “george”, etc. (The data was collected by George Forman, so his name occurs quite a lot.) 6 features, in [0, 100], giving the percentage of characters in the email that match a given character on the list. The characters are\n",
      "\n",
      ";\n",
      "\n",
      "(\n",
      "\n",
      "[\n",
      "\n",
      "!\n",
      "\n",
      "$\n",
      "\n",
      "#\n",
      "\n",
      "•\n",
      "\n",
      "\n",
      "\n",
      "Feature 55: The average length of an uninterrupted sequence of capital letters (max is 40.3, mean is 4.9) Feature 56: The length of the longest uninterrupted sequence of capital letters (max is 45.0, mean is 52.6) Feature 57: The sum of the lengts of uninterrupted sequence of capital letters (max is 25.6, mean is 282.2)\n",
      "\n",
      "Load the data from spamData.mat, which contains a training set (of size 3065) and a test set (of size 1536). One can imagine performing several kinds of preprocessing to this data. Try each of the following separately:\n",
      "\n",
      "a. Standardize the columns so they all have mean 0 and unit variance. b. Transform the features using log(xij + 0.1). c. Binarize the features using I(xij > 0).\n",
      "\n",
      "For each version of the data, ﬁt a logistic regression model. Use cross validation to choose the strength of the (cid:7)2 regularizer. Report the mean error rate on the training and test sets. You should get numbers similar to this:\n",
      "\n",
      "method stnd log binary\n",
      "\n",
      "train 0.082 0.052 0.065\n",
      "\n",
      "test 0.079 0.059 0.072\n",
      "\n",
      "(The precise values will depend on what regularization value you choose.) Turn in your code and numerical results. (See also Exercise 8.2.\n",
      "\n",
      "Exercise 8.2 Spam classiﬁcation using naive Bayes We will re-examine the dataset from Exercise 8.1.\n",
      "\n",
      "8.6. Generative vs discriminative classiﬁers\n",
      "\n",
      "277\n",
      "\n",
      "a. Use naiveBayesFit and naiveBayesPredict on the binarized spam data. What is the training and (You can try different settings of the pseudocount α if you like (this corresponds to the\n",
      "\n",
      "test error? Beta(α, α) prior each θjc), although the default of α = 1 is probably ﬁne.) Turn in your error rates. b. Modify the code so it can handle real-valued features. Use a Gaussian density for each feature; ﬁt it with maximum likelihood. What are the training and test error rates on the standardized data and the log transformed data? Turn in your 4 error rates and code.\n",
      "\n",
      "Exercise 8.3 Gradient and Hessian of log-likelihood for logistic regression a. Let σ(a) = 1\n",
      "\n",
      "1+e−a be the sigmoid function. Show that\n",
      "\n",
      "dσ(a) da\n",
      "\n",
      "= σ(a)(1 − σ(a))\n",
      "\n",
      "(8.124)\n",
      "\n",
      "b. Using the previous result and the chain rule of calculus, derive an expression for the gradient of the\n",
      "\n",
      "log likelihood (Equation 8.5).\n",
      "\n",
      "c. The Hessian can be written as H = XT SX, where S (cid:2) diag(μ1(1 − μ1), . . . , μn(1 − μn)). Show (You may assume that 0 < μi < 1, so the elements of S will be strictly\n",
      "\n",
      "that H is positive deﬁnite. positive, and that X is full rank.)\n",
      "\n",
      "Exercise 8.4 Gradient and Hessian of log-likelihood for multinomial logistic regression a. Let μik = S(ηi)k. Prove that the Jacobian of the softmax is\n",
      "\n",
      "∂μik ∂ηij\n",
      "\n",
      "= μik(δkj − μij)\n",
      "\n",
      "(8.125)\n",
      "\n",
      "where δkj = I(k = j).\n",
      "\n",
      "b. Hence show that\n",
      "\n",
      "∇wc (cid:7) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(yic − μic)xi\n",
      "\n",
      "(8.126)\n",
      "\n",
      "i\n",
      "\n",
      "Hint: use the chain rule and the fact that\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "c yic = 1.\n",
      "\n",
      "c. Show that the block submatrix of the Hessian for classes c and c(cid:2) is given by\n",
      "\n",
      "Hc,c(cid:2) = −\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "μic(δc,c(cid:2) − μi,c(cid:2) )xixT i\n",
      "\n",
      "(8.127)\n",
      "\n",
      "i\n",
      "\n",
      "Exercise 8.5 Symmetric version of (cid:7)2 regularized multinomial logistic regression (Source: Ex 18.3 of (Hastie et al. 2009).)\n",
      "\n",
      "Multiclass logistic regression has the form\n",
      "\n",
      "p(y = c|x, W) =\n",
      "\n",
      "(cid:2)C\n",
      "\n",
      "exp(wc0 + wT c x) k=1 exp(wk0 + wT\n",
      "\n",
      "k x)\n",
      "\n",
      "(8.128)\n",
      "\n",
      "where W is a (D + 1)× C weight matrix. We can arbitrarily deﬁne wc = 0 for one of the classes, say c = C, since p(y = C|x, W) = 1 −\n",
      "\n",
      "(cid:2)C−1\n",
      "\n",
      "c=1 p(y = c|x, w). In this case, the model has the form\n",
      "\n",
      "p(y = c|x, W) =\n",
      "\n",
      "1 +\n",
      "\n",
      "(cid:2)C−1\n",
      "\n",
      "exp(wc0 + wT k=1 exp(wk0 + wT\n",
      "\n",
      "c x)\n",
      "\n",
      "k x)\n",
      "\n",
      "(8.129)\n",
      "\n",
      "278\n",
      "\n",
      "Chapter 8. Logistic regression\n",
      "\n",
      "If we don’t “clamp” one of the vectors to some constant value, the parameters will be unidentiﬁable. However, suppose we don’t clamp wc = 0, so we are using Equation 8.128, but we add (cid:7)2 regularization by optimizing\n",
      "\n",
      "N(cid:12)\n",
      "\n",
      "log p(yi|xi, W) − λ\n",
      "\n",
      "C(cid:12)\n",
      "\n",
      "||wc||2 2\n",
      "\n",
      "(8.130)\n",
      "\n",
      "i=1\n",
      "\n",
      "c=1\n",
      "\n",
      "Show that at the optimum we have still need to enforce that w0C = 0 to ensure identiﬁability of the offset.)\n",
      "\n",
      "(cid:2)C\n",
      "\n",
      "c=1 ˆwcj = 0 for j = 1 :D .\n",
      "\n",
      "(For the unregularized ˆwc0 terms, we\n",
      "\n",
      "Exercise 8.6 Elementary properties of (cid:7)2 regularized logistic regression (Source: Jaaakkola.). Consider minimizing J(w) = −(cid:7)(w, Dtrain) + λ||w||2\n",
      "\n",
      "2\n",
      "\n",
      "(8.131)\n",
      "\n",
      "where\n",
      "\n",
      "(cid:7)(w, D) =\n",
      "\n",
      "1 |D|\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "i∈D\n",
      "\n",
      "log σ(yixT\n",
      "\n",
      "i w)\n",
      "\n",
      "(8.132)\n",
      "\n",
      "is the average log-likelihood on data set D, for yi ∈ {−1, +1}. Answer the following true/ false questions.\n",
      "\n",
      "a. J(w) has multiple locally optimal solutions: T/F? b. Let ˆw = arg minw J(w) be a global optimum. ˆw is sparse (has many zero entries): T/F? c. d. (cid:7)( ˆw, Dtrain) always increases as we increase λ: T/F? e. (cid:7)( ˆw, Dtest) always increases as we increase λ: T/F?\n",
      "\n",
      "If the training data is linearly separable, then some weights wj might become inﬁnite if λ = 0: T/F?\n",
      "\n",
      "Exercise 8.7 Regularizing separate terms in 2d logistic regression (Source: Jaaakkola.)\n",
      "\n",
      "a. Consider the data in Figure 8.13, where we ﬁt the model p(y = 1|x, w) =σ (w0 + w1x1 + w2x2).\n",
      "\n",
      "Suppose we ﬁt the model by maximum likelihood, i.e., we minimize\n",
      "\n",
      "(8.133) where (cid:7)(w, Dtrain) is the log likelihood on the training set. Sketch a possible decision boundary corresponding to ˆw. (Copy the ﬁgure ﬁrst (a rough sketch is enough), and then superimpose your answer on your copy, since you will need multiple versions of this ﬁgure). Is your answer (decision boundary) unique? How many classiﬁcation errors does your method make on the training set?\n",
      "\n",
      "J(w) = −(cid:7)(w, Dtrain)\n",
      "\n",
      "b. Now suppose we regularize only the w0 parameter, i.e., we minimize\n",
      "\n",
      "(8.134) Suppose λ is a very large number, so we regularize w0 all the way to 0, but all other parameters are unregularized. Sketch a possible decision boundary. How many classiﬁcation errors does your method make on the training set? Hint: consider the behavior of simple linear regression, w0 + w1x1 + w2x2 when x1 = x2 = 0.\n",
      "\n",
      "J0(w) = −(cid:7)(w, Dtrain) +λw 2\n",
      "\n",
      "0\n",
      "\n",
      "c. Now suppose we heavily regularize only the w1 parameter, i.e., we minimize\n",
      "\n",
      "J1(w) = −(cid:7)(w, Dtrain) +λw 2\n",
      "\n",
      "1\n",
      "\n",
      "(8.135)\n",
      "\n",
      "Sketch a possible decision boundary. How many classiﬁcation errors does your method make on the training set?\n",
      "\n",
      "8.6. Generative vs discriminative classiﬁers\n",
      "\n",
      "279\n",
      "\n",
      "Figure 8.13 Data for logistic regression question.\n",
      "\n",
      "d. Now suppose we heavily regularize only the w2 parameter. Sketch a possible decision boundary. How\n",
      "\n",
      "many classiﬁcation errors does your method make on the training set?\n",
      "\n",
      "9 Generalized linear models and the\n",
      "\n",
      "exponential family\n",
      "\n",
      "9.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "We have now encountered a wide variety of probability distributions: the Gaussian, the Bernoulli, the Student t, the uniform, the gamma, etc. It turns out that most of these are members of a broader class of distributions known as the exponential family.1 In this chapter, we discuss various properties of this family. This allows us to derive theorems and algorithms with very broad applicability.\n",
      "\n",
      "We will see how we can easily use any member of the exponential family as a class-conditional density in order to make a generative classiﬁer. In addition, we will discuss how to build discriminative models, where the response variable has an exponential family distribution, whose mean is a linear function of the inputs; this is known as a generalized linear model, and generalizes the idea of logistic regression to other kinds of response variables.\n",
      "\n",
      "9.2\n",
      "\n",
      "The exponential family\n",
      "\n",
      "Before deﬁning the exponential family, we mention several reasons why it is important:\n",
      "\n",
      "\n",
      "\n",
      "It can be shown that, under certain regularity conditions, the exponential family is the only family of distributions with ﬁnite-sized sufficient statistics, meaning that we can compress the data into a ﬁxed-sized summary without loss of information. This is particularly useful for online learning, as we will see later.\n",
      "\n",
      "The exponential family is the only family of distributions for which conjugate priors exist,\n",
      "\n",
      "which simpliﬁes the computation of the posterior (see Section 9.2.5).\n",
      "\n",
      "The exponential family can be shown to be the family of distributions that makes the least\n",
      "\n",
      "set of assumptions subject to some user-chosen constraints (see Section 9.2.6).\n",
      "\n",
      "The exponential family is at the core of generalized linear models, as discussed in Section 9.3. • The exponential family is at the core of variational inference, as discussed in Section 21.2.\n",
      "\n",
      "1. The exceptions are the Student t, which does not have the right form, and the uniform distribution, which does not have ﬁxed support independent of the parameter values.\n",
      "\n",
      "282\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "9.2.1\n",
      "\n",
      "Deﬁnition A pdf or pmf p(x|θ), for x = (x1, . . . , xm) ∈ X m and θ ∈ Θ ⊆ Rd, is said to be in the exponential family if it is of the form\n",
      "\n",
      "p(x|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "h(x) exp[θT φ(x)]\n",
      "\n",
      "(9.1)\n",
      "\n",
      "= h(x) exp[θT φ(x) − A(θ)]\n",
      "\n",
      "(9.2)\n",
      "\n",
      "where\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "Z(θ) =\n",
      "\n",
      "X m\n",
      "\n",
      "h(x) exp[θT φ(x)]dx\n",
      "\n",
      "(9.3)\n",
      "\n",
      "A(θ) = log Z(θ)\n",
      "\n",
      "(9.4)\n",
      "\n",
      "Here θ are called the natural parameters or canonical parameters, φ(x) ∈ Rd is called a vector of sufficient statistics, Z(θ) is called the partition function, A(θ) is called the log partition function or cumulant function, and h(x) is the a scaling constant, often 1. If φ(x) = x, we say it is a natural exponential family.\n",
      "\n",
      "Equation 9.2 can be generalized by writing\n",
      "\n",
      "p(x|θ) = h(x) exp[η(θ)T φ(x) − A(η(θ))]\n",
      "\n",
      "(9.5)\n",
      "\n",
      "where η is a function that maps the parameters θ to the canonical parameters η = η(θ). If dim(θ) < dim(η(θ)), it is called a curved exponential family, which means we have more If η(θ) =θ , the model is said to be in canonical form. sufficient statistics than parameters. We will assume models are in canonical form unless we state otherwise.\n",
      "\n",
      "9.2.2\n",
      "\n",
      "Examples\n",
      "\n",
      "Let us consider some examples to make things clearer.\n",
      "\n",
      "9.2.2.1\n",
      "\n",
      "Bernoulli\n",
      "\n",
      "The Bernoulli for x ∈ {0, 1} can be written in exponential family form as follows:\n",
      "\n",
      "Ber(x|μ) = μx(1 − μ)1−x = exp[x log(μ) + (1− x) log(1 − μ)] = exp[φ(x)T θ]\n",
      "\n",
      "(9.6)\n",
      "\n",
      "where φ(x) = [I(x = 0), I(x = 1)] and θ = [log(μ), log(1 − μ)]. However, this representation is over-complete since there is a linear dependendence between the features:\n",
      "\n",
      "1T φ(x) =I (x = 0) + I(x = 1) = 1\n",
      "\n",
      "(9.7)\n",
      "\n",
      "Consequently θ is not uniquely identiﬁable. It is common to require that the representation be minimal, which means there is a unique θ associated with the distribution. In this case, we can just deﬁne\n",
      "\n",
      "Ber(x|μ) = (1 − μ) exp\n",
      "\n",
      "(cid:29)\n",
      "\n",
      "x log\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "μ 1 − μ\n",
      "\n",
      "(cid:9)(cid:30)\n",
      "\n",
      "(9.8)\n",
      "\n",
      "9.2. The exponential family\n",
      "\n",
      "283\n",
      "\n",
      "μ Now we have φ(x) =x, θ = log 1−μ can recover the mean parameter μ from the canonical parameter using\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      ", which is the log-odds ratio, and Z = 1/(1 − μ). We\n",
      "\n",
      "μ = sigm(θ) =\n",
      "\n",
      "1 1 + e−θ\n",
      "\n",
      "(9.9)\n",
      "\n",
      "9.2.2.2 Multinoulli\n",
      "\n",
      "We can represent the multinoulli as a minimal exponential family as follows (where xk = I(x = k)):\n",
      "\n",
      "Cat(x|μ) =\n",
      "\n",
      "= exp\n",
      "\n",
      "k=1\n",
      "\n",
      "K(cid:27)\n",
      "\n",
      "μxk\n",
      "\n",
      "k = exp (cid:18)\n",
      "\n",
      "K−1(cid:2)\n",
      "\n",
      "xk log μk +\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "k=1\n",
      "\n",
      "K(cid:2)\n",
      "\n",
      "xk log μk\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "1 −\n",
      "\n",
      "K−1(cid:2)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "xk\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "log(1 −\n",
      "\n",
      "K−1(cid:2)\n",
      "\n",
      "μk)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(9.10)\n",
      "\n",
      "(9.11)\n",
      "\n",
      "= exp\n",
      "\n",
      "= exp\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "k=1 K−1(cid:2)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "k=1 K−1(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "xk log\n",
      "\n",
      "xk log\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "μk μK\n",
      "\n",
      "1 −\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "μk (cid:10)K−1 j=1 μj\n",
      "\n",
      "+ log μK\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "+ log(1−\n",
      "\n",
      "k=1 K−1(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "μk)\n",
      "\n",
      "(9.12)\n",
      "\n",
      "(9.13)\n",
      "\n",
      "where μK = 1 −\n",
      "\n",
      "(cid:10)K−1\n",
      "\n",
      "k=1 μk. We can write this in exponential family form as follows:\n",
      "\n",
      "Cat(x|θ) = exp(θT φ(x) − A(θ)) μK−1 μK\n",
      "\n",
      "θ = [log\n",
      "\n",
      "μ1 μK\n",
      "\n",
      ", . . . , log\n",
      "\n",
      "]\n",
      "\n",
      "(9.14)\n",
      "\n",
      "(9.15)\n",
      "\n",
      "φ(x) = [I(x = 1), . . . , I(x = K − 1)]\n",
      "\n",
      "(9.16)\n",
      "\n",
      "We can recover the mean parameters from the canonical parameters using\n",
      "\n",
      "μk =\n",
      "\n",
      "1 +\n",
      "\n",
      "eθk (cid:10)K−1 j=1 eθj\n",
      "\n",
      "(9.17)\n",
      "\n",
      "From this, we ﬁnd\n",
      "\n",
      "μK = 1 −\n",
      "\n",
      "1 +\n",
      "\n",
      "(cid:10)K−1\n",
      "\n",
      "j=1 eθj (cid:10)K−1\n",
      "\n",
      "j=1 eθj\n",
      "\n",
      "=\n",
      "\n",
      "1 (cid:10)K−1 j=1 eθj\n",
      "\n",
      "(9.18)\n",
      "\n",
      "and hence\n",
      "\n",
      "A(θ) = log\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "1 +\n",
      "\n",
      "K−1(cid:2)\n",
      "\n",
      "eθk\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(9.19)\n",
      "\n",
      "k=1\n",
      "\n",
      "If we deﬁne θK = 0, we can write μ = S(θ) and A(θ) = log softmax function in Equation 4.39.\n",
      "\n",
      "(cid:10)K\n",
      "\n",
      "k=1 eθk , where S is the\n",
      "\n",
      "284\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "9.2.2.3\n",
      "\n",
      "Univariate Gaussian\n",
      "\n",
      "The univariate Gaussian can be written in exponential family form as follows:\n",
      "\n",
      "N (x|μ, σ2) =\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "1 (2πσ2) 1 (2πσ2) 1 Z(θ)\n",
      "\n",
      "exp(θT φ(x))\n",
      "\n",
      "1 2\n",
      "\n",
      "1 2\n",
      "\n",
      "exp[− 1\n",
      "\n",
      "exp[− 1 2σ2\n",
      "\n",
      "2σ2 (x − μ)2] μ x2 + σ2\n",
      "\n",
      "x − 1 2σ2\n",
      "\n",
      "μ2]\n",
      "\n",
      "(9.20)\n",
      "\n",
      "(9.21)\n",
      "\n",
      "(9.22)\n",
      "\n",
      "where\n",
      "\n",
      "θ =\n",
      "\n",
      "φ(x) =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "μ/σ2 −1 2σ2 (cid:9) x x2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(9.23)\n",
      "\n",
      "(9.24)\n",
      "\n",
      "Z(μ, σ2) =\n",
      "\n",
      "A(θ) =\n",
      "\n",
      "√\n",
      "\n",
      "−θ2 1 4θ2\n",
      "\n",
      "2πσ exp[\n",
      "\n",
      "− 1 2\n",
      "\n",
      "μ2 2σ2 ] log(−2θ2) − 1 2\n",
      "\n",
      "log(2π)\n",
      "\n",
      "(9.25)\n",
      "\n",
      "(9.26)\n",
      "\n",
      "9.2.2.4\n",
      "\n",
      "Non-examples\n",
      "\n",
      "Not all distributions of interest belong to the exponential family. For example, the uniform distribution, X ∼ Unif(a, b), does not, since the support of the distribution depends on the parameters. Also, the Student T distribution (Section 11.4.5) does not belong, since it does not have the required form.\n",
      "\n",
      "9.2.3\n",
      "\n",
      "Log partition function\n",
      "\n",
      "An important property of the exponential family is that derivatives of the log partition function For this reason, A(θ) is can be used to generate cumulants of the sufficient statistics.2 sometimes called a cumulant function. We will prove this for a 1-parameter distribution; this can be generalized to a K-parameter distribution in a straightforward way. For the ﬁrst\n",
      "\n",
      "2. The ﬁrst and second cumulants of a distribution are its mean E [X] and variance var [X], whereas the ﬁrst and second moments are its mean E [X] and E\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "X 2\n",
      "\n",
      "(cid:6)\n",
      "\n",
      ".\n",
      "\n",
      "9.2. The exponential family\n",
      "\n",
      "285\n",
      "\n",
      "derivative we have (cid:8)\n",
      "\n",
      "dA dθ\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "d dθ d ) dθ\n",
      "\n",
      ")\n",
      "\n",
      "exp(θφ(x))h(x)dx φ(x) exp(θφ(x))h(x)dx exp(A(θ))\n",
      "\n",
      ")\n",
      "\n",
      "log\n",
      "\n",
      "exp(θφ(x))h(x)dx\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "exp(θφ(x))h(x)dx\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(9.27)\n",
      "\n",
      "(9.28)\n",
      "\n",
      "(9.29)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "φ(x) exp(θφ(x) − A(θ))h(x)dx\n",
      "\n",
      "(9.30)\n",
      "\n",
      "=\n",
      "\n",
      "φ(x)p(x)dx = E [φ(x)]\n",
      "\n",
      "(9.31)\n",
      "\n",
      "For the second derivative we have d2A dθ2 =\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "φ(x) exp (θφ(x) − A(θ)) h(x)(φ(x) − A(cid:4)(θ))dx\n",
      "\n",
      "(9.32)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "φ(x)p(x)(φ(x) − A(cid:4)(θ))dx\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(9.33)\n",
      "\n",
      "=\n",
      "\n",
      "= E\n",
      "\n",
      "φ2(x)p(x)dx − A(cid:4)(θ) (cid:31)\n",
      "\n",
      "φ2(X)\n",
      "\n",
      "− E [φ(x)]\n",
      "\n",
      "2\n",
      "\n",
      "= var [φ(x)]\n",
      "\n",
      "φ(x)p(x)dx\n",
      "\n",
      "(9.34)\n",
      "\n",
      "(9.35)\n",
      "\n",
      "where we used the fact that A(cid:4)(θ) = dA In the multivariate case, we have that\n",
      "\n",
      "dθ = E [φ(x)].\n",
      "\n",
      "∂2A ∂θi∂θj\n",
      "\n",
      "= E [φi(x)φj(x)] − E [φi(x)] E [φj(x)]\n",
      "\n",
      "(9.36)\n",
      "\n",
      "and hence\n",
      "\n",
      "∇2A(θ) = cov [φ(x)]\n",
      "\n",
      "(9.37)\n",
      "\n",
      "Since the covariance is positive deﬁnite, we see that A(θ) is a convex function (see Section 7.3.3).\n",
      "\n",
      "9.2.3.1\n",
      "\n",
      "Example: the Bernoulli distribution For example, consider the Bernoulli distribution. We have A(θ) = log(1 + eθ), so the mean is given by\n",
      "\n",
      "dA dθ\n",
      "\n",
      "=\n",
      "\n",
      "eθ 1 + eθ =\n",
      "\n",
      "1 + e−θ = sigm(θ) = μ\n",
      "\n",
      "1\n",
      "\n",
      "(9.38)\n",
      "\n",
      "The variance is given by\n",
      "\n",
      "d2A dθ2 =\n",
      "\n",
      "d dθ\n",
      "\n",
      "(1 + e−θ)−1 = (1 + e−θ)−2.e−θ\n",
      "\n",
      "(9.39)\n",
      "\n",
      "=\n",
      "\n",
      "e−θ 1 + e−θ\n",
      "\n",
      "1 1 + e−θ =\n",
      "\n",
      "1 eθ + 1\n",
      "\n",
      "1 + e−θ = (1 − μ)μ\n",
      "\n",
      "1\n",
      "\n",
      "(9.40)\n",
      "\n",
      "286\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "9.2.4 MLE for the exponential family\n",
      "\n",
      "The likelihood of an exponential family model has the form (cid:13)\n",
      "\n",
      "p(D|θ) =\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "N(cid:27)\n",
      "\n",
      "h(xi)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "g(θ)N exp\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "η(θ)T [\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "φ(xi)]\n",
      "\n",
      "(9.41)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "We see that the sufficient statistics are N and\n",
      "\n",
      "φ(D) = [\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "φ1(xi), . . . ,\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "φK(xi)]\n",
      "\n",
      "(9.42)\n",
      "\n",
      "For example, for the Bernoulli model we have φ = [ i xi, Gaussian, we have φ = [\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i x2\n",
      "\n",
      "i ]. (We also need to know the sample size, N .)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i I(xi = 1)], and for the univariate\n",
      "\n",
      "The Pitman-Koopman-Darmois theorem states that, under certain regularity conditions, the exponential family is the only family of distributions with ﬁnite sufficient statistics. (Here, ﬁnite means of a size independent of the size of the data set.)\n",
      "\n",
      "One of the conditions required in this theorem is that the support of the distribution not be dependent on the parameter. For a simple example of such a distribution, consider the uniform distribution\n",
      "\n",
      "p(x|θ) = U (x|θ) =\n",
      "\n",
      "1 θ\n",
      "\n",
      "I(0 ≤ x ≤ θ)\n",
      "\n",
      "(9.43)\n",
      "\n",
      "The likelihood is given by\n",
      "\n",
      "p(D|θ) = θ−N I(0 ≤ max{xi} ≤θ )\n",
      "\n",
      "(9.44)\n",
      "\n",
      "So the sufficient statistics are N and s(D) = maxi xi. This is ﬁnite in size, but the uni- form distribution is not in the exponential family because its support set, X , depends on the parameters.\n",
      "\n",
      "N iid data points D = (x1, . . . , xN ), the log-likelihood is\n",
      "\n",
      "We now descibe how to compute the MLE for a canonical exponential family model. Given\n",
      "\n",
      "(9.45) Since −A(θ) is concave in θ, and θT φ(D) is linear in θ, we see that the log likelihood is concave, and hence has a unique global maximum. To derive this maximum, we use the fact that the derivative of the log partition function yields the expected value of the sufficient statistic vector (Section 9.2.3):\n",
      "\n",
      "log p(D|θ) = θT φ(D) − N A(θ)\n",
      "\n",
      "∇θ log p(D|θ) = φ(D) − N E [φ(X)]\n",
      "\n",
      "(9.46)\n",
      "\n",
      "statistics must equal the model’s theoretical expected sufficient statistics, i.e., ˆθ must satisfy\n",
      "\n",
      "Setting this gradient to zero, we see that at the MLE, the empirical average of the sufficient\n",
      "\n",
      "E [φ(X)] =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "φ(xi)\n",
      "\n",
      "(9.47)\n",
      "\n",
      "9.2. The exponential family\n",
      "\n",
      "287\n",
      "\n",
      "This is called moment matching. For example, in the Bernoulli distribution, we have φ(X) = I(X = 1), so the MLE satisﬁes\n",
      "\n",
      "E [φ(X)] = p(X = 1) = ˆμ =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "I(xi = 1)\n",
      "\n",
      "(9.48)\n",
      "\n",
      "9.2.5\n",
      "\n",
      "Bayes for the exponential family *\n",
      "\n",
      "We have seen that exact Bayesian analysis is considerably simpliﬁed if the prior is conjugate to the likelihood. Informally this means that the prior p(θ|τ ) has the same form as the likelihood p(D|θ). For this to make sense, we require that the likelihood have ﬁnite sufficient statistics, so that we can write p(D|θ) = p(s(D)|θ). This suggests that the only family of distributions for which conjugate priors exist is the exponential family. We will derive the form of the prior and posterior below.\n",
      "\n",
      "9.2.5.1\n",
      "\n",
      "Likelihood\n",
      "\n",
      "The likelihood of the exponential family is given by (cid:3)\n",
      "\n",
      "where sN =\n",
      "\n",
      "p(D|θ) ∝ g(θ)N exp (cid:10)N\n",
      "\n",
      "i=1 s(xi). In terms of the canonical parameters this becomes\n",
      "\n",
      "η(θ)T sN\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(9.49)\n",
      "\n",
      "p(D|η) ∝ exp(N ηT s − N A(η))\n",
      "\n",
      "(9.50)\n",
      "\n",
      "where s = 1\n",
      "\n",
      "N sN .\n",
      "\n",
      "9.2.5.2\n",
      "\n",
      "Prior\n",
      "\n",
      "The natural conjugate prior has the form (cid:4) η(θ)T τ 0\n",
      "\n",
      "(9.51) Let us write τ 0 = ν0τ 0, to separate out the size of the prior pseudo-data, ν0, from the mean of the sufficient statistics on this pseudo-data, τ 0. In canonical form, the prior becomes\n",
      "\n",
      "p(θ|ν0, τ 0) ∝ g(θ)ν0 exp\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "p(η|ν0, τ 0) ∝ exp(ν0ηT τ 0 − ν0A(η))\n",
      "\n",
      "(9.52)\n",
      "\n",
      "9.2.5.3\n",
      "\n",
      "Posterior\n",
      "\n",
      "The posterior is given by\n",
      "\n",
      "p(θ|D) = p(θ|νN , τ N ) = p(θ|ν0 + N, τ 0 + sN )\n",
      "\n",
      "(9.53)\n",
      "\n",
      "So we see that we just update the hyper-parameters by adding. In canonical form, this becomes\n",
      "\n",
      "p(η|D) ∝ exp\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "ηT (ν0τ 0 + N s) − (ν0 + N )A(η))\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(9.54)\n",
      "\n",
      "= p(η|ν0 + N,\n",
      "\n",
      "ν0τ 0 + N s ν0 + N\n",
      "\n",
      ")\n",
      "\n",
      "(9.55)\n",
      "\n",
      "So we see that the posterior hyper-parameters are a convex combination of the prior mean hyper-parameters and the average of the sufficient statistics.\n",
      "\n",
      "288\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "9.2.5.4\n",
      "\n",
      "Posterior predictive density Let us derive a generic expression for the predictive density for future observables D(cid:4) = (˜x1, . . . , ˜xN (cid:2) ) given past data D = (x1, . . . , xN ) as follows. For notational brevity, we will combine the sufficient statistics with the size of the data, as follows: ˜τ 0 = (ν0, τ 0), ˜s(D) = (N, s(D)), and ˜s(D(cid:4)) = (N (cid:4), s(D(cid:4))). So the prior becomes\n",
      "\n",
      "p(θ|˜τ 0) =\n",
      "\n",
      "1 Z(˜τ 0)\n",
      "\n",
      "g(θ)ν0 exp(η(θ)T τ 0)\n",
      "\n",
      "(9.56)\n",
      "\n",
      "The likelihood and posterior have a similar form. Hence\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(D(cid:4)|D) =\n",
      "\n",
      "=\n",
      "\n",
      "⎡\n",
      "\n",
      "⎣\n",
      "\n",
      "N (cid:2)(cid:27)\n",
      "\n",
      "p(D(cid:4)|θ)p(θ|D)dθ ⎤\n",
      "\n",
      "h(˜xi)\n",
      "\n",
      "⎦ Z(˜τ 0 + ˜s(D))−1\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "g(θ)ν0+N +N (cid:2) dθ\n",
      "\n",
      "(9.57)\n",
      "\n",
      "(9.58)\n",
      "\n",
      "i=1\n",
      "\n",
      "× exp\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "ηk(θ)(τk +\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "sk(xi) +\n",
      "\n",
      "N (cid:2)(cid:2)\n",
      "\n",
      "⎞\n",
      "\n",
      "sk(˜xi)\n",
      "\n",
      "⎠ dθ\n",
      "\n",
      "(9.59)\n",
      "\n",
      "=\n",
      "\n",
      "⎡\n",
      "\n",
      "⎣\n",
      "\n",
      "N (cid:2)(cid:27)\n",
      "\n",
      "i=1\n",
      "\n",
      "h(˜xi)\n",
      "\n",
      "k ⎤ ⎦ Z(˜τ 0 + ˜s(D) + ˜s(D(cid:4))) Z(˜τ 0 + ˜s(D))\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "(9.60)\n",
      "\n",
      "If N = 0, this becomes the marginal likelihood of D(cid:4), which reduces to the familiar form of normalizer of the posterior divided by the normalizer of the prior, multiplied by a constant.\n",
      "\n",
      "9.2.5.5\n",
      "\n",
      "Example: Bernoulli distribution\n",
      "\n",
      "As a simple example, let us revisit the Beta-Bernoulli model in our new notation.\n",
      "\n",
      "The likelihood is given by\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(D|θ) = (1 − θ)N exp\n",
      "\n",
      "log(\n",
      "\n",
      "θ 1 − θ\n",
      "\n",
      ")\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "xi\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(9.61)\n",
      "\n",
      "Hence the conjugate prior is given by (cid:8)\n",
      "\n",
      "p(θ|ν0, τ0) ∝ (1 − θ)ν0 exp\n",
      "\n",
      "log(\n",
      "\n",
      "θ 1 − θ\n",
      "\n",
      "(cid:9)\n",
      "\n",
      ")τ0\n",
      "\n",
      "(9.62)\n",
      "\n",
      "= θτ0 (1 − θ)ν0−τ0\n",
      "\n",
      "(9.63)\n",
      "\n",
      "If we deﬁne α = τ0 + 1 and β = ν0 − τ0 + 1, we see that this is a beta distribution.\n",
      "\n",
      "We can derive the posterior as follows, where s = p(θ|D) ∝ θτ0+s(1 − θ)ν0−τ0+n−s = θτn (1 − θ)νn−τn\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i I(xi = 1) is the sufficient statistic:\n",
      "\n",
      "(9.64)\n",
      "\n",
      "(9.65)\n",
      "\n",
      "We can derive the posterior predictive distribution as follows. Assume p(θ) = Beta(θ|α, β), and let s = s(D) be the number of heads in the past data. We can predict the probability of a\n",
      "\n",
      "9.2. The exponential family\n",
      "\n",
      "289\n",
      "\n",
      "given sequence of future heads, D(cid:4) = (˜x1, . . . , ˜xm), with sufficient statistic s(cid:4) = 1), as follows:\n",
      "\n",
      "(cid:10)m\n",
      "\n",
      "i=1 I(˜xi =\n",
      "\n",
      "p(D(cid:4)|D) =\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "(cid:28) 1\n",
      "\n",
      "0 Γ(αn + βn) Γ(αn)Γ(βn) Γ(αn + βn) Γ(αn)Γ(βn)\n",
      "\n",
      "p(D(cid:4)|θ|Beta(θ|αn, βn)dθ (cid:28) 1\n",
      "\n",
      "Γ(αn+m)Γ(βn+m) Γ(αn+m + βn+m)\n",
      "\n",
      "0\n",
      "\n",
      "θαn+t(cid:2)−1(1 − θ)βn+m−t(cid:2)−1dθ\n",
      "\n",
      "(9.66)\n",
      "\n",
      "(9.67)\n",
      "\n",
      "(9.68)\n",
      "\n",
      "where\n",
      "\n",
      "αn+m = αn + s(cid:4) = α + s + s(cid:4) βn+m = βn + (m − s(cid:4)) = β + (n − s) + (m − s(cid:4))\n",
      "\n",
      "(9.69)\n",
      "\n",
      "(9.70)\n",
      "\n",
      "9.2.6 Maximum entropy derivation of the exponential family *\n",
      "\n",
      "Although the exponential family is convenient, is there any deeper justiﬁcation for its use? It turns out that there is: it is the distribution that makes the least number of assumptions about the data, subject to a speciﬁc set of user-speciﬁed constraints, as we explain below. In particular, suppose all we know is the expected values of certain features or functions:\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "fk(x)p(x) = Fk\n",
      "\n",
      "(9.71)\n",
      "\n",
      "x\n",
      "\n",
      "where Fk are known constants, and fk(x) is an arbitrary function. The principle of maximum entropy or maxent says we should pick the distribution with maximum entropy (closest to uniform), subject to the constraints that the moments of the distribution match the empirical moments of the speciﬁed functions.\n",
      "\n",
      "p(x) ≥ 0 and\n",
      "\n",
      "To maximize entropy subject to the constraints in Equation 9.71, and the constraints that x p(x) = 1, we need to use Lagrange multipliers. The Lagrangian is given by (cid:2)\n",
      "\n",
      "J(p, λ) = −\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "p(x) log p(x) +λ 0(1 −\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(x)) +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "λk(Fk −\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(x)fk(x))\n",
      "\n",
      "(9.72)\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "k\n",
      "\n",
      "x\n",
      "\n",
      "We can use the calculus of variations to take derivatives wrt the function p, but we will adopt a simpler approach and treat p as a ﬁxed length vector (since we are assuming x is discrete). Then we have ∂J ∂p(x)\n",
      "\n",
      "= −1 − log p(x) − λ0 −\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "k\n",
      "\n",
      "λkfk(x)\n",
      "\n",
      "(9.73)\n",
      "\n",
      "Setting\n",
      "\n",
      "p(x) =\n",
      "\n",
      "∂J ∂p(x) = 0 yields (cid:2)\n",
      "\n",
      "1 Z\n",
      "\n",
      "exp(−\n",
      "\n",
      "k\n",
      "\n",
      "λkfk(x))\n",
      "\n",
      "(9.74)\n",
      "\n",
      "290\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "w\n",
      "\n",
      "xi\n",
      "\n",
      "ηi\n",
      "\n",
      "g−1\n",
      "\n",
      "g\n",
      "\n",
      "μi\n",
      "\n",
      "Ψ\n",
      "\n",
      "Ψ−1\n",
      "\n",
      "θi\n",
      "\n",
      "Figure 9.1 A visualization of the various features of a GLM. Based on Figure 8.3 of (Jordan 2007).\n",
      "\n",
      "where Z = e1+λ0 . Using the sum to one constraint, we have\n",
      "\n",
      "1 =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "x\n",
      "\n",
      "p(x) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "x\n",
      "\n",
      "exp(−\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "k\n",
      "\n",
      "λkfk(x))\n",
      "\n",
      "(9.75)\n",
      "\n",
      "Hence the normalization constant is given by\n",
      "\n",
      "Z =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "exp(−\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "λkfk(x))\n",
      "\n",
      "(9.76)\n",
      "\n",
      "x\n",
      "\n",
      "k\n",
      "\n",
      "Thus the maxent distribution p(x) has the form of the exponential family (Section 9.2), also known as the Gibbs distribution.\n",
      "\n",
      "9.3\n",
      "\n",
      "Generalized linear models (GLMs)\n",
      "\n",
      "Linear and logistic regression are examples of generalized linear models, or GLMs (McCullagh and Nelder 1989). These are models in which the output density is in the exponential family (Section 9.2), and in which the mean parameters are a linear combination of the inputs, passed through a possibly nonlinear function, such as the logistic function. We describe GLMs in more (This excludes multinomial detail below. We focus on scalar outputs for notational simplicity. logistic regression, but this is just to simplify the presentation.)\n",
      "\n",
      "9.3.1\n",
      "\n",
      "Basics\n",
      "\n",
      "To understand GLMs, let us ﬁrst consider the case of an unconditional dstribution for a scalar response variable:\n",
      "\n",
      "p(yi|θ, σ2) = exp\n",
      "\n",
      "(cid:29)\n",
      "\n",
      "yiθ − A(θ) σ2\n",
      "\n",
      "+ c(yi, σ2)\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "(9.77)\n",
      "\n",
      "where σ2 is the dispersion parameter (often set to 1), θ is the natural parameter, A is the partition function, and c is a normalization constant. For example, in the case of logistic regression, θ is the log-odds ratio, θ = log( μ 1−μ ), where μ = E [y] =p( y = 1) is the mean parameter (see Section 9.2.2.1). To convert from the mean parameter to the natural parameter\n",
      "\n",
      "9.3. Generalized linear models (GLMs)\n",
      "\n",
      "291\n",
      "\n",
      "Distrib. N (μ, σ2) Bin(N, μ) Poi(μ)\n",
      "\n",
      "Link g(μ) identity logit log\n",
      "\n",
      "θ = ψ(μ) θ = μ θ = log( μ θ = log(μ)\n",
      "\n",
      "μ = ψ−1(θ) = E [y] μ = θ 1−μ ) μ = sigm(θ) μ = eθ\n",
      "\n",
      "Table 9.1 Canonical link functions ψ and their inverses for some common GLMs.\n",
      "\n",
      "we can use a function ψ, so θ = Ψ(μ). This function is uniquely determined by the form of the exponential family distribution. In fact, this is an invertible mapping, so we have μ = Ψ−1(θ). Furthermore, we know from Section 9.2.3 that the mean is given by the derivative of the partition function, so we have μ = Ψ−1(θ) = A(cid:4)(θ).\n",
      "\n",
      "Now let us add inputs/ covariates. We ﬁrst deﬁne a linear function of the inputs: ηi = wT xi\n",
      "\n",
      "(9.78)\n",
      "\n",
      "We now make the mean of the distribution be some invertible monotonic function of this linear combination. By convention, this function, known as the mean function, is denoted by g−1, so\n",
      "\n",
      "μi = g−1(ηi) = g−1(wT xi)\n",
      "\n",
      "(9.79)\n",
      "\n",
      "See Figure 9.1 for a summary of the basic model.\n",
      "\n",
      "The inverse of the mean function, namely g(), is called the link function. We are free to choose almost any function we like for g, so long as it is invertible, and so long as g−1 has the appropriate range. For example, in logistic regression, we set μi = g−1(ηi) = sigm(ηi).\n",
      "\n",
      "link function. In this case, θi = ηi = wT xi, so the model becomes\n",
      "\n",
      "One particularly simple form of link function is to use g = ψ; this is called the canonical\n",
      "\n",
      "p(yi|xi, w, σ2) = exp\n",
      "\n",
      "(cid:29)\n",
      "\n",
      "yiwT xi − A(wT xi) σ2\n",
      "\n",
      "+ c(yi, σ2)\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "(9.80)\n",
      "\n",
      "In Table 9.1, we list some distributions and their canonical link functions. We see that for the Bernoulli/ binomial distribution, the canonical link is the logit function, g(μ) = log(η/(1 − η)), whose inverse is the logistic function, μ = sigm(η).\n",
      "\n",
      "Based on the results in Section 9.2.3, we can show that the mean and variance of the response\n",
      "\n",
      "variable are as follows:\n",
      "\n",
      "var\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "y|xi, w, σ2 y|xi, w, σ2\n",
      "\n",
      "= μi = A(cid:4)(θi) = σ2\n",
      "\n",
      "i = A(cid:4)(cid:4)(θi)σ2\n",
      "\n",
      "(9.81)\n",
      "\n",
      "(9.82)\n",
      "\n",
      "To make the notation clearer, let us consider some simple examples.\n",
      "\n",
      "For linear regression, we have\n",
      "\n",
      "log p(yi|xi, w, σ2) =\n",
      "\n",
      "yiμi − μ2 σ2\n",
      "\n",
      "i 2\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "y2 i σ2 + log(2πσ2)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(9.83)\n",
      "\n",
      "where yi ∈ R, and θi = μi = wT xi Here A(θ) = θ2/2, soE [yi] = μi and var [yi] = σ2.\n",
      "\n",
      "292\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "For binomial regression, we have\n",
      "\n",
      "log p(yi|xi, w) =y\n",
      "\n",
      "i log(\n",
      "\n",
      "πi 1 − πi\n",
      "\n",
      ") +N i log(1 − πi) + log\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "Ni yi\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(9.84)\n",
      "\n",
      "where yi ∈ {0, 1, . . . , Ni}, πi = sigm(wT xi), θi = log(πi/(1 − πi)) = wT xi, and σ2 = 1. Here A(θ) = Ni log(1 + eθ), so E [yi] = Niπi = μi, var [yi] = Niπi(1 − πi).\n",
      "\n",
      "For poisson regression, we have\n",
      "\n",
      "log p(yi|xi, w) =y\n",
      "\n",
      "i log μi − μi − log(yi!)\n",
      "\n",
      "(9.85)\n",
      "\n",
      "where yi ∈ {0, 1, 2, . . .}, μi = exp(wT xi), θi = log(μi) = wT xi, and σ2 = 1. Here A(θ) =e θ, so E [yi] = var [yi] =μ i. Poisson regression is widely used in bio-statistical applications, where yi might represent the number of diseases of a given person or place, or the number of reads at a genomic location in a high-throughput sequencing context (see e.g., (Kuan et al. 2009)).\n",
      "\n",
      "9.3.2 ML and MAP estimation\n",
      "\n",
      "One of the appealing properties of GLMs is that they can be ﬁt using exactly the same methods that we used to ﬁt logistic regression. In particular, the log-likelihood has the following form:\n",
      "\n",
      "(cid:6)(w) = log p(D|w) =\n",
      "\n",
      "1 σ2\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:6)i\n",
      "\n",
      "(9.86)\n",
      "\n",
      "(cid:6)i (cid:2) θiyi − A(θi)\n",
      "\n",
      "(9.87)\n",
      "\n",
      "We can compute the gradient vector using the chain rule as follows:\n",
      "\n",
      "dθi dμi If we use a canonical link, θi = ηi, this simpliﬁes to\n",
      "\n",
      "∇w(cid:6)(w) =\n",
      "\n",
      "d(cid:6)i dwj\n",
      "\n",
      "=\n",
      "\n",
      "= (yi − A(cid:4)(θi))\n",
      "\n",
      "= (yi − μi)\n",
      "\n",
      "d(cid:6)i dθi\n",
      "\n",
      "1 σ2\n",
      "\n",
      "dθi dμi\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "dμi dηi\n",
      "\n",
      "(yi − μi)xi\n",
      "\n",
      "dηi dwj dθi dμi dμi dηi\n",
      "\n",
      "xij\n",
      "\n",
      "dμi dηi\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "xij\n",
      "\n",
      "(9.88)\n",
      "\n",
      "(9.89)\n",
      "\n",
      "(9.90)\n",
      "\n",
      "(9.91)\n",
      "\n",
      "which is a sum of the input vectors, weighted by the errors. This can be used inside a (stochastic) gradient descent procedure, discussed in Section 8.5.2. However, for improved efficiency, we should use a second-order method. If we use a canonical link, the Hessian is given by\n",
      "\n",
      "H = − 1 σ2\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "dμi dθi\n",
      "\n",
      "xixT\n",
      "\n",
      "i = − 1 σ2\n",
      "\n",
      "XT SX\n",
      "\n",
      "(9.92)\n",
      "\n",
      "9.4. Probit regression\n",
      "\n",
      "293\n",
      "\n",
      "Name Logistic Probit Log-log Complementary log-log\n",
      "\n",
      "Formula g−1(η) = sigm(η) = eη 1+eη g−1(η) = Φ(η) g−1(η) = exp(− exp(−η)) g−1(η) = 1 − exp(− exp(η))\n",
      "\n",
      "Table 9.2 Summary of some possible mean functions for binary regression.\n",
      "\n",
      "where S = diag( dμ1 dθ1 IRLS algorithm (Section 8.3.4). Speciﬁcally, we have the following Newton update:\n",
      "\n",
      ", . . . , dμN dθN\n",
      "\n",
      ") is a diagonal weighting matrix. This can be used inside the\n",
      "\n",
      "wt+1 = (XT StX)−1XT Stzt\n",
      "\n",
      "(9.93)\n",
      "\n",
      "zt = θt + S−1\n",
      "\n",
      "t (y − μt)\n",
      "\n",
      "(9.94)\n",
      "\n",
      "where θt = Xwt and μt = g−1(ηt).\n",
      "\n",
      "If we extend the derivation to handle non-canonical links, we ﬁnd that the Hessian has another term. However, it turns out that the expected Hessian is the same as in Equation 9.92; using the expected Hessian (known as the Fisher information matrix) instead of the actual Hessian is known as the Fisher scoring method.\n",
      "\n",
      "It is straightforward to modify the above procedure to perform MAP estimation with a Gaus- sian prior: we just modify the objective, gradient and Hessian, just as we added (cid:6)2 regularization to logistic regression in Section 8.3.6.\n",
      "\n",
      "9.3.3\n",
      "\n",
      "Bayesian inference\n",
      "\n",
      "Bayesian inference for GLMs is usually conducted using MCMC (Chapter 24). Possible methods include Metropolis Hastings with an IRLS-based proposal (Gamerman 1997), Gibbs sampling using adaptive rejection sampling (ARS) for each full-conditional (Dellaportas and Smith 1993), etc. See e.g., (Dey et al. 2000) for futher information. It is also possible to use the Gaussian approximation (Section 8.4.1) or variational inference (Section 21.8.1.1).\n",
      "\n",
      "9.4\n",
      "\n",
      "Probit regression\n",
      "\n",
      "In (binary) logistic regression, we use a model of the form p(y = 1|xi, w) = sigm(wT xi). In general, we can write p(y = 1|xi, w) = g−1(wT xi), for any function g−1 that maps [−∞, ∞] to [0, 1]. Several possible mean functions are listed in Table 9.2.\n",
      "\n",
      "In this section, we focus on the case where g−1(η) = Φ(η), where Φ(η) is the cdf of the standard normal. This is known as probit regression. The probit function is very similar to the logistic function, as shown in Figure 8.7(b). However, this model has some advantages over logistic regression, as we will see.\n",
      "\n",
      "294\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "9.4.1 ML/MAP estimation using gradient-based optimization\n",
      "\n",
      "We can ﬁnd the MLE for probit regression using standard gradient methods. Let μi = wT xi, and let ˜yi ∈ {−1, +1}. Then the gradient of the log-likelihod for a speciﬁc case is given by\n",
      "\n",
      "gi (cid:2)\n",
      "\n",
      "d dw\n",
      "\n",
      "log p(˜yi|wT xi) =\n",
      "\n",
      "dμi dw\n",
      "\n",
      "d dμi\n",
      "\n",
      "log p(˜yi|wT xi) = xi\n",
      "\n",
      "˜yiφ(μi) Φ(˜yiμi)\n",
      "\n",
      "(9.95)\n",
      "\n",
      "where φ is the standard normal pdf, and Φ is its cdf. Similarly, the Hessian for a single case is given by\n",
      "\n",
      "φ(μi)2 Φ(˜yiμi)2 + We can modify these expressions to compute the MAP estimate in a straightforward manner. In particular, if we use the prior p(w) =N (0, V0), the gradient and Hessian of the penalized i gi + 2V−1 0 w and 0 . These expressions can be log likelihood have the form passed to any gradient-based optimizer. See probitRegDemo for a demo.\n",
      "\n",
      "Hi =\n",
      "\n",
      "d dw2 log p(˜yi|wT xi) = −xi\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "˜yiμiφ(μi) Φ(˜yiμi)\n",
      "\n",
      "i Hi + 2V−1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "xT i\n",
      "\n",
      "(9.96)\n",
      "\n",
      "9.4.2\n",
      "\n",
      "Latent variable interpretation\n",
      "\n",
      "We can interpret the probit (and logistic) model as follows. First, let us associate each item xi with two latent utilities, u0i and u1i, corresponding to the possible choices of yi = 0 and yi = 1. We then assume that the observed choice is whichever action has larger utility. More precisely, the model is as follows:\n",
      "\n",
      "u0i (cid:2) wT 0 xi + δ0i u1i (cid:2) wT 1 xi + δ1i yi = I(u1i > u10)\n",
      "\n",
      "(9.97)\n",
      "\n",
      "(9.98)\n",
      "\n",
      "(9.99)\n",
      "\n",
      "where δ’s are error terms, representing all the other factors that might be relevant in decision making that we have chosen not to (or are unable to) model. This is called a random utility model or RUM (McFadden 1974; Train 2009).\n",
      "\n",
      "(cid:8)i = δ1i − δ0i. If the δ’s have a Gaussian distribution, then so does (cid:8)i. Thus we can write\n",
      "\n",
      "Since it is only the difference in utilities that matters, let us deﬁne zi = u1i − u0i + (cid:8)i, where\n",
      "\n",
      "zi (cid:2) wT xi + (cid:8)i (cid:8)i ∼ N (0, 1)\n",
      "\n",
      "yi = 1 = I(zi ≥ 0)\n",
      "\n",
      "(9.100)\n",
      "\n",
      "(9.101) (9.102)\n",
      "\n",
      "Following (Fruhwirth-Schnatter and Fruhwirth 2010), we call this the difference RUM or dRUM model.\n",
      "\n",
      "When we marginalize out zi, we recover the probit model:\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(yi = 1|xi, w) =\n",
      "\n",
      "I(zi ≥ 0)N (zi|wT xi, 1)dzi\n",
      "\n",
      "(9.103)\n",
      "\n",
      "= p(wT xi + (cid:8) ≥ 0) = p((cid:8) ≥ −wT xi) = 1 − Φ(−wT xi) = Φ(wT xi)\n",
      "\n",
      "(9.104)\n",
      "\n",
      "(9.105)\n",
      "\n",
      "9.4. Probit regression\n",
      "\n",
      "295\n",
      "\n",
      "where we used the symmetry of the Gaussian.3 This latent variable interpretation provides an alternative way to ﬁt the model, as discussed in Section 11.4.6.\n",
      "\n",
      "(cid:8)i, and the model reduces to logistic regression. See Section 24.5.1 for further details.\n",
      "\n",
      "Interestingly, if we use a Gumbel distribution for the δ’s, we induce a logistic distibution for\n",
      "\n",
      "9.4.3\n",
      "\n",
      "Ordinal probit regression *\n",
      "\n",
      "One advantage of the latent variable interpretation of probit regression is that it is easy to extend to the case where the response variable is ordinal, that is, it can take on C discrete values which can be ordered in some way, such as low, medium and high. This is called ordinal regression. The basic idea is as follows. We introduce C + 1 thresholds γj and set\n",
      "\n",
      "yi = j\n",
      "\n",
      "if γj−1 < zi ≤ γj\n",
      "\n",
      "(9.106)\n",
      "\n",
      "where γ0 ≤ · · · ≤ γC. For identiﬁability reasons, we set γ0 = −∞, γ1 = 0 and γC = ∞. For example, if C = 2, this reduces to the standard binary probit model, whereby zi < 0 produces yi = 0 and zi ≥ 0 produces yi = 1. If C = 3, we partition the real line into 3 intervals: (−∞, 0], (0, γ2], (γ2, ∞). We can vary the parameter γ2 to ensure the right relative amount of probability mass falls in each interval, so as to match the empirical frequencies of each class label.\n",
      "\n",
      "Finding the MLEs for this model is a bit trickier than for binary probit regression, since we need to optimize for w and γ, and the latter must obey an ordering constraint. See e.g., (Kawakatsu and Largey 2009) for an approach based on EM. It is also possible to derive a simple Gibbs sampling algorithm for this model (see e.g., (Hoff 2009, p216)).\n",
      "\n",
      "9.4.4 Multinomial probit models *\n",
      "\n",
      "Now consider the case where the response variable can take on C unordered categorical values, yi ∈ {1, . . . , C}. The multinomial probit model is deﬁned as follows:\n",
      "\n",
      "zic = wT xic + (cid:8)ic (cid:7) ∼ N (0, R) yi = arg max\n",
      "\n",
      "c\n",
      "\n",
      "zic\n",
      "\n",
      "(9.107) (9.108)\n",
      "\n",
      "(9.109)\n",
      "\n",
      "(Dow and Endersby 2004; Scott 2009; Fruhwirth-Schnatter and Fruhwirth 2010) for See e.g., more details on the model and its connection to multinomial logistic regression. (By deﬁning w = [w1, . . . , wC], and xic = [0, . . . , 0, xi, 0, . . . , 0], we can recover the more familiar formulation zic = xT i wc.) Since only relative utilities matter, we constrain R to be a correlation matrix. If instead of setting yi = argmaxc zic we use yic = I(zic > 0), we get a model known as multivariate probit, which is one way to model C correlated binary outcomes (see e.g., (Talhouk et al. 2011)).\n",
      "\n",
      "3. Note that the assumption that the Gaussian noise term is zero mean and unit variance is made without loss of generality. To see why, suppose we used some other mean μ and variance σ2. Then we could easily rescale w and add an offset term without changing the likelihood. since P (N (0, 1) ≥ −wT x) = P (N (μ, σ2) ≥ −(wT x + μ)/σ).\n",
      "\n",
      "296\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "9.5 Multi-task learning\n",
      "\n",
      "Sometimes we want to ﬁt many related classiﬁcation or regression models. It is often reasonable to assume the input-output mapping is similar across these different models, so we can get better performance by ﬁtting all the parameters at the same time. In machine learning, this setup is often called multi-task learning (Caruana 1998), transfer learning (e.g., (Raina et al. 2005)), or learning to learn (Thrun and Pratt 1997). In statistics, this is usually tackled using hierarchical Bayesian models (Bakker and Heskes 2003), as we discuss below, although there are other possible methods (see e.g., (Chai 2010)).\n",
      "\n",
      "9.5.1\n",
      "\n",
      "Hierarchical Bayes for multi-task learning\n",
      "\n",
      "Let yij be the response of the i’th item in groupj , for i = 1 :N j and j = 1 :J . For example, j might index schools, i might index students within a school, and yij might be the test score, as in Section 5.6.2. Or j might index people, and i might index purchaes, and yij might be the identity of the item that was purchased (this is known as discrete choice modeling (Train 2009)). Let xij be a feature vector associated with yij. The goal is to ﬁt the models p(yj|xj) for all j.\n",
      "\n",
      "Although some groups may have lots of data, there is often a long tail, where the majority of groups have little data. Thus we can’t reliably ﬁt each model separately, but we don’t want to use the same model for all groups. As a compromise, we can ﬁt a separate model for each group, but encourage the model parameters to be similar across groups. More precisely, suppose E [yij|xij] = g(xT ijβj), where g is the link function for the GLM. Furthermore, suppose βj ∼ N (β∗, σ2 ∗I). In this model, groups with small sample size borrow statistical strength from the groups with larger sample size, because the βj’s are correlated via the latent common parents β∗ (see Section 5.5 for further discussion of this point). The term σ2 ∗ term controls the strength of the overall prior.\n",
      "\n",
      "Suppose, for simplicity, that μ = 0, and that σ2\n",
      "\n",
      "j controls how much group j depends on the common parents and the σ2\n",
      "\n",
      "j I), and that β∗ ∼ N (μ, σ2\n",
      "\n",
      "j and σ2\n",
      "\n",
      "∗ are all known (e.g., they could be set\n",
      "\n",
      "by cross validation). The overall log probability has the form\n",
      "\n",
      "log p(D|β) + log p(β) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "j\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "log p(Dj|βj) −\n",
      "\n",
      "||βj − β∗||2 2σ2 j\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "−\n",
      "\n",
      "||β∗||2 2σ2 ∗\n",
      "\n",
      "(9.110)\n",
      "\n",
      "We can perform MAP estimation of β = (β1:J , β∗) using standard gradient methods. Alter- natively, we can perform an iterative optimization scheme, alternating between optimizing the βj and the β∗; since the likelihood and prior are convex, this is guaranteed to converge to the global optimum. Note that once the models are trained, we can discard β∗, and use each model separately.\n",
      "\n",
      "9.5.2\n",
      "\n",
      "Application to personalized email spam ﬁltering\n",
      "\n",
      "An interesting application of multi-task learning is personalized spam ﬁltering. Suppose we want to ﬁt one classiﬁer per user, βj. Since most users do not label their email as spam or not, it will be hard to estimate these models independently. So we will let the βj have a common prior β∗, representing the parameters of a generic user.\n",
      "\n",
      "9.5. Multi-task learning\n",
      "\n",
      "297\n",
      "\n",
      "In this case, we can emulate the behavior of the above model with a simple trick (Daume 2007b; Attenberg et al. 2009; Weinberger et al. 2009): we make two copies of each feature xi, one concatenated with the user id, and one not. The effect will be to learn a predictor of the form\n",
      "\n",
      "E [yi|xi, u] = (β∗, w1, · · · , wJ )T [xi, I(u = 1)xi, · · · , I(u = J)xi]\n",
      "\n",
      "(9.111)\n",
      "\n",
      "where u is the user id. In other words,\n",
      "\n",
      "E [yi|xi, u = j] = (βT\n",
      "\n",
      "∗ + wj)T xi\n",
      "\n",
      "(9.112)\n",
      "\n",
      "Thus β∗ will be estimated from everyone’s email, whereas wj will just be estimated from user j’s email.\n",
      "\n",
      "To see the correspondence with the above hierarchical Bayesian model, deﬁne wj = βj − β∗.\n",
      "\n",
      "Then the log probability of the original model can be rewritten as\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "j\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "log p(Dj|β∗ + wj) −\n",
      "\n",
      "||wj||2 2σ2 j\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "−\n",
      "\n",
      "||β∗||2 2σ2 ∗\n",
      "\n",
      "(9.113)\n",
      "\n",
      "If we assume σ2 ∗, the effect is the same as using the augmented feature trick, with the same regularizer strength for both wj and β∗. However, one typically gets better performance by not requiring that σ2\n",
      "\n",
      "j = σ2\n",
      "\n",
      "j be equal to σ2\n",
      "\n",
      "∗ (Finkel and Manning 2009).\n",
      "\n",
      "9.5.3\n",
      "\n",
      "Application to domain adaptation\n",
      "\n",
      "Domain adaptation is the problem of training a set of classiﬁers on data drawn from different distributions, such as email and newswire text. This problem is obviously a special case of multi-task learning, where the tasks are the same.\n",
      "\n",
      "(Finkel and Manning 2009) used the above hierarchical Bayesian model to perform domain adaptation for two NLP tasks, namely named entity recognition and parsing. They report reason- ably large improvements over ﬁtting separate models to each dataset, and small improvements over the approach of pooling all the data and ﬁtting a single model.\n",
      "\n",
      "9.5.4\n",
      "\n",
      "Other kinds of prior\n",
      "\n",
      "In multi-task learning, it is common to assume that the prior is Gaussian. However, sometimes other priors are more suitable. For example, consider the task of conjoint analysis, which requires ﬁguring out which features of a product customers like best. This can be modelled using the same hierarchical Bayesian setup as above, but where we use a sparsity-promoting prior on βj, rather than a Gaussian prior. This is called multi-task feature selection. See e.g., (Lenk et al. 1996; Argyriou et al. 2008) for some possible approaches.\n",
      "\n",
      "If we pool the parameters across tasks that are qualitatively different, the performance will be worse than not using pooling, because the inductive bias of our prior is wrong. Indeed, it has been found experimentally that sometimes multi-task learning does worse than solving each task separately (this is called negative transfer).\n",
      "\n",
      "It is not always reasonable to assume that all tasks are all equally similar.\n",
      "\n",
      "298\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "One way around this problem is to use a more ﬂexible prior, such as a mixture of Gaussians. Such ﬂexible priors can provide robustness against prior mis-speciﬁcation. See e.g., (Xue et al. 2007; Jacob et al. 2008) for details. One can of course combine mixtures with sparsity-promoting priors (Ji et al. 2009). Many other variants are possible.\n",
      "\n",
      "9.6\n",
      "\n",
      "Generalized linear mixed models *\n",
      "\n",
      "Suppose we generalize the multi-task learning scenario to allow the response to include infor- mation at the group level, xj, as well as at the item level, xij. Similarly, we can allow the parameters to vary across groups, βj, or to be tied across groups, α. This gives rise to the following model:\n",
      "\n",
      "E [yij|xij, xj] = g\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "φ1(xij)T βj + φ2(xj)T β(cid:4)\n",
      "\n",
      "j + φ3(xij)T α + φ4(xj)T α(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(9.114)\n",
      "\n",
      "where the φk are basis functions. This model can be represented pictorially as shown in (Such ﬁgures will be explained in Chapter 10.) Note that the number of βj Figure 9.2(a). parameters grows with the number of groups, whereas the size of α is ﬁxed.\n",
      "\n",
      "Frequentists call the terms βj random effects, since they vary randomly across groups, but they call α a ﬁxed effect, since it is viewed as a ﬁxed but unknown constant. A model with both ﬁxed and random effects is called a mixed model. If p(y|x) is a GLM, the overall model is called a generalized linear mixed effects model or GLMM. Such models are widely used in statistics.\n",
      "\n",
      "9.6.1\n",
      "\n",
      "Example: semi-parametric GLMMs for medical data\n",
      "\n",
      "Consider the following example from (Wand 2009). Suppose yij is the amount of spinal bone mineral density (SBMD) for person j at measurement i. Let xij be the age of person, and let xj be their ethnicity, which can be one of: White, Asian, Black, or Hispanic. The primary goal is to determine if there are signiﬁcant differences in the mean SBMD among the four ethnic groups, after accounting for age. The data is shown in the light gray lines in Figure 9.2(b). We see that there is a nonlinear effect of SBMD vs age, so we will use a semi-parametric model which combines linear regression with non-parametric regression (Ruppert et al. 2003). We also see that there is variation across individuals within each group, so we will use a mixed effects model. Speciﬁcally, we will use φ1(xij) = 1 to account for the random effect of each person; φ2(xij) = 0 since no other coefficients are person-speciﬁc; φ3(xij) = [bk(xij)], where bk is the k’th spline basis functions (see Section 15.4.6.2), to account for the nonlinear effect of age; and φ4(xj) = [I(xj = w), I(xj = a), I(xj = b), I(xj = h)] to account for the effect of the different ethnicities. Furthermore, we use a linear link function. The overall model is therefore\n",
      "\n",
      "E [yij|xij, xj] =β j + αT b(xij) +(cid:8) ij wI(xj = w) +α (cid:4)\n",
      "\n",
      "+α(cid:4)\n",
      "\n",
      "aI(xj = a) +α (cid:4)\n",
      "\n",
      "bI(xj = b) +α (cid:4)\n",
      "\n",
      "hI(xj = h)\n",
      "\n",
      "(9.115)\n",
      "\n",
      "(9.116)\n",
      "\n",
      "where (cid:8)ij ∼ N (0, σ2 y). α contains the non-parametric part of the model related to age, α(cid:4) contains the parametric part of the model related to ethnicity, and βj is a random offset for person j. We endow all of these regression coefficients with separate Gaussian priors. We can then perform posterior inference to compute p(α, α(cid:4), β, σ2|D) (see Section 9.6.2 for\n",
      "\n",
      "9.6. Generalized linear mixed models *\n",
      "\n",
      "299\n",
      "\n",
      "μ β\n",
      "\n",
      "σ2 β\n",
      "\n",
      "β\n",
      "\n",
      "j\n",
      "\n",
      "μ α σ2 α\n",
      "\n",
      "α\n",
      "\n",
      "xj\n",
      "\n",
      "y\n",
      "\n",
      "ij\n",
      "\n",
      "xij Nj\n",
      "\n",
      "J\n",
      "\n",
      "σ2 y\n",
      "\n",
      "(a)\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "Hispanic\n",
      "\n",
      "White\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "2\n",
      "\n",
      ")\n",
      "\n",
      "m c / g (\n",
      "\n",
      "y t i s n e d\n",
      "\n",
      "1.0\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "l\n",
      "\n",
      "a r e n m e n o b\n",
      "\n",
      "i\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "Asian\n",
      "\n",
      "Black\n",
      "\n",
      "l\n",
      "\n",
      "a n p s\n",
      "\n",
      "i\n",
      "\n",
      "1.0\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25 age in years\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 9.2 (a) Directed graphical model for generalized linear mixed effects model with J groups. (b) Spinal bone mineral density vs age for four different ethnic groups. Raw data is shown in the light gray lines. Fitted model shown in black (solid is the posterior predicted mean, dotted is the posterior predictive variance). From Figure 9 of (Wand 2009). Used with kind permission of Matt Wand\n",
      "\n",
      "300\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "computational details). After ﬁtting the model, we can compute the prediction for each group. See Figure 9.2(b) for the results. We can also perform signiﬁcance testing, by computing p(αg − αw|D) for each ethnic group g relative to some baseline (say, White), as we did in Section 5.2.3.\n",
      "\n",
      "9.6.2\n",
      "\n",
      "Computational issues\n",
      "\n",
      "The principle problem with GLMMs is that they can be difficult to ﬁt, for two reasons. First, p(yij|θ) may not be conjugate to the prior p(θ) where θ = (α, β). Second, there are two levels of unknowns in the model, namely the regression coefficients θ and the means and variances of the priors η = (μ, σ).\n",
      "\n",
      "One approach is to adopt fully Bayesian inference methods, such as variational Bayes (Hall et al. 2011) or MCMC (Gelman and Hill 2007). We discuss VB in Section 21.5, and MCMC in Section 24.1.\n",
      "\n",
      "An alternative approach is to use empirical Bayes, which we discuss in general terms in Section 5.6. In the context of a GLMM, we can use the EM algorithm (Section 11.4), where in the E step we compute p(θ|η, D), and in the M step we optimize η. If the linear regression setting, the E step can be performed exactly, but in general we need to use approximations. Traditional methods use numerical quadrature or Monte Carlo (see e.g., (Breslow and Clayton 1993)). A faster approach is to use variational EM; see (Braun and McAuliffe 2010) for an application of variational EM to a multi-level discrete choice modeling problem.\n",
      "\n",
      "In frequentist statistics, there is a popular method for ﬁtting GLMMs called generalized estimating equations or GEE (Hardin and Hilbe 2003). However, we do not recommend this approach, since it is not as statistically efficient as likelihood-based methods (see Section 6.4.3). In addition, it can only provide estimates of the population parameters α, but not the random effects βj, which are sometimes of interest in themselves.\n",
      "\n",
      "9.7\n",
      "\n",
      "Learning to rank *\n",
      "\n",
      "In this section, we discuss the learning to rank or LETOR problem. That is, we want to learn a function that can rank order a set of items (we will be more precise below). The most common application is to information retrieval. Speciﬁcally, suppose we have a query q and a set of documents d1, . . . , dm that might be relevant to q (e.g., all documents that contain the string q). We would like to sort these documents in decreasing order of relevance and show the top k to the user. Similar problems arise in other areas, such as collaborative ﬁltering. (Ranking players in a game or tournament setting is a slightly different kind of problem; see Section 22.5.5.)\n",
      "\n",
      "Below we summarize some methods for solving this problem, following the presentation of (Liu 2009). This material is not based on GLMs, but we include it in this chapter anyway for lack of a better place.\n",
      "\n",
      "A standard way to measure the relevance of a document d to a query q is to use a probabilistic language model based on a bag of words model. That is, we deﬁne sim(q, d) (cid:2) p(q|d) = ’n is the i’th word or term, and p(qi|d) is a multinoulli distribution i=1 p(qi|d), where qi estimated from document d. In practice, we need to smooth the estimated distribution, for example by using a Dirichlet prior, representing the overall frequency of each word. This can be\n",
      "\n",
      "9.7. Learning to rank *\n",
      "\n",
      "301\n",
      "\n",
      "estimated from all documents in the system. More precisely, we can use\n",
      "\n",
      "p(t|d) = (1 − λ)\n",
      "\n",
      "TF(t, d) LEN(d)\n",
      "\n",
      "+ λp(t|background)\n",
      "\n",
      "(9.117)\n",
      "\n",
      "where TF(t, d) is the frequency of term t in document d, LEN(d) is the number of words in d, and 0 < λ < 1 is a smoothing parameter (see e.g., Zhai and Lafferty (2004) for details).\n",
      "\n",
      "However, there might be many other signals that we can use to measure relevance. For example, the PageRank of a web document is a measure of its authoritativeness, derived from the web’s link structure (see Section 17.2.4 for details). We can also compute how often and where the query occurs in the document. Below we discuss how to learn how to combine all these signals.4\n",
      "\n",
      "9.7.1\n",
      "\n",
      "The pointwise approach\n",
      "\n",
      "Suppose we collect some training data representing the relevance of a set of documents for each query. Speciﬁcally, for each query q, suppose that we retrieve m possibly relevant documents dj, for j = 1 :m. For each query document pair, we deﬁne a feature vector, x(q, d). For example, this might contain the query-document similarity score and the page rank score of the document. Furthermore, suppose we have a set of labels yj representing the degree of relevance of document dj to query q. Such labels might be binary (e.g., relevant or irrelevant), or they may represent a degree of relevance (e.g., very relevant, somewhat relevant, irrelevant). Such labels can be obtained from query logs, by thresholding the number of times a document was clicked on for a given query.\n",
      "\n",
      "If we have binary relevance labels, we can solve the problem using a standard binary clas- siﬁcation scheme to estimate, p(y = 1|x(q, d)). If we have ordered relevancy labels, we can use ordinal regression to predict the rating, p(y = r|x(q, d)). In either case, we can then sort the documents by this scoring metric. This is called the pointwise approach to LETOR, and is widely used because of its simplicity. However, this method does not take into account the location of each document in the list. Thus it penalizes errors at the end of the list just as much as errors at the beginning, which is often not the desired behavior. In addition, each decision about relevance is made very myopically.\n",
      "\n",
      "9.7.2\n",
      "\n",
      "The pairwise approach\n",
      "\n",
      "There is evidence (e.g., (Carterette et al. 2008)) that people are better at judging the relative relevance of two items rather than absolute relevance. Consequently, the data might tell us that dj is more relevant than dk for a given query, or vice versa. We can model this kind of data using a binary classiﬁer of the form p(yjk|x(q, dj), x(q, dk)), where we set yjk = 1 if rel(dj, q) > rel(dk, q) and yjk = 0 otherwise.\n",
      "\n",
      "One way to model such a function is as follows:\n",
      "\n",
      "p(yjk = 1|xj, xk) = sigm(f (xj) − f (xk))\n",
      "\n",
      "(9.118)\n",
      "\n",
      "4. Rather surprisingly, Google does not (or at least, did not as of 2008) using such learning methods in its search engine. Source: Peter Norvig, quoted in http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-p rone-to-catastrophic-errors-than-machine-learned-models.html.\n",
      "\n",
      "302\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "where f (x) is a scoring function, often taken to be linear, f (x) =w T x. This is a special kind of neural network known as RankNet (Burges et al. 2005) (see Section 16.5 for a general discussion of neural networks). We can ﬁnd the MLE of w by maximizing the log likelihood, or equivalently, by minimizing the cross entropy loss, given by\n",
      "\n",
      "L =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "mi(cid:2)\n",
      "\n",
      "mi(cid:2)\n",
      "\n",
      "Lijk\n",
      "\n",
      "(9.119)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "k=j+1\n",
      "\n",
      "−Lijk = I(yijk = 1) log p(yijk = 1|xij, xik, w)\n",
      "\n",
      "+I(yijk = 0) log p(yijk = 0|xij, xik, w)\n",
      "\n",
      "(9.120)\n",
      "\n",
      "This can be optimized using gradient descent. A variant of RankNet is used by Microsoft’s Bing search engine.5\n",
      "\n",
      "9.7.3\n",
      "\n",
      "The listwise approach\n",
      "\n",
      "The pairwise approach suffers from the problem that decisions about relevance are made just based on a pair of items (documents), rather than considering the full context. We now consider methods that look at the entire list of items at the same time.\n",
      "\n",
      "We can deﬁne a total order on a list by specifying a permutation of its indices, π. To model our uncertainty about π, we can use the Plackett-Luce distribution, which derives its name from independent work by (Plackett 1975) and (Luce 1959). This has the following form:\n",
      "\n",
      "p(π|s) =\n",
      "\n",
      "m(cid:27)\n",
      "\n",
      "j=1\n",
      "\n",
      "sj(cid:10)m\n",
      "\n",
      "u=j su\n",
      "\n",
      "(9.121)\n",
      "\n",
      "where sj = s(π−1(j)) is the score of the document ranked at the j’th position.\n",
      "\n",
      "To understand Equation 9.121, let us consider a simple example. Suppose π = (A, B, C). Then we have that p(π) is the probability of A being ranked ﬁrst, times the probability of B being ranked second given that A is ranked ﬁrst, times the probabilty of C being ranked third given that A and B are ranked ﬁrst and second. In other words,\n",
      "\n",
      "p(π|s) =\n",
      "\n",
      "sA sA + sB + sC\n",
      "\n",
      "×\n",
      "\n",
      "sB sB + sC\n",
      "\n",
      "×\n",
      "\n",
      "sC sC\n",
      "\n",
      "(9.122)\n",
      "\n",
      "To incorporate features, we can deﬁne s(d) =f (x(q, d)), where we often take f to be a linear function, f (x) =w T x. This is known as the ListNet model (Cao et al. 2007). To train this model, let yi be the relevance scores of the documents for query i. We then minimize the cross entropy term\n",
      "\n",
      "−\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(π|yi) log p(π|si)\n",
      "\n",
      "(9.123)\n",
      "\n",
      "i\n",
      "\n",
      "π\n",
      "\n",
      "Of course, as stated, this is intractable, since the i’th term needs to sum over mi! permutations. To make this tractable, we can consider permutations over the top k positions only:\n",
      "\n",
      "p(π1:k|s1:m) =\n",
      "\n",
      "k(cid:27)\n",
      "\n",
      "j=1\n",
      "\n",
      "sj(cid:10)m\n",
      "\n",
      "u=1 su\n",
      "\n",
      "(9.124)\n",
      "\n",
      "5. Source: eatures-and-the-science-behind-bing.aspx.\n",
      "\n",
      "http://www.bing.com/community/site_blogs/b/search/archive/2009/06/01/user-needs-f\n",
      "\n",
      "9.7. Learning to rank *\n",
      "\n",
      "303\n",
      "\n",
      "There are only m!/(m − k)! such permutations. entropy term (and its derivative) in O(m) time.\n",
      "\n",
      "If we set k = 1, we can evaluate each cross\n",
      "\n",
      "yi = c, we can instead use multinomial logistic regression:\n",
      "\n",
      "In the special case where only one document from the presented list is deemed relevant, say\n",
      "\n",
      "p(yi = c|x) =\n",
      "\n",
      "(cid:10)m\n",
      "\n",
      "exp(sc) c(cid:2)=1 exp(sc(cid:2) )\n",
      "\n",
      "(9.125)\n",
      "\n",
      "This often performs at least as well as ranking methods, at least in the context of collaborative ﬁltering (Yang et al. 2011).\n",
      "\n",
      "9.7.4\n",
      "\n",
      "Loss functions for ranking\n",
      "\n",
      "There are a variety of ways to measure the performance of a ranking system, which we summa- rize below.\n",
      "\n",
      "Mean reciprocal rank (MRR). For a query q,\n",
      "\n",
      "let the rank position of its ﬁrst relevant document be denoted by r(q). Then we deﬁne the mean reciprocal rank to be 1/r(q). This is a very simple performance measure.\n",
      "\n",
      "Mean average precision (MAP). In the case of binary relevance labels, we can deﬁne the\n",
      "\n",
      "precision at k of some ordering as follows:\n",
      "\n",
      "P@k(π) (cid:2) num. relevant documents in the top k positions of π\n",
      "\n",
      "k\n",
      "\n",
      "(9.126)\n",
      "\n",
      "We then deﬁne the average precision as follows:\n",
      "\n",
      "AP(π) (cid:2)\n",
      "\n",
      "k P@k(π) · Ik num. relevant documents\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(9.127)\n",
      "\n",
      "where Ik is 1 iff document k is relevant. For example, y = (1, 0, 1, 0, 1), then the AP is 1 3 + 3 precision as the AP averaged over all queries.\n",
      "\n",
      "3 ( 1\n",
      "\n",
      "1 + 2\n",
      "\n",
      "if we have the relevancy labels 5 ) ≈ 0.76. Finally, we deﬁne the mean average\n",
      "\n",
      "Normalized discounted cumulative gain (NDCG). Suppose the relevance labels have multi- ple levels. We can deﬁne the discounted cumulative gain of the ﬁrst k items in an ordering as follows:\n",
      "\n",
      "DCG@k(r) = r1 +\n",
      "\n",
      "k(cid:2)\n",
      "\n",
      "i=2\n",
      "\n",
      "ri log2 i\n",
      "\n",
      "(9.128)\n",
      "\n",
      "where ri is the relevance of item i and the log2 term is used to discount items later in the list. Table 9.3 gives a simple numerical example. An alternative deﬁnition, that places stronger emphasis on retrieving relevant documents, uses\n",
      "\n",
      "DCG@k(r) =\n",
      "\n",
      "k(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "2ri − 1 log2(1 + i)\n",
      "\n",
      "(9.129)\n",
      "\n",
      "The trouble with DCG is that it varies in magnitude just because the length of a returned list may vary. It is therefore common to normalize this measure by the ideal DCG, which is\n",
      "\n",
      "304\n",
      "\n",
      "Chapter 9. Generalized linear models and the exponential family\n",
      "\n",
      "i ri log2 i ri log2 i\n",
      "\n",
      "1 3 0 N/A\n",
      "\n",
      "2 2 1 2\n",
      "\n",
      "3 3 1.59 1.887\n",
      "\n",
      "4 0 2.0 0\n",
      "\n",
      "5 1 2.32 0.431\n",
      "\n",
      "6 2 2.59 0.772\n",
      "\n",
      "Table 9.3 Illustration of how to compute NDCG, from http://en.wikipedia.org/wiki/Discounted _cumulative_gain. The value ri is the relevance score of the item in position i. From this, we see that DCG@6 = 3 + (2 + 1.887 + 0 + 0.431 + 0.772) = 8.09. The maximum DCG is obtained using the ordering with scores 3, 3, 2, 2, 1, 0. Hence the ideal DCG is 8.693, and so the normalized DCG is 8.09 / 8.693 = 0.9306.\n",
      "\n",
      "the DCG obtained by using the optimal ordering: IDCG@k(r) = argmaxπ DCG@k(r). This can be easily computed by sorting r1:m and then computing DCG@k. Finally, we deﬁne the normalized discounted cumulative gain or NDCG as DCG/IDCG. Table 9.3 gives a simple numerical example. The NDCG can be averaged over queries to give a measure of performance.\n",
      "\n",
      "Rank correlation. We can measure the correlation between the ranked list, π, and the relevance judegment, π∗, using a variety of methods. One approach, known as the (weighted) Kendall’s τ statistics, is deﬁned in terms of the weighted pairwise inconsistency between the two lists:\n",
      "\n",
      "τ (π, π∗) =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "u<v wuv [1 + sgn(πu − πv)sgn(π∗ (cid:10)\n",
      "\n",
      "2\n",
      "\n",
      "u<v wuv\n",
      "\n",
      "u − π∗\n",
      "\n",
      "v)]\n",
      "\n",
      "(9.130)\n",
      "\n",
      "A variety of other measures are commonly used.\n",
      "\n",
      "These loss functions can be used in different ways. In the Bayesian approach, we ﬁrst ﬁt the model using posterior inference; this depends on the likelihood and prior, but not the loss. We then choose our actions at test time to minimize the expected future loss. One way to do this is to sample parameters from the posterior, θs ∼ p(θ|D), and then evaluate, say, the precision@k for different thresholds, averaging over θs . See (Zhang et al. 2010) for an example of such an approach.\n",
      "\n",
      "In the frequentist approach, we try to minimize the empirical loss on the training set. The problem is that these loss functions are not differentiable functions of the model parameters. We can either use gradient-free optimization methods, or we can minimize a surrogate loss function instead. Cross entropy loss (i.e., negative log likelihood) is an example of a widely used surrogate loss function.\n",
      "\n",
      "Another loss, known as weighted approximate-rank pairwise or WARP loss, proposed in (Usunier et al. 2009) and extended in (Weston et al. 2010), provides a better approximation to the precision@k loss. WARP is deﬁned as follows: WARP(f (x, :), y) (cid:2) L(rank(f (x, :), y)) rank(f (x, :), y) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "I(f (x, y(cid:4)) ≥ f (x, y))\n",
      "\n",
      "(9.132)\n",
      "\n",
      "(9.131)\n",
      "\n",
      "y(cid:2)(cid:7)=y\n",
      "\n",
      "L(k) (cid:2)\n",
      "\n",
      "k(cid:2)\n",
      "\n",
      "αj, with α1 ≥ α2 ≥ · · · ≥0\n",
      "\n",
      "(9.133)\n",
      "\n",
      "j=1\n",
      "\n",
      "9.7. Learning to rank *\n",
      "\n",
      "305\n",
      "\n",
      "Here f (x, :) = [f (x, 1), . . . , f (x, |y|)] is the vector of scores for each possible output label, or, in IR terms, for each possible document corresponding to input query x. The expression rank(f (x, :), y) measures the rank of the true label y assigned by this scoring function. Finally, L transforms the integer rank into a real-valued penalty. Using α1 = 1 and αj>1 = 0 would optimize the proportion of top-ranked correct labels. Setting α1:k to be non-zero values would optimize the top k in the ranked list, which will induce good performance as measured by MAP or precision@k. As it stands, WARP loss is still hard to optimize, but it can be further approximated by Monte Carlo sampling, and then optimized by gradient descent, as described in (Weston et al. 2010).\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 9.1 Conjugate prior for univariate Gaussian in exponential family form Derive the conjugate prior for μ and λ = 1/σ2 for a univariate Gaussian using the exponential family, by analogy to Section 9.2.5.5. By suitable reparameterization, show that the prior has the form p(μ, λ) = N (μ|γ, λ(2α − 1))Ga(λ|α, β), and thus only has 3 free parameters.\n",
      "\n",
      "Exercise 9.2 The MVN is in the exponential family Show that we can write the MVN in exponential family form. Hint: use the information form deﬁned in Section 4.3.3.\n",
      "\n",
      "10 Directed graphical models (Bayes nets)\n",
      "\n",
      "10.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "I basically know of two principles for treating complicated systems in simple ways: the ﬁrst is the principle of modularity and the second is the principle of abstraction. I am an apologist for computational probability in machine learning because I believe that probability theory implements these two principles in deep and intriguing ways — namely through factorization and through averaging. Exploiting these two mechanisms as fully as possible seems to me to be the way forward in machine learning. — Michael Jordan, 1997 (quoted in (Frey 1998)).\n",
      "\n",
      "Suppose we observe multiple correlated variables, such as words in a document, pixels in an image, or genes in a microarray. How can we compactly represent the joint distribution p(x|θ)? How can we use this distribution to infer one set of variables given another in a reasonable amount of computation time? And how can we learn the parameters of this distribution with a reasonable amount of data? These questions are at the core of probabilistic modeling, inference and learning, and form the topic of this chapter.\n",
      "\n",
      "10.1.1\n",
      "\n",
      "Chain rule\n",
      "\n",
      "By the chain rule of probability, we can always represent a joint distribution as follows, using any ordering of the variables:\n",
      "\n",
      "p(x1:V ) = p(x1)p(x2|x1)p(x3|x2, x1)p(x4|x1, x2, x3) . . . p(xV |x1:V −1)\n",
      "\n",
      "(10.1)\n",
      "\n",
      "where V is the number of variables, the Matlab-like notation 1 :V denotes the set {1, 2, . . . , V }, and where we have dropped the conditioning on the ﬁxed parameters θ for brevity. The problem with this expression is that it becomes more and more complicated to represent the conditional distributions p(xt|x1:t−1) as t gets large.\n",
      "\n",
      "For example, suppose all the variables have K states. We can represent p(x1) as a table of O(K) numbers, representing a discrete distribution (there are actually only K − 1 free parameters, due to the sum-to-one constraint, but we write O(K) for simplicity). Similarly, we can represent p(x2|x1) as a table of O(K 2) numbers by writing p(x2 = j|x1 = i) = Tij; we j Tij = 1 for all rows i, say that T is a stochastic matrix, since it satisﬁes the constraint and 0 ≤ Tij ≤ 1 for all entries. Similarly, we can represent p(x3|x1, x2) as a 3d table with\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "308\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "O(K 3) numbers. These are called conditional probability tables or CPTs. We see that there are O(K V ) parameters in the model. We would need an awful lot of data to learn so many parameters.\n",
      "\n",
      "One solution is to replace each CPT with a more parsimonius conditional probability distri- bution or CPD, such as multinomial logistic regression, i.e., p(xt = k|x1:t−1) = S(Wtx1:t−1)k. The total number of parameters is now only O(K 2V 2), making this a compact density model (Neal 1992; Frey 1998). This is adequate if all we want to do is evaluate the probability of a fully observed vector x1:T . For example, we can use this model to deﬁne a class-conditional density, p(x|y = c), thus making a generative classiﬁer (Bengio and Bengio 2000). However, this model is not useful for other kinds of prediction tasks, since each variable depends on all the previous variables. So we need another approach.\n",
      "\n",
      "10.1.2\n",
      "\n",
      "Conditional independence\n",
      "\n",
      "The key to efficiently representing large joint distributions is to make some assumptions about conditional independence (CI). Recall from Section 2.2.4 that X and Y are conditionally inde- pendent given Z, denoted X ⊥ Y |Z, if and only if (iff) the conditional joint can be written as a product of conditional marginals, i.e.,\n",
      "\n",
      "X ⊥ Y |Z ⇐⇒ p(X, Y |Z) = p(X|Z)p(Y |Z)\n",
      "\n",
      "(10.2)\n",
      "\n",
      "Let us see why this might help. Suppose we assume that xt+1 ⊥ x1:t−1|xt, or in words, “the future is independent of the past given the present”. This is called the (ﬁrst order) Markov assumption. Using this assumption, plus the chain rule, we can write the joint distribution as follows:\n",
      "\n",
      "p(x1:V ) = p(x1)\n",
      "\n",
      "V(cid:27)\n",
      "\n",
      "p(xt|xt−1)\n",
      "\n",
      "(10.3)\n",
      "\n",
      "t=1\n",
      "\n",
      "This is called a (ﬁrst-order) Markov chain. They can be characterized by an initial distribution over states, p(x1 = i), plus a state transition matrix p(xt = j|xt−1 = i). See Section 17.2 for more information.\n",
      "\n",
      "10.1.3\n",
      "\n",
      "Graphical models\n",
      "\n",
      "Although the ﬁrst-order Markov assumption is useful for deﬁning distributions on 1d sequences, how can we deﬁne distributions on 2d images, or 3d videos, or, in general, arbitrary collections of variables (such as genes belonging to some biological pathway)? This is where graphical models come in.\n",
      "\n",
      "A graphical model (GM) is a way to represent a joint distribution by making CI assumptions. In particular, the nodes in the graph represent random variables, and the (lack of) edges represent (A better name for these models would in fact be “independence diagrams”, CI assumptions. but the term “graphical models” is now entrenched.) There are several kinds of graphical model, depending on whether the graph is directed, undirected, or some combination of directed and undirected. In this chapter, we just study directed graphs. We consider undirected graphs in Chapter 19.\n",
      "\n",
      "10.1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "309\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 10.1 5 are the leaves. {3, 5}.\n",
      "\n",
      "(a) A simple DAG on 5 nodes, numbered in topological order. Node 1 is the root, nodes 4 and (b) A simple undirected graph, with the following maximal cliques: {1, 2, 3}, {2, 3, 4},\n",
      "\n",
      "10.1.4\n",
      "\n",
      "Graph terminology\n",
      "\n",
      "Before we continue, we must deﬁne a few basic terms, most of which are very intuitive.\n",
      "\n",
      "A graph G = (V, E) consists of a set of nodes or vertices, V = {1, . . . , V }, and a set of edges, E = {(s, t) : s, t ∈ V}. We can represent the graph by its adjacency matrix, in which we write G(s, t) = 1 to denote (s, t) ∈ E, that is, if s → t is an edge in the graph. If G(s, t) = 1 iff G(t, s) = 1, we say the graph is undirected, otherwise it is directed. We usually assume G(s, s) = 0, which means there are no self loops.\n",
      "\n",
      "Here are some other terms we will commonly use:\n",
      "\n",
      "Parent For a directed graph, the parents of a node is the set of all nodes that feed into it:\n",
      "\n",
      "pa(s) (cid:2) {t : G(t, s) = 1}.\n",
      "\n",
      "Child For a directed graph, the children of a node is the set of all nodes that feed out of it:\n",
      "\n",
      "ch(s) (cid:2) {t : G(s, t) = 1}.\n",
      "\n",
      "Family For a directed graph, the family of a node is the node and its parents, fam(s) =\n",
      "\n",
      "{s} ∪ pa(s).\n",
      "\n",
      "Root For a directed graph, a root is a node with no parents. • Leaf For a directed graph, a leaf is a node with no children. • Ancestors For a directed graph, the ancestors are the parents, grand-parents, etc of a node. That is, the ancestors of t is the set of nodes that connect to t via a trail: anc(t) (cid:2) {s : s (cid:2) t}.\n",
      "\n",
      "Descendants For a directed graph, the descendants are the children, grand-children, etc of a node. That is, the descendants of s is the set of nodes that can be reached via trails from s: desc(s) (cid:2) {t : s (cid:2) t}.\n",
      "\n",
      "Neighbors For any graph, we deﬁne the neighbors of a node as the set of all immediately connected nodes, nbr(s) (cid:2) {t : G(s, t) = 1 ∨ G(t, s) = 1}. For an undirected graph, we\n",
      "\n",
      "310\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "write s ∼ t to indicate that s and t are neighbors (so (s, t) ∈ E is an edge in the graph). • Degree The degree of a node is the number of neighbors. For directed graphs, we speak of\n",
      "\n",
      "the in-degree and out-degree, which count the number of parents and children.\n",
      "\n",
      "Cycle or loop For any graph, we deﬁne a cycle or loop to be a series of nodes such that we can get back to where we started by following edges, s1 − s2 · · · − sn − s1, n ≥ 2. If the graph is directed, we may speak of a directed cycle. For example, in Figure 10.1(a), there are no directed cycles, but 1 → 2 → 4 → 3 → 1 is an undirected cycle.\n",
      "\n",
      "DAG A directed acyclic graph or DAG is a directed graph with no directed cycles. See\n",
      "\n",
      "Figure 10.1(a) for an example.\n",
      "\n",
      "Topological ordering For a DAG, a topological ordering or total ordering is a numbering of the nodes such that parents have lower numbers than their children. For example, in Figure 10.1(a), we can use (1, 2, 3, 4, 5), or(1 , 3, 2, 5, 4), etc.\n",
      "\n",
      "Path or trail A path or trail s (cid:2) t is a series of directed edges leading from s to t. • Tree An undirected tree is an undirectecd graph with no cycles. A directed tree is a DAG in which there are no directed cycles. If we allow a node to have multiple parents, we call it a polytree, otherwise we call it a moral directed tree.\n",
      "\n",
      "Forest A forest is a set of trees. • Subgraph A (node-induced) subgraph GA is the graph created by using the nodes in A and\n",
      "\n",
      "their corresponding edges, GA = (VA, EA).\n",
      "\n",
      "Clique For an undirected graph, a clique is a set of nodes that are all neighbors of each other. A maximal clique is a clique which cannot be made any larger without losing the clique property. For example, in Figure 10.1(b), {1, 2} is a clique but it is not maximal, since In fact, the maximal cliques are as we can add 3 and still maintain the clique property. follows: {1, 2, 3}, {2, 3, 4}, {3, 5}.\n",
      "\n",
      "10.1.5\n",
      "\n",
      "Directed graphical models\n",
      "\n",
      "A directed graphical model or DGM is a GM whose graph is a DAG. These are more commonly known as Bayesian networks. However, there is nothing inherently “Bayesian” about Bayesian networks: they are just a way of deﬁning probability distributions. These models are also called belief networks. The term “belief” here refers to subjective probability. Once again, there is nothing inherently subjective about the kinds of probability distributions represented by DGMs. Finally, these models are sometimes called causal networks, because the directed arrows are sometimes interpreted as representing causal relations. However, there is nothing inherently causal about DGMs. (See Section 26.6.1 for a discussion of causal DGMs.) For these reasons, we use the more neutral (but less glamorous) term DGM.\n",
      "\n",
      "The key property of DAGs is that the nodes can be ordered such that parents come before children. This is called a topological ordering, and it can be constructed from any DAG. Given such an order, we deﬁne the ordered Markov property to be the assumption that a node only depends on its immediate parents, not on all predecessors in the ordering, i.e.,\n",
      "\n",
      "(10.4) where pa(s) are the parents of node s, and pred(s) are the predecessors of node s in the ordering. This is a natural generalization of the ﬁrst-order Markov property to from chains to general DAGs.\n",
      "\n",
      "xs ⊥ xpred(s)\\pa(s)|xpa(s)\n",
      "\n",
      "10.2. Examples\n",
      "\n",
      "311\n",
      "\n",
      "Y\n",
      "\n",
      "Y\n",
      "\n",
      "X2\n",
      "\n",
      "X1\n",
      "\n",
      "X3\n",
      "\n",
      "X1\n",
      "\n",
      "X2\n",
      "\n",
      "X3\n",
      "\n",
      "X4\n",
      "\n",
      "X4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 10.2 (a) A naive Bayes classiﬁer represented as a DGM. We assume there are D = 4 features, for simplicity. Shaded nodes are observed, unshaded nodes are hidden. (b) Tree-augmented naive Bayes classiﬁer for D = 4 features. In general, the tree topology can change depending on the value of y.\n",
      "\n",
      "For example, the DAG in Figure 10.1(a) encodes the following joint distribution:\n",
      "\n",
      "p(x1:5) =p( x1)p(x2|x1)p(x3|x1,(cid:2)(cid:2)x2)p(x4|(cid:2)(cid:2)x1, x2, x3)p(x5|(cid:2)(cid:2)x1,(cid:2)(cid:2)x2, x3,(cid:2)(cid:2)x4)\n",
      "\n",
      "(10.5)\n",
      "\n",
      "= p(x1)p(x2|x1)p(x3|x1)p(x4|x2, x3)p(x5|x3)\n",
      "\n",
      "(10.6)\n",
      "\n",
      "In general, we have\n",
      "\n",
      "p(x1:V |G) =\n",
      "\n",
      "V(cid:27)\n",
      "\n",
      "p(xt|xpa(t))\n",
      "\n",
      "(10.7)\n",
      "\n",
      "t=1\n",
      "\n",
      "where each term p(xt|xpa(t)) is a CPD. We have written the distribution as p(x|G) to emphasize that this equation only holds if the CI assumptions encoded in DAG G are correct. However, If each node has O(F ) parents and we will usual drop this explicit conditioning for brevity. K states, the number of parameters in the model is O(V K F ), which is much less than the O(K V ) needed by a model which makes no CI assumptions.\n",
      "\n",
      "10.2\n",
      "\n",
      "Examples\n",
      "\n",
      "In this section, we show a wide variety of commonly used probabilistic models can be conve- niently represented as DGMs.\n",
      "\n",
      "10.2.1\n",
      "\n",
      "Naive Bayes classiﬁers\n",
      "\n",
      "In Section 3.5, we introduced the naive Bayes classiﬁer. This assumes the features are condi- tionally independent given the class label. This assumption is illustrated in Figure 10.2(a). This allows us to write the joint distirbution as follows:\n",
      "\n",
      "p(y, x) = p(y)\n",
      "\n",
      "D(cid:27)\n",
      "\n",
      "p(xj|y)\n",
      "\n",
      "(10.8)\n",
      "\n",
      "j=1\n",
      "\n",
      "The naive Bayes assumption is rather naive, since it assumes the features are conditionally independent. One way to capture correlation between the features is to use a graphical model. In particular, if the model is a tree, the method is known as a tree-augmented naive Bayes\n",
      "\n",
      "312\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "· ·\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "x4\n",
      "\n",
      "· ·\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 10.3 A ﬁrst and second order Markov chain.\n",
      "\n",
      "z1\n",
      "\n",
      "z2\n",
      "\n",
      "zT\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "xT\n",
      "\n",
      "Figure 10.4 A ﬁrst-order HMM.\n",
      "\n",
      "classiﬁer or TAN model (Friedman et al. 1997). This is illustrated in Figure 10.2(b). The reason to use a tree, as opposed to a generic graph, is two-fold. First, it is easy to ﬁnd the optimal tree structure using the Chow-Liu algorithm, as explained in Section 26.3. Second, it is easy to handle missing features in a tree-structured model, as we explain in Section 20.2.\n",
      "\n",
      "10.2.2 Markov and hidden Markov models\n",
      "\n",
      "Figure 10.3(a) illustrates a ﬁrst-order Markov chain as a DAG. Of course, the assumption that the immediate past, xt−1, captures everything we need to know about the entire history, x1:t−2, is a bit strong. We can relax it a little by adding a dependence from xt−2 to xt as well; this is called a second order Markov chain, and is illustrated in Figure 10.3(b). The corresponding joint has the following form:\n",
      "\n",
      "p(x1:T ) = p(x1, x2)p(x3|x1, x2)p(x4|x2, x3) . . . = p(x1, x2)\n",
      "\n",
      "T(cid:27)\n",
      "\n",
      "p(xt|xt−1, xt−2)\n",
      "\n",
      "(10.9)\n",
      "\n",
      "t=3\n",
      "\n",
      "We can create higher-order Markov models in a similar way. See Section 17.2 for a more detailed discussion of Markov models.\n",
      "\n",
      "Unfortunately, even the second-order Markov assumption may be inadequate if there are long- range correlations amongst the observations. We can’t keep building ever higher order models, since the number of parameters will blow up. An alternative approach is to assume that there is an underlying hidden process, that can be modeled by a ﬁrst-order Markov chain, but that the data is a noisy observation of this process. The result is known as a hidden Markov model or HMM, and is illustrated in Figure 10.4. Here zt is known as a hidden variable at “time” t, and xt is the observed variable. (We put “time” in quotation marks, since these models can be applied to any kind of sequence data, such as genomics or language, where t represents location rather than time.) The CPD p(zt|zt−1) is the transition model, and the CPD p(xt|zt) is the observation model.\n",
      "\n",
      "10.2. Examples\n",
      "\n",
      "313\n",
      "\n",
      "h0 1 1 1 1\n",
      "\n",
      "h1 0 1 0 1\n",
      "\n",
      "h2 P (v = 0|h1, h2) P (v = 1|h1, h2) 0 0 1 1\n",
      "\n",
      "θ0 θ0θ1 θ0θ2 θ0θ1θ2\n",
      "\n",
      "1 − θ0 1 − θ0θ1 1 − θ0θ2 1 − θ0θ1θ2\n",
      "\n",
      "Table 10.1 Noisy-OR CPD for 2 parents augmented with leak node. We have omitted the t subscript for brevity.\n",
      "\n",
      "The hidden variables often represent quantities of interest, such as the identity of the word that someone is currently speaking. The observed variables are what we measure, such as the acoustic waveform. What we would like to do is estimate the hidden state given the data, i.e., to compute p(zt|x1:t, θ). This is called state estimation, and is just another form of probabilistic inference. See Chapter 17 for further details on HMMs.\n",
      "\n",
      "10.2.3 Medical diagnosis\n",
      "\n",
      "Consider modeling the relationship between various variables that are measured in an intensive care unit (ICU), such as the breathing rate of a patient, their blood pressure, etc. The alarm network in Figure 10.5(a) is one way to represent these (in)dependencies (Beinlich et al. 1989). This model has 37 variables and 504 parameters.\n",
      "\n",
      "Since this model was created by hand, by a process called knowledge engineering, it is known as a probabilistic expert system. In Section 10.4, we discuss how to learn the parameters of DGMs from data, assuming the graph structure is known, and in Chapter 26, we discuss how to learn the graph structure itself.\n",
      "\n",
      "A different kind of medical diagnosis network, known as the quick medical reference or QMR network (Shwe et al. 1991), is shown in Figure 10.5(b). This was designed to model infectious diseases. The QMR model is a bipartite graph structure, with diseases (causes) at the top and symptoms or ﬁndings at the bottom. All nodes are binary. We can write the distribution as follows:\n",
      "\n",
      "p(v, h) =\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "p(hs)\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "p(vt|hpa(t))\n",
      "\n",
      "(10.10)\n",
      "\n",
      "s\n",
      "\n",
      "t\n",
      "\n",
      "where hs represent the hidden nodes (diseases), and vt represent the visible nodes (symptoms). The CPD for the root nodes are just Bernoulli distributions, representing the prior probability of that disease. Representing the CPDs for the leaves (symptoms) using CPTs would require too many parameters, because the fan-in (number of parents) of many leaf nodes is very high. A natural alternative is to use logistic regression to model the CPD, p(vt = 1|hpa(t)) = sigm(wT t hpa(t)). (A DGM in which the CPDs are logistic regression distributions is known as a sigmoid belief net (Neal 1992).) However, since the parameters of this model were created by hand, an alternative CPD, known as the noisy-OR model, was used.\n",
      "\n",
      "The noisy-OR model assumes that if a parent is on, then the child will usually also be on (since it is an or-gate), but occasionally the “links” from parents to child may fail, independently In this case, even if the parent is on, the child may be off. To model this more at random. precisely, let θst = 1 − qst be the probability that the s → t link fails, so qst = 1 − θst = p(vt =\n",
      "\n",
      "314\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "MinVolset\n",
      "\n",
      "Disconnect\n",
      "\n",
      "VentMach\n",
      "\n",
      "Pulm Embolus\n",
      "\n",
      "Intubation\n",
      "\n",
      "VentTube\n",
      "\n",
      "Kinked Tube\n",
      "\n",
      "PAP\n",
      "\n",
      "Shunt\n",
      "\n",
      "FIO2\n",
      "\n",
      "Press\n",
      "\n",
      "VentLung\n",
      "\n",
      "Hypo Volemia\n",
      "\n",
      "Anaphy Laxis\n",
      "\n",
      "MinVol\n",
      "\n",
      "VentAlv\n",
      "\n",
      "Stroke Volume\n",
      "\n",
      "TPR\n",
      "\n",
      "SAO2\n",
      "\n",
      "PVSAT\n",
      "\n",
      "Insuff Anesth\n",
      "\n",
      "Artco2\n",
      "\n",
      "LvFailure\n",
      "\n",
      "CO\n",
      "\n",
      "Catechol\n",
      "\n",
      "ExpCo2\n",
      "\n",
      "History\n",
      "\n",
      "Lved Volume\n",
      "\n",
      "Errlow Output\n",
      "\n",
      "HR\n",
      "\n",
      "ErrCauter\n",
      "\n",
      "CVP\n",
      "\n",
      "PCWP\n",
      "\n",
      "BP\n",
      "\n",
      "HRBP\n",
      "\n",
      "HrEKG\n",
      "\n",
      "HRSAT\n",
      "\n",
      "(a)\n",
      "\n",
      "(cid:24)(cid:26)(cid:19)(cid:3)(cid:71)(cid:76)(cid:86)(cid:72)(cid:68)(cid:86)(cid:72)(cid:86)\n",
      "\n",
      "(cid:73)(cid:79)(cid:88)\n",
      "\n",
      "(cid:75)(cid:72)(cid:68)(cid:85)(cid:87)(cid:3) (cid:71)(cid:76)(cid:86)(cid:72)(cid:68)(cid:86)(cid:72)\n",
      "\n",
      "(cid:69)(cid:82)(cid:87)(cid:88)(cid:79)(cid:76)(cid:86)(cid:80)\n",
      "\n",
      "(cid:86)(cid:72)(cid:91)(cid:32)(cid:41)\n",
      "\n",
      "(cid:58)(cid:37)(cid:38)(cid:3) (cid:70)(cid:82)(cid:88)(cid:81)(cid:87)\n",
      "\n",
      "(cid:68)(cid:69)(cid:71)(cid:82)(cid:80)(cid:72)(cid:81)(cid:3) (cid:83)(cid:68)(cid:76)(cid:81)\n",
      "\n",
      "(cid:23)(cid:19)(cid:26)(cid:24)(cid:3)(cid:86)(cid:92)(cid:80)(cid:83)(cid:87)(cid:82)(cid:80)(cid:86)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 10.5 network.\n",
      "\n",
      "(a) The alarm network. Figure generated by visualizeAlarmNetwork.\n",
      "\n",
      "(b) The QMR\n",
      "\n",
      "10.2. Examples\n",
      "\n",
      "315\n",
      "\n",
      "Gp Gm p(X = a) a a a b b b o o o\n",
      "\n",
      "a b o a b o a b o\n",
      "\n",
      "1 0 1 0 0 0 1 0 0\n",
      "\n",
      "p(X = b) 0 0 0 0 1 1 0 1 0\n",
      "\n",
      "p(X = o) 0 0 0 0 0 0 0 0 1\n",
      "\n",
      "p(X = ab) 0 1 0 1 0 0 0 0 0\n",
      "\n",
      "Table 10.2 CPT which encodes a mapping from genotype to phenotype (bloodtype). This is a determin- istic, but many-to-one, mapping.\n",
      "\n",
      "1|hs = 1, h−s = 0) is the probability that s can activate t on its own (its “causal power”). The only way for the child to be off is if all the links from all parents that are on fail independently at random. Thus\n",
      "\n",
      "p(vt = 0|h) =\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "θI(hs=1) st\n",
      "\n",
      "(10.11)\n",
      "\n",
      "s∈pa(t)\n",
      "\n",
      "Obviously, p(vt = 1|h) = 1 − p(vt = 0|h).\n",
      "\n",
      "If we observe that vt = 1 but all its parents are off, then this contradicts the model. Such a data case would get probability zero under the model, which is problematic, because it is possible that someone exhibits a symptom but does not have any of the speciﬁed diseases. To handle this, we add a dummy leak node h0, which is always on; this represents “all other causes”. The parameter q0t represents the probability that the background leak can cause the effect on its own. The modiﬁed CPD becomes p(vt = 0|h) = θ0t st . See Table 10.1 for a numerical example.\n",
      "\n",
      "If we deﬁne wst (cid:2) log(θst), we can rewrite the CPD as\n",
      "\n",
      "p(vt = 1|h) = 1 − exp\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "w0t +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "hswst\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "’\n",
      "\n",
      "s∈pa(t) θhs\n",
      "\n",
      "(10.12)\n",
      "\n",
      "s\n",
      "\n",
      "We see that this is similar to a logistic regression model.\n",
      "\n",
      "Bipartite models with noisy-OR CPDs are called BN2O models. It is relatively easy to set the θst parameters by hand, based on domain expertise. However, it is also possible to learn them from data (see e.g, (Neal 1992; Meek and Heckerman 1997)). Noisy-OR CPDs have also proved useful in modeling human causal learning (Griffiths and Tenenbaum 2005), as well as general binary classiﬁcation settings (Yuille and Zheng 2009).\n",
      "\n",
      "10.2.4\n",
      "\n",
      "Genetic linkage analysis *\n",
      "\n",
      "Another important (and historically very early) application of DGMs is to the problem of genetic linkage analysis. We start with a pedigree graph, which is a DAG that representing the relationship between parents and children, as shown in Figure 10.6(a). We then convert this to a DGM, as we explain below. Finally we perform probabilistic inference in the resulting model.\n",
      "\n",
      "316\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(a)\n",
      "\n",
      "(cid:47)(cid:82)(cid:70)(cid:88)(cid:86)(cid:3)(cid:6)(cid:3)(cid:20)\n",
      "\n",
      "(cid:47)(cid:82)(cid:70)(cid:88)(cid:86)(cid:3)(cid:6)(cid:3)(cid:21)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 10.6 Left: Individuals with the disease of interest are highlighted. Right: DGM for two loci. Blue nodes Xij is the observed phenotype for individual i at locus j. All other nodes are hidden. Orange nodes Gp/m is the paternal/ maternal allele. Small red nodes zp/m zm ij → zm switching variables. Based on Figure 3 from (Friedman et al. 2000).\n",
      "\n",
      "i,j+1 and zp\n",
      "\n",
      "ijl\n",
      "\n",
      "ij are the paternal/ maternal selection switching variables. These are linked across loci, i,j+1. The founder (root) nodes do not have any parents, and hence do no need\n",
      "\n",
      "ij → zp\n",
      "\n",
      "family tree, circles are females, squares are males.\n",
      "\n",
      "10.2. Examples\n",
      "\n",
      "317\n",
      "\n",
      "In more detail, for each person (or animal) i and location or locus j along the genome, we the observed marker Xij (which can be a property such as blood type, create three nodes: or just a fragment of DNA that can be measured), and two hidden alleles, Gm ij, one inherited from i’s mother (maternal allele) and the other from i’s father (paternal allele). Together, the ordered pair Gij = (Gm Obviously we must add Gm\n",
      "\n",
      "In addition, we add arcs from i’s mother and father into Gij, reﬂecting the Mendelian inheritance of genetic material from one’s parents. More precisely, let mi = k be i’s mother. Then Gm kj, that is, i’s maternal allele is a copy of one of its mother’s two alleles. Let Z m ij be a hidden variable than speciﬁes the choice. We can model this using the following CPD, known as the inheritance model: (cid:26)\n",
      "\n",
      "ij → Xij arcs representing the fact that genotypes cause phenotypes (observed manifestations of genotypes). The CPD p(Xij|Gm ij) is called the penetrance model. As a very simple example, suppose Xij ∈ {A, B, O, AB} represents person i’s observed bloodtype, and Gm ij ∈ {A, B, O} is their genotype. We can repre- sent the penetrance model using the deterministic CPD shown in Table 10.2. For example, A dominates O, so if a person has genotype AO or OA, their phenotype will be A.\n",
      "\n",
      "p(Gm\n",
      "\n",
      "ij |Gm\n",
      "\n",
      "ij could either be equal to Gm\n",
      "\n",
      "kj, Gp\n",
      "\n",
      "kj, Z m\n",
      "\n",
      "ij ) =\n",
      "\n",
      "ij , Gp ij → Xij and Gp\n",
      "\n",
      "I(Gm I(Gm\n",
      "\n",
      "ij) constitutes i’s hidden genotype at locus j.\n",
      "\n",
      "ij = Gm kj) ij = Gp kj)\n",
      "\n",
      "kj or Gp\n",
      "\n",
      "ij , Gp\n",
      "\n",
      "if Z m if Z m\n",
      "\n",
      "ij = m ij = p\n",
      "\n",
      "ij , Gp\n",
      "\n",
      "ij and Gp\n",
      "\n",
      "(10.13)\n",
      "\n",
      "We can deﬁne p(Gp kj, Z p are said to specify the phase of the genotype. The values of Gp the haplotype of person i at locus j.1\n",
      "\n",
      "ij). This is called the founder model, and represents the overall prevalence of difference kinds of alleles in the population. We usually assume independence between the loci for these founder alleles.\n",
      "\n",
      "Next, we need to specify the prior for the root nodes, p(Gm\n",
      "\n",
      "ij|Gm\n",
      "\n",
      "kj, Gp\n",
      "\n",
      "ij) similarly, where k = pi is i’s father. The values of the Zij i,j constitute\n",
      "\n",
      "i,j, Gm\n",
      "\n",
      "ij ) and p(Gp\n",
      "\n",
      "i,j, Z p\n",
      "\n",
      "i,j and Z m\n",
      "\n",
      "Finally, we need to specify priors for the switch variables that control the inheritance process. These variables are spatially correlated, since adjacent sites on the genome are typically inherited together (recombination events are rare). We can model this by imposing a two-state Markov chain on the Z’s, where the probability of switching state at locus j is given by θj = 1 2 (1 − e−2dj ), where dj is the distance between loci j and j + 1. This is called the recombination model.\n",
      "\n",
      "it is a series of replicated pedigree DAGs, augmented with switching Z variables, which are linked using Markov chains. (There is a related model known as phylogenetic HMM (Siepel and Haussler 2003), which is used to model evolution amongst phylogenies.)\n",
      "\n",
      "The resulting DGM is shown in Figure 10.6(b):\n",
      "\n",
      "As a simpliﬁed example of how this model can be used, suppose we only have one locus, corresponding to blood type. For brevity, we will drop the j index. Suppose we observe xi = A. Then there are 3 possible genotypes: Gi is (A, A), (A, O) or (O, A). There is ambiguity because the genotype to phenotype mapping is many-to-one. We want to reverse this mapping. This is known as an inverse problem. Fortunately, we can use the blood types of relatives to help disambiguate the evidence. Information will “ﬂow” from the other xi(cid:2) ’s up to their Gi(cid:2) ’s, then across to i’s Gi via the pedigree DAG. Thus we can combine our local evidence p(xi|Gi)\n",
      "\n",
      "1. Sometimes the observed marker is equal to the unphased genotype, which is the unordered set {Gp the phased or hidden genotype is not directly measurable.\n",
      "\n",
      "ij , Gm\n",
      "\n",
      "ij }; however,\n",
      "\n",
      "318\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "with an informative prior, p(Gi|x−i), conditioned on the other data, to get a less entropic local posterior, p(Gi|x) ∝ p(xi|Gi)p(Gi|x−i).\n",
      "\n",
      "In practice, the model is used to try to determine where along the genome a given disease- causing gene is assumed to lie — this is the genetic linkage analysis task. The method works as follows. First, suppose all the parameters of the model, including the distance between all the marker loci, are known. The only unknown is the location of the disease-causing gene. If there are L marker loci, we construct L + 1 models: in model (cid:6), we postulate that the disease gene comes after marker (cid:6), for 0 < (cid:6) < L + 1. We can estimate the Markov switching parameter ˆθ(cid:14), and hence the distance d(cid:14) between the disease gene and its nearest known locus. We measure the quality of that model using its likelihood, p(D|ˆθ(cid:14)). We then can then pick the model with highest likelihood (which is equivalent to the MAP model under a uniform prior).\n",
      "\n",
      "Note, however, that computing the likelihood requires marginalizing out all the hidden Z and G variables. See (Fishelson and Geiger 2002) and the references therein for some exact methods for this task; these are based on the variable elimination algorithm, which we discuss in Section 20.3. Unfortunately, for reasons we explain in Section 20.5, exact methods can be computationally intractable if the number of individuals and/or loci is large. See (Albers et al. 2006) for an approximate method for computing the likelihood; this is based on a form of variational inference, which we will discuss in Section 22.4.1.\n",
      "\n",
      "10.2.5\n",
      "\n",
      "Directed Gaussian graphical models *\n",
      "\n",
      "Consider a DGM where all the variables are real-valued, and all the CPDs have the following form:\n",
      "\n",
      "p(xt|xpa(t)) = N (xt|μt + wT\n",
      "\n",
      "t xpa(t), σ2 t )\n",
      "\n",
      "(10.14)\n",
      "\n",
      "This is called a linear Gaussian CPD. As we show below, multiplying all these CPDs together results in a large joint Gaussian distribution of the form p(x) =N (x|μ, Σ). This is called a directed GGM, or a Gaussian Bayes net.\n",
      "\n",
      "We now explain how to derive μ and Σ from the CPD parameters, following (Shachter and\n",
      "\n",
      "Kenley 1989, App. B). For convenience, we will rewrite the CPDs in the following form:\n",
      "\n",
      "xt = μt +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "wts(xs − μs) +σ tzt\n",
      "\n",
      "(10.15)\n",
      "\n",
      "s∈pa(t)\n",
      "\n",
      "where zt ∼ N (0, 1), σt is the conditional standard deviation of xt given its parents, wts is the strength of the s → t edge, and μt is the local mean.2\n",
      "\n",
      "It is easy to see that the global mean is just the concatenation of the local means, μ = (μ1, . . . , μD). We now derive the global covariance, Σ. Let S (cid:2) diag(σ) be a diagonal matrix containing the standard deviations. We can rewrite Equation 10.15 in matrix-vector form as follows:\n",
      "\n",
      "(x − μ) = W(x − μ) +Sz\n",
      "\n",
      "(10.16)\n",
      "\n",
      "2. If we do not subtract off the parent’s mean (i.e., if we use xt = μt + is much messier, as can be seen by looking at (Bishop 2006b, p370).\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "s∈pa(t) wtsxs + σtzt), the derivation of Σ\n",
      "\n",
      "10.3.\n",
      "\n",
      "Inference\n",
      "\n",
      "319\n",
      "\n",
      "Now let e be a vector of noise terms:\n",
      "\n",
      "e (cid:2) Sz\n",
      "\n",
      "(10.17)\n",
      "\n",
      "We can rearrange this to get e = (I − W)(x − μ)\n",
      "\n",
      "(10.18) Since W is lower triangular (because wts = 0 if t > s in the topological ordering), we have that I − W is lower triangular with 1s on the diagonal. Hence\n",
      "\n",
      "(10.20) where we deﬁned U = (I − W)−1. Thus the regression weights correspond to a Cholesky decomposition of Σ, as we now show: Σ = cov [x] = cov [x − μ]\n",
      "\n",
      "1 Since I − W is always invertible, we can write x − μ = (I − W)−1e (cid:2) Ue = USz\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "e1 e2 ... ed\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "=\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "−wd1 −wd2\n",
      "\n",
      "1 −w21 −w32 −w31\n",
      "\n",
      "...\n",
      "\n",
      "1\n",
      "\n",
      ". . . −wd,d−1\n",
      "\n",
      "1\n",
      "\n",
      ". . .\n",
      "\n",
      "⎟ ⎟ ⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "⎞\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "x1 − μ1 x2 − μ2 ... xd − μd\n",
      "\n",
      "⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "⎞\n",
      "\n",
      "(10.19)\n",
      "\n",
      "(10.21)\n",
      "\n",
      "= cov [USz] = US cov [z] SUT = US2UT\n",
      "\n",
      "(10.22)\n",
      "\n",
      "10.3\n",
      "\n",
      "Inference\n",
      "\n",
      "We have seen that graphical models provide a compact way to deﬁne joint probability distribu- tions. Given such a joint distribution, what can we do with it? The main use for such a joint distribution is to perform probabilistic inference. This refers to the task of estimating unknown quantities from known quantities. For example, in Section 10.2.2, we introduced HMMs, and said that one of the goals is to estimate the hidden states (e.g., words) from the observations (e.g., speech signal). And in Section 10.2.4, we discussed genetic linkage analysis, and said that one of the goals is to estimate the likelihood of the data under various DAGs, corresponding to different hypotheses about the location of the disease-causing gene.\n",
      "\n",
      "In general, we can pose the inference problem as follows. Suppose we have a set of correlated random variables with joint distribution p(x1:V |θ). (In this section, we are assuming the parameters θ of the model are known. We discuss how to learn the parameters in Section 10.4.) Let us partition this vector into the visible variables xv, which are observed, and the hidden variables, xh, which are unobserved. Inference refers to computing the posterior distribution of the unknowns given the knowns: p(xh, xv|θ) p(xv|θ)\n",
      "\n",
      "p(xh|xv, θ) =\n",
      "\n",
      "=\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "x(cid:2) h\n",
      "\n",
      "p(xh, xv|θ) p(x(cid:4)\n",
      "\n",
      "h, xv|θ)\n",
      "\n",
      "(10.23)\n",
      "\n",
      "Essentially we are conditioning on the data by clamping the visible variables to their observed values, xv, and then normalizing, to go from p(xh, xv) to p(xh|xv). The normalization constant p(xv|θ) is the likelihood of the data, also called the probability of the evidence.\n",
      "\n",
      "320\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "Sometimes only some of the hidden variables are of interest to us. So let us partition the hidden variables into query variables, xq, whose value we wish to know, and the remaining nuisance variables, xn, which we are not interested in. We can compute what we are interested in by marginalizing out the nuisance variables:\n",
      "\n",
      "p(xq|xv, θ) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(xq, xn|xv, θ)\n",
      "\n",
      "(10.24)\n",
      "\n",
      "xn\n",
      "\n",
      "In Section 4.3.1, we saw how to perform all these operations for a multivariate Gaussian in O(V 3) time, where V is the number of variables. What if we have discrete random variables, with say K states each? If the joint distribution is represented as a multi-dimensional table, we can always perform these operations exactly, but this will take O(K V ) time. In Chapter 20, we explain how to exploit the factorization encoded by the GM to perform these operations in O(V K w+1) time, where w is a quantity known as the treewidth of the graph. This measures If the graph is a tree (or a chain), we have w = 1, so for these how “tree-like” the graph is. models, inference takes time linear in the number of nodes. Unfortunately, for more general graphs, exact inference can take time exponential in the number of nodes, as we explain in Section 20.5. We will therefore examine various approximate inference schemes later in the book.\n",
      "\n",
      "10.4\n",
      "\n",
      "Learning\n",
      "\n",
      "In the graphical models literature, it is common to distinguish between inference and learning. Inference means computing (functions of) p(xh|xv, θ), where v are the visible nodes, h are the hidden nodes, and θ are the parameters of the model, assumed to be known. Learning usually means computing a MAP estimate of the parameters given data:\n",
      "\n",
      "ˆθ = argmax\n",
      "\n",
      "θ\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "log p(xi,v|θ) + log p(θ)\n",
      "\n",
      "(10.25)\n",
      "\n",
      "where xi,v are the visible variables in case i. If we have a uniform prior, p(θ) ∝ 1, this reduces to the MLE, as usual.\n",
      "\n",
      "If we adopt a Bayesian view, the parameters are unknown variables and should also be inferred. Thus to a Bayesian, there is no distinction between inference and learning. In fact, we can just add the parameters as nodes to the graph, condition on D, and then infer the values of all the nodes. (We discuss this in more detail below.)\n",
      "\n",
      "In this view, the main difference between hidden variables and parameters is that the number of hidden variables grows with the amount of training data (since there is usually a set of hidden variables for each observed data case), whereas the number of parameters in usually ﬁxed (at least in a parametric model). This means that we must integrate out the hidden variables to avoid overﬁtting, but we may be able to get away with point estimation techniques for parameters, which are fewer in number.\n",
      "\n",
      "10.4.1\n",
      "\n",
      "Plate notation\n",
      "\n",
      "When inferring parameters from data, we often assume the data is iid. We can represent this assumption explicitly using a graphical model, as shown in Figure 10.7(a). This illustrates the\n",
      "\n",
      "10.4. Learning\n",
      "\n",
      "321\n",
      "\n",
      "θ\n",
      "\n",
      "θ\n",
      "\n",
      "X1\n",
      "\n",
      "XN\n",
      "\n",
      "Xi\n",
      "\n",
      "N\n",
      "\n",
      "Left: data points xi are conditionally independent given θ. Right: Plate notation. This Figure 10.7 represents the same model as the one on the left, except the repeated xi nodes are inside a box, known as a plate; the number in the lower right hand corner, N , speciﬁes the number of repetitions of the Xi node.\n",
      "\n",
      "assumption that each data case was generated independently but from the same distribution. Notice that the data cases are only independent conditional on the parameters θ; marginally, the data cases are dependent. Nevertheless, we can see that, in this example, the order in which the data cases arrive makes no difference to our beliefs about θ, since all orderings will have the same sufficient statistics. Hence we say the data is exchangeable.\n",
      "\n",
      "To avoid visual clutter, it is common to use a form of syntactic sugar called plates: we simply draw a little box around the repeated variables, with the convention that nodes within the box will get repeated when the model is unrolled. We often write the number of copies or repetitions in the bottom right corner of the box. See Figure 10.7(b) for a simple example. The corresponding joint distribution has the form\n",
      "\n",
      "p(θ, D) = p(θ)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "N(cid:27)\n",
      "\n",
      "p(xi|θ)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(10.26)\n",
      "\n",
      "i=1\n",
      "\n",
      "This DGM represents the CI assumptions behind the models we considered in Chapter 5.\n",
      "\n",
      "A slightly more complex example is shown in Figure 10.8. On the left we show a naive Bayes classiﬁer that has been “unrolled” for D features, but uses a plate to represent repetition over cases i = 1 :N . The version on the right shows the same model using nested plate notation. When a variable is inside two plates, it will have two sub-indices. For example, we write θjc to represent the parameter for feature j in class-conditional density c. Note that plates can be nested or crossing. Notational devices for modeling more complex parameter tying patterns can be devised (e.g., (Heckerman et al. 2004)), but these are not widely used. What is not clear from the ﬁgure is that θjc is used to generate xij iff yi = c, otherwise it is ignored. This is an example of context speciﬁc independence, since the CI relationship xij ⊥ θjc only holds if yi (cid:7)= c.\n",
      "\n",
      "322\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "π\n",
      "\n",
      "π\n",
      "\n",
      "Yi\n",
      "\n",
      "Yi\n",
      "\n",
      "Xi1\n",
      "\n",
      ". . .\n",
      "\n",
      "XiD N\n",
      "\n",
      "Xij\n",
      "\n",
      "N\n",
      "\n",
      "θc1\n",
      "\n",
      ". . .\n",
      "\n",
      "θcD\n",
      "\n",
      "θjc\n",
      "\n",
      "C\n",
      "\n",
      "C\n",
      "\n",
      "D\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 10.8 Naive Bayes classiﬁer as a DGM. (a) With single plates. (b) WIth nested plates.\n",
      "\n",
      "10.4.2\n",
      "\n",
      "Learning from complete data\n",
      "\n",
      "If all the variables are fully observed in each case, so there is no missing data and there are no hidden variables, we say the data is complete. For a DGM with complete data, the likelihood is given by\n",
      "\n",
      "p(D|θ) =\n",
      "\n",
      "N(cid:27)\n",
      "\n",
      "p(xi|θ) =\n",
      "\n",
      "N(cid:27)\n",
      "\n",
      "V(cid:27)\n",
      "\n",
      "p(xit|xi,pa(t), θt) =\n",
      "\n",
      "V(cid:27)\n",
      "\n",
      "p(Dt|θt)\n",
      "\n",
      "(10.27)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "t=1\n",
      "\n",
      "t=1\n",
      "\n",
      "where Dt is the data associated with node t and its parents, i.e., the t’th family. This is a product of terms, one per CPD. We say that the likelihood decomposes according to the graph structure.\n",
      "\n",
      "Now suppose that the prior factorizes as well:\n",
      "\n",
      "p(θ) =\n",
      "\n",
      "V(cid:27)\n",
      "\n",
      "p(θt)\n",
      "\n",
      "(10.28)\n",
      "\n",
      "t=1\n",
      "\n",
      "Then clearly the posterior also factorizes:\n",
      "\n",
      "p(θ|D) ∝ p(D|θ)p(θ) =\n",
      "\n",
      "V(cid:27)\n",
      "\n",
      "p(Dt|θt)p(θt)\n",
      "\n",
      "(10.29)\n",
      "\n",
      "t=1\n",
      "\n",
      "This means we can compute the posterior of each CPD independently. In other words,\n",
      "\n",
      "factored prior plus factored likelihood implies factored posterior\n",
      "\n",
      "(10.30)\n",
      "\n",
      "Let us consider an example, where all CPDs are tabular, thus extending the earlier results of Secion 3.5.1.2, where discussed Bayesian naive Bayes. We have a separate row (i.e., a separate multinoulli distribution) for each conditioning case, i.e., for each combination of parent values, as in Table 10.2. Formally, we can write the t’th CPT as xt|xpa(t) = c ∼ Cat(θtc), where θtck (cid:2) p(xt = k|xpa(t) = c), for k = 1 : Kt, c = 1 : Ct and t = 1 : T . Here Kt is the number\n",
      "\n",
      "10.4. Learning\n",
      "\n",
      "323\n",
      "\n",
      "’ s∈pa(t) Ks is the number of parent combinations, and T is the (cid:10) k θtck = 1 for each row of each CPT. Let us put a separate Dirichlet prior on each row of each CPT, i.e., θtc ∼ Dir(αtc). Then we can compute the posterior by simply adding the pseudo counts to the empirical counts to get θtc|D ∼ Dir(Ntc + αtc), whereN tck is the number of times that node t is in state k while its parents are in state c:\n",
      "\n",
      "of states for node t, Ct (cid:2) number of nodes. Obviously\n",
      "\n",
      "Ntck (cid:2)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "I(xi,t = k, xi,pa(t) = c)\n",
      "\n",
      "(10.31)\n",
      "\n",
      "i=1\n",
      "\n",
      "From Equation 2.77, the mean of this distribution is given by the following:\n",
      "\n",
      "θtck =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "Ntck + αtck k(cid:2) (Ntck(cid:2) + αtck(cid:2) )\n",
      "\n",
      "(10.32)\n",
      "\n",
      "For example, consider the DGM in Figure 10.1(a). Suppose the training data consists of the\n",
      "\n",
      "following 5 cases:\n",
      "\n",
      "x1 0 0 1 0 0\n",
      "\n",
      "x2 0 1 1 1 1\n",
      "\n",
      "x3 1 1 0 1 1\n",
      "\n",
      "x4 0 1 1 0 1\n",
      "\n",
      "x5 0 1 0 0 0\n",
      "\n",
      "a Dirichlet prior with αick = 1 (corresponding to add-one smoothing) for the t = 4 node:\n",
      "\n",
      "Below we list all the sufficient statistics Ntck, and the posterior mean parameters θick under\n",
      "\n",
      "x2 0 1 0 1\n",
      "\n",
      "x3 Ntck=1 Ntck=0 0 0 1 1\n",
      "\n",
      "0 1 0 2\n",
      "\n",
      "0 0 1 1\n",
      "\n",
      "θtck=1 1/2 2/3 1/3 3/5\n",
      "\n",
      "θtck=0 1/2 1/3 2/3 2/5\n",
      "\n",
      "It is easy to show that the MLE has the same form as Equation 10.32, except without the αtck\n",
      "\n",
      "terms, i.e.,\n",
      "\n",
      "ˆθtck =\n",
      "\n",
      "Ntck(cid:10)\n",
      "\n",
      "k(cid:2) Ntck(cid:2)\n",
      "\n",
      "(10.33)\n",
      "\n",
      "Of course, the MLE suffers from the zero-count problem discussed in Section 3.3.4.1, so it is important to use a prior to regularize the estimation problem.\n",
      "\n",
      "10.4.3\n",
      "\n",
      "Learning with missing and/or latent variables\n",
      "\n",
      "If we have missing data and/or hidden variables, the likelihood no longer factorizes, and indeed it is no longer convex, as we explain in detail in Section 11.3. This means we will usually can only compute a locally optimal ML or MAP estimate. Bayesian inference of the parameters is even harder. We discuss suitable approximate inference techniques in later chapters.\n",
      "\n",
      "324\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "10.5\n",
      "\n",
      "Conditional independence properties of DGMs\n",
      "\n",
      "At the heart of any graphical model is a set of conditional indepence (CI) assumptions. We write xA ⊥G xB|xC if A is independent of B given C in the graph G, using the semantics to be deﬁned below. Let I(G) be the set of all such CI statements encoded by the graph.\n",
      "\n",
      "iff I(G) ⊆ I(p), where I(p) is the set of all CI statements that hold for distribution p. In other words, the graph is an I-map if it does not make any assertions of CI that are not true of the distribution. This allows us to use the graph as a safe proxy for p when reasoning about p’s CI properties. This is helpful for designing algorithms that work for large classes of distributions, regardless of their speciﬁc numerical parameters θ.\n",
      "\n",
      "We say that G is an I-map (independence map) for p, or that p is Markov wrt G,\n",
      "\n",
      "Note that the fully connected graph is an I-map of all distributions, since it makes no CI assertions at all (since it is not missing any edges). We therefore say G is a minimal I-map of p if G is an I-map of p, and if there is no G(cid:4) ⊆ G which is an I-map of p.\n",
      "\n",
      "It remains to specify how to determine if xA ⊥G xB|xC. Deriving these independencies for undirected graphs is easy (see Section 19.2), but the DAG situation is somewhat complicated, because of the need to respect the orientation of the directed edges. We give the details below.\n",
      "\n",
      "10.5.1\n",
      "\n",
      "d-separation and the Bayes Ball algorithm (global Markov properties)\n",
      "\n",
      "First, we introduce some deﬁnitions. We say an undirected path P is d-separated by a set of nodes E (containing the evidence) iff at least one of the following conditions hold:\n",
      "\n",
      "1. P contains a chain, s → m → t or s ← m ← t, wherem ∈ E\n",
      "\n",
      "2. P contains a tent or fork, s (cid:10)m(cid:11) t, wherem ∈ E\n",
      "\n",
      "3. P contains a collider or v-structure, s (cid:11)m(cid:10) t, where m is not in E and nor is any\n",
      "\n",
      "descendant of m.\n",
      "\n",
      "Next, we say that a set of nodes A is d-separated from a different set of nodes B given a third observed set E iff each undirected path from every node a ∈ A to every node b ∈ B is d-separated by E. Finally, we deﬁne the CI properties of a DAG as follows:\n",
      "\n",
      "xA ⊥G xB|xE ⇐⇒ A is d-separated from B given E\n",
      "\n",
      "(10.34)\n",
      "\n",
      "The Bayes ball algorithm (Shachter 1998) is a simple way to see if A is d-separated from B given E, based on the above deﬁnition. The idea is this. We “shade” all nodes in E, indicating that they are observed. We then place “balls” at each node in A, let them “bounce around” according to some rules, and then ask if any of the balls reach any of the nodes in B. The three main rules are shown in Figure 10.9. Notice that balls can travel opposite to edge directions. We see that a ball can pass through a chain, but not if it is shaded in the middle. Similarly, a ball can pass through a fork, but not if it is shaded in the middle. However, a ball cannot pass through a v-structure, unless it is shaded in the middle.\n",
      "\n",
      "Z, which encodes\n",
      "\n",
      "We can justify the 3 rules of Bayes ball as follows. First consider a chain structure X → Y →\n",
      "\n",
      "p(x, y, z) = p(x)p(y|x)p(z|y)\n",
      "\n",
      "(10.35)\n",
      "\n",
      "10.5. Conditional independence properties of DGMs\n",
      "\n",
      "325\n",
      "\n",
      "X\n",
      "\n",
      "Y\n",
      "\n",
      "Z\n",
      "\n",
      "Y\n",
      "\n",
      "X\n",
      "\n",
      "Z\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "X\n",
      "\n",
      "Z\n",
      "\n",
      "X\n",
      "\n",
      "Y\n",
      "\n",
      "Z\n",
      "\n",
      "Y\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Y\n",
      "\n",
      "X\n",
      "\n",
      "Z\n",
      "\n",
      "X\n",
      "\n",
      "Z\n",
      "\n",
      "Y\n",
      "\n",
      "(e)\n",
      "\n",
      "(f)\n",
      "\n",
      "Figure 10.9 Bayes ball rules. A shaded node is one we condition on. If there is an arrow hitting a bar, it means the ball cannot pass through; otherwise the ball can pass through. Based on (Jordan 2007).\n",
      "\n",
      "When we condition on y, arex and z independent? We have\n",
      "\n",
      "p(x, z|y) =\n",
      "\n",
      "p(x)p(y|x)p(z|y) p(y)\n",
      "\n",
      "=\n",
      "\n",
      "p(x, y)p(z|y) p(y)\n",
      "\n",
      "= p(x|y)p(z|y)\n",
      "\n",
      "(10.36)\n",
      "\n",
      "and therefore x ⊥ z|y. So observing the middle node of chain breaks it in two (as in a Markov chain).\n",
      "\n",
      "Now consider the tent structure X ← Y → Z. The joint is\n",
      "\n",
      "p(x, y, z) = p(y)p(x|y)p(z|y)\n",
      "\n",
      "(10.37)\n",
      "\n",
      "326\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "x\n",
      "\n",
      "z\n",
      "\n",
      "y\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "y(cid:14)\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 10.10 (a-b) Bayes ball boundary conditions. (c) Example of why we need boundary conditions. y(cid:2) is an observed child of y, rendering y “effectively observed”, so the ball bounces back up on its way from x to z.\n",
      "\n",
      "When we condition on y, arex and z independent? We have\n",
      "\n",
      "p(x, z|y) =\n",
      "\n",
      "p(x, y, z) p(y)\n",
      "\n",
      "=\n",
      "\n",
      "p(y)p(x|y)p(z|y) p(y)\n",
      "\n",
      "= p(x|y)p(z|y)\n",
      "\n",
      "(10.38)\n",
      "\n",
      "and therefore x ⊥ z|y. So observing a root node separates its children (as in a naive Bayes classiﬁer: see Section 3.5).\n",
      "\n",
      "Finally consider a v-structure X → Y ← Z. The joint is\n",
      "\n",
      "p(x, y, z) = p(x)p(z)p(y|x, z)\n",
      "\n",
      "(10.39)\n",
      "\n",
      "When we condition on y, arex and z independent? We have\n",
      "\n",
      "p(x, z|y) =\n",
      "\n",
      "p(x)p(z)p(y|x, z) p(y)\n",
      "\n",
      "(10.40)\n",
      "\n",
      "so x (cid:7)⊥ z|y. However, in the unconditional distribution, we have\n",
      "\n",
      "p(x, z) = p(x)p(z)\n",
      "\n",
      "(10.41)\n",
      "\n",
      "so we see that x and z are marginally independent. So we see that conditioning on a common child at the bottom of a v-structure makes its parents become dependent. This important effect is called explaining away, inter-causal reasoning, or Berkson’s paradox. As an example of explaining away, suppose we toss two coins, representing the binary numbers 0 and 1, and we observe the “sum” of their values. A priori, the coins are independent, but once we observe their sum, they become coupled (e.g., if the sum is 1, and the ﬁrst coin is 0, then we know the second coin is 1).\n",
      "\n",
      "Finally, Bayes Ball also needs the “boundary conditions” shown in Figure 10.10(a-b). To understand where these rules come from, consider Figure 10.10(c). Suppose Y (cid:4) is a noise-free copy of Y . Then if we observe Y (cid:4), we effectively observe Y as well, so the parents X and Z have to compete to explain this. So if we send a ball down X → Y → Y (cid:4), it should “bounce back” up along Y (cid:4) → Y → Z. However, if Y and all its children are hidden, the ball does not bounce back.\n",
      "\n",
      "10.5. Conditional independence properties of DGMs\n",
      "\n",
      "327\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "1\n",
      "\n",
      "5\n",
      "\n",
      "7\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "\n",
      "Figure 10.11 A DGM.\n",
      "\n",
      "For example, in Figure 10.11, we see that x2 ⊥ x6|x5, since the 2 → 5 → 6 path is blocked by x5 (which is observed), the 2 → 4 → 7 → 6 path is blocked by x7 (which is hidden), and the 2 → 1 → 3 → 6 path is blocked by x1 (which is hidden). However, we also see that x2 (cid:7)⊥ x6|x5, x7, since now the 2 → 4 → 7 → 6 path is no longer blocked by x7 (which is observed). Exercise 10.2 gives you some more practice in determining CI relationships for DGMs.\n",
      "\n",
      "10.5.2\n",
      "\n",
      "Other Markov properties of DGMs\n",
      "\n",
      "From the d-separation criterion, one can conclude that\n",
      "\n",
      "t ⊥ nd(t) \\ pa(t)|pa(t)\n",
      "\n",
      "(10.42)\n",
      "\n",
      "where the non-descendants of a node nd(t) are all the nodes except for its descendants, nd(t) =V \\ { t ∪ desc(t)}. Equation 10.42 is called the directed local Markov property. For example, in Figure 10.11, we have nd(3) = {2, 4}, and pa(3) = 1, so3 ⊥ 2, 4|1.\n",
      "\n",
      "A special case of this property is when we only look at predecessors of a node according to\n",
      "\n",
      "some topological ordering. We have\n",
      "\n",
      "t ⊥ pred(t) \\ pa(t)|pa(t)\n",
      "\n",
      "(10.43)\n",
      "\n",
      "which follows since pred(t) ⊆ nd(t). This is called the ordered Markov property, which justiﬁes Equation 10.7. For example, in Figure 10.11, if we use the ordering 1, 2, . . . , 7. we ﬁnd pred(3) = {1, 2} and pa(3) = 1, so3 ⊥ 2|1.\n",
      "\n",
      "We have now described three Markov properties for DAGs: the directed global Markov property G in Equation 10.34, the ordered Markov property O in Equation 10.43, and the directed local It is obvious that G =⇒ L =⇒ O. What is less Markov property L in Equation 10.42. obvious, but nevertheless true, is that O =⇒ L =⇒ G (see e.g., (Koller and Friedman 2009) for the proof). Hence all these properties are equivalent.\n",
      "\n",
      "Furthermore, any distribution p that is Markov wrt G can be factorized as in Equation 10.7; this is called the factorization property F. It is obvious that O =⇒ F , but one can show that the converse also holds (see e.g., (Koller and Friedman 2009) for the proof).\n",
      "\n",
      "10.5.3 Markov blanket and full conditionals\n",
      "\n",
      "The set of nodes that renders a node t conditionally independent of all the other nodes in the graph is called t’s Markov blanket; we will denote this by mb(t). One can show that the Markov blanket of a node in a DGM is equal to the parents, the children, and the co-parents,\n",
      "\n",
      "328\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "i.e., other nodes who are also parents of its children:\n",
      "\n",
      "mb(t) (cid:2) ch(t) ∪ pa(t) ∪ copa(t)\n",
      "\n",
      "(10.44)\n",
      "\n",
      "For example, in Figure 10.11, we have\n",
      "\n",
      "mb(5) = {6, 7} ∪ {2, 3} ∪ {4} = {2, 3, 4, 6, 7}\n",
      "\n",
      "(10.45)\n",
      "\n",
      "where 4 is a co-parent of 5 because they share a common child, namely 7.\n",
      "\n",
      "To see why the co-parents are in the Markov blanket, note that when we derive p(xt|x−t) = p(xt, x−t)/p(x−t), all the terms that do not involve xt will cancel out between numerator and denominator, so we are left with a product of CPDs which contain xt in their scope. Hence\n",
      "\n",
      "p(xt|x−t) ∝ p(xt|xpa(t))\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "p(xs|xpa(s))\n",
      "\n",
      "(10.46)\n",
      "\n",
      "s∈ch(t)\n",
      "\n",
      "For example, in Figure 10.11 we have\n",
      "\n",
      "(10.47) The resulting expression is called t’s full conditional, and will prove to be important when we study Gibbs sampling (Section 24.2).\n",
      "\n",
      "p(x5|x−5) ∝ p(x5|x2, x3)p(x6|x3, x5)p(x7|x4, x5, x6)\n",
      "\n",
      "10.6\n",
      "\n",
      "Inﬂuence (decision) diagrams *\n",
      "\n",
      "We can represent multi-stage (Bayesian) decision problems by using a graphical notation known as a decision diagram or an inﬂuence diagram (Howard and Matheson 1981; Kjaerulff and Madsen 2008). This extends directed graphical models by adding decision nodes (also called ac- tion nodes), represented by rectangles, and utility nodes (also called value nodes), represented by diamonds. The original random variables are called chance nodes, and are represented by ovals, as usual.\n",
      "\n",
      "Figure 10.12(a) gives a simple example, illustrating the famous oil wild-catter problem.3 In this problem, you have to decide whether to drill an oil well or not. You have two possible actions: d = 1 means drill, d = 0 means don’t drill. You assume there are 3 states of nature: o = 0 means the well is dry, o = 1 means it is wet (has some oil), and o = 2 means it is soaking (has a lot of oil). Suppose your prior beliefs are p(o) = [0.5, 0.3, 0.2]. Finally, you must specify the utility function U (d, o). Since the states and actions are discrete, we can represent it as a table (analogous to a CPT in a DGM). Suppose we use the following numbers, in dollars:\n",
      "\n",
      "d = 0 d = 1\n",
      "\n",
      "o = 0 0 -70\n",
      "\n",
      "o = 1 0 50\n",
      "\n",
      "o = 2 0 200\n",
      "\n",
      "We see that if you don’t drill, you incur no costs, but also make no money. If you drill a dry well, you lose $70; if you drill a wet well, you gain $50; and if you drill a soaking well, you gain $200. Your prior expected utility if you drill is given by\n",
      "\n",
      "EU (d = 1) =\n",
      "\n",
      "2(cid:2)\n",
      "\n",
      "p(o)U (d, o) = 0.5 · (−70) + 0.3 · 50 + 0.2 · 200 = 20\n",
      "\n",
      "(10.48)\n",
      "\n",
      "o=0\n",
      "\n",
      "3. This example is originally from (Raiffa 1968). Our presentation is based on some notes by Daphne Koller.\n",
      "\n",
      "10.6.\n",
      "\n",
      "Inﬂuence (decision) diagrams *\n",
      "\n",
      "329\n",
      "\n",
      "Oil\n",
      "\n",
      "Oil\n",
      "\n",
      "Sound\n",
      "\n",
      "Drill\n",
      "\n",
      "Drill\n",
      "\n",
      "Utility\n",
      "\n",
      "Utility\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Test\n",
      "\n",
      "Oil\n",
      "\n",
      "Sound\n",
      "\n",
      "Cost\n",
      "\n",
      "Drill\n",
      "\n",
      "Utility\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 10.12 (a) Inﬂuence diagram for basic oil wild catter problem. (b) An extension in which we have an information arc from the Sound chance node to the Drill decision node. (c) An extension in which we get to decide whether to perform the test or not.\n",
      "\n",
      "Your expected utility if you don’t drill is 0. So your maximum expected utility is\n",
      "\n",
      "M EU = max{EU (d = 0), EU (d = 1)} = max{0, 20} = 20\n",
      "\n",
      "(10.49)\n",
      "\n",
      "and therefore the optimal action is to drill:\n",
      "\n",
      "d∗ = arg max{EU (d = 0), EU (d = 1)} = 1\n",
      "\n",
      "(10.50)\n",
      "\n",
      "Now let us consider a slight extension to the model. Suppose you perform a sounding to estimate the state of the well. The sounding observation can be in one of 3 states: s = 0 is a diffuse reﬂection pattern, suggesting no oil; s = 1 is an open reﬂection pattern, suggesting some oil; and s = 2 is a closed reﬂection pattern, indicating lots of oil. Since S is caused by O, we add an O → S arc to our model. In addition, we assume that the outcome of the sounding test will be available before we decide whether to drill or not; hence we add an information arc from S to D. This is illustrated in Figure 10.12(b).\n",
      "\n",
      "p(s|o):\n",
      "\n",
      "Let us model the reliability of our sensor using the following conditional distribution for\n",
      "\n",
      "330\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "o = 0 o = 1 o = 2\n",
      "\n",
      "s = 0 0.6 0.3 0.1\n",
      "\n",
      "s = 1 0.3 0.4 0.4\n",
      "\n",
      "s = 2 0.1 0.3 0.5\n",
      "\n",
      "Suppose we do the sounding test and we observe s = 0. The posterior over the oil state is\n",
      "\n",
      "p(o|s = 0) = [0.732, 0.219, 0.049]\n",
      "\n",
      "(10.51)\n",
      "\n",
      "Now your posterior expected utility of performing action d is\n",
      "\n",
      "EU (d|s = 0) =\n",
      "\n",
      "2(cid:2)\n",
      "\n",
      "p(o|s = 0)U (o, d)\n",
      "\n",
      "(10.52)\n",
      "\n",
      "o=0\n",
      "\n",
      "If d = 1, this gives\n",
      "\n",
      "EU (d = 1|s = 0) = 0.732 × (−70) + 0.219 × 50 + 0.049 × 200 = −30.5\n",
      "\n",
      "(10.53)\n",
      "\n",
      "However, if d = 0, then EU (d = 0|s = 0) = 0, since not drilling incurs no cost. So if we observe s = 0, we are better off not drilling, which makes sense.\n",
      "\n",
      "Now suppose we do the sounding test and we observe s = 1. By similar reasoning, one can show that EU (d = 1|s = 1) = 32.9, which is higher than EU (d = 0|s = 1) = 0. Similarly, if we observe s = 2, we have EU (d = 1|s = 2) = 87.5 which is much higher than EU (d = 0|s = 2) = 0. Hence the optimal policy d∗(s) is as follows: if s = 0, choose d∗(0) = 0 and get $0; if s = 1, choose d∗(1) = 1 and get $32.9; and if s = 2, choose d∗(2) = 1 and get $87.5.\n",
      "\n",
      "You can compute your expected proﬁt or maximum expected utility as follows:\n",
      "\n",
      "M EU =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(s)EU (d∗(s)|s)\n",
      "\n",
      "(10.54)\n",
      "\n",
      "s\n",
      "\n",
      "This is the expected utility given possible outcomes of the sounding test, assuming you act optimally given the outcome. The prior marginal on the outcome of the test is\n",
      "\n",
      "p(s) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(o)p(s|o) = [0.41, 0.35, 0.24]\n",
      "\n",
      "(10.55)\n",
      "\n",
      "o\n",
      "\n",
      "Hence your maximum expected utility is\n",
      "\n",
      "M EU = 0.41 × 0 + 0.35 × 32.9 + 0.24 × 87.5 = 32.2\n",
      "\n",
      "(10.56)\n",
      "\n",
      "Now suppose you can choose whether to do the test or not. This can be modelled as shown in Figure 10.12(c), where we add a new test node T . If T = 1, we do the test, and S can enter 1 of 3 states, determined by O, exactly as above. If T = 0, we don’t do the test, and S enters a special unknown state. There is also some cost associated with performing the test.\n",
      "\n",
      "Is it worth doing the test? This depends on how much our MEU changes if we know the If you don’t do the test, we have M EU = 20 outcome of the test (namely the state of S). If you do the test, you have M EU = 32.2 from Equation 10.56. So the from Equation 10.49. improvement in utility if you do the test (and act optimally on its outcome) is $12.2. This is\n",
      "\n",
      "10.6.\n",
      "\n",
      "Inﬂuence (decision) diagrams *\n",
      "\n",
      "331\n",
      "\n",
      "at\n",
      "\n",
      "at\n",
      "\n",
      "xt\n",
      "\n",
      "xt+1\n",
      "\n",
      "zt\n",
      "\n",
      "zt+1\n",
      "\n",
      "xt\n",
      "\n",
      "xt+1\n",
      "\n",
      "Rt\n",
      "\n",
      "Rt\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 10.13 (a) A POMDP, shown as an inﬂuence diagram. zt are hidden world states. We implicitly make the no forgetting assumption, which effectively means that at has arrows coming into it from all previous observations, x1:t. (b) An MDP, shown as an inﬂuence diagram.\n",
      "\n",
      "called the value of perfect information (VPI). So we should do the test as long as it costs less than $12.2.\n",
      "\n",
      "In terms of graphical models, the VPI of a variable T can be determined by computing the MEU for the base inﬂuence diagram, I, and then computing the MEU for the same inﬂuence diagram where we add information arcs from T to the action nodes, and then computing the difference. In other words,\n",
      "\n",
      "VPI = MEU(I + T → D) − MEU(I)\n",
      "\n",
      "(10.57)\n",
      "\n",
      "where D is the decision node and T is the variable we are measuring.\n",
      "\n",
      "It is possible to modify the variable elimination algorithm (Section 20.3) so that it computes the optimal policy given an inﬂuence diagram. These methods essentially work backwards from the ﬁnal time-step, computing the optimal decision at each step assuming all following actions are chosen optimally. See e.g., (Lauritzen and Nilsson 2001; Kjaerulff and Madsen 2008) for details.\n",
      "\n",
      "We could continue to extend the model in various ways. For example, we could imagine a dynamical system in which we test, observe outcomes, perform actions, move on to the next oil well, and continue drilling (and polluting) in this way. In fact, many problems in robotics, business, medicine, public policy, etc. can be usefully formulated as inﬂuence diagrams unrolled over time (Raiffa 1968; Lauritzen and Nilsson 2001; Kjaerulff and Madsen 2008).\n",
      "\n",
      "A generic model of this form is shown in Figure 10.13(a). This is known as a partially observed Markov decision process or POMDP (pronounced “pom-d-p”). This is basically a hidden Markov model (Section 17.3) augmented with action and reward nodes. This can be used to model the perception-action cycle that all intelligent agents use (see e.g., (Kaelbling et al. 1998) for details).\n",
      "\n",
      "A special case of a POMDP, in which the states are fully observed, is called a Markov decision process or MDP, shown in Figure 10.13(b). This is much easier to solve, since we only have to compute a mapping from observed states to actions. This can be solved using dynamic programming (see e.g., (Sutton and Barto 1998) for details).\n",
      "\n",
      "In the POMDP case, the information arc from xt to at is not sufficient to uniquely determine\n",
      "\n",
      "332\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "G\n",
      "\n",
      "H\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "D\n",
      "\n",
      "I\n",
      "\n",
      "A\n",
      "\n",
      "D\n",
      "\n",
      "E\n",
      "\n",
      "F\n",
      "\n",
      "E\n",
      "\n",
      "F\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "G\n",
      "\n",
      "H\n",
      "\n",
      "I\n",
      "\n",
      "A\n",
      "\n",
      "J\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 10.14\n",
      "\n",
      "Some DGMs.\n",
      "\n",
      "the best action, since the state is not fully observed. Instead, we need to choose actions based on our belief state, p(zt|x1:t, a1:t). Since the belief updating process is deterministic (see Section 17.4.2), we can compute a belief state MDP. For details on to compute the policies for such models, see e.g., (Kaelbling et al. 1998; Spaan and Vlassis 2005).\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 10.1 Marginalizing a node in a DGM (Source: Koller.) Consider the DAG G in Figure 10.14(a). Assume it is a minimal I-map for p(A, B, C, D, E, F, X). Now consider marginalizing out X. Construct a new DAG G(cid:2) which is a minimal I-map for p(A, B, C, D, E, F ). Specify (and justify) which extra edges need to be added.\n",
      "\n",
      "Exercise 10.2 Bayes Ball (Source: Jordan.)\n",
      "\n",
      "Here we compute some global independence statements from some directed graphical models. You can use the “Bayes ball” algorithm, the d-separation criterion, or the method of converting to an undirected graph (all should give the same results).\n",
      "\n",
      "a. Consider the DAG in Figure 10.14(b). List all variables that are independent of A given evidence on B. b. Consider the DAG in Figure 10.14(c). List all variables that are independent of A given evidence on J.\n",
      "\n",
      "Exercise 10.3 Markov blanket for a DGM Prove that the full conditional for node i in a DGM is given by\n",
      "\n",
      "p(Xi|X−i) ∝ p(Xi|P a(Xi))\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "p(Yj|P a(Yj))\n",
      "\n",
      "(10.58)\n",
      "\n",
      "Yj ∈ch(Xi)\n",
      "\n",
      "where ch(Xi) are the children of Xi and P a(Yj) are the parents of Yj.\n",
      "\n",
      "Exercise 10.4 Hidden variables in DGMs Consider the DGMs in Figure 11.1 which both deﬁne p(X1:6), where we number empty nodes left to right, top to bottom. The graph on the left deﬁnes the joint as\n",
      "\n",
      "p(X1:6) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(X1)p(X2)p(X3)p(H = h|X1:3)p(X4|H = h)p(X5|H = h)p(X6|H = h) (10.59)\n",
      "\n",
      "h\n",
      "\n",
      "10.6.\n",
      "\n",
      "Inﬂuence (decision) diagrams *\n",
      "\n",
      "333\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 10.15\n",
      "\n",
      "(a) Weather BN. (b) Fishing BN.\n",
      "\n",
      "where we have marginalized over the hidden variable H. The graph on the right deﬁnes the joint as\n",
      "\n",
      "p(X1:6) = p(X1)p(X2)p(X3)p(X4|X1:3)p(X5|X1:4)p(X6|X1:5)\n",
      "\n",
      "(10.60)\n",
      "\n",
      "a.\n",
      "\n",
      "b.\n",
      "\n",
      "c.\n",
      "\n",
      "(5 points) Assuming all nodes (including H) are binary and all CPDs are tabular, prove that the model on the left has 17 free parameters. (5 points) Assuming all nodes are binary and all CPDs are tabular, prove that the model on the right has 59 free parameters. (5 points) Suppose we have a data set D = X n 1:6 for n = 1 : N , where we observe the Xs but not H, and we want to estimate the parameters of the CPDs using maximum likelihood. For which model is this easier? Explain your answer.\n",
      "\n",
      "Exercise 10.5 Bayes nets for a rainy day (Source: Nando de Freitas.). In this question you must model a problem with 4 binary variables: G =”gray”, V =”Vancouver”, R =”rain” and S =”sad”. Consider the directed graphical model describing the relation- ship between these variables shown in Figure 10.15(a).\n",
      "\n",
      "a. Write down an expression for P (S = 1|V = 1) in terms of α, β, γ, δ. b. Write down an expression for P (S = 1|V = 0). Is this the same or different to P (S = 1|V = 1)?\n",
      "\n",
      "Explain why.\n",
      "\n",
      "c. Find maximum likelihood estimates of α, β, γ using the following data set, where each row is a training\n",
      "\n",
      "case. (You may state your answers without proof.)\n",
      "\n",
      "V G R S 1 1 1 1 0 1\n",
      "\n",
      "1 1 0\n",
      "\n",
      "1 0 0\n",
      "\n",
      "(10.61)\n",
      "\n",
      "Exercise 10.6 Fishing nets (Source: the following variables\n",
      "\n",
      "(Duda et al. 2001)..) Consider the Bayes net shown in Figure 10.15(b). Here, the nodes represent\n",
      "\n",
      "X1 ∈ {winter, spring, summer, autumn}, X2 ∈ {salmon, sea bass} X3 ∈ {light, medium, dark}, X4 ∈ {wide, thin}\n",
      "\n",
      "(10.62) (10.63)\n",
      "\n",
      "334\n",
      "\n",
      "Chapter 10. Directed graphical models (Bayes nets)\n",
      "\n",
      "Z1\n",
      "\n",
      "Z2\n",
      "\n",
      "Z3\n",
      "\n",
      "Z1\n",
      "\n",
      "Z2\n",
      "\n",
      "Z3\n",
      "\n",
      "X1\n",
      "\n",
      "X2\n",
      "\n",
      "X3\n",
      "\n",
      "X4\n",
      "\n",
      "X5\n",
      "\n",
      "X1\n",
      "\n",
      "X2\n",
      "\n",
      "X4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 10.16 (a) A QMR-style network with some hidden leaves. (b) Removing the barren nodes.\n",
      "\n",
      "The corresponding conditional probability tables are\n",
      "\n",
      "p(x1) =\n",
      "\n",
      "p(x3|x2) =\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      ".25\n",
      "\n",
      ".33 .8\n",
      "\n",
      ".25\n",
      "\n",
      ".33 .1\n",
      "\n",
      ".25\n",
      "\n",
      ".34 .1\n",
      "\n",
      "⎛\n",
      "\n",
      ".25\n",
      "\n",
      "(cid:25)\n",
      "\n",
      ", p(x2|x1) =\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "(cid:4)\n",
      "\n",
      ", p(x4|x2) =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      ".4 .95\n",
      "\n",
      ".9 .3 .4 .8\n",
      "\n",
      ".6 .05\n",
      "\n",
      ".1 .7 .6 .2 (cid:4)\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠\n",
      "\n",
      "(10.64)\n",
      "\n",
      "(10.65)\n",
      "\n",
      "Note that in p(x4|x2), the rows represent x2 and the columns x4 (so each row sums to one and represents the child of the CPD). Thus p(x4 = thin|x2 = sea bass) = 0.05, p(x4 = thin|x2 = salmon) = 0.6, etc. Answer the following queries. You may use matlab or do it by hand. In either case, show your work.\n",
      "\n",
      "a. Suppose the ﬁsh was caught on December 20 — the end of autumn and the beginning of winter — and thus let p(x1) = (.5, 0, 0, .5) instead of the above prior. (This is called soft evidence, since we do not know the exact value of X1, but we have a distribution over it.) Suppose the lightness has not been measured but it is known that the ﬁsh is thin. Classify the ﬁsh as salmon or sea bass.\n",
      "\n",
      "b. Suppose all we know is that the ﬁsh is thin and medium lightness. What season is it now, most likely?\n",
      "\n",
      "Use p(x1) =\n",
      "\n",
      "(cid:24)\n",
      "\n",
      ".25\n",
      "\n",
      ".25\n",
      "\n",
      ".25\n",
      "\n",
      ".25\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "Exercise 10.7 Removing leaves in BN20 networks a. Consider the QMR network, where only some of the symtpoms are observed. For example, in Fig- ure 10.16(a), X4 and X5 are hidden. Show that we can safely remove all the hidden leaf nodes without affecting the posterior over the disease nodes, i.e., prove that we can compute p(z1:3|x1, x2, x4) using the network in Figure 10.16(b). This is called barren node removal, and can be applied to any DGM.\n",
      "\n",
      "b. Now suppose we partition the leaves into three groups: on, off and unknown. Clearly we can remove the unknown leaves, since they are hidden and do not affect their parents. Show that we can analytically remove the leaves that are in the “off state”, by absorbing their effect into the prior of the parents. (This trick only works for noisy-OR CPDs.)\n",
      "\n",
      "Exercise 10.8 Handling negative ﬁndings in the QMR network Consider the QMR network. Let d be the hidden diseases, f − be the negative ﬁndings (leaf nodes that are off), and f − be the positive ﬁndings (leaf nodes that are on). We can compute the posterior p(d|f ,f +) in two steps: ﬁrst absorb the negative ﬁndings, p(d|f −) ∝ p(d)p(f −|d), then absorb the positive ﬁndings, p(d|f −, f +) ∝ p(d|f −)p(f +|d). Show that the ﬁrst step can be done in O(|d||f −|) time, where |d| is the number of dieases and |f −| is the number of negative ﬁndings. For simplicity, you can ignore leak nodes. (Intuitively, the reason for this is that there is no correlation induced amongst the parents when the ﬁnding is off, since there is no explaining away.)\n",
      "\n",
      "10.6.\n",
      "\n",
      "Inﬂuence (decision) diagrams *\n",
      "\n",
      "335\n",
      "\n",
      "Exercise 10.9 Moralization does not introduce new independence statements Recall that the process of moralizing a DAG means connecting together all “unmarried” parents that share a common child, and then dropping all the arrows. Let M be the moralization of DAG G. Show that CI(M ) ⊆ CI(G), where CI are the set of conditional independence statements implied by the model.\n",
      "\n",
      "11 Mixture models and the EM algorithm\n",
      "\n",
      "11.1\n",
      "\n",
      "Latent variable models\n",
      "\n",
      "In Chapter 10 we showed how graphical models can be used to deﬁne high-dimensional joint probability distributions. The basic idea is to model dependence between two variables by adding an edge between them in the graph. (Technically the graph represents conditional independence, but you get the point.)\n",
      "\n",
      "An alternative approach is to assume that the observed variables are correlated because they arise from a hidden common “cause”. Model with hidden variables are also known as latent variable models or LVMs. As we will see in this chapter, such models are harder to ﬁt than models with no latent variables. However, they can have signiﬁcant advantages, for two main reasons. First, LVMs often have fewer parameters than models that directly represent correlation in the visible space. This is illustrated in Figure 11.1. If all nodes (including H) are binary and all CPDs are tabular, the model on the left has 17 free parameters, whereas the model on the right has 59 free parameters.\n",
      "\n",
      "Second, the hidden variables in an LVM can serve as a bottleneck, which computes a compressed representation of the data. This forms the basis of unsupervised learning, as we will see. Figure 11.2 illustrates some generic LVM structures that can be used for this purpose. In general there are L latent variables, zi1, . . . , zIL, and D visible variables, xi1, . . . , xiD, If we have L >1, there are many latent factors contributing to each where usually D (cid:16) L. If L = 1, we we only have a single latent observation, so we have a many-to-many mapping. in this case, zi is usually discrete, and we have a one-to-many mapping. We can variable; also have a many-to-one mapping, representing different competing factors or causes for each observed variable; such models form the basis of probabilistic matrix factorization, discussed in Section 27.6.2. Finally, we can have a one-to-one mapping, which can be represented as zi → xi. By allowing zi and/or xi to be vector-valued, this representation can subsume all the others. Depending on the form of the likelihood p(xi|zi) and the prior p(zi), we can generate a variety of different models, as summarized in Table 11.1.\n",
      "\n",
      "11.2 Mixture models\n",
      "\n",
      "The simplest form of LVM is when zi ∈ {1, . . . , K}, representing a discrete latent state. We will use a discrete prior for this, p(zi) = Cat(π). For the likelihood, we use p(xi|zi = k) = pk(xi),\n",
      "\n",
      "338\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "(cid:43)\n",
      "\n",
      "(cid:20)(cid:26)(cid:3)(cid:83)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)\n",
      "\n",
      "(cid:24)(cid:28)(cid:3)(cid:83)(cid:68)(cid:85)(cid:68)(cid:80)(cid:72)(cid:87)(cid:72)(cid:85)(cid:86)\n",
      "\n",
      "Figure 11.1 A DGM with and without hidden variables. The leaves represent medical symptoms. The roots represent primary causes, such as smoking, diet and exercise. The hidden variable can represent mediating factors, such as heart disease, which might not be directly visible. zi\n",
      "\n",
      "zi1 . . . ziL\n",
      "\n",
      "xi1\n",
      "\n",
      ". . .\n",
      "\n",
      "xiD\n",
      "\n",
      "xi1\n",
      "\n",
      ". . .\n",
      "\n",
      "xiD\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "zi1 . . . ziL\n",
      "\n",
      "zi\n",
      "\n",
      "xi\n",
      "\n",
      "xi\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 11.2 A latent variable model represented as a DGM. Many-to-one. (d) One-to-one.\n",
      "\n",
      "(a) Many-to-many.\n",
      "\n",
      "(b) One-to-many.\n",
      "\n",
      "(c)\n",
      "\n",
      "where pk is the k’th base distribution for the observations; this can be of any type. The overall model is known as a mixture model, since we are mixing together the K base distributions as follows:\n",
      "\n",
      "p(xi|θ) =\n",
      "\n",
      "K(cid:2)\n",
      "\n",
      "πkpk(xi|θ)\n",
      "\n",
      "(11.1)\n",
      "\n",
      "k=1\n",
      "\n",
      "This is a convex combination of the pk’s, since we are taking a weighted sum, where the (cid:10)K mixing weights πk satisfy 0 ≤ πk ≤ 1 and\n",
      "\n",
      "k=1 πk = 1. We give some examples below.\n",
      "\n",
      "11.2. Mixture models\n",
      "\n",
      "339\n",
      "\n",
      "p(xi|zi) MVN Prod. Discrete Prod. Gaussian Prod. Gaussian Prod. Discrete Prod. Discrete Prod. Noisy-OR Prod. Bernoulli\n",
      "\n",
      "p(zi) Discrete Discrete Prod. Gaussian Prod. Laplace Prod. Gaussian Multinomial PCA Dirichlet Prod. Bernoulli Prod. Bernoulli\n",
      "\n",
      "Name Mixture of Gaussians Mixture of multinomials Factor analysis/ probabilistic PCA Probabilistic ICA/ sparse coding\n",
      "\n",
      "Latent Dirichlet allocation BN20/ QMR Sigmoid belief net\n",
      "\n",
      "Section 11.2.1 11.2.2 12.1.5 12.6 27.2.3 27.3 10.2.3 27.7\n",
      "\n",
      "Table 11.1 “Prod. Discrete” in the likelihood means a factored distribution of the form Gaussian” means a factored distribution of the form analysis”. “ICA” stands for “indepedendent components analysis”.\n",
      "\n",
      "Summary of some popular directed latent variable models. Here “Prod” means product, so j Cat(xij|zi), and “Prod. j N (xij|zi). “PCA” stands for “principal components\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.8\n",
      "\n",
      "0.9\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.3 A mixture of 3 Gaussians in 2d. (a) We show the contours of constant probability for each component in the mixture. (b) A surface plot of the overall density. Based on Figure 2.23 of (Bishop 2006a). Figure generated by mixGaussPlotDemo.\n",
      "\n",
      "11.2.1 Mixtures of Gaussians\n",
      "\n",
      "The most widely used mixture model is the mixture of Gaussians (MOG), also called a Gaussian mixture model or GMM. In this model, each base distribution in the mixture is a multivariate Gaussian with mean μk and covariance matrix Σk. Thus the model has the form\n",
      "\n",
      "p(xi|θ) =\n",
      "\n",
      "K(cid:2)\n",
      "\n",
      "πkN (xi|μk, Σk)\n",
      "\n",
      "(11.2)\n",
      "\n",
      "k=1\n",
      "\n",
      "Figure 11.3 shows a mixture of 3 Gaussians in 2D. Each mixture component is represented by a different set of eliptical contours. Given a sufficiently large number of mixture components, a GMM can be used to approximate any density deﬁned on RD.\n",
      "\n",
      "340\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "11.2.2 Mixture of multinoullis\n",
      "\n",
      "We can use mixture models to deﬁne density models on many kinds of data. For example, suppose our data consist of D-dimensional bit vectors. In this case, an appropriate class- conditional density is a product of Bernoullis:\n",
      "\n",
      "p(xi|zi = k, θ) =\n",
      "\n",
      "D(cid:27)\n",
      "\n",
      "Ber(xij|μjk) =\n",
      "\n",
      "D(cid:27)\n",
      "\n",
      "μxij jk (1 − μjk)1−xij\n",
      "\n",
      "(11.3)\n",
      "\n",
      "j=1\n",
      "\n",
      "j=1\n",
      "\n",
      "where μjk is the probability that bit j turns on in cluster k.\n",
      "\n",
      "The latent variables do not have to any meaning, we might simply introduce latent variables in order to make the model more powerful. For example, one can show (Exercise 11.8) that the mean and covariance of the mixture distribution are given by\n",
      "\n",
      "E [x] =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "πkμk\n",
      "\n",
      "(11.4)\n",
      "\n",
      "cov [x] =\n",
      "\n",
      "k (cid:2)\n",
      "\n",
      "πk[Σk + μkμT\n",
      "\n",
      "k ] − E [x] E [x]\n",
      "\n",
      "T\n",
      "\n",
      "(11.5)\n",
      "\n",
      "k\n",
      "\n",
      "where Σk = diag(μjk(1 − μjk)). So although the component distributions are factorized, the joint distribution is not. Thus the mixture distribution can capture correlations between variables, unlike a single product-of-Bernoullis model.\n",
      "\n",
      "11.2.3\n",
      "\n",
      "Using mixture models for clustering\n",
      "\n",
      "There are two main applications of mixture models. The ﬁrst is to use them as a black-box density model, p(xi). This can be useful for a variety of tasks, such as data compression, outlier detection, and creating generative classiﬁers, where we model each class-conditional density p(x|y = c) by a mixture distribution (see Section 14.7.3).\n",
      "\n",
      "The second, and more common, application of mixture models is to use them for clustering. We discuss this topic in detail in Chapter 25, but the basic idea is simple. We ﬁrst ﬁt the mixture model, and then compute p(zi = k|xi, θ), which represents the posterior probability that point i belongs to cluster k. This is known as the responsibility of cluster k for point i, and can be computed using Bayes rule as follows:\n",
      "\n",
      "rik (cid:2) p(zi = k|xi, θ) =\n",
      "\n",
      "(cid:10)K\n",
      "\n",
      "p(zi = k|θ)p(xi|zi = k, θ) k(cid:2)=1 p(zi = k(cid:4)|θ)p(xi|zi = k(cid:4), θ)\n",
      "\n",
      "(11.6)\n",
      "\n",
      "This procedure is called soft clustering, and is identical to the computations performed when using a generative classiﬁer. The difference between the two models only arises at training time: in the mixture case, we never observe zi, whereas with a generative classiﬁer, we do observe yi (which plays the role of zi).\n",
      "\n",
      "We can represent the amount of uncertainty in the cluster assignment by using 1 − maxk rik. Assuming this is small, it may be reasonable to compute a hard clustering using the MAP estimate, given by\n",
      "\n",
      "z∗ i = arg max\n",
      "\n",
      "k\n",
      "\n",
      "rik = arg max\n",
      "\n",
      "k\n",
      "\n",
      "log p(xi|zi = k, θ) + log p(zi = k|θ)\n",
      "\n",
      "(11.7)\n",
      "\n",
      "11.2. Mixture models\n",
      "\n",
      "341\n",
      "\n",
      "5\n",
      "\n",
      "yeast microarray data\n",
      "\n",
      "K−Means centroids\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "s e n e g\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "9.5\n",
      "\n",
      "time\n",
      "\n",
      "11.5\n",
      "\n",
      "13.5\n",
      "\n",
      "15.5\n",
      "\n",
      "18.5\n",
      "\n",
      "20.5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.4 centers produced by K-means. Figure generated by kmeansYeastDemo.\n",
      "\n",
      "(a) Some yeast gene expression data plotted as a time series.\n",
      "\n",
      "(c) Visualizing the 16 cluster\n",
      "\n",
      "0.12\n",
      "\n",
      "0.14\n",
      "\n",
      "0.12\n",
      "\n",
      "0.06\n",
      "\n",
      "0.13\n",
      "\n",
      "0.07\n",
      "\n",
      "0.05\n",
      "\n",
      "0.15\n",
      "\n",
      "0.07\n",
      "\n",
      "0.09\n",
      "\n",
      "Figure 11.5 We ﬁt a mixture of 10 Bernoullis to the binarized MNIST digit data. We show the MLE for the corresponding cluster means, μk. The numbers on top of each image represent the mixing weights ˆπk. No labels were used when training the model. Figure generated by mixBerMnistEM.\n",
      "\n",
      "Hard clustering using a GMM is illustrated in Figure 1.8, where we cluster some data rep- resenting the height and weight of people. The colors represent the hard assignments. Note that the identity of the labels (colors) used is immaterial; we are free to rename all the clusters, without affecting the partitioning of the data; this is called label switching.\n",
      "\n",
      "Another example is shown in Figure 11.4. Here the data vectors xi ∈ R7 represent the expression levels of different genes at 7 different time points. We clustered them using a GMM. We see that there are several kinds of genes, such as those whose expression level goes up monotonically over time (in response to a given stimulus), those whose expression level goes down monotonically, and those with more complex response patterns. We have clustered the series into K = 16 groups. (See Section 11.5 for details on how to choose K.) For example, we can represent each cluster by a prototype or centroid. This is shown in Figure 11.4(b).\n",
      "\n",
      "As an example of clustering binary data, consider a binarized version of the MNIST handwrit- ten digit dataset (see Figure 1.5(a)), where we ignore the class labels. We can ﬁt a mixture of\n",
      "\n",
      "342\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "Bernoullis to this, using K = 10, and then visualize the resulting centroids, ˆμk, as shown in Figure 11.5. We see that the method correctly discovered some of the digit classes, but overall the it has created multiple clusters for some digits, and no clusters for others. results aren’t great: There are several possible reasons for these “errors”:\n",
      "\n",
      "The model is very simple and does not capture the relevant visual characteristics of a digit. For example, each pixel is treated independently, and there is no notion of shape or a stroke.\n",
      "\n",
      "Although we think there should be 10 clusters, some of the digits actually exhibit a fair degree of visual variety. For example, there are two ways of writing 7’s (with and without the cross bar). Figure 1.5(a) illustrates some of the range in writing styles. Thus we need K (cid:16) 10 clusters to adequately model this data. However, if we set K to be large, there is nothing in the model or algorithm preventing the extra clusters from being used to create multiple versions of the same digit, and indeed this is what happens. We can use model selection to prevent too many clusters from being chosen but what looks visually appealing and what makes a good density estimator may be quite different.\n",
      "\n",
      "The likelihood function is not convex, so we may be stuck in a local optimum, as we explain\n",
      "\n",
      "in Section 11.3.2.\n",
      "\n",
      "trying to “interpret” any clusters that are discovered by the method. supervision, or using informative priors, can help a lot.)\n",
      "\n",
      "This example is typical of mixture modeling, and goes to show one must be very cautious (Adding a little bit of\n",
      "\n",
      "11.2.4 Mixtures of experts\n",
      "\n",
      "Section 14.7.3 described how to use mixture models in the context of generative classiﬁers. We can also use them to create discriminative models for classiﬁcation and regression. For example, It seems like a good model would be three different linear consider the data in Figure 11.6(a). regression functions, each applying to a different part of the input space. We can model this by allowing the mixing weights and the mixture densities to be input-dependent:\n",
      "\n",
      "p(yi|xi, zi = k, θ) =N (yi|wT\n",
      "\n",
      "k xi, σ2 k) p(zi|xi, θ) = Cat(zi|S(VT xi))\n",
      "\n",
      "(11.8)\n",
      "\n",
      "(11.9)\n",
      "\n",
      "See Figure 11.7(a) for the DGM.\n",
      "\n",
      "This model is called a mixture of experts or MoE (Jordan and Jacobs 1994). The idea is that each submodel is considered to be an “expert” in a certain region of input space. The function p(zi = k|xi, θ) is called a gating function, and decides which expert to use, depending on the input values. For example, Figure 11.6(b) shows how the three experts have “carved up” the 1d input space, Figure 11.6(a) shows the predictions of each expert individually (in this case, the experts are just linear regression models), and Figure 11.6(c) shows the overall prediction of the model, obtained using (cid:2)\n",
      "\n",
      "p(yi|xi, θ) =\n",
      "\n",
      "p(zi = k|xi, θ)p(yi|xi, zi = k, θ)\n",
      "\n",
      "(11.10)\n",
      "\n",
      "k\n",
      "\n",
      "We discuss how to ﬁt this model in Section 11.4.3.\n",
      "\n",
      "11.2. Mixture models\n",
      "\n",
      "343\n",
      "\n",
      "expert predictions, fixed mixing weights=0\n",
      "\n",
      "gating functions, fixed mixing weights=0\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.5\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "−0.5\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "−1\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "predicted mean and var, fixed mixing weights=0\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2 −1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 11.6 (a) Some data ﬁt with three separate regression lines. (b) Gating functions for three different “experts”. (c) The conditionally weighted average of the three expert predictions. Figure generated by mixexpDemo.\n",
      "\n",
      "xi\n",
      "\n",
      "xi\n",
      "\n",
      "z1 i\n",
      "\n",
      "zi\n",
      "\n",
      "z2 i\n",
      "\n",
      "yi\n",
      "\n",
      "yi\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.7\n",
      "\n",
      "(a) A mixture of experts. (b) A hierarchical mixture of experts.\n",
      "\n",
      "344\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "1.2\n",
      "\n",
      "forwards problem\n",
      "\n",
      "1.2\n",
      "\n",
      "expert predictions\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "1.2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "1\n",
      "\n",
      "prediction\n",
      "\n",
      "0.9\n",
      "\n",
      "mean mode\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "1.2\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 11.8 (a) Some data from a simple forwards model. (b) Some data from the inverse model, ﬁt (c) The with a mixture of 3 linear regressions. Training points are color coded by their responsibilities. predictive mean (red cross) and mode (black square). Based on Figures 5.20 and 5.21 of (Bishop 2006b). Figure generated by mixexpDemoOneToMany.\n",
      "\n",
      "It should be clear that we can “plug in” any model for the expert. For example, we can use neural networks (Chapter 16) to represent both the gating functions and the experts. The result is known as a mixture density network. Such models are slower to train, but can be more ﬂexible than mixtures of experts. See (Bishop 1994) for details.\n",
      "\n",
      "It is also possible to make each expert be itself a mixture of experts. This gives rise to a model known as the hierarchical mixture of experts. See Figure 11.7(b) for the DGM, and Section 16.2.6 for further details.\n",
      "\n",
      "11.2.4.1\n",
      "\n",
      "Application to inverse problems\n",
      "\n",
      "Mixtures of experts are useful in solving inverse problems. These are problems where we have to invert a many-to-one mapping. A typical example is in robotics, where the location of the end effector (hand) y is uniquely determined by the joint angles of the motors, x. However, for any given location y, there are many settings of the joints x that can produce it. Thus the inverse mapping x = f −1(y) is not unique. Another example is kinematic tracking of people from video (Bo et al. 2008), where the mapping from image appearance to pose is not unique, due to self occlusion, etc.\n",
      "\n",
      "11.3. Parameter estimation for mixture models\n",
      "\n",
      "345\n",
      "\n",
      "θz\n",
      "\n",
      "θz\n",
      "\n",
      "z1\n",
      "\n",
      "zN\n",
      "\n",
      "zi\n",
      "\n",
      "· ·\n",
      "\n",
      "x1\n",
      "\n",
      "xN\n",
      "\n",
      "xi\n",
      "\n",
      "N\n",
      "\n",
      "θx\n",
      "\n",
      "θx\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.9 A LVM represented as a DGM. Left: Model is unrolled for N examples. Right: same model using plate notation.\n",
      "\n",
      "is shown in Figure 11.8(a). We see that this deﬁnes a function, y = f (x), since for every value x along the horizontal axis, there is a unique response y. This is sometimes called the forwards model. Now consider the problem of computing x = f −1(y). The corresponding inverse model is shown in Figure 11.8(b); this is obtained by simply interchanging the x and y axes. Now we see that for some values along the horizontal axis, there are multiple possible outputs, so the inverse is not uniquely deﬁned. For example, if y = 0.8, then x could be 0.2 or 0.8. Consequently, the predictive distribution, p(x|y, θ) is multimodal.\n",
      "\n",
      "A simpler example, for illustration purposes,\n",
      "\n",
      "We can ﬁt a mixture of linear experts to this data. Figure 11.8(b) shows the prediction of each expert, and Figure 11.8(c) shows (a plugin approximation to) the posterior predictive mode and mean. Note that the posterior mean does not yield good predictions. In fact, any model which is trained to minimize mean squared error — even if the model is a ﬂexible nonlinear model, such as neural network — will work poorly on inverse problems such as this. However, the posterior mode, where the mode is input dependent, provides a reasonable approximation.\n",
      "\n",
      "11.3\n",
      "\n",
      "Parameter estimation for mixture models\n",
      "\n",
      "We have seen how to compute the posterior over the hidden variables given the observed variables, assuming the parameters are known. In this section, we discuss how to learn the parameters.\n",
      "\n",
      "In Section 10.4.2, we showed that when we have complete data and a factored prior, the posterior over the parameters also factorizes, making computation very simple. Unfortunately this is no longer true if we have hidden variables and/or missing data. The reason is apparent from looking at Figure 11.9. If the zi were observed, then by d-separation, we see that θz ⊥ θx|D, and hence the posterior will factorize. But since, in an LVM, the zi are hidden, the parameters are no longer independent, and the posterior does not factorize, making it much harder to\n",
      "\n",
      "346\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "35\n",
      "\n",
      "19.5\n",
      "\n",
      "30\n",
      "\n",
      "14.5\n",
      "\n",
      "9.5\n",
      "\n",
      "25\n",
      "\n",
      "4.5\n",
      "\n",
      "20\n",
      "\n",
      "μ\n",
      "\n",
      "2\n",
      "\n",
      "−0.5\n",
      "\n",
      "15\n",
      "\n",
      "−5.5\n",
      "\n",
      "10\n",
      "\n",
      "−10.5\n",
      "\n",
      "5\n",
      "\n",
      "−15.5\n",
      "\n",
      "0 −25\n",
      "\n",
      "−20\n",
      "\n",
      "−15\n",
      "\n",
      "−10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "−15.5\n",
      "\n",
      "−10.5\n",
      "\n",
      "−5.5\n",
      "\n",
      "−0.5 μ 1\n",
      "\n",
      "4.5\n",
      "\n",
      "9.5\n",
      "\n",
      "14.5\n",
      "\n",
      "19.5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.10 Left: N = 200 data points sampled from a mixture of 2 Gaussians in 1d, with πk = 0.5, σk = 5, μ1 = −10 and μ2 = 10. Right: Likelihood surface p(D|μ1, μ2), with all other parameters set to their true values. We see the two symmetric modes, reﬂecting the unidentiﬁability of the parameters. Figure generated by mixGaussLikSurfaceDemo.\n",
      "\n",
      "compute. This also complicates the computation of MAP and ML estimates, as we discus below.\n",
      "\n",
      "11.3.1\n",
      "\n",
      "Unidentiﬁability\n",
      "\n",
      "The main problem with computing p(θ|D) for an LVM is that the posterior may have multiple modes. To see why, consider a GMM. If the zi were all observed, we would have a unimodal posterior for the parameters:\n",
      "\n",
      "p(θ|D) = Dir(π|D)\n",
      "\n",
      "K(cid:27)\n",
      "\n",
      "NIW(μk, Σk|D)\n",
      "\n",
      "(11.11)\n",
      "\n",
      "k=1\n",
      "\n",
      "Consequently we can easily ﬁnd the globally optimal MAP estimate (and hence globally optimal MLE).\n",
      "\n",
      "But now suppose the zi’s are hidden. In this case, for each of the possible ways of “ﬁlling in” the zi’s, we get a different unimodal likelihood. Thus when we marginalize out over the zi’s, we get a multi-modal posterior for p(θ|D).1 These modes correspond to different labelings of the clusters. This is illustrated in Figure 11.10(b), where we plot the likelihood function, p(D|μ1, μ2), for a 2D GMM with K = 2 for the data is shown in Figure 11.10(a). We see two peaks, one corresponding to the case where μ1 = −10, μ2 = 10, and the other to the case where μ1 = 10, μ2 = −10. We say the parameters are not identiﬁable, since there is not a unique MLE. Therefore there cannot be a unique MAP estimate (assuming the prior does not rule out certain labelings), and hence the posterior must be multimodal. The question of how many modes there\n",
      "\n",
      "1. Do not confuse multimodality of the parameter posterior, p(θ|D), with the multimodality deﬁned by the model, p(x|θ). In the latter case, if we have K clusters, we would expect to only get K peaks, although it is theoretically possible to get more than K, at least if D > 1 (Carreira-Perpinan and Williams 2003).\n",
      "\n",
      "11.3. Parameter estimation for mixture models\n",
      "\n",
      "347\n",
      "\n",
      "are in the parameter posterior is hard to answer. There are K! possible labelings, but some of the peaks might get merged. Nevertheless, there can be an exponential number, since ﬁnding the optimal MLE for a GMM is NP-hard (Aloise et al. 2009; Drineas et al. 2004).\n",
      "\n",
      "For example, suppose we draw some samples from the posterior, θ(s) ∼ p(θ|D), and then average them, to try to s=1 θ(s) approximate the posterior mean, θ = 1 (This kind of Monte Carlo approach is S explained in more detail in Chapter 24.) If the samples come from different modes, the average will be meaningless. Note, however, that it is reasonable to average the posterior predictive s=1 p(x|θ(s)), since the likelihood function is invariant to which distributions, p(x) ≈ 1 S mode the parameters came from.\n",
      "\n",
      "Unidentiﬁability can cause a problem for Bayesian inference.\n",
      "\n",
      "(cid:10)S\n",
      "\n",
      "(cid:10)S\n",
      "\n",
      ".\n",
      "\n",
      "A variety of solutions have been proposed to the unidentiﬁability problem. These solutions depend on the details of the model and the inference algorithm that is used. For example, see (Stephens 2000) for an approach to handling unidentiﬁability in mixture models using MCMC.\n",
      "\n",
      "The approach we will adopt in this chapter is much simpler: we just compute a single (We say “approximate” since ﬁnding local mode, i.e., we perform approximate MAP estimation. the globally optimal MLE, and hence MAP estimate, is NP-hard, at least for mixture models (Aloise et al. 2009).) This is by far the most common approach, because of its simplicity. It is also a reasonable approximation, at least if the sample size is sufficiently large. To see why, consider Figure 11.9(a). We see that there are N latent variables, each of which gets to “see” one data point each. However, there are only two latent parameters, each of which gets to see N data points. So the posterior uncertainty about the parameters is typically much less than the posterior uncertainty about the latent variables. This justiﬁes the common strategy of computing p(zi|xi, ˆθ), but not bothering to compute p(θ|D). In Section 5.6, we will study hierarchical Bayesian models, which essentially put structure on top of the parameters. In such models, it is important to model p(θ|D), so that the parameters can send information between themselves. If we used a point estimate, this would not be possible.\n",
      "\n",
      "11.3.2\n",
      "\n",
      "Computing a MAP estimate is non-convex\n",
      "\n",
      "In the previous sections, we have argued, rather heuristically, that the likelihood function has multiple modes, and hence that ﬁnding an MAP or ML estimate will be hard. In this section, we show this result by more algebraic means, which sheds some additional insight into the problem. Our presentation is based in part on (Rennie 2004).\n",
      "\n",
      "Consider the log-likelihood for an LVM:\n",
      "\n",
      "log p(D|θ) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "log\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(xi, zi|θ)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(11.12)\n",
      "\n",
      "i\n",
      "\n",
      "zi\n",
      "\n",
      "Unfortunately, this objective is hard to maximize. since we cannot push the log inside the sum. This precludes certain algebraic simplications, but does not prove the problem is hard.\n",
      "\n",
      "Now suppose the joint probability distribution p(zi, xi|θ) is in the exponential family, which\n",
      "\n",
      "means it can be written as follows:\n",
      "\n",
      "p(x, z|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "exp[θT φ(x, z)]\n",
      "\n",
      "(11.13)\n",
      "\n",
      "348\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "where φ(x, z) are the sufficient statistics, and Z(θ) is the normalization constant (see Sec- tion 9.2 for more details). It can be shown (Exercise 9.2) that the MVN is in the exponential family, as are nearly all of the distributions we have encountered so far, including Dirichlet, multinomial, Gamma, Wishart, etc. (The Student distribution is a notable exception.) Further- more, mixtures of exponential families are also in the exponential family, providing the mixing indicator variables are observed (Exercise 11.11).\n",
      "\n",
      "With this assumption, the complete data log likelihood can be written as follows:\n",
      "\n",
      "(cid:6)c(θ) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "log p(xi, zi|θ) = θT (\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "φ(xi, zi)) − N Z(θ)\n",
      "\n",
      "(11.14)\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "The ﬁrst term is clearly linear in θ. One can show that Z(θ) is a convex function (Boyd and Vandenberghe 2004), so the overall objective is concave (due to the minus sign), and hence has a unique maximum.\n",
      "\n",
      "Now consider what happens when we have missing data. The observed data log likelihood\n",
      "\n",
      "is given by\n",
      "\n",
      "(cid:6)(θ) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "log\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(xi, zi|θ) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "log\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "eθT φ(zi,xi)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "− N log Z(θ)\n",
      "\n",
      "(11.15)\n",
      "\n",
      "i\n",
      "\n",
      "zi\n",
      "\n",
      "i\n",
      "\n",
      "zi\n",
      "\n",
      "One can show that the log-sum-exp function is convex (Boyd and Vandenberghe 2004), and we know that Z(θ) is convex. However, the difference of two convex functions is not, in general, convex. So the objective is neither convex nor concave, and has local optima.\n",
      "\n",
      "The disadvantage of non-convex functions is that it is usually hard to ﬁnd their global op- timum. Most optimization algorithms will only ﬁnd a local optimum; which one they ﬁnd depends on where they start. There are some algorithms, such as simulated annealing (Sec- tion 24.6.1) or genetic algorithms, that claim to always ﬁnd the global optimum, but this is only under unrealistic assumptions (e.g., if they are allowed to be cooled “inﬁnitely slowly”, or al- lowed to run “inﬁnitely long”). In practice, we will run a local optimizer, perhaps using multiple random restarts to increase out chance of ﬁnding a “good” local optimum. Of course, careful initialization can help a lot, too. We give examples of how to do this on a case-by-case basis.\n",
      "\n",
      "Note that a convex method for ﬁtting mixtures of Gaussians has been proposed. The idea is to assign one cluster per data point, and select from amongst them, using a convex (cid:6)1-type penalty, rather than trying to optimize the locations of the cluster centers. See (Lashkari and Golland 2007) for details. This is essentially an unsupervised version of the approach used in sparse kernel logistic regression, which we will discuss in Section 14.3.2. Note, however, that the (cid:6)1 penalty, although convex, is not necessarily a good way to promote sparsity, as discussed in Chapter 13. In fact, as we will see in that Chapter, some of the best sparsity-promoting methods use non-convex penalties, and use EM to optimie them! The moral of the story is: do not be afraid of non-convexity.\n",
      "\n",
      "11.4\n",
      "\n",
      "The EM algorithm\n",
      "\n",
      "For many models in machine learning and statistics, computing the ML or MAP parameter estimate is easy provided we observe all the values of all the relevant random variables, i.e., if\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "349\n",
      "\n",
      "Model Mix. Gaussians Mix. experts Factor analysis Student T Probit regression DGM with hidden variables MVN with missing data HMMs Shrinkage estimates of Gaussian means\n",
      "\n",
      "Section 11.4.2 11.4.3 12.1.5 11.4.5 11.4.6 11.4.4 11.6.1 17.5.2 Exercise 11.13\n",
      "\n",
      "Table 11.2 Some models discussed in this book for which EM can be easily applied to ﬁnd the ML/ MAP parameter estimate.\n",
      "\n",
      "we have complete data. However, if we have missing data and/or latent variables, then computing the ML/MAP estimate becomes hard.\n",
      "\n",
      "One approach is to use a generic gradient-based optimizer to ﬁnd a local minimum of the\n",
      "\n",
      "negative log likelihood or NLL, given by\n",
      "\n",
      "NLL(θ) = − (cid:2) 1 N\n",
      "\n",
      "log p(D|θ)\n",
      "\n",
      "(11.16)\n",
      "\n",
      "However, we often have to enforce constraints, such as the fact that covariance matrices must be positive deﬁnite, mixing weights must sum to one, etc., which can be tricky (see Exercise 11.5). In such cases, it is often much simpler (but not always faster) to use an algorithm called expectation maximization, or EM for short (Dempster et al. 1977; Meng and van Dyk 1997; McLachlan and Krishnan 1997). This is a simple iterative algorithm, often with closed-form updates at each step. Furthermore, the algorithm automatically enforce the required constraints.\n",
      "\n",
      "EM exploits the fact that if the data were fully observed, then the ML/ MAP estimate would be easy to compute. In particular, EM is an iterative algorithm which alternates between inferring the missing values given the parameters (E step), and then optimizing the parameters given the “ﬁlled in” data (M step). We give the details below, followed by several examples. We end with a more theoretical discussion, where we put the algorithm in a larger context. See Table 11.2 for a summary of the applications of EM in this book.\n",
      "\n",
      "11.4.1\n",
      "\n",
      "Basic idea\n",
      "\n",
      "Let xi be the visible or observed variables in case i, and let zi be the hidden or missing variables. The goal is to maximize the log likelihood of the observed data: (cid:18)\n",
      "\n",
      "(cid:6)(θ) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "log p(xi|θ) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "log\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(xi, zi|θ)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(11.17)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "zi\n",
      "\n",
      "Unfortunately this is hard to optimize, since the log cannot be pushed inside the sum.\n",
      "\n",
      "350\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "EM gets around this problem as follows. Deﬁne the complete data log likelihood to be\n",
      "\n",
      "(cid:6)c(θ) (cid:2)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "log p(xi, zi|θ)\n",
      "\n",
      "(11.18)\n",
      "\n",
      "i=1\n",
      "\n",
      "This cannot be computed, since zi is unknown. So let us deﬁne the expected complete data log likelihood as follows: Q(θ, θt−1) = E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(cid:6)c(θ)\n",
      "\n",
      "( (D, θt−1\n",
      "\n",
      "(11.19)\n",
      "\n",
      "where t is the current iteration number. Q is called the auxiliary function. The expectation is taken wrt the old parameters, θt−1 , and the observed data D. The goal of the E stepis to compute Q(θ, θt−1), or rather, the terms inside of it which the MLE depends on; these are known as the expected sufficient statistics or ESS. In the M step, we optimize the Q function wrt θ:\n",
      "\n",
      "θt = arg max\n",
      "\n",
      "θ\n",
      "\n",
      "Q(θ, θt−1)\n",
      "\n",
      "(11.20)\n",
      "\n",
      "To perform MAP estimation, we modify the M step as follows:\n",
      "\n",
      "θt = argmax\n",
      "\n",
      "θ\n",
      "\n",
      "Q(θ, θt−1) + log p(θ)\n",
      "\n",
      "(11.21)\n",
      "\n",
      "The E step remains unchanged.\n",
      "\n",
      "In Section 11.4.7 we show that the EM algorithm monotonically increases the log likelihood of the observed data (plus the log prior, if doing MAP estimation), or it stays the same. So if the objective ever goes down, there must be a bug in our math or our code. (This is a surprisingly useful debugging tool!)\n",
      "\n",
      "Below we explain how to perform the E and M steps for several simple models, that should\n",
      "\n",
      "make things clearer.\n",
      "\n",
      "11.4.2\n",
      "\n",
      "EM for GMMs\n",
      "\n",
      "In this section, we discuss how to ﬁt a mixture of Gaussians using EM. Fitting other kinds of mixture models requires a straightforward modiﬁcation — see Exercise 11.3. We assume the number of mixture components, K, is known (see Section 11.5 for discussion of this point).\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "351\n",
      "\n",
      "11.4.2.1\n",
      "\n",
      "Auxiliary function\n",
      "\n",
      "The expected complete data log likelihood is given by (cid:18)\n",
      "\n",
      "Q(θ, θ(t−1)) (cid:2) E\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "log p(xi, zi|θ)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(11.22)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "E\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "log\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "K(cid:27)\n",
      "\n",
      "(πkp(xi|θk))I(zi=k)\n",
      "\n",
      "(cid:19)(cid:19)\n",
      "\n",
      "(11.23)\n",
      "\n",
      "=\n",
      "\n",
      "i (cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "E [I(zi = k)] log[πkp(xi|θk)]\n",
      "\n",
      "(11.24)\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "i (cid:2)\n",
      "\n",
      "i (cid:2)\n",
      "\n",
      "k (cid:2)\n",
      "\n",
      "k (cid:2)\n",
      "\n",
      "p(zi = k|xi, θt−1) log[πkp(xi|θk)] (cid:2)\n",
      "\n",
      "rik log πk +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "rik log p(xi|θk)\n",
      "\n",
      "(11.25)\n",
      "\n",
      "(11.26)\n",
      "\n",
      "i\n",
      "\n",
      "k\n",
      "\n",
      "i\n",
      "\n",
      "k\n",
      "\n",
      "where rik (cid:2) p(zi = k|xi, θ(t−1)) is the responsibility that cluster k takes for data point i. This is computed in the E step, described below.\n",
      "\n",
      "11.4.2.2\n",
      "\n",
      "E step\n",
      "\n",
      "The E step has the following simple form, which is the same for any mixture model:\n",
      "\n",
      "rik =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "πkp(xi|θ(t−1) ) k(cid:2) πk(cid:2) p(xi|θ(t−1)\n",
      "\n",
      "k\n",
      "\n",
      "k(cid:2)\n",
      "\n",
      ")\n",
      "\n",
      "(11.27)\n",
      "\n",
      "11.4.2.3 M step\n",
      "\n",
      "In the M step, we optimize Q wrt π and the θk. For π, we obviously have\n",
      "\n",
      "and Σk. We see that the result is\n",
      "\n",
      "where rk (cid:2)\n",
      "\n",
      "To derive the M step for the μk and Σk terms, we look at the parts of Q that depend on μk\n",
      "\n",
      "πk =\n",
      "\n",
      "(cid:6)(μk, Σk) =\n",
      "\n",
      "1 N (cid:10)\n",
      "\n",
      "i rik is the weighted number of points assigned to cluster k.\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "rik =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "rk N\n",
      "\n",
      "rik log p(xi|θk)\n",
      "\n",
      "(11.28)\n",
      "\n",
      "(11.29)\n",
      "\n",
      "k\n",
      "\n",
      "= − 1 2\n",
      "\n",
      "i (cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "rik\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "log |Σk| + (xi − μk)T Σ−1\n",
      "\n",
      "k (xi − μk)\n",
      "\n",
      "(11.30)\n",
      "\n",
      "This is just a weighted version of the standard problem of computing the MLEs of an MVN (see Section 4.1.3). One can show (Exercise 11.2) that the new parameter estimates are given by\n",
      "\n",
      "Σk =\n",
      "\n",
      "μk =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i rikxi rk i rik(xi − μk)(xi − μk)T rk\n",
      "\n",
      "=\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i rikxixT i rk\n",
      "\n",
      "− μkμT k\n",
      "\n",
      "(11.32)\n",
      "\n",
      "(11.31)\n",
      "\n",
      "352\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "These equations make intuitive sense: the mean of cluster k is just the weighted average of all points assigned to cluster k, and the covariance is proportional to the weighted empirical scatter matrix.\n",
      "\n",
      "After computing the new estimates, we set θt = (πk, μk, Σk) for k = 1 :K , and go to the\n",
      "\n",
      "next E step.\n",
      "\n",
      "11.4.2.4\n",
      "\n",
      "Example\n",
      "\n",
      "An example of the algorithm in action is shown in Figure 11.11. We start with μ1 = (−1, 1), Σ1 = I, μ2 = (1, −1), Σ2 = I. We color code points such that blue points come from cluster 1 and red points from cluster 2. More precisely, we set the color to\n",
      "\n",
      "color(i) =r i1blue + ri2red\n",
      "\n",
      "(11.33)\n",
      "\n",
      "so ambiguous points appear purple. After 20 iterations, the algorithm has converged on a good (The data was standardized, by removing the mean and dividing by the standard clustering. deviation, before processing. This often helps convergence.)\n",
      "\n",
      "11.4.2.5\n",
      "\n",
      "K-means algorithm\n",
      "\n",
      "There is a popular variant of the EM algorithm for GMMs known as the K-means algorithm, which we now discuss. Consider a GMM in which we make the following assumptions: Σk = σ2ID is ﬁxed, and πk = 1/K is ﬁxed, so only the cluster centers, μk ∈ RD, have to be estimated. Now consider the following delta-function approximation to the posterior computed during the E step:\n",
      "\n",
      "p(zi = k|xi, θ) ≈ I(k = z∗ i )\n",
      "\n",
      "(11.34)\n",
      "\n",
      "where zi∗ = argmaxk p(zi = k|xi, θ). This is sometimes called hard EM, since we are making a hard assignment of points to clusters. Since we assumed an equal spherical covariance matrix for each cluster, the most probable cluster for xi can be computed by ﬁnding the nearest prototype:\n",
      "\n",
      "z∗ i = arg min\n",
      "\n",
      "k\n",
      "\n",
      "||xi − μk||2 2\n",
      "\n",
      "(11.35)\n",
      "\n",
      "Hence in each E step, we must ﬁnd the Euclidean distance between N data points and K cluster centers, which takes O(N KD) time. However, this can be sped up using various techniques, such as applying the triangle inequality to avoid some redundant computations (Elkan 2003). Given the hard cluster assignments, the M step updates each cluster center by computing the mean of all points assigned to it: (cid:2)\n",
      "\n",
      "μk =\n",
      "\n",
      "1 Nk\n",
      "\n",
      "i:zi=k\n",
      "\n",
      "xi\n",
      "\n",
      "(11.36)\n",
      "\n",
      "See Algorithm 5 for the pseudo-code.\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "353\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(e)\n",
      "\n",
      "(f)\n",
      "\n",
      "Figure 11.11 Illustration of the EM for a GMM applied to the Old Faithful data. (a) Initial (random) values (b) Posterior responsibility of each point computed in the ﬁrst E step. The degree of of the parameters. redness indicates the degree to which the point belongs to the red cluster, and similarly for blue; this purple points have a roughly uniform posterior over clusters. (c) We show the updated parameters after the ﬁrst M step. (d) After 3 iterations. (e) After 5 iterations. (f) After 16 iterations. Based on (Bishop 2006a) Figure 9.8. Figure generated by mixGaussDemoFaithful.\n",
      "\n",
      "354\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "Algorithm 11.1: K-means algorithm 1 initialize mk; 2 repeat 3\n",
      "\n",
      "Assign each data point to its closest cluster center: zi = arg mink ||xi − μk||2 2; Update each cluster center by computing the mean of all points assigned to it: μk = 1 Nk 5 until converged;\n",
      "\n",
      "4\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i:zi=k xi;\n",
      "\n",
      "K=2\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "120\n",
      "\n",
      "140\n",
      "\n",
      "160\n",
      "\n",
      "180\n",
      "\n",
      "200\n",
      "\n",
      "50\n",
      "\n",
      "100\n",
      "\n",
      "150\n",
      "\n",
      "200\n",
      "\n",
      "250\n",
      "\n",
      "300\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.12 An image compressed using vector quantization with a codebook of size K. (a) K = 2. (b) K = 4. Figure generated by vqDemo.\n",
      "\n",
      "11.4.2.6\n",
      "\n",
      "Vector quantization\n",
      "\n",
      "Since K-means is not a proper EM algorithm, it is not maximizing likelihood. Instead, it can be interpreted as a greedy algorithm for approximately minimizing a loss function related to data compression, as we now explain.\n",
      "\n",
      "Suppose we want to perform lossy compression of some real-valued vectors, xi ∈ RD. A very simple approach to this is to use vector quantization or VQ. The basic idea is to replace each real-valued vector xi ∈ RD with a discrete symbol zi ∈ {1, . . . , K}, which is an index into a codebook of K prototypes, μk ∈ RD. Each data vector is encoded by using the index of the most similar prototype, where similarity is measured in terms of Euclidean distance:\n",
      "\n",
      "encode(xi) = arg min\n",
      "\n",
      "k\n",
      "\n",
      "||xi − μk||2\n",
      "\n",
      "(11.37)\n",
      "\n",
      "We can deﬁne a cost function that measures the quality of a codebook by computing the reconstruction error or distortion it induces:\n",
      "\n",
      "J(μ, z|K, X) (cid:2) 1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "||xi − decode(encode(xi))||2 =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "||xi − μzi\n",
      "\n",
      "||2\n",
      "\n",
      "(11.38)\n",
      "\n",
      "where decode(k) = μk. The K-means algorithm can be thought of as a simple iterative scheme for minimizing this objective.\n",
      "\n",
      "Of course, we can achieve zero distortion if we assign one prototype to every data vector, but that takes O(N DC) space, where N is the number of real-valued data vectors, each of\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "355\n",
      "\n",
      "length D, and C is the number of bits needed to represent a real-valued scalar (the quantization accuracy). However, in many data sets, we see similar vectors repeatedly, so rather than storing them many times, we can store them once and then create pointers to them. Hence we can reduce the space requirement to O(N log2 K + KDC): the O(N log2 K) term arises because each of the N data vectors needs to specify which of the K codewords it is using (the pointers); and the O(KDC) term arises because we have to store each codebook entry, each of which is a D-dimensional vector. Typically the ﬁrst term dominates the second, so we can approximate the rate of the encoding scheme (number of bits needed per object) as O(log2 K), which is typically much less than O(DC).\n",
      "\n",
      "One application of VQ is to image compression. Consider the N = 200 × 320 = 64, 000 pixel image in Figure 11.12; this is gray-scale, so D = 1. If we use one byte to represent each pixel (a gray-scale intensity of 0 to 255), then C = 8, so we need N C = 512, 000 bits to represent the image. For the compressed image, we need N log2 K + KC bits. For K = 4, this is about 128kb, a factor of 4 compression. For K = 8, this is about 192kb, a factor of 2.6 compression, at negligible perceptual loss (see Figure 11.12(b)). Greater compression could be achieved if we modelled spatial correlation between the pixels, e.g., if we encoded 5x5 blocks (as used by JPEG). This is because the residual errors (differences from the model’s predictions) would be smaller, and would take fewer bits to encode.\n",
      "\n",
      "11.4.2.7\n",
      "\n",
      "Initialization and avoiding local minima\n",
      "\n",
      "Both K-means and EM need to be initialized. It is common to pick K data points at random, and to make these be the initial cluster centers. Or we can pick the centers sequentially so as to try to “cover” the data. That is, we pick the initial point uniformly at random. Then each subsequent point is picked from the remaining points with probability proportional to its squared distance to the points’s closest cluster center. This is known as farthest point clustering (Gonzales 1985), or k-means++ (Arthur and Vassilvitskii 2007; Bahmani et al. 2012). Surprisingly, this simple trick can be shown to guarantee that the distortion is never more than O(log K) worse than optimal (Arthur and Vassilvitskii 2007).\n",
      "\n",
      "An heuristic that is commonly used in the speech recognition community is to incrementally “grow” GMMs: we initially give each cluster a score based on its mixture weight; after each round of training, we consider splitting the cluster with the highest score into two, with the new centroids being random perturbations of the original centroid, and the new scores being half of the old scores. If a new cluster has too small a score, or too narrow a variance, it is removed. We continue in this way until the desired number of clusters is reached. See (Figueiredo and Jain 2002) for a similar incremental approach.\n",
      "\n",
      "11.4.2.8 MAP estimation\n",
      "\n",
      "As usual, the MLE may overﬁt. The overﬁtting problem is particularly severe in the case of GMMs. To understand the problem, suppose for simplicity that Σk = σ2 kI, and that K = 2. It is possible to get an inﬁnite likelihood by assigning one of the centers, say μ2, to a single data point, say x1, since then the 1st term makes the following contribution to the likelihood:\n",
      "\n",
      "N (x1|μ2, σ2\n",
      "\n",
      "2I) =\n",
      "\n",
      "1(cid:17)\n",
      "\n",
      "2πσ2 2\n",
      "\n",
      "e0\n",
      "\n",
      "(11.39)\n",
      "\n",
      "356\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "s\n",
      "\n",
      "l i\n",
      "\n",
      "a f\n",
      "\n",
      "0.8\n",
      "\n",
      "M M G\n",
      "\n",
      "0.7\n",
      "\n",
      "r o f\n",
      "\n",
      "0.6\n",
      "\n",
      ") x ( p\n",
      "\n",
      "M E s e m\n",
      "\n",
      "i t\n",
      "\n",
      "f\n",
      "\n",
      "o\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "MLE MAP\n",
      "\n",
      "n o\n",
      "\n",
      "i t c a r f\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "x\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "dimensionality\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.13 (a) Illustration of how singularities can arise in the likelihood function of GMMs. Based on (Bishop 2006a) Figure 9.7. Figure generated by mixGaussSingularity. (b) Illustration of the beneﬁt of MAP estimation vs ML estimation when ﬁtting a Gaussian mixture model. We plot the fraction of times (out of 5 random trials) each method encounters numerical problems vs the dimensionality of the problem, for N = 100 samples. Solid red (upper curve): MLE. Dotted black (lower curve): MAP. Figure generated by mixGaussMLvsMAP.\n",
      "\n",
      "Hence we can drive this term to inﬁnity by letting σ2 → 0, as shown in Figure 11.13(a). We will call this the “collapsing variance problem”.\n",
      "\n",
      "An easy solution to this is to perform MAP estimation. The new auxiliary function is the\n",
      "\n",
      "expected complete data log-likelihood plus the log prior:\n",
      "\n",
      "Q(cid:4)(θ, θold) =\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "rik log πik +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "rik log p(xi|θk)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "+ log p(π) +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "log p(θk)(11.40)\n",
      "\n",
      "i\n",
      "\n",
      "k\n",
      "\n",
      "i\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "Note that the E step remains unchanged, but the M step needs to be modiﬁed, as we now explain.\n",
      "\n",
      "For the prior on the mixture weights, it is natural to use a Dirichlet prior, π ∼ Dir(α), since\n",
      "\n",
      "this is conjugate to the categorical distribution. The MAP estimate is given by\n",
      "\n",
      "πk =\n",
      "\n",
      "N +\n",
      "\n",
      "rk + αk − 1\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "k αk − K\n",
      "\n",
      "(11.41)\n",
      "\n",
      "If we use a uniform prior, αk = 1, this reduces to Equation 11.28.\n",
      "\n",
      "The prior on the parameters of the class conditional densities, p(θk), depends on the form of the class conditional densities. We discuss the case of GMMs below, and leave MAP estimation for mixtures of Bernoullis to Exercise 11.3.\n",
      "\n",
      "For simplicity, let us consider a conjugate prior of the form\n",
      "\n",
      "p(μk, Σk) = NIW(μk, Σk|m0, κ0, ν0, S0)\n",
      "\n",
      "(11.42)\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "357\n",
      "\n",
      "From Section 4.6.3, the MAP estimate is given by\n",
      "\n",
      "ˆμk =\n",
      "\n",
      "rkxk + κ0m0 rk + κ0\n",
      "\n",
      "(11.43)\n",
      "\n",
      "xk (cid:2)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i rikxi rk\n",
      "\n",
      "(11.44)\n",
      "\n",
      "(11.45)\n",
      "\n",
      "ˆΣk =\n",
      "\n",
      "Sk (cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "S0 + Sk + κ0rk κ0+rk ν0 + rk + D + 2 rik(xi − xk)(xi − xk)T\n",
      "\n",
      "(xk − m0)(xk − m0)T\n",
      "\n",
      "(11.46)\n",
      "\n",
      "(11.47)\n",
      "\n",
      "i\n",
      "\n",
      "We now illustrate the beneﬁts of using MAP estimation instead of ML estimation in the context of GMMs. We apply EM to some synthetic data in D dimensions, using either ML or MAP estimation. We count the trial as a “failure” if there are numerical issues involving singular matrices. For each dimensionality, we conduct 5 random trials. The results are illustrated in Figure 11.13(b) using N = 100. We see that as soon as D becomes even moderately large, ML estimation crashes and burns, whereas MAP estimation never encounters numerical problems.\n",
      "\n",
      "When using MAP estimation, we need to specify the hyper-parameters. Here we mention some simple heuristics for setting them (Fraley and Raftery 2007, p163). We can set κ0 = 0, so that the μk are unregularized, since the numerical problems only arise from Σk. In this case, the MAP estimates simplify to ˆμk = xk and ˆΣk = ν0+rk+D+2 , which is not quite so scary-looking.\n",
      "\n",
      "Now we discuss how to set S0. One possibility is to use\n",
      "\n",
      "S0+Sk\n",
      "\n",
      "i=1(xij − xj)2 is the pooled variance for dimension j. where sj = (1/N ) (The reason 1 K1/D term is that the resulting volume of each ellipsoid is then given by |S0| = for the 1 1, . . . , s2 K |diag(s2 D)|.) The parameter ν0 controls how strongly we believe this prior. The weakest prior we can use, while still being proper, is to set ν0 = D + 2, so this is a common choice.\n",
      "\n",
      "S0 =\n",
      "\n",
      "K 1/D diag(s2 (cid:10)N\n",
      "\n",
      "1\n",
      "\n",
      "1, . . . , s2\n",
      "\n",
      "D)\n",
      "\n",
      "(11.48)\n",
      "\n",
      "11.4.3\n",
      "\n",
      "EM for mixture of experts\n",
      "\n",
      "We can ﬁt a mixture of experts model using EM in a straightforward manner. The expected complete data log likelihood is given by\n",
      "\n",
      "Q(θ, θold) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "K(cid:2)\n",
      "\n",
      "rik log[πikN (yi|wT\n",
      "\n",
      "k xi, σ2\n",
      "\n",
      "k)]\n",
      "\n",
      "(11.49)\n",
      "\n",
      "i=1\n",
      "\n",
      "k=1\n",
      "\n",
      "πi,k (cid:2) S(VT xi)k rik ∝ πold\n",
      "\n",
      "ik N (yi|xT\n",
      "\n",
      "i wold\n",
      "\n",
      "k , (σold\n",
      "\n",
      "k )2)\n",
      "\n",
      "(11.50)\n",
      "\n",
      "(11.51)\n",
      "\n",
      "So the E step is the same as in a standard mixture model, except we have to replace πk with πi,k when computing rik.\n",
      "\n",
      "358\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "parameters for model k, the objective has the form (cid:26)\n",
      "\n",
      "In the M step, we need to maximize Q(θ, θold) wrt wk, σ2\n",
      "\n",
      "Q(θk, θold) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "rik\n",
      "\n",
      "− 1 σ2 k\n",
      "\n",
      "(yi − wT\n",
      "\n",
      "k xi)\n",
      "\n",
      "\n",
      "\n",
      "k and V. For the regression\n",
      "\n",
      "(11.52)\n",
      "\n",
      "if rik is We recognize this as a weighted least squares problem, which makes intuitive sense: small, then data point i will be downweighted when estimating model k’s parameters. From Section 8.3.4 we can immediately write down the MLE as\n",
      "\n",
      "wk = (XT RkX)−1XT Rky\n",
      "\n",
      "(11.53)\n",
      "\n",
      "where Rk = diag(r:,k). The MLE for the variance is given by\n",
      "\n",
      "σ2\n",
      "\n",
      "k =\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 rik(yi − wT (cid:10)N\n",
      "\n",
      "i=1 rik\n",
      "\n",
      "k xi)2\n",
      "\n",
      "(11.54)\n",
      "\n",
      "We replace the estimate of the unconditional mixing weights π with the estimate of the gating parameters, V. The objective has the form (cid:2)\n",
      "\n",
      "(cid:6)(V) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "rik log πi,k\n",
      "\n",
      "(11.55)\n",
      "\n",
      "i\n",
      "\n",
      "k\n",
      "\n",
      "We recognize this as equivalent to the log-likelihood for multinomial logistic regression in Equation 8.34, except we replace the “hard” 1-of-C encoding yi with the “soft” 1-of-K encoding ri. Thus we can estimate V by ﬁtting a logistic regression model to soft target labels.\n",
      "\n",
      "11.4.4\n",
      "\n",
      "EM for DGMs with hidden variables\n",
      "\n",
      "We can generalize the ideas behind EM for mixtures of experts to compute the MLE or MAP estimate for an arbitrary DGM. We could use gradient-based methods (Binder et al. 1997), but it is much simpler to use EM (Lauritzen 1995): in the E step, we just estimate the hidden variables, and in the M step, we will compute the MLE using these ﬁlled-in values. We give the details below.\n",
      "\n",
      "For simplicity of presentation, we will assume all CPDs are tabular. Based on Section 10.4.2,\n",
      "\n",
      "let us write each CPT as follows:\n",
      "\n",
      "p(xit|xi,pa(t), θt) =\n",
      "\n",
      "Kpa(t)(cid:27)\n",
      "\n",
      "Kt(cid:27)\n",
      "\n",
      "θI(xit=i,xi,pa(t)=c) tck\n",
      "\n",
      "(11.56)\n",
      "\n",
      "c=1\n",
      "\n",
      "k=1\n",
      "\n",
      "The log-likelihood of the complete data is given by Kpa(t)(cid:2)\n",
      "\n",
      "log p(D|θ) =\n",
      "\n",
      "V(cid:2)\n",
      "\n",
      "Kt(cid:2)\n",
      "\n",
      "Ntck log θtck\n",
      "\n",
      "(11.57)\n",
      "\n",
      "where Ntck = complete data log-likelihood has the form\n",
      "\n",
      "E [log p(D|θ)] =\n",
      "\n",
      "t=1 (cid:10)N\n",
      "\n",
      "i=1 I(xit = i, xi,pa(t) = c) are the empirical counts. Hence the expected\n",
      "\n",
      "c=1\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "N tck log θtck\n",
      "\n",
      "(11.58)\n",
      "\n",
      "t\n",
      "\n",
      "c\n",
      "\n",
      "k\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "359\n",
      "\n",
      "where\n",
      "\n",
      "N tck =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "I(xit = i, xi,pa(t) = c)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(xit = k, xi,pa(t) = c|Di)\n",
      "\n",
      "(11.59)\n",
      "\n",
      "i=1\n",
      "\n",
      "i\n",
      "\n",
      "where Di are all the visible variables in case i.\n",
      "\n",
      "The quantity p(xit, xi,pa(t)|Di, θ) is known as a family marginal, and can be computed using any GM inference algorithm. The N tjk are the expected sufficient statistics, and constitute the output of the E step.\n",
      "\n",
      "Given these ESS, the M step has the simple form\n",
      "\n",
      "k θtjk = 1) This can be proved by adding Lagrange multipliers (to enforce the constraint to the expected complete data log likelihood, and then optimizing each parameter vector θtc separately. We can modify this to perform MAP estimation with a Dirichlet prior by simply adding pseudo counts to the expected counts.\n",
      "\n",
      "ˆθtck =\n",
      "\n",
      "N tck(cid:10)\n",
      "\n",
      "k(cid:2) N tjk(cid:2)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(11.60)\n",
      "\n",
      "11.4.5\n",
      "\n",
      "EM for the Student distribution *\n",
      "\n",
      "One problem with the Gaussian distribution is that it is sensitive to outliers, since the log- probability only decays quadratically with distance from the center. A more robust alternative is the Student t distribution, as discussed in Section ??.\n",
      "\n",
      "Unlike the case of a Gaussian, there is no closed form formula for the MLE of a Student, even if we have no missing data, so we must resort to iterative optimization methods. The easiest one to use is EM, since it automatically enforces the constraints that ν is positive and that Σ is symmetric positive deﬁnite. In addition, the resulting algorithm turns out to have a simple intuitive form, as we see below.\n",
      "\n",
      "At ﬁrst blush, it might not be apparent why EM can be used, since there is no missing data. The key idea is to introduce an “artiﬁcial” hidden or auxiliary variable in order to simplify the algorithm. In particular, we will exploit the fact that a Student distribution can be written as a Gaussian scale mixture:\n",
      "\n",
      "T (xi|μ, Σ, ν) =\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "N (xi|μ, Σ/zi)Ga(zi|\n",
      "\n",
      "ν\n",
      "\n",
      "2\n",
      "\n",
      ",\n",
      "\n",
      "ν\n",
      "\n",
      "2\n",
      "\n",
      ")dzi\n",
      "\n",
      "(11.61)\n",
      "\n",
      "(See Exercise 11.1 for a proof of this in the 1d case.) This can be thought of as an “inﬁnite” mixture of Gaussians, each one with a slightly different covariance matrix.\n",
      "\n",
      "Treating the zi as missing data, we can write the complete data log likelihood as\n",
      "\n",
      "(cid:6)c(θ) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "[log N (xi|μ, Σ/zi) + log Ga(zi|ν/2, ν/2)]\n",
      "\n",
      "(11.62)\n",
      "\n",
      "=\n",
      "\n",
      "i=1 N(cid:2)\n",
      "\n",
      "i=1 ν\n",
      "\n",
      "+\n",
      "\n",
      "2\n",
      "\n",
      "(cid:29)\n",
      "\n",
      "−\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "log(2π) − 1 2\n",
      "\n",
      "(log zi − zi) + (\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "log |Σ| −\n",
      "\n",
      "zi 2 (cid:30)\n",
      "\n",
      "− 1) log zi\n",
      "\n",
      "δi +\n",
      "\n",
      "ν\n",
      "\n",
      "2\n",
      "\n",
      "log\n",
      "\n",
      "ν\n",
      "\n",
      "2\n",
      "\n",
      "− log Γ(\n",
      "\n",
      "ν\n",
      "\n",
      "2\n",
      "\n",
      ")\n",
      "\n",
      "(11.63)\n",
      "\n",
      "(11.64)\n",
      "\n",
      "360\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "where we have deﬁned the Mahalanobis distance to be\n",
      "\n",
      "δi = (xi − μ)T Σ−1(xi − μ)\n",
      "\n",
      "(11.65)\n",
      "\n",
      "We can partition this into two terms, one involving μ and Σ, and the other involving ν. We have, dropping irrelevant constants,\n",
      "\n",
      "LN (μ, Σ) (cid:2) − 1 2\n",
      "\n",
      "(cid:6)c(θ) =L N (μ, Σ) +L G(ν) N log |Σ| − 1 2\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "ziδi\n",
      "\n",
      "(11.66)\n",
      "\n",
      "(11.67)\n",
      "\n",
      "LG(ν) (cid:2) −N log Γ(ν/2) +\n",
      "\n",
      "1 2\n",
      "\n",
      "N ν log(ν/2) +\n",
      "\n",
      "1 2\n",
      "\n",
      "ν\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(log zi − zi)\n",
      "\n",
      "i=1\n",
      "\n",
      "(11.68)\n",
      "\n",
      "11.4.5.1\n",
      "\n",
      "EM with ν known\n",
      "\n",
      "Let us ﬁrst derive the algorithm with ν assumed known, for simplicity. In this case, we can ignore the LG term, so we only need to ﬁgure out how to compute E [zi] wrt the old parameters.\n",
      "\n",
      "From Section 4.6.2.2 we have\n",
      "\n",
      "p(zi|xi, θ) = Ga(zi|\n",
      "\n",
      "ν + D 2\n",
      "\n",
      ",\n",
      "\n",
      "ν + δi 2\n",
      "\n",
      ")\n",
      "\n",
      "(11.69)\n",
      "\n",
      "Now if zi ∼ Ga(a, b), then E [zi] = a/b. Hence the E step at iteration t is\n",
      "\n",
      "z(t) i (cid:2) E\n",
      "\n",
      "+\n",
      "\n",
      "zi|xi, θ(t)\n",
      "\n",
      ",\n",
      "\n",
      "=\n",
      "\n",
      "ν(t) + D ν(t) + δ(t) i\n",
      "\n",
      "(11.70)\n",
      "\n",
      "The M step is obtained by maximizing E [LN (μ, Σ)] to yield\n",
      "\n",
      "ˆμ(t+1) =\n",
      "\n",
      "ˆΣ\n",
      "\n",
      "(t+1)\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "1 N\n",
      "\n",
      "1 N\n",
      "\n",
      "i z(t) i xi (cid:10) i z(t) i (cid:2) z(t) i (xi − ˆμ(t+1))(xi − ˆμ(t+1))T\n",
      "\n",
      "i (cid:18) (cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "z(t) i xixT\n",
      "\n",
      "i −\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "z(t) i\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "ˆμ(t+1)( ˆμ(t+1))T\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(11.71)\n",
      "\n",
      "(11.72)\n",
      "\n",
      "(11.73)\n",
      "\n",
      "These results are quite intuitive: the quantity zi is the precision of measurement i, so if it is small, the corresponding data point is down-weighted when estimating the mean and covariance. This is how the Student achieves robustness to outliers.\n",
      "\n",
      "11.4.5.2\n",
      "\n",
      "EM with ν unknown\n",
      "\n",
      "To compute the MLE for the degrees of freedom, we ﬁrst need to compute the expectation of LG(ν), which involves zi and log zi. Now if zi ∼ Ga(a, b), then one can show that\n",
      "\n",
      "(cid:6)(t) i (cid:2) E\n",
      "\n",
      "+\n",
      "\n",
      "log zi|θ(t)\n",
      "\n",
      ",\n",
      "\n",
      "= Ψ(a) − log b\n",
      "\n",
      "(11.74)\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "361\n",
      "\n",
      "3\n",
      "\n",
      "14 errors using gauss (red=error)\n",
      "\n",
      "2\n",
      "\n",
      "4 errors using student (red=error)\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−5\n",
      "\n",
      "−5\n",
      "\n",
      "−6\n",
      "\n",
      "Bankrupt Solvent\n",
      "\n",
      "−6\n",
      "\n",
      "Bankrupt Solvent\n",
      "\n",
      "−7\n",
      "\n",
      "−5\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "−7\n",
      "\n",
      "−5\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.14 Mixture modeling on the bankruptcy data set. Left: Gaussian class conditional densities. Right: Student class conditional densities. Points that belong to class 1 are shown as triangles, points that belong to class 2 are shown as circles The estimated labels, based on the posterior probability of belonging to each mixture component, are computed. If these are incorrect, the point is colored red, otherwise it is colored blue. (Training data is in black.) Figure generated by mixStudentBankruptcyDemo.\n",
      "\n",
      "where Ψ(x) = d\n",
      "\n",
      "dx log Γ(x) is the digamma function. Hence, from Equation 11.69, we have\n",
      "\n",
      "(cid:6)(t) i\n",
      "\n",
      "ν(t) + D 2 = log(z(t)\n",
      "\n",
      "= Ψ(\n",
      "\n",
      "i ) + Ψ(\n",
      "\n",
      ") − log(\n",
      "\n",
      "ν(t) + D 2\n",
      "\n",
      "ν(t) + δ(t) i 2\n",
      "\n",
      ") − log(\n",
      "\n",
      ")\n",
      "\n",
      "ν(t) + D 2\n",
      "\n",
      ")\n",
      "\n",
      "(11.75)\n",
      "\n",
      "(11.76)\n",
      "\n",
      "Substituting into Equation 11.68, we have\n",
      "\n",
      "E [LG(ν)] = −N log Γ(ν/2) +\n",
      "\n",
      "N ν\n",
      "\n",
      "2\n",
      "\n",
      "log(ν/2) +\n",
      "\n",
      "ν\n",
      "\n",
      "2\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "((cid:6)(t)\n",
      "\n",
      "i − z(t) i )\n",
      "\n",
      "i\n",
      "\n",
      "(11.77)\n",
      "\n",
      "The gradient of this expression is equal to N\n",
      "\n",
      "d dν\n",
      "\n",
      "E [LG(ν)] = −\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "Ψ(ν/2) +\n",
      "\n",
      "2\n",
      "\n",
      "log(ν/2) +\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "+\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "((cid:6)(t)\n",
      "\n",
      "i − z(t) i )\n",
      "\n",
      "i\n",
      "\n",
      "(11.78)\n",
      "\n",
      "This has a unique solution in the interval (0, +∞] which can be found using a 1d constrained\n",
      "\n",
      "optimizer.\n",
      "\n",
      "Performing a gradient-based optimization in the M step, rather than a closed-form update, is an example of what is known as the generalized EM algorithm. One can show that EM will still converge to a local optimum even if we only perform a “partial” improvement to the parameters in the M step.\n",
      "\n",
      "11.4.5.3 Mixtures of Student distributions\n",
      "\n",
      "It is easy to extend the above methods to ﬁt a mixture of Student distributions. See Exercise 11.4 for the details.\n",
      "\n",
      "Let us consider a small example from (Lo 2009, ch3). We have a N = 66, D = 2 data set regarding the bankrupty patterns of certain companies. The ﬁrst feature speciﬁes the ratio\n",
      "\n",
      "362\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "of retained earnings (RE) to total assets, and the second feature speciﬁes the ratio of earnings before interests and taxes (EBIT) to total assets. We ﬁt two models to this data, ignoring the class labels: a mixture of 2 Gaussians, and a mixture of 2 Students. We then use each ﬁtted model to classify the data. We compute the most probable cluster membership and treat this as ˆyi. We then compare ˆyi to the true labels yi and compute an error rate. If this is more than 50%, we permute the latent labels (i.e., we consider cluster 1 to represent class 2 and vice versa), and then recompute the error rate. Points which are misclassiﬁed are then shown in red. The result is shown in Figure 11.14. We see that the Student model made 4 errors, the Gaussian model made 21. This is because the class-conditional densities contain some extreme values, causing the Gaussian to be a poor choice.\n",
      "\n",
      "11.4.6\n",
      "\n",
      "EM for probit regression *\n",
      "\n",
      "In Section 9.4.2, we described the latent variable interpretation of probit regression. Recall that this has the form p(yi = 1|zi) =I( zi > 0), where zi ∼ N (wT xi, 1) is latent. We now show how to ﬁt this model using EM. (Although it is possible to ﬁt probit regression models using gradient based methods, as shown in Section 9.4.1, this EM-based approach has the advantage that it generalized to many other kinds of models, as we will see later on.)\n",
      "\n",
      "The complete data log likelihood has the following form, assuming a N (0, V0) prior on w:\n",
      "\n",
      "(cid:6)(z, w|V0) = log p(y|z) + log N (z|Xw, I) + log N (w|0, V0) (z − Xw)T (z − Xw) − 1 2\n",
      "\n",
      "=\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "log p(yi|zi) − 1 2\n",
      "\n",
      "wT V−1\n",
      "\n",
      "0 w + const (11.80)\n",
      "\n",
      "(11.79)\n",
      "\n",
      "The posterior in the E step is a truncated Gaussian:\n",
      "\n",
      "p(zi|yi, xi, w) =\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "N (zi|wT xi, 1)I(zi > 0) N (zi|wT xi, 1)I(zi < 0)\n",
      "\n",
      "if yi = 1 if yi = 0\n",
      "\n",
      "(11.81)\n",
      "\n",
      "E [zi|yi, xi, w]. Exercise 11.15 asks you to show that the posterior mean is given by\n",
      "\n",
      "In Equation 11.80, we see that w only depends linearly on z, so we just need to compute\n",
      "\n",
      "E [zi|w, xi] =\n",
      "\n",
      "\n",
      "\n",
      "μi + μi − φ(μi)\n",
      "\n",
      "1−Φ(−μi) = μi + Φ(−μi) = μi − φ(μi)\n",
      "\n",
      "φ(μi)\n",
      "\n",
      "1−Φ(μi)\n",
      "\n",
      "φ(μi) Φ(μi)\n",
      "\n",
      "if yi = 1 if yi = 0\n",
      "\n",
      "(11.82)\n",
      "\n",
      "where μi = wT xi.\n",
      "\n",
      "In the M step, we estimate w using ridge regression, where μ = E [z] is the output we are\n",
      "\n",
      "trying to predict. Speciﬁcally, we have\n",
      "\n",
      "ˆw = (V−1\n",
      "\n",
      "0 + XT X)−1XT μ\n",
      "\n",
      "(11.83)\n",
      "\n",
      "The EM algorithm is simple, but can be much slower than direct gradient methods, as illustrated in Figure 11.15. This is because the posterior entropy in the E step is quite high, since we only observe that z is positive or negative, but are given no information from the likelihood about its magnitude. Using a stronger regularizer can help speed convergence, because it constrains the range of plausible z values. In addition, one can use various speedup tricks, such as data augmentation (van Dyk and Meng 2001), but we do not discuss that here.\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "363\n",
      "\n",
      "probit regression with L2 regularizer of 0.100\n",
      "\n",
      "70\n",
      "\n",
      "em minfunc\n",
      "\n",
      "60\n",
      "\n",
      "50\n",
      "\n",
      "L L N d e z\n",
      "\n",
      "i l\n",
      "\n",
      "a n e p\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60 iter\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "120\n",
      "\n",
      "Figure 11.15 by probitRegDemo.\n",
      "\n",
      "Fitting a probit regression model in 2d using a quasi-Newton method or EM. Figure generated\n",
      "\n",
      "11.4.7\n",
      "\n",
      "Theoretical basis for EM *\n",
      "\n",
      "In this section, we show that EM monotonically increases the observed data log likelihood until it reaches a local maximum (or saddle point, although such points are usually unstable). Our derivation will also serve as the basis for various generalizations of EM that we will discuss later.\n",
      "\n",
      "11.4.7.1\n",
      "\n",
      "Expected complete data log likelihood is a lower bound\n",
      "\n",
      "Consider an arbitrary distribution q(zi) over the hidden variables. The observed data log likelihood can be written as follows: (cid:19)\n",
      "\n",
      "(cid:6)(θ) (cid:2)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "log\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "zi\n",
      "\n",
      "p(xi, zi|θ)\n",
      "\n",
      "=\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "log\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "zi\n",
      "\n",
      "q(zi)\n",
      "\n",
      "p(xi, zi|θ) q(zi)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(11.84)\n",
      "\n",
      "Now log(u) is a concave function, so from Jensen’s inequality (Equation 2.113) we have the following lower bound:\n",
      "\n",
      "(cid:6)(θ) ≥\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "zi\n",
      "\n",
      "qi(zi) log\n",
      "\n",
      "p(xi, zi|θ) qi(zi)\n",
      "\n",
      "(11.85)\n",
      "\n",
      "Let us denote this lower bound as follows:\n",
      "\n",
      "Q(θ, q) (cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "Eqi [log p(xi, zi|θ)] + H (qi)\n",
      "\n",
      "(11.86)\n",
      "\n",
      "i\n",
      "\n",
      "where H (qi) is the entropy of qi.\n",
      "\n",
      "The above argument holds for any positive distribution q. Which one should we choose? Intuitively we should pick the q that yields the tightest lower bound. The lower bound is a sum\n",
      "\n",
      "364\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "over i of terms of the following form: (cid:2)\n",
      "\n",
      "L(θ, qi) =\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "zi (cid:2)\n",
      "\n",
      "zi (cid:2)\n",
      "\n",
      "zi\n",
      "\n",
      "qi(zi) log\n",
      "\n",
      "qi(zi) log\n",
      "\n",
      "qi(zi) log\n",
      "\n",
      "p(zi|xi, θ)p(xi|θ) qi(zi)\n",
      "\n",
      "p(zi|xi, θ) qi(zi)\n",
      "\n",
      "p(xi, zi|θ) qi(zi)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "zi\n",
      "\n",
      "qi(zi) log p(xi|θ)\n",
      "\n",
      "(11.87)\n",
      "\n",
      "(11.88)\n",
      "\n",
      "(11.89)\n",
      "\n",
      "= −KL (qi(zi)||p(zi|xi, θ)) + log p(xi|θ)\n",
      "\n",
      "(11.90)\n",
      "\n",
      "The p(xi|θ) term is independent of qi, so we can maximize the lower bound by setting qi(zi) = p(zi|xi, θ). Of course, θ is unknown, so instead we use qt is our estimate of the parameters at iteration t. This is the output of the E step.\n",
      "\n",
      "i (zi) = p(zi|xi, θt), where θt\n",
      "\n",
      "Plugging this in to the lower bound we get\n",
      "\n",
      "Q(θ, qt) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "Eqt\n",
      "\n",
      "i\n",
      "\n",
      "[log p(xi, zi|θ)] + H\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "qt i\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(11.91)\n",
      "\n",
      "We recognize the ﬁrst term as the expected complete data log likelihood. The second term is a constant wrt θ. So the M step becomes\n",
      "\n",
      "θt+1 = arg max\n",
      "\n",
      "θ\n",
      "\n",
      "Q(θ, θt) = arg max\n",
      "\n",
      "θ\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "Eqt\n",
      "\n",
      "i\n",
      "\n",
      "[log p(xi, zi|θ)]\n",
      "\n",
      "(11.92)\n",
      "\n",
      "as usual.\n",
      "\n",
      "zero, so L(θt, qi) = log p(xi|θt), and hence\n",
      "\n",
      "Now comes the punchline. Since we used qt\n",
      "\n",
      "Q(θt, θt) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "log p(xi|θt) = (cid:6)(θt)\n",
      "\n",
      "i (zi) = p(zi|xi, θt), the KL divergence becomes\n",
      "\n",
      "(11.93)\n",
      "\n",
      "i\n",
      "\n",
      "We see that the lower bound is tight after the E step. Since the lower bound “touches” the function, maximizing the lower bound will also “push up” on the function itself. That is, the M step is guaranteed to modify the parameters so as to increase the likelihood of the observed data (unless it is already at a local maximum).\n",
      "\n",
      "This process is sketched in Figure 11.16. The dashed red curve is the original function (the observed data log-likelihood). The solid blue curve is the lower bound, evaluated at θt ; this . We then set θt+1 touches the objective function at θt to the maximum of the lower bound (blue curve), and ﬁt a new bound at that point (dotted green curve). The maximum of this new bound becomes θt+2 , etc. (Compare this to Newton’s method in Figure 8.4(a), which repeatedly ﬁts and then optimizes a quadratic approximation.)\n",
      "\n",
      "11.4.7.2\n",
      "\n",
      "EM monotonically increases the observed data log likelihood\n",
      "\n",
      "We now prove that EM monotonically increases the observed data log likelihood until it reaches a local optimum. We have\n",
      "\n",
      "(cid:6)(θt+1) ≥ Q(θt+1, θt) ≥ Q(θt, θt) = (cid:6)(θt)\n",
      "\n",
      "(11.94)\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "365\n",
      "\n",
      "Q(θ,θ ) t Q(θ,θ l(θ)\n",
      "\n",
      ") t+1\n",
      "\n",
      "θ t\n",
      "\n",
      "θ\n",
      "\n",
      "t+1\n",
      "\n",
      "θ\n",
      "\n",
      "t+2\n",
      "\n",
      "Figure 11.16 Illustration of EM as a bound optimization algorithm. Based on Figure 9.14 of (Bishop 2006a). Figure generated by emLogLikelihoodMax.\n",
      "\n",
      "where the ﬁrst inequality follows since Q(θ, ·) is a lower bound on (cid:6)(θ); the second inequality follows since, by deﬁnition, Q(θt+1, θt) = maxθ Q(θ, θt) ≥ Q(θt, θt); and the ﬁnal equality follows Equation 11.93.\n",
      "\n",
      "As a consequence of this result, if you do not observe monotonic increase of the observed (If you are performing data log likelihood, you must have an error in your math and/or code. MAP estimation, you must add on the log prior term to the objective.) This is a surprisingly powerful debugging tool.\n",
      "\n",
      "11.4.8\n",
      "\n",
      "Online EM\n",
      "\n",
      "When dealing with large or streaming datasets, it is important to be able to learn online, as we discussed in Section 8.5. There are two main approaches to online EM in the literature. The ﬁrst approach, known as incremental EM (Neal and Hinton 1998), optimizes the lower bound Q(θ, q1, . . . , qN ) one qi at a time; however, this requires storing the expected sufficient statistics for each data case. The second approach, known as stepwise EM (Sato and Ishii 2000; Cappe and Mouline 2009; Cappe 2010), is based on stochastic approximation theory, and only requires constant memory use. We explain both approaches in more detail below, following the presentation of (Liang and Klein Liang and Klein).\n",
      "\n",
      "11.4.8.1\n",
      "\n",
      "Batch EM review\n",
      "\n",
      "Before explaining online EM, we review batch EM in a more abstract setting. Let φ(x, z) be a (For example, for a mixture of multinoullis, vector of sufficient statistics for a single data case. this would be the count vector a(j), which is the number of cluster j was used in z, plus the matrix B(j, v), which is of the number of times the hidden state was j and the observed letter was v.) Let si = z p(z|xi, θ)φ(xi, z) be the expected sufficient statistics for case i, and (cid:10)N μ = i=1 si be the sum of the ESS. Given μ, we can derive an ML or MAP estimate of the parameters in the M step; we will denote this operation by θ(μ). (For example, in the case of mixtures of multinoullis, we just need to normalize a and each row of B.) With this notation under our belt, the pseudo code for batch EM is as shown in Algorithm 8.\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "366\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "Algorithm 11.2: Batch EM algorithm 1 initialize μ; 2 repeat 3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "μnew = 0 ; for each example i = 1 :N do (cid:10)\n",
      "\n",
      "si := μnew := μnew + si; ;\n",
      "\n",
      "z p(z|xi, θ(μ))φ(xi, z) ;\n",
      "\n",
      "μ := μnew; 7 8 until converged;\n",
      "\n",
      "11.4.8.2\n",
      "\n",
      "Incremental EM\n",
      "\n",
      "In incremental EM (Neal and Hinton 1998), we keep track of μ as well as the si. When we come to a data case, we swap out the old si and replace it with the new snew , as shown in the code in Algorithm 8. Note that we can exploit the sparsity of snew to speedup the computation of θ, since most components of μ wil not have changed.\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "Algorithm 11.3: Incremental EM algorithm 1 initialize si for i = 1 : N ; 2 μ = 3 repeat 4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "for each example i = 1 :N in a random order do\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i si;\n",
      "\n",
      "snew := i μ := μ + snew si := snew i\n",
      "\n",
      "(cid:10)\n",
      "\n",
      ";\n",
      "\n",
      "z p(z|xi, θ(μ))φ(xi, z) ; i − si;\n",
      "\n",
      "8 until converged;\n",
      "\n",
      "This can be viewed as maximizing the lower bound Q(θ, q1, . . . , qN ) by optimizing q1, then θ, then q2, then θ, etc. As such, this method is guaranteed to monotonically converge to a local maximum of the lower bound and to the log likelihood itself.\n",
      "\n",
      "11.4.8.3\n",
      "\n",
      "Stepwise EM\n",
      "\n",
      "In stepwise EM, whenever we compute a new si, we move μ towards it, as shown in Algorithm 7.2 At iteration k, the stepsize has value ηk, which must satisfy the Robbins-Monro conditions in (Liang and Klein Liang and Klein) use ηk = (2 + k)−κ for Equation 8.82. 0.5 < κ ≤ 1. We can get somewhat better behavior by using a minibatch of size m before It is possible to optimize m and κ to maximize the training set likelihood, by each update.\n",
      "\n",
      "For example,\n",
      "\n",
      "2. A detail: As written the update for μ does not exploit the sparsity of si. We can ﬁx this by storing m = instead of μ, and then using the sparse update m := m + θ(μ) =θ (m)), since scaling the counts by a global constant has no effect.\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "j<k(1−ηj ) j<k(1−ηj ) si. This will not affect the results (i.e.,\n",
      "\n",
      "ηk\n",
      "\n",
      "μ(cid:3)\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "367\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:17)(cid:17)(cid:17)\n",
      "\n",
      "Figure 11.17 uated_optimization.\n",
      "\n",
      "Illustration of deterministic annealing. Based on http://en.wikipedia.org/wiki/Grad\n",
      "\n",
      "trying different values in parallel for an initial trial period; this can signiﬁcantly speed up the algorithm.\n",
      "\n",
      "Algorithm 11.4: Stepwise EM algorithm 1 initialize μ; k = 0 ; 2 repeat 3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "for each example i = 1 :N in a random order do z p(z|xi, θ(μ))φ(xi, z) ;\n",
      "\n",
      "si := μ := (1 − ηk)μ + ηksi; k := k + 1\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "7 until converged;\n",
      "\n",
      "(Liang and Klein Liang and Klein) compare batch EM, incremental EM, and stepwise EM on four different unsupervised language modeling tasks. They found that stepwise EM (using κ ≈ 0.7 and m ≈ 1000) was faster than incremental EM, and both were much faster than batch EM. In terms of accuracy, stepwise EM was usually as good or sometimes even better than batch EM; incremental EM was often worse than either of the other methods.\n",
      "\n",
      "11.4.9\n",
      "\n",
      "Other EM variants *\n",
      "\n",
      "EM is one of the most widely used algorithms in statistics and machine learning. Not surpris- ingly, many variations have been proposed. We brieﬂy mention a few below, some of which we will use in later chapters. See (McLachlan and Krishnan 1997) for more information.\n",
      "\n",
      "Annealed EM In general, EM will only converge to a local maximum. To increase the chance of ﬁnding the global maximum, we can use a variety of methods. One approach is to use a method known as deterministic annealing (Rose 1998). The basic idea is to “smooth” the posterior “landscape” by raising it to a temperature, and then gradually cooling it, all the while slowly tracking the global maximum. See Figure 11.17. for a sketch. (A stochastic version\n",
      "\n",
      "368\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "true log−likelihood\n",
      "\n",
      "true log−likelihood\n",
      "\n",
      "lower bound\n",
      "\n",
      "lower bound\n",
      "\n",
      "training time\n",
      "\n",
      "training time\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.18 Illustration of possible behaviors of variational EM. (a) The lower bound increases at each iteration, and so does the likelihood. In this case, the algorithm is closing the gap between the approximate and true posterior. This can have a regularizing effect. Based on Figure 6 of (Saul et al. 1996). Figure generated by varEMbound.\n",
      "\n",
      "(b) The lower bound increases but the likelihood decreases.\n",
      "\n",
      "of this algorithm is described in Section 24.6.1.) An annealed version of EM is described in (Ueda and Nakano 1998).\n",
      "\n",
      "Variational EM In Section 11.4.7, we showed that the optimal thing to do in the E step is to i (zi) =p( zi|xi, θt). In this case, make qi be the exact posterior over the latent variables, qt the lower bound on the log likelihood will be tight, so the M step will “push up” on the log-likelihood itself. However, sometimes it is computationally intractable to perform exact If we can inference in the E step, but we may be able to perform approximate inference. ensure that the E step is performing inference based on a a lowerbound to the likelihood, then the M step can be seen as monotonically increasing this lower bound (see Figure 11.18). This is called variational EM (Neal and Hinton 1998). See Chapter 21 for some variational inference methods that can be used in the E step.\n",
      "\n",
      "Monte Carlo EM Another approach to handling an intractable E step is to use a Monte Carlo approximation to the expected sufficient statistics. That is, we draw samples from the i ∼ p(zi|xi, θt), and then compute the sufficient statistics for each completed posterior, zs vector, (xi, zs i ), and then average the results. This is called Monte Carlo EM or MCEM (Wei (If we only draw a single sample, it is called stochastic EM (Celeux and and Tanner 1990). Diebolt 1985).) One way to draw samples is to use MCMC (see Chapter 24). However, if we have to wait for MCMC to converge inside each E step, the method becomes very slow. An alternative is to use stochastic approximation, and only perform “brief” sampling in the E step, followed by a partial parameter update. This is called stochastic approximation EM (Delyon et al. 1999) and tends to work better than MCEM. Another alternative is to apply MCMC to infer the parameters as well as the latent variables (a fully Bayesian approach), thus eliminating the distinction between E and M steps. See Chapter 24 for details.\n",
      "\n",
      "Generalized EM Sometimes we can perform the E step exactly, but we cannot perform the M step exactly. However, we can still monotonically increase the log likelihood by performing a “partial” M step, in which we merely increase the expected complete data log likelihood, rather than maximizing it. For example, we might follow a few gradient steps. This is called\n",
      "\n",
      "11.4. The EM algorithm\n",
      "\n",
      "369\n",
      "\n",
      "K=5, D=15, N=5000\n",
      "\n",
      "K=5, D=15, N=5000\n",
      "\n",
      "−38.5\n",
      "\n",
      "−36\n",
      "\n",
      "−39\n",
      "\n",
      "−37\n",
      "\n",
      "−39.5\n",
      "\n",
      "−38\n",
      "\n",
      "−40\n",
      "\n",
      "k\n",
      "\n",
      "i l\n",
      "\n",
      "g o\n",
      "\n",
      "k\n",
      "\n",
      "i l\n",
      "\n",
      "g o\n",
      "\n",
      "−39\n",
      "\n",
      "−40.5\n",
      "\n",
      "−41\n",
      "\n",
      "−41.5\n",
      "\n",
      "EM (1.080) OR(1) (1.358) OR(1.25) (1.141) OR(2) (1.219) OR(5) (1.433)\n",
      "\n",
      "−40\n",
      "\n",
      "−41\n",
      "\n",
      "EM (1.315) OR(1) (1.368) OR(1.25) (1.381) OR(2) (1.540) OR(5) (1.474)\n",
      "\n",
      "−42\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8 10 iterations\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "18\n",
      "\n",
      "−42\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8 10 iterations\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "18\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.19 Illustration of adaptive over-relaxed EM applied to a mixture of 5 Gaussians in 15 dimensions. We show the algorithm applied to two different datasets, randomly sampled from a mixture of 10 Gaussians. We plot the convergence for different update rates η. Using η = 1 gives the same results as regular EM. The actual running time is printed in the legend. Figure generated by mixGaussOverRelaxedEmDemo.\n",
      "\n",
      "the generalized EM or GEM algorithm. ways to generalize EM....)\n",
      "\n",
      "(This is an unfortunate term, since there are many\n",
      "\n",
      "ECM(E) algorithm The ECM algorithm stands for “expectation conditional maximization”, and refers to optimizing the parameters in the M step sequentially, if they turn out to be dependent. The ECME algorithm, which stands for “ECM either” (Liu and Rubin 1995), is a variant of ECM in which we maximize the expected complete data log likelihood (the Q function) as usual, or the observed data log likelihood, during one or more of the conditional maximization steps. The latter can be much faster, since it ignores the results of the E step, and directly optimizes the objective of interest. A standard example of this is when ﬁtting the Student T distribution. For ﬁxed ν, we can update Σ as usual, but then to update ν, we replace the standard update of the form νt+1 = arg maxν Q((μt+1, Σt+1, ν), θt) with νt+1 = arg maxν log p(D|μt+1, Σt+1, ν). See (McLachlan and Krishnan 1997) for more information.\n",
      "\n",
      "Over-relaxed EM Vanilla EM can be quite slow, especially if there is lots of missing data. The adaptive overrelaxed EM algorithm (Salakhutdinov and Roweis 2003) performs an update of the form θt+1 = θt + η(M (θt) − θt), where η is a step-size parameter, and M (θt) is the usual update computed during the M step. Obviously this reduces to standard EM if η = 1, but using larger values of η can result in faster convergence. See Figure 11.19 for an illustration. Unfortunately, using too large a value of η can cause the algorithm to fail to converge.\n",
      "\n",
      "Finally, note that EM is in fact just a special case of a larger class of algorithms known as bound optimization or MM algorithms (MM stands for minorize-maximize). See (Hunter and Lange 2004) for further discussion.\n",
      "\n",
      "370\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "11.5 Model selection for latent variable models\n",
      "\n",
      "When using LVMs, we must specify the number of latent variables, which controls the model in the case of mixture models, we must specify K, the number complexity. of clusters. Choosing these parameters is an example of model selection. We discuss some approaches below.\n",
      "\n",
      "In particuarl,\n",
      "\n",
      "11.5.1 Model selection for probabilistic models\n",
      "\n",
      "The optimal Bayesian approach, discussed in Section 5.3, is to pick the model with the largest marginal likelihood, K ∗ = argmaxk p(D|K).\n",
      "\n",
      "likelihood for LVMs is First, evaluating the marginal quite difficult. In practice, simple approximations, such as BIC, can be used (see e.g., (Fraley and Raftery 2002)). Alternatively, we can use the cross-validated likelihood as a performance measure, although this can be slow, since it requires ﬁtting each model F times, where F is the number of CV folds.\n",
      "\n",
      "There are two problems with this.\n",
      "\n",
      "The second issue is the need to search over a potentially large number of models. The usual approach is to perform exhaustive search over all candidate values of K. However, sometimes we can set the model to its maximal size, and then rely on the power of the Bayesian Occam’s razor to “kill off” unwanted components. An example of this will be shown in Section 21.6.1.6, when we discuss variational Bayes.\n",
      "\n",
      "An alternative approach is to perform stochastic sampling in the space of models. Traditional approaches, such as (Green 1998, 2003; Lunn et al. 2009), are based on reversible jump MCMC, and use birth moves to propose new centers, and death moves to kill off old centers. However, this can be slow and difficult to implement. A simpler approach is to use a Dirichlet process mixture model, which can be ﬁt using Gibbs sampling, but still allows for an unbounded number of mixture components; see Section 25.2 for details.\n",
      "\n",
      "Perhaps surprisingly, these sampling-based methods can be faster than the simple approach of evaluating the quality of each K separately. The reason is that ﬁtting the model for each K is often slow. By contrast, the sampling methods can often quickly determine that a certain value of K is poor, and thus they need not waste time in that part of the posterior.\n",
      "\n",
      "11.5.2 Model selection for non-probabilistic methods\n",
      "\n",
      "What if we are not using a probabilistic model? For example, how do we choose K for the K- means algorithm? Since this does not correspond to a probability model, there is no likelihood, so none of the methods described above can be used.\n",
      "\n",
      "struction error of a data set D, using model complexity K, as follows:\n",
      "\n",
      "An obvious proxy for the likelihood is the reconstruction error. Deﬁne the squared recon-\n",
      "\n",
      "E(D, K) =\n",
      "\n",
      "1 |D|\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i∈D\n",
      "\n",
      "||xi − ˆxi||2\n",
      "\n",
      "(11.95)\n",
      "\n",
      "In the case of K-means, the reconstruction is given by ˆxi = μzi , where zi = argmink ||xi − μk||2\n",
      "\n",
      "2, as explained in Section 11.4.2.6.\n",
      "\n",
      "Figure 11.20(a) plots the reconstruction error on the test set for K-means. We notice that the error decreases with increasing model complexity! The reason for this behavior is as follows:\n",
      "\n",
      "11.5. Model selection for latent variable models\n",
      "\n",
      "371\n",
      "\n",
      "0.25\n",
      "\n",
      "MSE on test vs K for K−means\n",
      "\n",
      "1245\n",
      "\n",
      "NLL on test set vs K for GMM\n",
      "\n",
      "1240\n",
      "\n",
      "0.2\n",
      "\n",
      "1235\n",
      "\n",
      "1230\n",
      "\n",
      "0.15\n",
      "\n",
      "1225\n",
      "\n",
      "1220\n",
      "\n",
      "0.1\n",
      "\n",
      "1215\n",
      "\n",
      "1210\n",
      "\n",
      "0.05\n",
      "\n",
      "1205\n",
      "\n",
      "1200\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "1195\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.20 Test set performance vs K for data generated from a mixture of 3 Gaussians in 1d (data is shown in Figure 11.21(a)). (a) MSE on test set for K-means. (b) Negative log likelihood on test set for GMM. Figure generated by kmeansModelSel1d.\n",
      "\n",
      "60\n",
      "\n",
      "Xtrain\n",
      "\n",
      "1\n",
      "\n",
      "K=2, mse=0.2023\n",
      "\n",
      "1\n",
      "\n",
      "K=3, mse=0.0818\n",
      "\n",
      "1\n",
      "\n",
      "K=4, mse=0.0562\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "50\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "40\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0 −1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "30\n",
      "\n",
      "K=5, mse=0.0368\n",
      "\n",
      "K=6, mse=0.0275\n",
      "\n",
      "K=10, mse=0.0111\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "20\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "10\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0 −3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "0.5\n",
      "\n",
      "K=2, nll=1244.7882\n",
      "\n",
      "0.5\n",
      "\n",
      "K=3, nll=1198.9738\n",
      "\n",
      "0.5\n",
      "\n",
      "K=4, nll=1196.9937\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "K=5, nll=1202.5869\n",
      "\n",
      "K=6, nll=1199.5574\n",
      "\n",
      "K=10, nll=1203.2931\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0 −2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 11.21 (Test data looks essentially the same.) (c) GMM density model estimated by EM for for the same values of K. kmeansModelSel1d.\n",
      "\n",
      "Synthetic data generated from a mixture of 3 Gaussians in 1d. (a) Histogram of training data. (b) Centroids estimated by K-means for K ∈ {2, 3, 4, 5, 6, 10}. Figure generated by\n",
      "\n",
      "372\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "when we add more and more centroids to K-means, we can “tile” the space more densely, as shown in Figure 11.21(b). Hence any given test vector is more likely to ﬁnd a close prototype to accurately represent it as K increases, thus decreasing reconstruction error. However, if we use a probabilistic model, such as the GMM, and plot the negative log-likelihood, we get the usual U-shaped curve on the test set, as shown in Figure 11.20(b).\n",
      "\n",
      "In supervised learning, we can always use cross validation to select between non-probabilistic models of different complexity, but this is not the case with unsupervised learning. Although this is not a novel observation (e.g., it is mentioned in passing in (Hastie et al. 2009, p519), one of the standard references in this ﬁeld), it is perhaps not as widely appreciated as it should be. In fact, it is one of the more compelling arguments in favor of probabilistic models.\n",
      "\n",
      "Given that cross validation doesn’t work, and supposing one is unwilling to use probabilistic models (for some bizarre reason...), how can one choose K? The most common approach is to plot the reconstruction error on the training set versus K, and to try to identify a knee or kink in the curve. The idea is that for K < K ∗, where K ∗ is the “true” number of clusters, the rate of decrease in the error function will be high, since we are splitting apart things that should not be grouped together. However, for K > K ∗, we are splitting apart “natural” clusters, which does not reduce the error by as much.\n",
      "\n",
      "This kink-ﬁnding process can be automated by use of the gap statistic (Tibshirani et al. 2001). Nevertheless, identifying such kinks can be hard, as shown in Figure 11.20(a), since the loss function usually drops off gradually. A different approach to “kink ﬁnding” is described in Section 12.3.2.1.\n",
      "\n",
      "11.6\n",
      "\n",
      "Fitting models with missing data\n",
      "\n",
      "Suppose we want to ﬁt a joint density model by maximum likelihood, but we have “holes” in our data matrix, due to missing data (usually represented by NaNs). More formally, let Oij = 1 if component j of data case i is observed, and let Oij = 0 otherwise. Let Xv = {xij : Oij = 1} be the visible data, and Xh = {xij : Oij = 0} be the missing or hidden data. Our goal is to compute\n",
      "\n",
      "ˆθ = argmax\n",
      "\n",
      "θ\n",
      "\n",
      "p(Xv|θ, O)\n",
      "\n",
      "(11.96)\n",
      "\n",
      "Under the missing at random assumption (see Section 8.6.2), we have\n",
      "\n",
      "p(Xv|θ, O) =\n",
      "\n",
      "N(cid:27)\n",
      "\n",
      "p(xiv|θ)\n",
      "\n",
      "(11.97)\n",
      "\n",
      "i=1\n",
      "\n",
      "where xiv is a vector created from row i and the columns indexed by the set {j : Oij = 1}. Hence the log-likelihood has the form\n",
      "\n",
      "log p(Xv|θ) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "log p(xiv|θ)\n",
      "\n",
      "(11.98)\n",
      "\n",
      "i\n",
      "\n",
      "where\n",
      "\n",
      "p(xiv|θ) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(xiv, xih|θ)\n",
      "\n",
      "(11.99)\n",
      "\n",
      "xih\n",
      "\n",
      "11.6. Fitting models with missing data\n",
      "\n",
      "373\n",
      "\n",
      "and xih is the vector of hidden variables for case i (assumed discrete for notational simplicity). Substituting in, we get (cid:2)\n",
      "\n",
      "log p(Xv|θ) =\n",
      "\n",
      "log\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "p(xiv, xih|θ)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(11.100)\n",
      "\n",
      "i\n",
      "\n",
      "xih\n",
      "\n",
      "Unfortunately, this objective is hard to maximize. since we cannot push the log inside the sum. However, we can use the EM algorithm to compute a local optimum. We give an example of this below.\n",
      "\n",
      "11.6.1\n",
      "\n",
      "EM for the MLE of an MVN with missing data\n",
      "\n",
      "Suppose we want to ﬁt an MVN by maximum likelihood, but we have missing data. We can use EM to ﬁnd a local maximum of the objective, as we explain below.\n",
      "\n",
      "11.6.1.1\n",
      "\n",
      "Getting started\n",
      "\n",
      "To get the algorithm started, we can compute the MLE based on those rows of the data ma- trix that are fully observed. If there are no such rows, we can use some ad-hoc imputation procedures, and then compute an initial MLE.\n",
      "\n",
      "11.6.1.2\n",
      "\n",
      "E step Once we have θt−1 follows:\n",
      "\n",
      "Q(θ, θt−1) =E\n",
      "\n",
      ", we can compute the expected complete data log likelihood at iteration t as\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "log N (xi|μ, Σ)|D, θt−1\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(11.101)\n",
      "\n",
      "= −\n",
      "\n",
      "= −\n",
      "\n",
      "i=1\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log |2πΣ| − 1 2 log |2πΣ| − 1 2\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(xi − μ)T Σ−1(xi − μ)\n",
      "\n",
      "i\n",
      "\n",
      "tr(Σ−1\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(xi − μ)(xi − μ)T\n",
      "\n",
      "i\n",
      "\n",
      "(11.102)\n",
      "\n",
      "(11.103)\n",
      "\n",
      "= −\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log |Σ| −\n",
      "\n",
      "N D\n",
      "\n",
      "2\n",
      "\n",
      "log(2π) − 1 2\n",
      "\n",
      "tr(Σ−1E [S(μ)])\n",
      "\n",
      "(11.104)\n",
      "\n",
      "where\n",
      "\n",
      "E [S(μ)] (cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xixT i\n",
      "\n",
      "+ μμT − 2μE [xi]\n",
      "\n",
      "T\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(11.105)\n",
      "\n",
      "i\n",
      "\n",
      "(We drop the conditioning of the expectation on D and θt−1 i E to compute\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i E [xi] and\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xixT i\n",
      "\n",
      "; these are the expected sufficient statistics.\n",
      "\n",
      "for brevity.) We see that we need\n",
      "\n",
      "i, where components v are observed and components h are unobserved. We have\n",
      "\n",
      "To compute these quantities, we use the results from Section 4.3.1. Speciﬁcally, consider case\n",
      "\n",
      "xih|xiv, θ ∼ N (mi, Vi)\n",
      "\n",
      "(11.106)\n",
      "\n",
      "mi (cid:2) μh + ΣhvΣ−1 Vi (cid:2) Σhh − ΣhvΣ−1\n",
      "\n",
      "vv (xiv − μv) vv Σvh\n",
      "\n",
      "(11.107)\n",
      "\n",
      "(11.108)\n",
      "\n",
      "374\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "Hence the expected sufficient statistics are E [xi] = (E [xih] ; xiv) = (mi; xiv)\n",
      "\n",
      "(11.109)\n",
      "\n",
      "where we have assumed (without loss of generality) that the unobserved variables come before the observed variables in the node ordering.\n",
      "\n",
      "To compute E\n",
      "\n",
      "E\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xihxT ih\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xixT i\n",
      "\n",
      "= E\n",
      "\n",
      "= E [xih] E [xih]\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xixT i\n",
      "\n",
      ", we use the result that cov [x] = E (cid:29)(cid:8) (cid:4) xihxT E ih xivE [xih]\n",
      "\n",
      "xih xiv\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "ih xT xT iv\n",
      "\n",
      "T\n",
      "\n",
      "+ Vi\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "T\n",
      "\n",
      "xxT\n",
      "\n",
      "E [xih] xT iv xivxT iv\n",
      "\n",
      "− E [x] E (cid:9)\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xT\n",
      "\n",
      ". Hence\n",
      "\n",
      "(11.110)\n",
      "\n",
      "(11.111)\n",
      "\n",
      "11.6.1.3 M step\n",
      "\n",
      "By solving ∇Q(θ, θ(t−1)) =0, we can show that the M step is equivalent to plugging these ESS into the usual MLE equations to get\n",
      "\n",
      "Σt =\n",
      "\n",
      "μt =\n",
      "\n",
      "1 N\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i (cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "E [xi]\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xixT i\n",
      "\n",
      "− μt(μt)T\n",
      "\n",
      "(11.112)\n",
      "\n",
      "(11.113)\n",
      "\n",
      "Thus we see that EM is not equivalent to simply replacing variables by their expectations and applying the standard MLE formula; that would ignore the posterior variance and would result Instead we must compute the expectation of the sufficient statistics, in an incorrect estimate. and plug that into the usual equation for the MLE. We can easily modify the algorithm to perform MAP estimation, by plugging in the ESS into the equation for the MAP estimate. For an implementation, see gaussMissingFitEm.\n",
      "\n",
      "11.6.1.4\n",
      "\n",
      "Example\n",
      "\n",
      "As an example of this procedure in action, let us reconsider the imputation problem from Section 4.3.2.3, which had N = 100 10-dimensional data cases, with 50% missing data. Let us ﬁt the parameters using EM. Call the resulting parameters ˆθ. We can use our model for predictions by computing E . Figure 11.22(a-b) indicates that the results obtained using the learned parameters are almost as good as with the true parameters. Not surprisingly, performance improves with more data, or as the fraction of missing data is reduced.\n",
      "\n",
      "+\n",
      "\n",
      "xih|xiv, ˆθ\n",
      "\n",
      ",\n",
      "\n",
      "11.6.1.5\n",
      "\n",
      "Extension to the GMM case\n",
      "\n",
      "It is straightforward to ﬁt a mixture of Gaussians in the presence of partially observed data vectors xi. We leave the details as an exercise.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 11.1 Student T as inﬁnite mixture of Gaussians Derive Equation 11.61. For simplicity, assume a one-dimensional distribution.\n",
      "\n",
      "11.6. Fitting models with missing data\n",
      "\n",
      "375\n",
      "\n",
      "imputation with true params\n",
      "\n",
      "imputation with em\n",
      "\n",
      "R2 = 0.260\n",
      "\n",
      "R2 = 0.685\n",
      "\n",
      "R2 = 0.220\n",
      "\n",
      "R2 = 0.609\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "d e t u p m\n",
      "\n",
      "i\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "d e t u p m\n",
      "\n",
      "i\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "d e t u p m\n",
      "\n",
      "i\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "d e t u p m\n",
      "\n",
      "i\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "−10\n",
      "\n",
      "−10\n",
      "\n",
      "0 truth R2 = 0.399\n",
      "\n",
      "10\n",
      "\n",
      "−10\n",
      "\n",
      "−20\n",
      "\n",
      "0 truth R2 = 0.531\n",
      "\n",
      "20\n",
      "\n",
      "−10\n",
      "\n",
      "−10\n",
      "\n",
      "0 truth R2 = 0.113\n",
      "\n",
      "10\n",
      "\n",
      "−10\n",
      "\n",
      "−20\n",
      "\n",
      "0 truth R2 = 0.532\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "d e t u p m\n",
      "\n",
      "i\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "d e t u p m\n",
      "\n",
      "i\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "d e t u p m\n",
      "\n",
      "i\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "d e t u p m\n",
      "\n",
      "i\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "−10\n",
      "\n",
      "−10\n",
      "\n",
      "0 truth\n",
      "\n",
      "10\n",
      "\n",
      "−10\n",
      "\n",
      "−10\n",
      "\n",
      "0 truth\n",
      "\n",
      "10\n",
      "\n",
      "−10\n",
      "\n",
      "−10\n",
      "\n",
      "0 truth\n",
      "\n",
      "10\n",
      "\n",
      "−10\n",
      "\n",
      "−10\n",
      "\n",
      "0 truth\n",
      "\n",
      "10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 11.22 Illustration of data imputation. ing true parameters. gaussImputationDemo.\n",
      "\n",
      "(a) Scatter plot of true values vs imputed values us- (b) Same as (b), but using parameters estimated with EM. Figure generated by\n",
      "\n",
      "Exercise 11.2 EM for mixtures of Gaussians Show that the M step for ML estimation of a mixture of Gaussians is given by\n",
      "\n",
      "Σk =\n",
      "\n",
      "μk =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i rikxi rk i rik(xi − μk)(xi − μk)T rk\n",
      "\n",
      "=\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i rikxixT i − rkμkμT rk\n",
      "\n",
      "k\n",
      "\n",
      "(11.114)\n",
      "\n",
      "(11.115)\n",
      "\n",
      "Exercise 11.3 EM for mixtures of Bernoullis •\n",
      "\n",
      "Show that the M step for ML estimation of a mixture of Bernoullis is given by\n",
      "\n",
      "μkj =\n",
      "\n",
      "(cid:2) i rikxij(cid:2) i rik\n",
      "\n",
      "(11.116)\n",
      "\n",
      "\n",
      "\n",
      "Show that the M step for MAP estimation of a mixture of Bernoullis with a β(α, β) prior is given by\n",
      "\n",
      "μkj =\n",
      "\n",
      "(cid:2) ( (cid:2) (\n",
      "\n",
      "i rikxij) +α − 1 i rik) +α + β − 2\n",
      "\n",
      "(11.117)\n",
      "\n",
      "Exercise 11.4 EM for mixture of Student distributions Derive the EM algorithm for ML estimation of a mixture of multivariate Student T distributions.\n",
      "\n",
      "Exercise 11.5 Gradient descent for ﬁtting GMM Consider the Gaussian mixture model\n",
      "\n",
      "p(x|θ) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "πkN (x|μk, Σk)\n",
      "\n",
      "(11.118)\n",
      "\n",
      "k\n",
      "\n",
      "Deﬁne the log likelihood as\n",
      "\n",
      "(cid:7)(θ) =\n",
      "\n",
      "N(cid:12)\n",
      "\n",
      "log p(xn|θ)\n",
      "\n",
      "(11.119)\n",
      "\n",
      "n=1\n",
      "\n",
      "376\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "p\n",
      "\n",
      "q\n",
      "\n",
      "Jn\n",
      "\n",
      "Kn\n",
      "\n",
      "xn\n",
      "\n",
      "N\n",
      "\n",
      "μj\n",
      "\n",
      "σk\n",
      "\n",
      "m\n",
      "\n",
      "l\n",
      "\n",
      "Figure 11.23 A mixture of Gaussians with two discrete latent indicators. Jn speciﬁes which mean to use, and Kn speciﬁes which variance to use.\n",
      "\n",
      "Deﬁne the posterior responsibility that cluster k has for datapoint n as follows:\n",
      "\n",
      "rnk (cid:2) p(zn = k|xn, θ) =\n",
      "\n",
      "(cid:2)K\n",
      "\n",
      "πkN (xn|μk, Σk) k(cid:2)=1 πk(cid:2) N (xn|μk(cid:2) , Σk(cid:2) )\n",
      "\n",
      "(11.120)\n",
      "\n",
      "a. Show that the gradient of the log-likelihood wrt μk is\n",
      "\n",
      "d dμk\n",
      "\n",
      "(cid:7)(θ) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "n\n",
      "\n",
      "rnkΣ−1\n",
      "\n",
      "k (xn − μk)\n",
      "\n",
      "(11.121)\n",
      "\n",
      "b. Derive the gradient of the log-likelihood wrt πk. (For now, ignore any constraints on πk.) c. One way to handle the constraint that\n",
      "\n",
      "(cid:2)K\n",
      "\n",
      "k=1 πk = 1 is to reparameterize using the softmax function:\n",
      "\n",
      "πk (cid:2)\n",
      "\n",
      "(cid:2)K\n",
      "\n",
      "ewk k(cid:2)=1 ewk(cid:2)\n",
      "\n",
      "(11.122)\n",
      "\n",
      "Here wk ∈ R are unconstrained parameters. Show that\n",
      "\n",
      "d dwk\n",
      "\n",
      "(cid:7)(θ) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "n\n",
      "\n",
      "rnk − πk\n",
      "\n",
      "(11.123)\n",
      "\n",
      "(There may be a constant factor missing in the above expression...) Hint: use the chain rule and the fact that dπj dwk\n",
      "\n",
      "=\n",
      "\n",
      "πj(1 − πj) −πjπk\n",
      "\n",
      "if j = k if j (cid:8)= k\n",
      "\n",
      "(11.124)\n",
      "\n",
      "which follows from Exercise 8.4(1).\n",
      "\n",
      "d. Derive the gradient of the log-likelihood wrt Σk. (For now, ignore any constraints on Σk.) e. One way to handle the constraint that Σk be a symmetric positive deﬁnite matrix is to reparame- k R, where R is an upper-triangular, but otherwise\n",
      "\n",
      "terize using a Cholesky decomposition, Σk = RT unconstrained matrix. Derive the gradient of the log-likelihood wrt Rk.\n",
      "\n",
      "Exercise 11.6 EM for a ﬁnite scale mixture of Gaussians (Source: Jaakkola..) Consider the graphical model in Figure 11.23 which deﬁnes the following: (cid:19)\n",
      "\n",
      "p(xn|θ) =\n",
      "\n",
      "m(cid:12)\n",
      "\n",
      "pj\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "l(cid:12)\n",
      "\n",
      "qkN (xn|μj, σ2 k)\n",
      "\n",
      "(11.125)\n",
      "\n",
      "j=1\n",
      "\n",
      "k=1\n",
      "\n",
      "11.6. Fitting models with missing data\n",
      "\n",
      "377\n",
      "\n",
      "where θ = {p1, . . . , pm, μ1, . . . , μm, q1, . . . , ql, σ2 l } are all the parameters. Here pj (cid:2) P (Jn = j) and qk (cid:2) P (Kn = k) are the equivalent of mixture weights. We can think of this as a mixture of m non-Gaussian components, where each component distribution is a scale mixture, p(x|j; θ) = (cid:2)l\n",
      "\n",
      "k=1 qkN (x; μj, σ2\n",
      "\n",
      "k), combining Gaussians with different variances (scales).\n",
      "\n",
      "1, . . . , σ2\n",
      "\n",
      "We will now derive a generalized EM algorithm for this model. partial update in the M step, rather than ﬁnding the exact maximum.)\n",
      "\n",
      "(Recall that in generalized EM, we do a\n",
      "\n",
      "a. Derive an expression for the responsibilities, P (Jn = j, Kn = k|xn, θ), needed for the E step. b. Write out a full expression for the expected complete log-likelihood\n",
      "\n",
      "Q(θnew, θold) = Eθold\n",
      "\n",
      "N(cid:12)\n",
      "\n",
      "log P (Jn, Kn, xn|θnew)\n",
      "\n",
      "(11.126)\n",
      "\n",
      "n=1\n",
      "\n",
      "c. Solving the M-step would require us to jointly optimize the means μ1, . . . , μm and the variances l . It will turn out to be simpler to ﬁrst solve for the μj’s given ﬁxed σ2 j ’s, and subsequently j ’s given the new values of μj’s. For brevity, we will just do the ﬁrst part. Derive an\n",
      "\n",
      "σ2 1, . . . , σ2 solve for σ2 expression for the maximizing μj’s given ﬁxed σ2\n",
      "\n",
      "1:l, i.e., solve\n",
      "\n",
      "∂μnew = 0.\n",
      "\n",
      "∂Q\n",
      "\n",
      "Exercise 11.7 Manual calculation of the M step for a GMM (Source: de Freitas.) In this question we consider clustering 1D data with a mixture of 2 Gaussians using the EM algorithm. You are given the 1-D data points x = [1 20]. Suppose the output of the E step is the following matrix:\n",
      "\n",
      "10\n",
      "\n",
      "R =\n",
      "\n",
      "⎡\n",
      "\n",
      "⎣\n",
      "\n",
      "1 0.4 0\n",
      "\n",
      "0 0.6 1\n",
      "\n",
      "⎤\n",
      "\n",
      "⎦\n",
      "\n",
      "(11.127)\n",
      "\n",
      "where entry ri,c is the probability of obervation xi belonging to cluster c (the responsibility of cluster c for data point i). You just have to compute the M step. You may state the equations for maximum likelihood estimates of these quantities (which you should know) without proof; you just have to apply the equations to this data set. You may leave your answer in fractional form. Show your work.\n",
      "\n",
      "a. Write down the likelihood function you are trying to optimize. b. After performing the M step for the mixing weights π1, π2, what are the new values? c. After performing the M step for the means μ1 and μ2, what are the new values?\n",
      "\n",
      "Exercise 11.8 Moments of a mixture of Gaussians Consider a mixture of K Gaussians\n",
      "\n",
      "p(x) =\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "πkN (x|μk, Σk)\n",
      "\n",
      "(11.128)\n",
      "\n",
      "k=1\n",
      "\n",
      "a. Show that\n",
      "\n",
      "E [x] =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "πkμk\n",
      "\n",
      "(11.129)\n",
      "\n",
      "k\n",
      "\n",
      "378\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2 0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "18\n",
      "\n",
      "Figure 11.24\n",
      "\n",
      "Some data points in 2d. Circles represent the initial guesses for m1 and m2.\n",
      "\n",
      "b. Show that\n",
      "\n",
      "cov [x] =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "πk[Σk + μkμT\n",
      "\n",
      "k ] − E [x] E [x]T\n",
      "\n",
      "(11.130)\n",
      "\n",
      "k\n",
      "\n",
      "Hint: use the fact that cov [x] = E\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "xxT\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "− E [x] E [x]T\n",
      "\n",
      ".\n",
      "\n",
      "Exercise 11.9 K-means clustering by hand (Source: Jaakkola.) In Figure 11.24, we show some data points which lie on the integer grid. (Note that the x-axis has been compressed; distances should be measured using the actual grid coordinates.) Suppose we apply the K- means algorithm to this data, using K = 2 and with the centers initialized at the two circled data points. Draw the ﬁnal clusters obtained after K-means converges (show the approximate location of the new centers and group together all the points assigned to each center). Hint: think about shortest Euclidean distance.\n",
      "\n",
      "Exercise 11.10 Deriving the K-means cost function Show that\n",
      "\n",
      "JW (z) =\n",
      "\n",
      "1 2\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "i:zi=k\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(xi − xi(cid:2) )2 =\n",
      "\n",
      "i(cid:2):zi(cid:2) =k\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "k=1\n",
      "\n",
      "nk\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(xi − xk)2\n",
      "\n",
      "i:zi=k\n",
      "\n",
      "(11.131)\n",
      "\n",
      "Hint: note that, for any μ,\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(xi − μ)2 =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "[(xi − x) − (μ − x)]2\n",
      "\n",
      "(11.132)\n",
      "\n",
      "i\n",
      "\n",
      "=\n",
      "\n",
      "i (cid:12)\n",
      "\n",
      "(xi − x)2 +\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(x − μ)2 − 2\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(xi − x)(μ − x)\n",
      "\n",
      "(11.133)\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "= ns2 + n(x − μ)2\n",
      "\n",
      "(11.134)\n",
      "\n",
      "where s2 = 1 n\n",
      "\n",
      "(cid:2)n\n",
      "\n",
      "i=1(xi − x)2, since\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(xi − x)(μ − x) = (μ − x)\n",
      "\n",
      "%\n",
      "\n",
      "(\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "&\n",
      "\n",
      "xi) − nx\n",
      "\n",
      "= (μ − x)(nx − nx) = 0\n",
      "\n",
      "(11.135)\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "Exercise 11.11 Visible mixtures of Gaussians are in the exponential family Show that the joint distribution p(x, z|θ) for a 1d GMM can be represented in exponential family form.\n",
      "\n",
      "11.6. Fitting models with missing data\n",
      "\n",
      "379\n",
      "\n",
      "regression with censored data; red x = censored, green * = predicted\n",
      "\n",
      "4.6\n",
      "\n",
      "4.4\n",
      "\n",
      "EM OLS\n",
      "\n",
      "4.2\n",
      "\n",
      "4\n",
      "\n",
      "e m\n",
      "\n",
      "i t\n",
      "\n",
      "3.8\n",
      "\n",
      "l\n",
      "\n",
      "a v v r u s\n",
      "\n",
      "i\n",
      "\n",
      "3.6\n",
      "\n",
      "3.4\n",
      "\n",
      "3.2\n",
      "\n",
      "3\n",
      "\n",
      "2.8\n",
      "\n",
      "2.6\n",
      "\n",
      "2\n",
      "\n",
      "2.05\n",
      "\n",
      "2.1\n",
      "\n",
      "2.15\n",
      "\n",
      "2.2 inverse temperature\n",
      "\n",
      "2.25\n",
      "\n",
      "2.3\n",
      "\n",
      "2.35\n",
      "\n",
      "2.4\n",
      "\n",
      "Figure 11.25 Example of censored linear regression. Black circles are observed training points, red crosses are observed but censored training points. Green stars are predicted values of the censored training points. We also show the lines ﬁt by least squares (ignoring censoring) and by EM. Based on Figure 5.6 of (Tanner 1996). Figure generated by linregCensoredSchmeeHahnDemo, written by Hannes Bretschneider.\n",
      "\n",
      "Exercise 11.12 EM for robust linear regression with a Student t likelihood Consider a model of the form\n",
      "\n",
      "p(yi|xi, w, σ2, ν) = T (yi|wT xi, σ2, ν)\n",
      "\n",
      "(11.136)\n",
      "\n",
      "Derive an EM algorithm to compute the MLE for w. You may assume ν and σ2 are ﬁxed, for simplicity. Hint: see Section 11.4.5.\n",
      "\n",
      "Exercise 11.13 EM for EB estimation of Gaussian shrinkage model Extend the results of Section 5.6.2.2 to the case where the σ2 j are not equal (but are known). Hint: treat the θj as hidden variables, and then to integrate them out in the E step, and maximize η = (μ, τ 2) in the M step.\n",
      "\n",
      "Exercise 11.14 EM for censored linear regression Censored regression refers to the case where one knows the outcome is at least (or at most) a certain value, but the precise value is unknown. This arises in many different settings. For example, suppose one is trying to learn a model that can predict how long a program will take to run, for different settings of its parameters. One may abort certain runs if they seem to be taking too long; the resulting run times are said to be right censored. For such runs, all we know is that yi ≥ ci, where ci is the censoring time, that is, yi = min(zi, ci), where zi is the true running time and yi is the observed running time. We can also deﬁne left censored and interval censored models.3 Derive an EM algorithm for ﬁtting a linear regression model to right-censored data. Hint: use the results from Exercise 11.15. See Figure 11.25 for an example, based on the data from (Schmee and Hahn 1979). We notice that the EM line is tilted upwards more, since the model takes into account the fact that the truncated values are actually higher than the observed values.\n",
      "\n",
      "3. There is a closely related model in econometrics called the Tobit model, in which yi = max(zi, 0), so we only get to observe positive outcomes. An example of this is when zi represents “desired investment”, and yi is actual investment. Probit regression (Section 9.4) is another example.\n",
      "\n",
      "380\n",
      "\n",
      "Chapter 11. Mixture models and the EM algorithm\n",
      "\n",
      "Exercise 11.15 Posterior mean and variance of a truncated Gaussian Let zi = μi + σ(cid:20)i, where (cid:20)i ∼ N (0, 1). Sometimes, such as in probit regression or censored regression, we do not observe zi, but we observe the fact that it is above some threshold, namely we observe the event E = I(zi ≥ ci) =I ((cid:20)i ≥ ci−μi ). (See Exercise 11.14 for details on censored regression, and Section 11.4.6 for probit regression.) Show that\n",
      "\n",
      "σ\n",
      "\n",
      "E [zi|zi ≥ ci] =μ i + σH\n",
      "\n",
      "(cid:16) ci − μi σ\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(11.137)\n",
      "\n",
      "and\n",
      "\n",
      "E\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "z2 i |zi ≥ ci\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "= μ2\n",
      "\n",
      "i + σ2 + σ(ci + μi)H\n",
      "\n",
      "(cid:16) ci − μi σ\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(11.138)\n",
      "\n",
      "where we have deﬁned\n",
      "\n",
      "H(u) (cid:2)\n",
      "\n",
      "φ(u) 1 − Φ(u)\n",
      "\n",
      "(11.139)\n",
      "\n",
      "and where φ(u) is the pdf of a standard Gaussian, and Φ(u) is its cdf. Hint 1: we have p((cid:20)i|E) = p((cid:7)i,E) p(E) , whereE is some event of interest.\n",
      "\n",
      "Hint 2: It can be shown that\n",
      "\n",
      "d dw\n",
      "\n",
      "N (w|0, 1) = −wN (w|0, 1)\n",
      "\n",
      "(11.140)\n",
      "\n",
      "and hence (cid:5) c\n",
      "\n",
      "wN (w|0, 1) = N (b|0, 1) − N (c|0, 1)\n",
      "\n",
      "(11.141)\n",
      "\n",
      "b\n",
      "\n",
      "12 Latent linear models\n",
      "\n",
      "12.1\n",
      "\n",
      "Factor analysis\n",
      "\n",
      "One problem with mixture models is that they only use a single latent variable to generate the observations. In particular, each observation can only come from one of K prototypes. One can think of a mixture model as using K hidden binary variables, representing a one-hot encoding of the cluster identity. But because these variables are mutually exclusive, the model is still limited in its representational power.\n",
      "\n",
      "An alternative is to use a vector of real-valued latent variables, zi ∈ RL. The simplest prior\n",
      "\n",
      "to use is a Gaussian (we will consider other choices later):\n",
      "\n",
      "(12.1) If the observations are also continuous, so xi ∈ RD, we may use a Gaussian for the likelihood. Just as in linear regression, we will assume the mean is a linear function of the (hidden) inputs, thus yielding\n",
      "\n",
      "p(zi) = N (zi|μ0, Σ0)\n",
      "\n",
      "(12.2) where W is a D ×L matrix, known as the factor loading matrix, and Ψ is a D ×D covariance matrix. We take Ψ to be diagonal, since the whole point of the model is to “force” zi to explain the correlation, rather than “baking it in” to the observation’s covariance. This overall model is called factor analysis or FA. The special case in which Ψ = σ2I is called probabilistic principal components analysis or PPCA. The reason for this name will become apparent later. The generative process, where L = 1, D = 2 and Ψ is diagonal, is illustrated in Figure 12.1. We take an isotropic Gaussian “spray can” and slide it along the 1d line deﬁned by wzi + μ. This induces an ellongated (and hence correlated) Gaussian in 2d.\n",
      "\n",
      "p(xi|zi, θ) = N (Wzi + μ, Ψ)\n",
      "\n",
      "12.1.1\n",
      "\n",
      "FA is a low rank parameterization of an MVN\n",
      "\n",
      "FA can be thought of as a way of specifying a joint density model on x using a small number of parameters. To see this, note that from Equation 4.126, the induced marginal distribution p(xi|θ) is a Gaussian:\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(xi|θ) =\n",
      "\n",
      "N (xi|Wzi + μ, Ψ)N (zi|μ0, Σ0)dzi\n",
      "\n",
      "(12.3)\n",
      "\n",
      "= N (xi|Wμ0 + μ, Ψ + WΣ0WT )\n",
      "\n",
      "(12.4)\n",
      "\n",
      "382\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "(cid:91)(cid:21)\n",
      "\n",
      "(cid:90)\n",
      "\n",
      "(cid:91)(cid:21)\n",
      "\n",
      "(cid:83)(cid:11)(cid:91)(cid:95)(cid:93)(cid:12)\n",
      "\n",
      "(cid:541)\n",
      "\n",
      "(cid:541)\n",
      "\n",
      "(cid:83)(cid:11)(cid:93)(cid:12) (cid:83)(cid:11)(cid:93)(cid:12)\n",
      "\n",
      "(cid:93)(cid:3)(cid:95)(cid:90)(cid:95)\n",
      "\n",
      "(cid:83)(cid:11)(cid:91)(cid:12)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:93)\n",
      "\n",
      "(cid:91)(cid:20)\n",
      "\n",
      "(cid:91)(cid:20)\n",
      "\n",
      "Figure 12.1 D = 2 observed dimensions. Based on Figure 12.9 of (Bishop 2006b).\n",
      "\n",
      "Illustration of the PPCA generative process, where we have L = 1 latent dimension generating\n",
      "\n",
      "From this, we see that we can set μ0 = 0 without loss of generality, since we can always absorb Wμ0 into μ. Similarly, we can set Σ0 = I without loss of generality, because we can always “emulate” a correlated prior by using deﬁning a new weight matrix, ˜W = WΣ− 1 . Then we ﬁnd\n",
      "\n",
      "cov [x|θ] = ˜WT + E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(cid:7)(cid:7)T\n",
      "\n",
      "= (WΣ− 1\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      ")Σ0(WΣ− 1\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      ")T + Ψ = WWT + Ψ\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "(12.5)\n",
      "\n",
      "We thus see that FA approximates the covariance matrix of the visible vector using a low-rank\n",
      "\n",
      "decomposition:\n",
      "\n",
      "(12.6) This only uses O(LD) parameters, which allows a ﬂexible compromise between a full covariance Gaussian, with O(D2) parameters, and a diagonal covariance, with O(D) parameters. Note that if we did not restrict Ψ to be diagonal, we could trivially set Ψ to a full covariance matrix; then we could set W = 0, in which case the latent factors would not be required.\n",
      "\n",
      "C (cid:2) cov [x] = WWT + Ψ\n",
      "\n",
      "12.1.2\n",
      "\n",
      "Inference of the latent factors\n",
      "\n",
      "Although FA can be thought of as just a way to deﬁne a density on x, it is often used because we hope that the latent factors z will reveal something interesting about the data. To do this, we need to compute the posterior over the latent factors. We can use Bayes rule for Gaussians to give\n",
      "\n",
      "(12.9) Note that in the FA model, Σi is actually independent of i, so we can denote it by Σ. Computing this matrix takes O(L3 + L2D) time, and computing each mi = E [zi|xi, θ] takes O(L2 + LD) time. The mi are sometimes called the latent scores, or latent factors.\n",
      "\n",
      "p(zi|xi, θ) = N (zi|mi, Σi) Σi (cid:2) (Σ−1 mi (cid:2) Σi(WT Ψ−1(xi − μ) + Σ−1\n",
      "\n",
      "0 + WT Ψ−1W)−1\n",
      "\n",
      "0 μ0)\n",
      "\n",
      "(12.7)\n",
      "\n",
      "(12.8)\n",
      "\n",
      "12.1. Factor analysis\n",
      "\n",
      "383\n",
      "\n",
      "rotation=none\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "Width Wheelbase Length\n",
      "\n",
      "Weight\n",
      "\n",
      "Engine\n",
      "\n",
      "0.6\n",
      "\n",
      "Cylinders\n",
      "\n",
      "0.4\n",
      "\n",
      "GMC Yukon XL 2500 SLT Nissan Pathfinder Armada SE\n",
      "\n",
      "Horsepower\n",
      "\n",
      "2\n",
      "\n",
      "t n e n o p m o C\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "Saturn Ion1\n",
      "\n",
      "Kia Sorento LX\n",
      "\n",
      "Mercedes−Benz G500\n",
      "\n",
      "Mercedes−Benz CL600\n",
      "\n",
      "Retail Dealer\n",
      "\n",
      "−0.4\n",
      "\n",
      "Honda Insight\n",
      "\n",
      "−0.6\n",
      "\n",
      "CityMPG HighwayMPG\n",
      "\n",
      "Porsche 911\n",
      "\n",
      "−0.8\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0 Component 1\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "Figure 12.2 2D projection of 2004 cars data based on factor analysis. The blue text are the names of cars corresponding to certain chosen points. Figure generated by faBiplotDemo.\n",
      "\n",
      "Let us give a simple example, based (Shalizi 2009). We consider a dataset of D = 11 variables and N = 387 cases describing various aspects of cars, such as the engine size, the number of cylinders, the miles per gallon (MPG), the price, etc. We ﬁrst ﬁt a L = 2 dimensional model. We can plot the mi scores as points in R2, to visualize the data, as shown in Figure 12.2.\n",
      "\n",
      "To get a better understanding of the “meaning” of the latent factors, we can project unit vectors corresponding to each of the feature dimensions, e1 = (1, 0, . . . , 0), e2 = (0, 1, 0, . . . , 0), etc. into the low dimensional space. These are shown as blue lines in Figure 12.2; this is known as a biplot. We see that the horizontal axis represents price, corresponding to the features labeled “dealer” and “retail”, with expensive cars on the right. The vertical axis represents fuel efficiency (measured in terms of MPG) versus size: heavy vehicles are less efficient and are higher up, whereas light vehicles are more efficient and are lower down. We can “verify” this interpretation by clicking on some points, and ﬁnding the closest exemplars in the training set, and printing their names, as in Figure 12.2. However, in general, interpreting latent variable models is fraught with difficulties, as we discuss in Section 12.1.3.\n",
      "\n",
      "12.1.3\n",
      "\n",
      "Unidentiﬁability\n",
      "\n",
      "Just like with mixture models, FA is also unidentiﬁable. To see this, suppose R is an arbitrary orthogonal rotation matrix, satisfying RRT = I. Let us deﬁne ˜W = WR; then the likelihood\n",
      "\n",
      "384\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "function of this modiﬁed matrix is the same as for the unmodiﬁed matrix, since\n",
      "\n",
      "cov [x] = ˜WE\n",
      "\n",
      "˜WT + E = WRRT WT + Ψ = WWT + Ψ\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "zzT\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(cid:7)(cid:7)T\n",
      "\n",
      "(12.10)\n",
      "\n",
      "(12.11)\n",
      "\n",
      "Geometrically, multiplying W by an orthogonal matrix is like rotating z before generating x; but since z is drawn from an isotropic Gaussian, this makes no difference to the likelihood. Consequently, we cannot unique identify W, and therefore cannot uniquely identify the latent factors, either.\n",
      "\n",
      "To ensure a unique solution, we need to remove L(L − 1)/2 degrees of freedom, since that In total, the FA model has D + LD − is the number of orthonormal matrices of size L × L.1 L(L − 1)/2 free parameters (excluding the mean), where the ﬁrst term arises from Ψ. Obviously we require this to be less than or equal to D(D + 1)/2, which is the number of parameters in an unconstrained (but symmetric) covariance matrix. This gives us an upper bound on L, as follows:\n",
      "\n",
      "Lmax = (cid:13)D + 0.5(1 −\n",
      "\n",
      "√\n",
      "\n",
      "1 + 8D)(cid:15)\n",
      "\n",
      "(12.12)\n",
      "\n",
      "For example, D = 6 implies L ≤ 3. But we usually never choose this upper bound, since it would result in overﬁtting (see discussion in Section 12.3 on how to choose L).\n",
      "\n",
      "Unfortunately, even if we set L < Lmax, we still cannot uniquely identify the parameters, since the rotational ambiguity still exists. Non-identiﬁability does not affect the predictive per- formance of the model. However, it does affect the loading matrix, and hence the interpretation of the latent factors. Since factor analysis is often used to uncover structure in the data, this problem needs to be addressed. Here are some commonly used solutions:\n",
      "\n",
      "Forcing W to be orthonormal Perhaps the cleanest solution to the identiﬁability problem is to force W to be orthonormal, and to order the columns by decreasing variance of the corresponding latent factors. This is the approach adopted by PCA, which we will discuss in Section 12.2. The result is not necessarily more interpretable, but at least it is unique.\n",
      "\n",
      "Forcing W to be lower triangular One way to achieve identiﬁability, which is popular in the Bayesian community (e.g., (Lopes and West 2004)), is to ensure that the ﬁrst visible feature is only generated by the ﬁrst latent factor, the second visible feature is only generated by the ﬁrst two latent factors, and so on. For example, if L = 3 and D = 4, the correspond factor loading matrix is given by\n",
      "\n",
      "W =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "w11 0 0 w21 w22 0 w31 w32 w33 w41 w42 w43\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠\n",
      "\n",
      "(12.13)\n",
      "\n",
      "We also require that wjj > 0 for j = 1 :L. The total number of parameters in this constrained matrix is D + DL − L(L − 1)/2, which is equal to the number of uniquely identiﬁable parameters. The disadvantage of this method is that the ﬁrst L visible variables,\n",
      "\n",
      "1. To see this, note that there are L − 1 free parameters in R in the ﬁrst column (since the column vector must be normalized to unit length), there are L − 2 free parameters in the second column (which must be orthogonal to the ﬁrst), and so on.\n",
      "\n",
      "12.1. Factor analysis\n",
      "\n",
      "385\n",
      "\n",
      "π\n",
      "\n",
      "qi\n",
      "\n",
      "zi\n",
      "\n",
      "Ψ\n",
      "\n",
      "xi\n",
      "\n",
      "N\n",
      "\n",
      "μk W k K\n",
      "\n",
      "Figure 12.3 Mixture of factor analysers as a DGM.\n",
      "\n",
      "known as the founder variables, affect the interpretation of the latent factors, and so must be chosen carefully.\n",
      "\n",
      "Sparsity promoting priors on the weights Instead of pre-specifying which entries in W are zero, we can encourage the entries to be zero, using (cid:6)1 regularization (Zou et al. 2006), ARD (Bishop 1999; Archambeau and Bach 2008), or spike-and-slab priors (Rattray et al. 2009). This is called sparse factor analysis. This does not necessarily ensure a unique MAP estimate, but it does encourage interpretable solutions. See Section 13.8.\n",
      "\n",
      "Choosing an informative rotation matrix There are a variety of heuristic methods that try to ﬁnd rotation matrices R which can be used to modify W (and hence the latent factors) so as to try to increase the interpretability, typically by encouraging them to be (approximately) sparse. One popular method is known as varimax (Kaiser 1958).\n",
      "\n",
      "Use of non-Gaussian priors for the latent factors In Section 12.6, we will dicuss how re- placing p(zi) with a non-Gaussian distribution can enable us to sometimes uniquely identify W as well as the latent factors. This technique is known as ICA.\n",
      "\n",
      "12.1.4 Mixtures of factor analysers\n",
      "\n",
      "The FA model assumes that the data lives on a low dimensional linear manifold. In reality, most data is better modeled by some form of low dimensional curved manifold. We can approximate let the a curved manifold by a piecewise linear manifold. This suggests the following model: k’th linear subspace of dimensionality Lk be represented by Wk, for k = 1 :K . Suppose we have a latent indicator qi ∈ {1, . . . , K} specifying which subspace we should use to generate the data. We then sample zi from a Gaussian prior and pass it through the Wk matrix (where k = qi), and add noise. More precisely, the model is as follows:\n",
      "\n",
      "p(xi|zi, qi = k, θ) =N (xi|μk + Wkzi, Ψ) p(zi|θ) =N (zi|0, I) p(qi|θ) = Cat(qi|π)\n",
      "\n",
      "(12.14)\n",
      "\n",
      "(12.15)\n",
      "\n",
      "(12.16)\n",
      "\n",
      "386\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 12.4 Mixture of mixPpcaDemoNetlab.\n",
      "\n",
      "1d PPCAs ﬁt\n",
      "\n",
      "to a dataset,\n",
      "\n",
      "for K = 1, 10.\n",
      "\n",
      "Figure generated by\n",
      "\n",
      "This is called a mixture of factor analysers (MFA) (Hinton et al. 1997). The CI assumptions are represented in Figure 12.3.\n",
      "\n",
      "Another way to think about this model is as a low-rank version of a mixture of Gaussians. In particular, this model needs O(KLD) parameters instead of the O(KD2) parameters needed for a mixture of full covariance Gaussians. This can reduce overﬁtting. In fact, MFA is a good generic density model for high-dimensional real-valued data.\n",
      "\n",
      "12.1.5\n",
      "\n",
      "EM for factor analysis models\n",
      "\n",
      "Using the results from Chapter 4, it is straightforward to derive an EM algorithm to ﬁt an FA model. With just a little more work, we can ﬁt a mixture of FAs. Below we state the results without proof. The derivation can be found in (Ghahramani and Hinton 1996a); however, deriving these equations yourself is a useful exercise if you want to become proﬁcient at the math.\n",
      "\n",
      "To obtain the results for a single factor analyser, just set ric = 1 and c = 1 in the equations below. In Section 12.2.5 we will see a further simpliﬁcation of these equations that arises when ﬁtting a PPCA model, where the results will turn out to have a particularly simple and elegant intepretation.\n",
      "\n",
      "In the E step, we compute the posterior responsibility of cluster c for data point i using\n",
      "\n",
      "ric (cid:2) p(qi = c|xi, θ) ∝ πcN (xi|μc, WcWT\n",
      "\n",
      "c + Ψ)\n",
      "\n",
      "(12.17)\n",
      "\n",
      "The conditional posterior for zi is given by\n",
      "\n",
      "p(zi|xi, qi = c, θ) =N (zi|mic, Σic) c Ψ−1\n",
      "\n",
      "Σic (cid:2) (IL + WT mic (cid:2) Σic(WT\n",
      "\n",
      "c Ψ−1\n",
      "\n",
      "c Wc)−1 c (xi − μc))\n",
      "\n",
      "(12.18)\n",
      "\n",
      "(12.19)\n",
      "\n",
      "(12.20)\n",
      "\n",
      "In the M step, it is easiest to estimate μc and Wc at the same time, by deﬁning ˜Wc =\n",
      "\n",
      "12.2. Principal components analysis (PCA)\n",
      "\n",
      "387\n",
      "\n",
      "(Wc, μc), ˜z = (z, 1), Also, deﬁne\n",
      "\n",
      "Cic (cid:2) E\n",
      "\n",
      "bic (cid:2) E [˜z|xi, qi = c] = [mic; 1] (cid:31) zzT |xi, qi = c E T E [z|xi, qi = c]\n",
      "\n",
      "(cid:31) ˜z˜zT |xi, qi = c\n",
      "\n",
      "=\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "E [z|xi, qi = c] 1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(12.21)\n",
      "\n",
      "(12.22)\n",
      "\n",
      "Then the M step is as follows: (cid:18)\n",
      "\n",
      "ˆ˜Wc =\n",
      "\n",
      "ˆΨ =\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "diag\n",
      "\n",
      "i\n",
      "\n",
      "ricxibT ic -\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "ic\n",
      "\n",
      "ric\n",
      "\n",
      "(cid:19) (cid:18)\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "xi − ˆ˜Wcbic\n",
      "\n",
      "i\n",
      "\n",
      "ricCic\n",
      "\n",
      "(cid:19)−1\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "xT i\n",
      "\n",
      ".\n",
      "\n",
      "(12.23)\n",
      "\n",
      "(12.24)\n",
      "\n",
      "ˆπc =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "ric\n",
      "\n",
      "(12.25)\n",
      "\n",
      "Note that these updates are for “vanilla” EM. A much faster version of this algorithm, based\n",
      "\n",
      "on ECM, is described in (Zhao and Yu 2008).\n",
      "\n",
      "12.1.6\n",
      "\n",
      "Fitting FA models with missing data\n",
      "\n",
      "In many applications, such as collaborative ﬁltering, we have missing data. One virtue of the EM approach to ﬁtting an FA/PPCA model is that it is easy to extend to this case. However, overﬁtting can be a problem if there is a lot of missing data. Consequently it is important to perform MAP estimation or to use Bayesian inference. See e.g., (Ilin and Raiko 2010) for details.\n",
      "\n",
      "12.2\n",
      "\n",
      "Principal components analysis (PCA)\n",
      "\n",
      "Consider the FA model where we constrain Ψ = σ2I, and W to be orthonormal. It can be shown (Tipping and Bishop 1999) that, as σ2 → 0, this model reduces to classical (non- probabilistic) principal components analysis ( PCA), also known as the Karhunen Loeve transform. The version where σ2 > 0 is known as probabilistic PCA (PPCA) (Tipping and Bishop 1999), or sensible PCA (Roweis 1997). (An equivalent result was derived independently, from a different perspective, in (Moghaddam and Pentland 1995).)\n",
      "\n",
      "To make sense of this result, we ﬁrst have to learn about classical PCA. We then connect PCA\n",
      "\n",
      "to the SVD. And ﬁnally we return to discuss PPCA.\n",
      "\n",
      "12.2.1\n",
      "\n",
      "Classical PCA: statement of the theorem\n",
      "\n",
      "The synthesis view of classical PCA is summarized in the forllowing theorem. Theorem 12.2.1. Suppose we want to ﬁnd an orthogonal set of L linear basis vectors wj ∈ RD, and the corresponding scores zi ∈ RL, such that we minimize the average reconstruction error\n",
      "\n",
      "J(W, Z) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "||xi − ˆxi||2\n",
      "\n",
      "(12.26)\n",
      "\n",
      "388\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−5 −4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 12.5 An illustration of PCA and PPCA where D = 2 and L = 1. Circles are the original data points, crosses are the reconstructions. The red star is the data mean. (a) PCA. The points are orthogonally projected onto the line. Figure generated by pcaDemo2d. (b) PPCA. The projection is no longer orthogonal: the reconstructions are shrunk towards the data mean (red star). Based on Figure 7.6 of (Nabney 2001). Figure generated by ppcaDemo2d.\n",
      "\n",
      "where ˆxi = Wzi, subject to the constraint that W is orthonormal. Equivalently, we can write this objective as follows:\n",
      "\n",
      "J(W, Z) = ||X − WZT ||2 F\n",
      "\n",
      "(12.27)\n",
      "\n",
      "where Z is an N × L matrix with the zi in its rows, and ||A||F is the Frobenius norm of matrix A, deﬁned by\n",
      "\n",
      "||A||F =\n",
      "\n",
      "/ 0 0 1\n",
      "\n",
      "m(cid:2)\n",
      "\n",
      "n(cid:2)\n",
      "\n",
      "a2 ij =\n",
      "\n",
      "2\n",
      "\n",
      "tr(AT A) = ||A(:)||2\n",
      "\n",
      "(12.28)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "The optimal solution is obtained by setting ˆW = VL, where VL contains the L eigenvectors with largest eigenvalues of the empirical covariance matrix, ˆΣ = 1 i . (We assume the N xi have zero mean, for notational simplicity.) Furthermore, the optimal low-dimensional encoding of the data is given by ˆzi = WT xi, which is an orthogonal projection of the data onto the column space spanned by the eigenvectors.\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 xixT\n",
      "\n",
      "An example of this is shown in Figure 12.5(a) for D = 2 and L = 1. The diagonal line is the vector w1; this is called the ﬁrst principal component or principal direction. The data points xi ∈ R2 are orthogonally projected onto this line to get zi ∈ R. This is the best 1-dimensional approximation to the data. (We will discuss Figure 12.5(b) later.)\n",
      "\n",
      "In general, it is hard to visualize higher dimensional data, but if the data happens to be a set of images, it is easy to do so. Figure 12.6 shows the ﬁrst three principal vectors, reshaped as images, as well as the reconstruction of a speciﬁc image using a varying number of basis vectors. (We discuss how to choose L in Section 11.5.)\n",
      "\n",
      "Below we will show that the principal directions are the ones along which the data shows maximal variance. This means that PCA can be “misled” by directions in which the variance is high merely because of the measurement scale. Figure 12.7(a) shows an example, where the vertical axis (weight) uses a large range than the horizontal axis (height), resulting in a line that It is therefore standard practice to standardize the data ﬁrst, or looks somewhat “unnatural”.\n",
      "\n",
      "12.2. Principal components analysis (PCA)\n",
      "\n",
      "389\n",
      "\n",
      "mean\n",
      "\n",
      "principal basis 1\n",
      "\n",
      "reconstructed with 2 bases\n",
      "\n",
      "reconstructed with 10 bases\n",
      "\n",
      "principal basis 2\n",
      "\n",
      "principal basis 3\n",
      "\n",
      "reconstructed with 100 bases\n",
      "\n",
      "reconstructed with 506 bases\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 12.6 (a) The mean and the ﬁrst three PC basis vectors (eigendigits) based on 25 images of the digit 3 (from the MNIST dataset). (b) Reconstruction of an image based on 2, 10, 100 and all the basis vectors. Figure generated by pcaImageDemo.\n",
      "\n",
      "300\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "250\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "t h g e w\n",
      "\n",
      "i\n",
      "\n",
      "200\n",
      "\n",
      "150\n",
      "\n",
      "t h g e w\n",
      "\n",
      "i\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "100\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "65\n",
      "\n",
      "70 height\n",
      "\n",
      "75\n",
      "\n",
      "80\n",
      "\n",
      "85\n",
      "\n",
      "−4 −4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0 height\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 12.7 Effect of standardization on PCA applied to the height/ weight dataset. Left: PCA of raw data. Right: PCA of standardized data. Figure generated by pcaDemoHeightWeight.\n",
      "\n",
      "equivalently, to work with correlation matrices instead of covariance matrices. The beneﬁts of this are apparent from Figure 12.7(b).\n",
      "\n",
      "12.2.2\n",
      "\n",
      "Proof * Proof. We use wj ∈ RD to denote the j’th principal direction, xi ∈ RD to denote the i’th high-dimensional observation, zi ∈ RL to denote the i’th low-dimensional representation, and ˜zj ∈ RN to denote the [z1j, . . . , zN j], which is the j’th component of all the low-dimensional vectors.\n",
      "\n",
      "Let us start by estimating the best 1d solution, w1 ∈ RD, and the corresponding projected points ˜z1 ∈ RN . We will ﬁnd the remaining bases w2, w3, etc. later. The reconstruction error\n",
      "\n",
      "390\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "is given by\n",
      "\n",
      "J(w1, z1) =\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "1 N\n",
      "\n",
      "1 N\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1 N(cid:2)\n",
      "\n",
      "[xT\n",
      "\n",
      "||xi − zi1w1||2 =\n",
      "\n",
      "i xi − 2zi1wT\n",
      "\n",
      "1 xi + z2\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "i1wT\n",
      "\n",
      "(xi − zi1w1)T (xi − zi1w1)\n",
      "\n",
      "1 w1]\n",
      "\n",
      "i=1 N(cid:2)\n",
      "\n",
      "[xT\n",
      "\n",
      "i xi − 2zi1wT\n",
      "\n",
      "1 xi + z2 i1]\n",
      "\n",
      "i=1\n",
      "\n",
      "(12.29)\n",
      "\n",
      "(12.30)\n",
      "\n",
      "(12.31)\n",
      "\n",
      "since wT to zero gives ∂ ∂zi1\n",
      "\n",
      "1 w1 = 1 (by the orthonormality assumption). Taking derivatives wrt zi1 and equating\n",
      "\n",
      "J(w1, z1) =\n",
      "\n",
      "1 N\n",
      "\n",
      "[−2wT\n",
      "\n",
      "1 xi + 2zi1] = 0 ⇒ zi1 = wT\n",
      "\n",
      "1 xi\n",
      "\n",
      "(12.32)\n",
      "\n",
      "So the optimal reconstruction weights are obtained by orthogonally projecting the data onto the ﬁrst principal direction, w1 (see Figure 12.5(a)). Plugging back in gives\n",
      "\n",
      "J(w1) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "[xT\n",
      "\n",
      "i xi − z2\n",
      "\n",
      "i1] = const − 1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "z2 i1\n",
      "\n",
      "(12.33)\n",
      "\n",
      "Now the variance of the projected coordinates is given by\n",
      "\n",
      "var [˜z1] = E\n",
      "\n",
      "(cid:31) ˜z2 1\n",
      "\n",
      "− (E [˜z1])2 =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "z2 i1 − 0\n",
      "\n",
      "(12.34)\n",
      "\n",
      "since\n",
      "\n",
      "E [zi1] = E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xT i w1\n",
      "\n",
      "= E [xi]\n",
      "\n",
      "T w1 = 0\n",
      "\n",
      "(12.35)\n",
      "\n",
      "because the data has been centered. From this, we see that minimizing the reconstruction error is equivalent to maximizing the variance of the projected data, i.e.,\n",
      "\n",
      "arg min w1\n",
      "\n",
      "J(w1) = arg max w1\n",
      "\n",
      "var [˜z1]\n",
      "\n",
      "(12.36)\n",
      "\n",
      "This is why it is often said that PCA ﬁnds the directions of maximal variance. This is called the analysis view of PCA.\n",
      "\n",
      "The variance of the projected data can be written as\n",
      "\n",
      "where ˆΣ = 1 i=1 N data is standardized).\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "z2 i1 =\n",
      "\n",
      "1 N (cid:10)N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1 (cid:10)\n",
      "\n",
      "wT\n",
      "\n",
      "i xixT i\n",
      "\n",
      "1 xixT\n",
      "\n",
      "i w1 = wT\n",
      "\n",
      "is the empirical covariance matrix (or correlation matrix if the\n",
      "\n",
      "1 ˆΣw1\n",
      "\n",
      "(12.37)\n",
      "\n",
      "12.2. Principal components analysis (PCA)\n",
      "\n",
      "391\n",
      "\n",
      "We can trivially maximize the variance of the projection (and hence minimize the recon- struction error) by letting ||w1|| → ∞, so we impose the constraint ||w1|| = 1 and instead maximize\n",
      "\n",
      "˜J(w1) = wT\n",
      "\n",
      "1 ˆΣw1 + λ1(wT\n",
      "\n",
      "1 w1 − 1)\n",
      "\n",
      "(12.38)\n",
      "\n",
      "where λ1 is the Lagrange multiplier. Taking derivatives and equating to zero we have\n",
      "\n",
      "∂ ∂w1\n",
      "\n",
      "˜J(w1) = 2 ˆΣw1 − 2λ1w1 = 0\n",
      "\n",
      "(12.39)\n",
      "\n",
      "ˆΣw1 = λ1w1\n",
      "\n",
      "(12.40)\n",
      "\n",
      "Hence the direction that maximizes the variance is an eigenvector of the covariance matrix. Left multiplying by w1 (and using wT\n",
      "\n",
      "1 w1 = 1) we ﬁnd that the variance of the projected data is\n",
      "\n",
      "wT\n",
      "\n",
      "1 ˆΣw1 = λ1\n",
      "\n",
      "(12.41)\n",
      "\n",
      "Since we want to maximize the variance, we pick the eigenvector which corresponds to the largest eigenvalue.\n",
      "\n",
      "wT\n",
      "\n",
      "Now let us ﬁnd another direction w2 to further minimize the reconstruction error, subject to 1 w2 = 0 and wT\n",
      "\n",
      "J(w1, z1, w2, z2) =\n",
      "\n",
      "2 w2 = 1. The error is N(cid:2)\n",
      "\n",
      "1 N\n",
      "\n",
      "i=1\n",
      "\n",
      "||xi − zi1w1 − zi2w2||2\n",
      "\n",
      "(12.42)\n",
      "\n",
      "Optimizing wrt w1 and z1 gives the same solution as before. Exercise 12.4 asks you to show that ∂J In other words, the second principal encoding is gotten by ∂z2 projecting onto the second principal direction. Substituting in yields\n",
      "\n",
      "= 0 yields zi2 = wT\n",
      "\n",
      "2 xi.\n",
      "\n",
      "J(w2) =\n",
      "\n",
      "1 n\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "[xT\n",
      "\n",
      "i xi − wT\n",
      "\n",
      "1 xixT\n",
      "\n",
      "i w1 − wT\n",
      "\n",
      "2 xixT\n",
      "\n",
      "i w2] = const − wT\n",
      "\n",
      "2 ˆΣw2\n",
      "\n",
      "i=1\n",
      "\n",
      "(12.43)\n",
      "\n",
      "Dropping the constant term and adding the constraints yields 2 w1 − 0)\n",
      "\n",
      "˜J(w2) = −wT\n",
      "\n",
      "2 ˆΣw2 + λ2(wT\n",
      "\n",
      "2 w2 − 1) + λ12(wT\n",
      "\n",
      "(12.44)\n",
      "\n",
      "Exercise 12.4 asks you to show that the solution is given by the eigenvector with the second largest eigenvalue: ˆΣw2 = λ2w2\n",
      "\n",
      "(12.45)\n",
      "\n",
      "The proof continues in this way. (Formally one can use induction.)\n",
      "\n",
      "392\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "12.2.3\n",
      "\n",
      "Singular value decomposition (SVD)\n",
      "\n",
      "We have deﬁned the solution to PCA in terms of eigenvectors of the covariance matrix. However, there is another way to obtain the solution, based on the singular value decomposition, or SVD. This basically generalizes the notion of eigenvectors from square matrices to any kind of matrix.\n",
      "\n",
      "In particular, any (real) N × D matrix X can be decomposed as follows\n",
      "\n",
      "X3456\n",
      "\n",
      "N ×D\n",
      "\n",
      "= U3456\n",
      "\n",
      "N ×N\n",
      "\n",
      "S3456\n",
      "\n",
      "N ×D\n",
      "\n",
      "VT 3456 D×D\n",
      "\n",
      "(12.46)\n",
      "\n",
      "where U is an N × N matrix whose columns are orthornormal (so UT U = IN ), V is D × D matrix whose rows and columns are orthonormal (so VT V = VVT = ID), and S is a N × D matrix containing the r = min(N, D) singular values σi ≥ 0 on the main diagonal, with 0s ﬁlling the rest of the matrix. The columns of U are the left singular vectors, and the columns of V are the right singular vectors. See Figure 12.8(a) for an example.\n",
      "\n",
      "Since there are at most D singular values (assuming N > D), the last N − D columns of U are irrelevant, since they will be multiplied by 0. The economy sized SVD, orthin SVD, avoids computing these unnecessary elements. Let us denote this decomposition by ˆUˆS ˆV. If N > D, we have\n",
      "\n",
      "X3456\n",
      "\n",
      "N ×D\n",
      "\n",
      "= ˆU3456\n",
      "\n",
      "N ×D\n",
      "\n",
      "ˆS3456\n",
      "\n",
      "D×D\n",
      "\n",
      "ˆVT 3456 D×D\n",
      "\n",
      "(12.47)\n",
      "\n",
      "as in Figure 12.8(a). If N < D, we have ˆS3456\n",
      "\n",
      "N ×D\n",
      "\n",
      "X3456\n",
      "\n",
      "= ˆU3456\n",
      "\n",
      "N ×N\n",
      "\n",
      "N ×N\n",
      "\n",
      "ˆVT 3456 N ×D\n",
      "\n",
      "(12.48)\n",
      "\n",
      "Computing the economy-sized SVD takes O(N D min(N, D)) time (Golub and van Loan 1996, p254).\n",
      "\n",
      "real matrix X, ifX = USVT , we have\n",
      "\n",
      "The connection between eigenvectors and singular vectors is the following. For an arbitrary\n",
      "\n",
      "XT X = VST UT USVT = V(ST S)VT = VDVT\n",
      "\n",
      "(12.49)\n",
      "\n",
      "where D = S2 is a diagonal matrix containing the squares singular values. Hence\n",
      "\n",
      "(XT X)V = VD\n",
      "\n",
      "(12.50)\n",
      "\n",
      "so the eigenvectors of XT X are equal to V, the right singular vectors of X, and the eigenvalues of XT X are equal to D, the squared singular values. Similarly\n",
      "\n",
      "XXT = USVT VST UT = U(SST )UT\n",
      "\n",
      "(12.51)\n",
      "\n",
      "(XXT )U = U(SST ) = UD\n",
      "\n",
      "(12.52)\n",
      "\n",
      "so the eigenvectors of XXT are equal to U, the left singular vectors of X. Also, the eigenvalues of XXT are equal to the squared singular values. We can summarize all this as follows:\n",
      "\n",
      "U = evec(XXT ), V = evec(XT X), S2 = eval(XXT ) = eval(XT X)\n",
      "\n",
      "(12.53)\n",
      "\n",
      "12.2. Principal components analysis (PCA)\n",
      "\n",
      "393\n",
      "\n",
      "D\n",
      "\n",
      "D\n",
      "\n",
      "N − D\n",
      "\n",
      "D\n",
      "\n",
      "D\n",
      "\n",
      "N\n",
      "\n",
      "=\n",
      "\n",
      "σ1\n",
      "\n",
      ". . .\n",
      "\n",
      "σD\n",
      "\n",
      "D\n",
      "\n",
      "0\n",
      "\n",
      "X\n",
      "\n",
      "=\n",
      "\n",
      "U\n",
      "\n",
      "S\n",
      "\n",
      "V T\n",
      "\n",
      "(a)\n",
      "\n",
      "D\n",
      "\n",
      "L\n",
      "\n",
      "L\n",
      "\n",
      "D\n",
      "\n",
      "σ1 . . .σL\n",
      "\n",
      "L\n",
      "\n",
      "N\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "X\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "U L\n",
      "\n",
      "SL\n",
      "\n",
      "V T L\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 12.8 (a) SVD decomposition of non-square matrices X = USVT . The shaded parts of S, and all the off-diagonal terms, are zero. The shaded entries in U and S are not computed in the economy-sized version, since they are not needed. (b) Truncated SVD approximation of rank L.\n",
      "\n",
      "Since the eigenvectors are unaffected by linear scaling of a matrix, we see that the right singular vectors of X are equal to the eigenvectors of the empirical covariance ˆΣ. Furthermore, the eigenvalues of ˆΣ are a scaled version of the squared singular values. This means we can perform PCA using just a few lines of code (see pcaPmtk).\n",
      "\n",
      "represent a rank r matrix as follows: ⎞\n",
      "\n",
      "However, the connection between PCA and SVD goes deeper. From Equation 12.46, we can\n",
      "\n",
      "X = σ1\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "| u1 |\n",
      "\n",
      "⎠\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "− vT\n",
      "\n",
      "1 −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "+ · · · + σr\n",
      "\n",
      "⎝\n",
      "\n",
      "⎛\n",
      "\n",
      "| ur |\n",
      "\n",
      "⎠\n",
      "\n",
      "⎞\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "− vT\n",
      "\n",
      "r −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(12.54)\n",
      "\n",
      "If the singular values die off quickly as in Figure 12.10, we can produce a rank L approximation to the matrix as follows:\n",
      "\n",
      "X ≈ U:,1:L S1:L,1:L VT\n",
      "\n",
      ":,1:L\n",
      "\n",
      "(12.55)\n",
      "\n",
      "This is called a truncated SVD (see Figure 12.8(b)). The total number of parameters needed to represent an N × D matrix using a rank L approximation is\n",
      "\n",
      "N L + LD + L = L(N + D + 1)\n",
      "\n",
      "(12.56)\n",
      "\n",
      "394\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "rank 200\n",
      "\n",
      "rank 2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "rank 5\n",
      "\n",
      "rank 20\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 12.9 Low rank approximations to an image. Top left: The original image is of size 200 × 320, so has rank 200. Subsequent images have ranks 2, 5, and 20. Figure generated by svdImageDemo.\n",
      "\n",
      "10\n",
      "\n",
      "original randomized\n",
      "\n",
      "9\n",
      "\n",
      "8\n",
      "\n",
      ") σ ( g o\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "7\n",
      "\n",
      "6\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50 i\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "Figure 12.10 First 50 log singular values for the clown image (solid red line), and for a data matrix obtained by randomly shuffling the pixels (dotted green line). Figure generated by svdImageDemo.\n",
      "\n",
      "12.2. Principal components analysis (PCA)\n",
      "\n",
      "395\n",
      "\n",
      "As an example, consider the 200 × 320 pixel image in Figure 12.9(top left). This has 64,000 numbers in it. We see that a rank 20 approximation, with only (200 + 320 + 1) × 20 = 10, 420 numbers is a very good approximation.\n",
      "\n",
      "One can show that the error in this approximation is given by\n",
      "\n",
      "||X − XL||F ≈ σL+1\n",
      "\n",
      "(12.57)\n",
      "\n",
      "Furthermore, one can show that the SVD offers the best rank L approximation to a matrix (best in the sense of minimizing the above Frobenius norm).\n",
      "\n",
      "ˆW = V, and that ˆZ = X ˆW, so\n",
      "\n",
      "Let us connect this back to PCA. Let X = USVT be a truncated SVD of X. We know that\n",
      "\n",
      "ˆZ = USVT V = US\n",
      "\n",
      "(12.58)\n",
      "\n",
      "Furthermore, the optimal reconstruction is given by ˆX = Z ˆWT , so we ﬁnd\n",
      "\n",
      "ˆX = USVT\n",
      "\n",
      "(12.59)\n",
      "\n",
      "This is precisely the same as a truncated SVD approximation! This is another illustration of the fact that PCA is the best low rank approximation to the data.\n",
      "\n",
      "12.2.4\n",
      "\n",
      "Probabilistic PCA\n",
      "\n",
      "We are now ready to revisit PPCA. One can show the following remarkable result. Theorem 12.2.2 ((Tipping and Bishop 1999)). Consider a factor analysis model in which Ψ = σ2I and W is orthogonal. The observed data log likelihood is given by\n",
      "\n",
      "where C = WWT + σ2I and S = 1 N data, for notational simplicity.) The maxima of the log-likelihood are given by\n",
      "\n",
      "log p(X|W, σ2) =−\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "ln |C| − 1 2 (cid:10)N\n",
      "\n",
      "i=1 xixT\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "i C−1xi = − xT\n",
      "\n",
      "i = (1/N )XT X. (We are assuming centered\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "ln |C| + tr(C−1 ˆΣ)\n",
      "\n",
      "(12.60)\n",
      "\n",
      "ˆW = V(Λ − σ2I)\n",
      "\n",
      "1 2 R\n",
      "\n",
      "(12.61)\n",
      "\n",
      "where R is an arbitrary L × L orthogonal matrix, V is the D × L matrix whose columns are the ﬁrst L eigenvectors of S, and Λ is the corresponding diagonal matrix of eigenvalues. Without loss of generality, we can set R = I. Furthermore, the MLE of the noise variance is given by\n",
      "\n",
      "ˆσ2 =\n",
      "\n",
      "1 D − L\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=L+1\n",
      "\n",
      "λj\n",
      "\n",
      "(12.62)\n",
      "\n",
      "which is the average variance associated with the discarded dimensions.\n",
      "\n",
      "Thus, as σ2 → 0, we have ˆW → V, as in classical PCA. What about ˆZ? It is easy to see that\n",
      "\n",
      "the posterior over the latent factors is given by p(zi|xi, ˆθ) =N (zi| ˆF−1 ˆWT xi, σ2 ˆF−1)\n",
      "\n",
      "(12.63)\n",
      "\n",
      "ˆF (cid:2) ˆWT ˆW + ˆσ2I\n",
      "\n",
      "(12.64)\n",
      "\n",
      "396\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "(Do not confuse F = WT W + σ2I with C = WWT + σ2I.) Hence, as σ2 → 0, we ﬁnd ˆW → V, ˆF → I and ˆzi → VT xi. Thus the posterior mean is obtained by an orthogonal projection of the data onto the column space of V, as in classical PCA.\n",
      "\n",
      "Note, however, that if σ2 >, the posterior mean is not an orthogonal projection, since it is shrunk somewhat towards the prior mean, as illustrated in Figure 12.5(b). This sounds like an undesirable property, but it means that the reconstructions will be closer to the overall data mean, ˆμ = x.\n",
      "\n",
      "12.2.5\n",
      "\n",
      "EM algorithm for PCA\n",
      "\n",
      "Although the usual way to ﬁt a PCA model uses eigenvector methods, or the SVD, we can also use EM, which will turn out to have some advantages that we discuss below. EM for PCA relies on the probabilistic formulation of PCA. However the algorithm continues to work in the zero noise limit, σ2 = 0, as shown by (Roweis 1997).\n",
      "\n",
      "along its columns. Similarly, Equation 12.63, when σ2 = 0, we have\n",
      "\n",
      "Let ˜Z be a L × N matrix storing the posterior means (low-dimensional representations) let ˜X = XT store the original data along its columns. From\n",
      "\n",
      "˜Z = (WT W)−1WT ˜X\n",
      "\n",
      "(12.65)\n",
      "\n",
      "This constitutes the E step. Notice that this is just an orthogonal projection of the data.\n",
      "\n",
      "From Equation 12.23, the M step is given by\n",
      "\n",
      "ˆW =\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "xiE [zi]\n",
      "\n",
      "T\n",
      "\n",
      "(cid:19) (cid:18)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "E [zi] E [zi]\n",
      "\n",
      "T\n",
      "\n",
      "(cid:19)−1\n",
      "\n",
      "(12.66)\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "where we exploited the fact that Σ = cov [zi|xi, θ] = 0I when σ2 = 0. It is worth comparing this expression to the MLE for multi-output linear regression (Equation 7.89), which has the form i )−1. Thus we see that the M step is like linear regression where we W = ( replace the observed inputs by the expected values of the latent variables.\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i yixT\n",
      "\n",
      "i )(\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i xixT\n",
      "\n",
      "In summary, here is the entire algorithm:\n",
      "\n",
      "E step ˜Z = (WT W)−1WT ˜X • M stepW = ˜X˜ZT (˜Z˜ZT )−1\n",
      "\n",
      "(Tipping and Bishop 1999) showed that the only stable ﬁxed point of the EM algorithm is the globally optimal solution. That is, the EM algorithm converges to a solution where W spans the same linear subspace as that deﬁned by the ﬁrst L eigenvectors. However, if we want W to be orthogonal, and to contain the eigenvectors in descending order of eigenvalue, we have to orthogonalize the resulting matrix (which can be done quite cheaply). Alternatively, we can modify EM to give the principal basis directly (Ahn and Oh 2003).\n",
      "\n",
      "This algorithm has a simple physical analogy in the case D = 2 and L = 1 (Roweis 1997). Consider some points in R2 attached by springs to a rigid rod, whose orientation is deﬁned by a vector w. Let zi be the location where the i’th spring attaches to the rod. In the E step, we hold the rod ﬁxed, and let the attachment points slide around so as to minimize the spring energy (which is proportional to the sum of squared residuals). In the M step, we hold the attachment\n",
      "\n",
      "12.2. Principal components analysis (PCA)\n",
      "\n",
      "397\n",
      "\n",
      "2.5\n",
      "\n",
      "E step 1\n",
      "\n",
      "2.5\n",
      "\n",
      "M step 1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−2.5\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "−2.5\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "E step 2\n",
      "\n",
      "M step 2\n",
      "\n",
      "2.5\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−2.5\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Illustration of EM for PCA when D = 2 and L = 1. Green stars are the original data points, Figure 12.11 black circles are their reconstructions. The weight vector w is represented by blue line. (a) We start with a random initial guess of w. The E step is represented by the orthogonal projections. (b) We update the rod w in the M step, keeping the projections onto the rod (black circles) ﬁxed. (c) Another E step. The black circles can ’slide’ along the rod, but the rod stays ﬁxed. (d) Another M step. Based on Figure 12.12 of (Bishop 2006b). Figure generated by pcaEmStepByStep.\n",
      "\n",
      "points ﬁxed and let the rod rotate so as to minimize the spring energy. See Figure 12.11 for an illustration.\n",
      "\n",
      "Apart from this pleasing intuitive interpretation, EM for PCA has the following advantages\n",
      "\n",
      "over eigenvector methods:\n",
      "\n",
      "EM can be faster. In particular, assuming N, D (cid:18) L, the dominant cost of EM is the pro- jection operation in the E step, so the overall time is O(T LN D), where T is the number of\n",
      "\n",
      "398\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(e)\n",
      "\n",
      "(f)\n",
      "\n",
      "Figure 12.12 Illustration of estimating the effective dimensionalities in a mixture of factor analysers using VBEM. The blank columns have been forced to 0 via the ARD mechanism. The data was generated from 6 clusters with intrinsic dimensionalities of 7, 4, 3, 2, 2, 1, which the method has successfully estimated. Source: Figure 4.4 of (Beal 2003). Used with kind permission of Matt Beal.\n",
      "\n",
      "iterations. (Roweis 1997) showed experimentally that the number of iterations is usually very small (the mean was 3.6), regardless of N or D. (This results depends on the ratio of eigenval- ues of the empirical covariance matrix.) This is much faster than the O(min(N D2, DN 2)) time required by straightforward eigenvector methods, although more sophisticated eigenvec- tor methods, such as the Lanczos algorithm, have running times comparable to EM.\n",
      "\n",
      "EM can be implemented in an online fashion, i.e., we can update our estimate of W as the\n",
      "\n",
      "data streams in.\n",
      "\n",
      "EM can handle missing data in a simple way (see Section 12.1.6).\n",
      "\n",
      "EM can be extended to handle mixtures of PPCA/ FA models.\n",
      "\n",
      "EM can be modiﬁed to variational EM or to variational Bayes EM to ﬁt more complex models.\n",
      "\n",
      "12.3\n",
      "\n",
      "Choosing the number of latent dimensions\n",
      "\n",
      "In Section 11.5, we discussed how to choose the number of components K in a mixture model. In this section, we discuss how to choose the number of latent dimensions L in a FA/PCA model.\n",
      "\n",
      "12.3.1 Model selection for FA/PPCA\n",
      "\n",
      "If we use a probabilistic model, we can in principle compute L∗ = argmaxL p(L|D). However, there are two problems with this. First, evaluating the marginal likelihood for LVMs is quite difficult. lower bounds (see Section 21.5), can be used (see also (Minka 2000a)). Alternatively, we can use the cross-validated likelihood as a performance measure, although this can be slow, since it requires ﬁtting each model F times, where F is the number of CV folds.\n",
      "\n",
      "In practice, simple approximations, such as BIC or variational\n",
      "\n",
      "The second issue is the need to search over a potentially large number of models. The usual approach is to perform exhaustive search over all candidate values of L. However, sometimes we can set the model to its maximal size, and then use a technique called automatic relevancy determination (Section 13.7), combined with EM, to automatically prune out irrelevant weights.\n",
      "\n",
      "12.3. Choosing the number of latent dimensions\n",
      "\n",
      "399\n",
      "\n",
      "number of points per cluster\n",
      "\n",
      "1\n",
      "\n",
      "intrinsic dimensionalities\n",
      "\n",
      "7\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "8 8 16 32 64 128\n",
      "\n",
      "1 1 1 1 1\n",
      "\n",
      "2\n",
      "\n",
      "6 7 7\n",
      "\n",
      "4 3 4 4\n",
      "\n",
      "2\n",
      "\n",
      "3 3 3\n",
      "\n",
      "1\n",
      "\n",
      "2 2 2\n",
      "\n",
      "2 2 2 2\n",
      "\n",
      "Figure 12.13 We show the estimated number of clusters, and their estimated dimensionalities, as a function of sample size. The VBEM algorithm found two different solutions when N = 8. Note that more clusters, with larger effective dimensionalities, are discovered as the sample sizes increases. Source: Table 4.1 of (Beal 2003). Used with kind permission of Matt Beal.\n",
      "\n",
      "This technique will be described in a supervised context in Chapter 13, but can be adapted to the (M)FA context as shown in (Bishop 1999; Ghahramani and Beal 2000).\n",
      "\n",
      "Figure 12.12 illustrates this approach applied to a mixture of FAs ﬁt to a small synthetic dataset. The ﬁgures visualize the weight matrices for each cluster, using Hinton diagrams, where where the size of the square is proportional to the value of the entry in the matrix.2 We see that many of them are sparse. Figure 12.13 shows that the degree of sparsity depends on the amount of training data, in accord with the Bayesian Occam’s razor. In particular, when the sample size is small, the method automatically prefers simpler models, but as the sample size gets sufficiently large, the method converges on the “correct” solution, which is one with 6 subspaces of dimensionality 1, 2, 2, 3, 4 and 7.\n",
      "\n",
      "Although the ARD/ EM method is elegant, it still needs to perform search over K. This is done using “birth” and “death” moves (Ghahramani and Beal 2000). An alternative approach is to perform stochastic sampling in the space of models. Traditional approaches, such as (Lopes and West 2004), are based on reversible jump MCMC, and also use birth and death moves. However, this can be slow and difficult to implement. More recent approaches use non-parametric priors, combined with Gibbs sampling, see e.g., (Paisley and Carin 2009).\n",
      "\n",
      "12.3.2 Model selection for PCA\n",
      "\n",
      "Since PCA is not a probabilistic model, we cannot use any of the methods described above. An obvious proxy for the likelihood is the reconstruction error:\n",
      "\n",
      "E(D, L) =\n",
      "\n",
      "1 |D|\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i∈D\n",
      "\n",
      "||xi − ˆxi||2\n",
      "\n",
      "(12.67)\n",
      "\n",
      "In the case of PCA, the reconstruction is given by by ˆxi = Wzi + μ, where zi = WT (xi − μ) and W and μ are estimated from Dtrain.\n",
      "\n",
      "2. Geoff Hinton is an English professor of computer science at the University of Toronto.\n",
      "\n",
      "400\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "train set reconstruction error\n",
      "\n",
      "test set reconstruction error\n",
      "\n",
      "60\n",
      "\n",
      "60\n",
      "\n",
      "50\n",
      "\n",
      "50\n",
      "\n",
      "40\n",
      "\n",
      "40\n",
      "\n",
      "e s m\n",
      "\n",
      "r\n",
      "\n",
      "30\n",
      "\n",
      "e s m\n",
      "\n",
      "r\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "num PCs\n",
      "\n",
      "num PCs\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 12.14 Reconstruction error on MNIST vs number of latent dimensions used by PCA. (a) Training set. (b) Test set. Figure generated by pcaOverfitDemo.\n",
      "\n",
      "Figure 12.14(a) plots E(Dtrain, L) vs L on the MNIST training data in Figure 12.6. We see that it drops off quite quickly, indicating that we can capture most of the empirical correlation of the pixels with a small number of factors, as illustrated qualitatively in Figure 12.6.\n",
      "\n",
      "Exercise 12.5 asks you to prove that the residual error from only using L terms is given by the\n",
      "\n",
      "sum of the discarded eigenvalues:\n",
      "\n",
      "E(Dtrain, L) =\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "λj\n",
      "\n",
      "(12.68)\n",
      "\n",
      "j=L+1\n",
      "\n",
      "Therefore an alternative to plotting the error is to plot the retained eigenvalues, in decreasing order. This is called a scree plot, because “the plot looks like the side of a mountain, and ’scree’ refers to the debris fallen from a mountain and lying at its base”.3 This will have the same shape as the residual error plot.\n",
      "\n",
      "A related quantity is the fraction of variance explained, deﬁned as\n",
      "\n",
      "F (Dtrain, L) =\n",
      "\n",
      "(cid:10)L (cid:10)Lmax\n",
      "\n",
      "j=1 λj j(cid:2)=1 λj(cid:2)\n",
      "\n",
      "(12.69)\n",
      "\n",
      "This captures the same information as the scree plot.\n",
      "\n",
      "Of course, if we use L = rank(X), we get zero reconstruction error on the training set. To avoid overﬁtting, it is natural to plot reconstruction error on the test set. This is shown in Figure 12.14(b). Here we see that the error continues to go down even as the model becomes more complex! Thus we do not get the usual U-shaped curve that we typically expect to see.\n",
      "\n",
      "What is going on? The problem is that PCA is not a proper generative model of the data. If you give it more latent dimensions, it will be able to It is merely a compression technique. approximate the test data more accurately. By contrast, a probabilistic model enjoys a Bayesian Occam’s razor effect (Section 5.3.1), in that it gets “punished” if it wastes probability mass on parts of the space where there is little data. This is illustrated in Figure 12.15, which plots the\n",
      "\n",
      "3. Quotation from http://janda.org/workshop/factoranalysis/SPSSrun/SPSS08.htm.\n",
      "\n",
      "12.3. Choosing the number of latent dimensions\n",
      "\n",
      "401\n",
      "\n",
      "2.2\n",
      "\n",
      "6 x 10\n",
      "\n",
      "train set negative loglik\n",
      "\n",
      "2.5\n",
      "\n",
      "6 x 10\n",
      "\n",
      "test set negative loglik\n",
      "\n",
      "2.1\n",
      "\n",
      "2.4\n",
      "\n",
      "2\n",
      "\n",
      "2.3\n",
      "\n",
      "1.9\n",
      "\n",
      "k\n",
      "\n",
      "i l\n",
      "\n",
      "g o g e n\n",
      "\n",
      "l\n",
      "\n",
      "1.8\n",
      "\n",
      "1.7\n",
      "\n",
      "k\n",
      "\n",
      "i l\n",
      "\n",
      "g o g e n\n",
      "\n",
      "l\n",
      "\n",
      "2.2\n",
      "\n",
      "2.1\n",
      "\n",
      "1.6\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "1.4\n",
      "\n",
      "1.9\n",
      "\n",
      "1.3\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "1.8\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "num PCs\n",
      "\n",
      "num PCs\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 12.15 Negative log likelihood on MNIST vs number of latent dimensions used by PPCA. (a) Training set. (b) Test set. Figure generated by pcaOverfitDemo.\n",
      "\n",
      "negative log likelihood, computed using PPCA, vs L. Here, on the test set, we see the usual U-shaped curve.\n",
      "\n",
      "K in the K-means algorithm vs using a GMM.\n",
      "\n",
      "These results are analogous to those in Section 11.5.2, where we discussed the issue of choosing\n",
      "\n",
      "12.3.2.1\n",
      "\n",
      "Proﬁle likelihood\n",
      "\n",
      "Although there is no U-shape, there is sometimes a “regime change” in the plots, from relatively large errors to relatively small. One way to automate the detection of this is described in (Zhu and Ghodsi 2006). The idea is this. Let λk be some measure of the error incurred by a model of size k, such that λ1 ≥ λ2 ≥ · · · ≥λ Lmax . In PCA, these are the eigenvalues, but the method can also be applied to K-means. Now consider partitioning these values into two groups, depending on whether k < L or k > L, where L is some threshold which we will determine. To measure the quality of L, we will use a simple change-point model, where λk ∼ N (μ1, σ2) if k ≤ L, and λk ∼ N (μ2, σ2) if k > L. (It is important that σ2 be the same in both models, to prevent overﬁtting in the case where one regime has less data than the other.) Within each of the two regimes, we assume the λk are iid, which is obviously incorrect, but is adequate for our present purposes. We can ﬁt this model for each L = 1 : Lmax by partitioning the data and computing the MLEs, using a pooled estimate of the variance:\n",
      "\n",
      "μ1(L) =\n",
      "\n",
      "σ2(L) =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "k≤L λk L k≤L(λk − μ1(L))2 + N\n",
      "\n",
      ", μ2(L) =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "k>L λk N − L (cid:10)\n",
      "\n",
      "k>L(λk − μ2(L))2\n",
      "\n",
      "(12.70)\n",
      "\n",
      "(12.71)\n",
      "\n",
      "We can then evaluate the proﬁle log likelihood\n",
      "\n",
      "(cid:6)(L) =\n",
      "\n",
      "L(cid:2)\n",
      "\n",
      "log N (λk|μ1(L), σ2(L)) +\n",
      "\n",
      "K(cid:2)\n",
      "\n",
      "log N (λk|μ2(L), σ2(L))\n",
      "\n",
      "(12.72)\n",
      "\n",
      "k=L+1 Finally, we choose L∗ = arg max (cid:6)(L). This is illustrated in Figure 12.16. On the left, we plot the scree plot, which has the same shape as in Figure 12.14(a). On the right, we plot the proﬁle\n",
      "\n",
      "k=1\n",
      "\n",
      "402\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "4\n",
      "\n",
      "5 x 10\n",
      "\n",
      "scree plot\n",
      "\n",
      "−5450\n",
      "\n",
      "3.5\n",
      "\n",
      "−5500\n",
      "\n",
      "3\n",
      "\n",
      "e u a v n e g e\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "2.5\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "d o o h\n",
      "\n",
      "i l\n",
      "\n",
      "e k\n",
      "\n",
      "i l\n",
      "\n",
      "g o\n",
      "\n",
      "l\n",
      "\n",
      "e\n",
      "\n",
      "l i f\n",
      "\n",
      "o r p\n",
      "\n",
      "−5550\n",
      "\n",
      "−5600\n",
      "\n",
      "−5650\n",
      "\n",
      "1\n",
      "\n",
      "−5700\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "−5750\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "num PCs\n",
      "\n",
      "num PCs\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 12.16 (a) Scree plot for training set, corresponding to Figure 12.14(a). generated by pcaOverfitDemo.\n",
      "\n",
      "(b) Proﬁle likelihood. Figure\n",
      "\n",
      "likelihood. Rather miraculously, we see a fairly well-determined peak.\n",
      "\n",
      "12.4\n",
      "\n",
      "PCA for categorical data\n",
      "\n",
      "In this section, we consider extending the factor analysis model to the case where the observed data is categorical rather than real-valued. That is, the data has the form yij ∈ {1, . . . , C}, where j = 1 :R is the number of observed response variables. We assume each yij is generated from a latent variable zi ∈ RL, with a Gaussian prior, which is passed through the softmax function as follows:\n",
      "\n",
      "p(zi) =N (0, I)\n",
      "\n",
      "(12.73)\n",
      "\n",
      "p(yi|zi, θ) =\n",
      "\n",
      "R(cid:27)\n",
      "\n",
      "Cat(yir|S(WT\n",
      "\n",
      "r zi + w0r))\n",
      "\n",
      "(12.74)\n",
      "\n",
      "r=1\n",
      "\n",
      "where Wr ∈ RL×M is the factor loading matrix for response j, and w0r ∈ RM is the offset term for response r, and θ = (Wr, w0r)R r=1. (We need an explicit offset term, since clamping one element of zi to 1 can cause problems when computing the posterior covariance.) As in factor analysis, we have deﬁned the prior mean to be m0 = 0 and the prior covariance V0 = I, since we can capture non-zero mean by changing w0j and non-identity covariance by changing Wr. We will call this categorical PCA. See Chapter 27 for a discussion of related models.\n",
      "\n",
      "It is interesting to study what kinds of distributions we can induce on the observed variables by varying the parameters. For simplicity, we assume there is a single ternary response variable, so yi lives in the 3d probability simplex. Figure 12.17 shows what happens when we vary the parameters of the prior, m0 and V0, which is equivalent to varying the parameters of the likelihood, W1 and w01. We see that this can deﬁne fairly complex distributions over the simplex. This induced distribution is known as the logistic normal distribution (Aitchison 1982).\n",
      "\n",
      "We can ﬁt this model to data using a modiﬁed version of EM. The basic idea is to infer a Gaussian approximation to the posterior p(zi|yi, θ) in the E step, and then to maximize θ in the M step. The details for the multiclass case, can be found in (Khan et al. 2010) (see\n",
      "\n",
      "12.4. PCA for categorical data\n",
      "\n",
      "403\n",
      "\n",
      "Figure 12.17 (a) Diagonal covariance and non-zero mean. (c) Positive correlation between states 1 and 2. Source: Figure 1 of (Blei and Lafferty 2007). Used with kind permission of David Blei.\n",
      "\n",
      "Some examples of the logistic normal distribution deﬁned on the 3d simplex.\n",
      "\n",
      "(b) Negative correlation between states 1 and 2.\n",
      "\n",
      "2\n",
      "\n",
      "20\n",
      "\n",
      "1.5\n",
      "\n",
      "40\n",
      "\n",
      "1\n",
      "\n",
      "60\n",
      "\n",
      "0.5\n",
      "\n",
      "80\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "−0.5\n",
      "\n",
      "120\n",
      "\n",
      "−1\n",
      "\n",
      "140\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 12.18 Left: 150 synthetic 16 dimensional bit vectors. Right: the 2d embedding learned by binary PCA, using variational EM. We have color coded points by the identity of the true “prototype” that generated them. Figure generated by binaryFaDemoTipping.\n",
      "\n",
      "also Section 21.8.1.1). The details for the binary case for the the sigmoid link can be found in Exercise 21.9, and for the probit link in Exercise 21.10.\n",
      "\n",
      "is to visualize high dimensional categorical data. Fig- ure 12.18(a) shows a simple example where we have 150 6-dimensional bit vectors. It is clear that each sample is just a noisy copy of one of three binary prototypes. We ﬁt a 2d catFA to this model, yielding approximate MLEs ˆθ. In Figure 12.18(b), we plot E . We see that there are three distinct clusters, as is to be expected.\n",
      "\n",
      "One application of such a model\n",
      "\n",
      "+\n",
      "\n",
      "zi|xi, ˆθ\n",
      "\n",
      ",\n",
      "\n",
      "In (Khan et al. 2010), we show that this model outperforms ﬁnite mixture models on the task of imputing missing entries in design matrices consisting of real and categorical data. This is useful for analysing social science survey data, which often has missing data and variables of mixed type.\n",
      "\n",
      "404\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "wy W x\n",
      "\n",
      "W x W y\n",
      "\n",
      "zi\n",
      "\n",
      "zx i\n",
      "\n",
      "zs i\n",
      "\n",
      "yi\n",
      "\n",
      "xi\n",
      "\n",
      "N\n",
      "\n",
      "Bx\n",
      "\n",
      "xi\n",
      "\n",
      "yi\n",
      "\n",
      "N\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "W x W y\n",
      "\n",
      "zx i\n",
      "\n",
      "zs i\n",
      "\n",
      "zy i\n",
      "\n",
      "Bx\n",
      "\n",
      "xi\n",
      "\n",
      "yi\n",
      "\n",
      "N\n",
      "\n",
      "By\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 12.19 Gaussian latent factor models for paired data. (c) Canonical correlation analysis.\n",
      "\n",
      "(a) Supervised PCA. (b) Partial least squares.\n",
      "\n",
      "12.5\n",
      "\n",
      "PCA for paired and multi-view data\n",
      "\n",
      "It is common to have a pair of related datasets, e.g., gene expression and gene copy number, or movie ratings by users and movie reviews. It is natural to want to combine these together into a low-dimensional embedding. This is an example of data fusion. In some cases, we might want to predict one element of the pair, say xi1, from the other one, xi2, via the low-dimensional “bottleneck”.\n",
      "\n",
      "Below we discuss various latent Gaussian models for these tasks, following the presentation of (Virtanen 2010). The models easily generalize from pairs to sets of data, xim, for m = 1 :M . We focus on the case where xim ∈ RDm . In this case, the joint distribution is multivariate Gaussian, so we can easily ﬁt the models using EM, or Gibbs sampling.\n",
      "\n",
      "We can generalize the models to handle discrete and count data by using the exponential family as a response distribution instead of the Gaussian, as we explain in Section 27.2.2. However, this will require the use of approximate inference in the E step (or an analogous modiﬁcation to MCMC).\n",
      "\n",
      "12.5. PCA for paired and multi-view data\n",
      "\n",
      "405\n",
      "\n",
      "12.5.1\n",
      "\n",
      "Supervised PCA (latent factor regression)\n",
      "\n",
      "Consider the following model, illustrated in Figure 12.19(a):\n",
      "\n",
      "p(zi) =N (0, IL)\n",
      "\n",
      "(12.75)\n",
      "\n",
      "p(yi|zi) =N (wT y zi + μy, σ2 y) p(xi|zi) =N (Wxzi + μx, σ2 xID)\n",
      "\n",
      "(12.76)\n",
      "\n",
      "(12.77)\n",
      "\n",
      "In (Yu et al. 2006), this is called supervised PCA. In (West 2003), this is called Bayesian factor regression. This model is like PCA, except that the target variable yi is taken into account when learning the low dimensional embedding. Since the model is jointly Gaussian, we have\n",
      "\n",
      "yi|xi ∼ N (xT\n",
      "\n",
      "i w, σ2\n",
      "\n",
      "y + wT\n",
      "\n",
      "y Cwy)\n",
      "\n",
      "(12.78)\n",
      "\n",
      "where w = Ψ−1WxCwy, Ψ = σ2 joint density model of (yi, xi), we can infer the implied conditional distribution.\n",
      "\n",
      "and let X = RVT be the SVD of X, where VT V = I and RT R = Σ2 = diag(σ2 the squared singular values. Then one can show (West 2003) that\n",
      "\n",
      "We now show an interesting connection to Zellner’s g-prior. Suppose p(wy) = N (0, 1\n",
      "\n",
      "xID, and C−1 = I + WT\n",
      "\n",
      "x Ψ−1Wx. So although this is a\n",
      "\n",
      "g Σ2), j ) contains\n",
      "\n",
      "p(w) = N (0, gV−T Σ−2V−1) = N (0, g(XT X)−1)\n",
      "\n",
      "(12.79)\n",
      "\n",
      "So the dependence of the prior for w on X arises from the fact that w is derived indirectly by a joint model of X and y.\n",
      "\n",
      "The above discussion focussed on regression. (Guo 2009) generalizes CCA to the exponential family, which is more appropriate if xi and/or yi are discrete. Although we can no longer compute the conditional p(yi|xi, θ) in closed form, the model has a similar interpretation to the regression case, namely that we are predicting the response via a latent “bottleneck”.\n",
      "\n",
      "In particular, we might want to ﬁnd an encoding distribution p(z|x) such that we minimize\n",
      "\n",
      "The basic idea of compressing xi to predict yi can be formulated using information theory.\n",
      "\n",
      "I (X; Z) − βI (X; Y )\n",
      "\n",
      "(12.80)\n",
      "\n",
      "where β ≥ 0 is some parameter controlling the tradeoff between compression and predictive accuracy. This is known as the information bottleneck (Tishby et al. 1999). Often Z is taken to be discrete, as in clustering. However, in the Gaussian case, IB is closely related to CCA (Chechik et al. 2005).\n",
      "\n",
      "We can easily generalize CCA to the case where yi is a vector of responses to be predicted, as in multi-label classiﬁcation. (Ma et al. 2008; Williamson and Ghahramani 2008) used this model to perform collaborative ﬁltering, where the goal is to predict yij ∈ {1, . . . , 5}, the rating person i gives to movie j, where the “side information” xi takes the form of a list of i’s friends. The intuition behind this approach is that knowledge of who your friends are, as well as the ratings of all other users, should help predict which movies you will like. In general, any setting where the tasks are correlated could beneﬁt from CCA. Once we adopt a probabilistic view, various extensions are straightforward. For example, we can easily generalize to the semi-supervised case, where we do not observe yi for all i (Yu et al. 2006).\n",
      "\n",
      "406\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "12.5.1.1\n",
      "\n",
      "Discriminative supervised PCA\n",
      "\n",
      "One problem with this model is that it puts as much weight on predicting the inputs xi as the outputs yi. This can be partially alleviated by using a weighted objective of the following form (Rish et al. 2008):\n",
      "\n",
      "(cid:6)(θ) =\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "p(yi|ηiy)αy p(xi|ηix)αx\n",
      "\n",
      "(12.81)\n",
      "\n",
      "i\n",
      "\n",
      "where the αm control the relative importance of the data sources, and ηim = Wmzi. For Gaussian data, we can see that αm just controls the noise variance:\n",
      "\n",
      "(cid:6)(θ) ∝\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "i\n",
      "\n",
      "exp(− 1 2\n",
      "\n",
      "αx||xT\n",
      "\n",
      "i − ηix||2) exp(− 1 2\n",
      "\n",
      "αy||yT\n",
      "\n",
      "i − ηiy||2)\n",
      "\n",
      "(12.82)\n",
      "\n",
      "This interpretation holds more generally for the exponential family. Note, however, that it is hard to estimate the αm parameters, because changing them changes the normalization constant of the likelihood. We give an alternative approach to weighting y more heavily below.\n",
      "\n",
      "12.5.2\n",
      "\n",
      "Partial least squares\n",
      "\n",
      "The technique of partial least squares (PLS) (Gustafsson 2001; Sun et al. 2009) is an asym- metric or more “discriminative” form of supervised PCA. The key idea is to allow some of the (co)variance in the input features to be explained by its own subspace, zx i , and to let the rest of the subspace, zs\n",
      "\n",
      "i , be shared between input and output. The model has the form\n",
      "\n",
      "p(zi) =N (zs p(yi|zi) =N (Wyzs p(xi|zi) =N (Wxzs\n",
      "\n",
      "i |0, ILs )N (zx\n",
      "\n",
      "i + μy, σ2IDy ) i + Bxzx\n",
      "\n",
      "i + μx, σ2IDx )\n",
      "\n",
      "i |0, ILx )\n",
      "\n",
      "(12.83)\n",
      "\n",
      "(12.84)\n",
      "\n",
      "(12.85)\n",
      "\n",
      "See Figure 12.19(b). The corresponding induced distribution on the visible variables has the form\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(vi|θ) =\n",
      "\n",
      "N (vi|Wzi + μ, σ2I)N (zi|0, I)dzi = N (vi|μ, WWT + σ2I)\n",
      "\n",
      "(12.86)\n",
      "\n",
      "where vi = (xi; yi), μ = (μy; μx) and (cid:8)\n",
      "\n",
      "WWT =\n",
      "\n",
      "W =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "0 Wy Wx Bx WyWT y x WxWT WxWT\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "WxWT x x + BxBT x\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(12.87)\n",
      "\n",
      "(12.88)\n",
      "\n",
      "We should choose L large enough so that the shared subspace does not capture covariate- speciﬁc variation.\n",
      "\n",
      "This model can be easily generalized to discrete data using the exponential family (Virtanen\n",
      "\n",
      "2010).\n",
      "\n",
      "12.6.\n",
      "\n",
      "Independent Component Analysis (ICA)\n",
      "\n",
      "407\n",
      "\n",
      "12.5.3\n",
      "\n",
      "Canonical correlation analysis\n",
      "\n",
      "Canonical correlation analysis or CCA is like a symmetric unsupervised version of PLS: it allows each view to have its own “private” subspace, but there is also a shared subspace. If we have two observed variables, xi and yi, then we have three latent variables, zs i ∈ RL0 which is shared, zx i ∈ RLy which are private. We can write the model as follows (Bach and Jordan 2005):\n",
      "\n",
      "i ∈ RLx and zy\n",
      "\n",
      "p(zi) =N (zs\n",
      "\n",
      "i |0, ILs )N (zx\n",
      "\n",
      "i |0, ILx )N (zy\n",
      "\n",
      "i |0, ILy )\n",
      "\n",
      "(12.89)\n",
      "\n",
      "p(xi|zi) =N (xi|Bxzx p(yi|zi) =N (yi|Byzy\n",
      "\n",
      "i + Wxzs i + Wyzs\n",
      "\n",
      "i + μx, σ2IDx ) i + μy, σ2IDy )\n",
      "\n",
      "(12.90)\n",
      "\n",
      "(12.91)\n",
      "\n",
      "See Figure 12.19(c). The corresponding observed joint distribution has the form\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(vi|θ) =\n",
      "\n",
      "N (vi|Wzi + μ, σ2I)N (zi|0, I)dzi = N (vi|μ, WWT + σ2ID)\n",
      "\n",
      "(12.92)\n",
      "\n",
      "where\n",
      "\n",
      "W =\n",
      "\n",
      "WWT =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "Wx Bx Wy WxWT\n",
      "\n",
      "0 0 By x + BxBT x WyWT y\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "WyWT\n",
      "\n",
      "WxWT y y + ByBT y\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(12.93)\n",
      "\n",
      "(12.94)\n",
      "\n",
      "One can compute the MLE for this model using EM. (Bach and Jordan 2005) show that the resulting MLE is equivalent (up to rotation and scaling) to the classical, non-probabilistic view. However, the advantages of the probabilistic view are many: we can trivially generalize to M > 2 observed variables; we can create mixtures of CCA (Viinikanoja et al. 2010); we can create sparse versions of CCA using ARD (Archambeau and Bach 2008); we can generalize to the exponential family (Klami et al. 2010); we can perform Bayesian inference of the parameters (Wang 2007; Klami and Kaski 2008); we can handle non-parametric sparsity-promoting priors for W and B (Rai and Daume 2009); and so on.\n",
      "\n",
      "12.6\n",
      "\n",
      "Independent Component Analysis (ICA)\n",
      "\n",
      "Consider the following situation. You are in a crowded room and many people are speaking. Your ears essentially act as two microphones, which are listening to a linear combination of the different speech signals in the room. Your goal is to deconvolve the mixed signals into their constituent parts. This is known as the cocktail party problem, and is an example of blind signal separation (BSS), or blind source separation, where “blind” means we know “nothing” about the source of the signals. Besides the obvious applications to acoustic signal processing, this problem also arises when analysing EEG and MEG signals, ﬁnancial data, and any other dataset (not necessarily temporal) where latent sources or factors get mixed together in a linear way.\n",
      "\n",
      "at “time” t, and zt ∈ RL be the vector of source signals. We assume that\n",
      "\n",
      "We can formalize the problem as follows. Let xt ∈ RD be the observed signal at the sensors\n",
      "\n",
      "xt = Wzt + (cid:7)t\n",
      "\n",
      "(12.95)\n",
      "\n",
      "408\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "truth\n",
      "\n",
      "observed signals\n",
      "\n",
      "2\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "2\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "−10\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "PCA estimate\n",
      "\n",
      "ICA estimate\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 12.20 Illustration of ICA applied to 500 iid samples of a 4d source signal. (b) Observations. (c) PCA estimate. (d) ICA estimate. Figure generated by icaDemo, written by Aapo Hyvarinen.\n",
      "\n",
      "(a) Latent signals.\n",
      "\n",
      "where W is an D × L matrix, and (cid:7)t ∼ N (0, Ψ). In this section, we treat each time point as an independent observation, i.e., we do not model temporal correlation (so we could replace the t index with i, but we stick with t to be consistent with much of the ICA literature). The goal is to infer the source signals, p(zt|xt, θ), as illustrated in Figure 12.20. In this context, W is called the mixing matrix. If L = D (number of sources = number of sensors), it will be a square matrix. Often we will assume the noise level, |Ψ|, is zero, for simplicity.\n",
      "\n",
      "So far, the model is identical to factor analysis (or PCA if there is no noise, except we don’t in general require orthogonality of W). However, we will use a different prior for p(zt). In PCA, we assume each source is independent, and has a Gaussian distribution\n",
      "\n",
      "p(zt) =\n",
      "\n",
      "L(cid:27)\n",
      "\n",
      "N (ztj|0, 1)\n",
      "\n",
      "(12.96)\n",
      "\n",
      "j=1\n",
      "\n",
      "We will now relax this Gaussian assumption and let the source distributions be any non-Gaussian\n",
      "\n",
      "12.6.\n",
      "\n",
      "Independent Component Analysis (ICA)\n",
      "\n",
      "409\n",
      "\n",
      "uniform data\n",
      "\n",
      "uniform data after linear mixing\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "3\n",
      "\n",
      "PCA applied to mixed data from uniform source\n",
      "\n",
      "3\n",
      "\n",
      "ICA applied to mixed data from uniform source\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 12.21 distribution. icaDemoUniform, written by Aapo Hyvarinen.\n",
      "\n",
      "(a) Latent signals.\n",
      "\n",
      "Illustration of ICA and PCA applied to 100 iid samples of a 2d source signal with a uniform (d) ICA estimate. Figure generated by\n",
      "\n",
      "(b) Observations.\n",
      "\n",
      "(c) PCA estimate.\n",
      "\n",
      "distribution\n",
      "\n",
      "p(zt) =\n",
      "\n",
      "L(cid:27)\n",
      "\n",
      "pj(ztj)\n",
      "\n",
      "(12.97)\n",
      "\n",
      "j=1\n",
      "\n",
      "Without loss of generality, we can constrain the variance of the source distributions to be 1, because any other variance can be modelled by scaling the rows of W appropriately. The resulting model is known as independent component analysis or ICA.\n",
      "\n",
      "The reason the Gaussian distribution is disallowed as a source prior in ICA is that it does not permit unique recovery of the sources, as illustrated in Figure 12.20(c). This is because the PCA likelihood is invariant to any orthogonal transformation of the sources zt and mixing matrix W. PCA can recover the best linear subspace in which the signals lie, but cannot uniquely recover the signals themselves.\n",
      "\n",
      "410\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "To illustrate this, suppose we have two independent sources with uniform distributions, as\n",
      "\n",
      "shown in Figure 12.21(a). Now suppose we have the following mixing matrix\n",
      "\n",
      "W =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "2 3 2 1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(12.98)\n",
      "\n",
      "Then we observe the data shown in Figure 12.21(b) (assuming no noise). If we apply PCA followed by scaling to this, we get the result in Figure 12.21(c). This corresponds to a whitening of the data. To uniquely recover the sources, we need to perform an additional rotation. The trouble is, there is no information in the symmetric Gaussian posterior to tell us which angle to rotate by. In a sense, PCA solves “half” of the problem, since it identiﬁes the linear subspace; all (Hence we see that ICA is not that ICA has to do is then to identify the appropriate rotation. that different from methods such as varimax, which seek good rotations of the latent factors to enhance interpretability.)\n",
      "\n",
      "Figure 12.21(d) shows that ICA can recover the source, up to a permutation of the indices and ICA requires that W is square and hence invertible. In the non-square possible sign change. case (e.g., where we have more sources than sensors), we cannot uniquely recover the true signal, but we can compute the posterior p(zt|xt, ˆW), which represents our beliefs about the source. In both cases, we need to estimate W as well as the source distributions pj. We discuss how to do this below.\n",
      "\n",
      "12.6.1 Maximum likelihood estimation\n",
      "\n",
      "In this section, we discuss ways to estimate square mixing matrices W for the noise-free ICA model. As usual, we will assume that the observations have been centered; hence we can also assume z is zero-mean. In addition, we assume the observations have been whitened, which can be done with PCA.\n",
      "\n",
      "If the data is centered and whitened, we have E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xxT\n",
      "\n",
      "= I. But in the noise free case, we\n",
      "\n",
      "also have\n",
      "\n",
      "cov [x] = E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xxT\n",
      "\n",
      "= WE\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "zzT\n",
      "\n",
      "WT = WWT\n",
      "\n",
      "(12.99)\n",
      "\n",
      "Hence we see that W must be orthogonal. This reduces the number of parameters we have to estimate from D2 to D(D − 1)/2. It will also simplify the math and the algorithms.\n",
      "\n",
      "Let V = W−1; these are often called the recognition weights, as opposed to W, which are\n",
      "\n",
      "the generative weights.4\n",
      "\n",
      "Since x = Wz, we have, from Equation 2.89,\n",
      "\n",
      "px(Wzt) = pz(zt)| det(W−1)| = pz(Vxt)| det(V)|\n",
      "\n",
      "(12.100)\n",
      "\n",
      "Hence we can write the log-likelihood, assuming T iid samples, as follows:\n",
      "\n",
      "1 T\n",
      "\n",
      "log p(D|V) = log | det(V)| +\n",
      "\n",
      "1 T\n",
      "\n",
      "L(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "T(cid:2)\n",
      "\n",
      "t=1\n",
      "\n",
      "log pj(vT\n",
      "\n",
      "j xt)\n",
      "\n",
      "(12.101)\n",
      "\n",
      "4. In the literature, it is common to denote the generative weights by A and the recognition weights by W, but we are trying to be consistent with the notation used earlier in this chapter.\n",
      "\n",
      "12.6.\n",
      "\n",
      "Independent Component Analysis (ICA)\n",
      "\n",
      "411\n",
      "\n",
      "where vj is the j’th row of V. Since we are constraining V to be orthogonal, the ﬁrst term is a constant, so we can drop it. We can also replace the average over the data with an expectation operator to get the following objective\n",
      "\n",
      "NLL(V) =\n",
      "\n",
      "L(cid:2)\n",
      "\n",
      "E [Gj(zj)]\n",
      "\n",
      "(12.102)\n",
      "\n",
      "j=1\n",
      "\n",
      "where zj = vT j x and Gj(z) (cid:2) − log pj(z). We want to minimize this subject to the constraint that the rows of V are orthogonal. We also want them to be unit norm, since this ensures = ||vj||2), which is that the variance of the factors is unity (since, with whitened data, E necessary to ﬁx the scale of the weights. In otherwords, V should be an orthonormal matrix. It is straightforward to derive a gradient descent algorithm to ﬁt this model; however, it is rather slow. One can also derive a faster algorithm that follows the natural gradient; see e.g., (MacKay 2003, ch 34) for details. A popular alternative is to use an approximate Newton method, which we discuss in Section 12.6.2. Another approach is to use EM, which we discuss in Section 12.6.3.\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "vT j x\n",
      "\n",
      "12.6.2\n",
      "\n",
      "The FastICA algorithm\n",
      "\n",
      "We now describe the fast ICA algorithm, based on (Hyvarinen and Oja 2000), which we will show is an approximate Newton method for ﬁtting ICA models.\n",
      "\n",
      "For simplicity of presentation, we initially assume there is only one latent factor. In addition, we initially assume all source distributions are known and are the same, so we can just write G(z) =− log p(z). Let g(z) = d dz G(z). The constrained objective, and its gradient and Hessian, are given by (cid:31)\n",
      "\n",
      "f (v) =E ∇f (v) =E H(v) =E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "G(vT x) xg(vT x) xxT g(cid:4)(vT x)\n",
      "\n",
      "+ λ(1 − vT v) − βv\n",
      "\n",
      "− βI\n",
      "\n",
      "(12.103)\n",
      "\n",
      "(12.104)\n",
      "\n",
      "(12.105)\n",
      "\n",
      "where β = 2λ is a Lagrange multiplier. Let us make the approximation (cid:31)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xxT g(cid:4)(vT x)\n",
      "\n",
      "≈ E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xxT\n",
      "\n",
      "E\n",
      "\n",
      "g(cid:4)(vT x)\n",
      "\n",
      "= E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "g(cid:4)(vT x)\n",
      "\n",
      "(12.106)\n",
      "\n",
      "This makes the Hessian very easy to invert, giving rise to the following Newton update:\n",
      "\n",
      "v∗ (cid:2) v −\n",
      "\n",
      "E\n",
      "\n",
      "E [g(cid:4)(vT x)] − β\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xg(vT x)\n",
      "\n",
      "− βv\n",
      "\n",
      "(12.107)\n",
      "\n",
      "One can rewrite this in the following way\n",
      "\n",
      "v∗ (cid:2) E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "xg(vT x)\n",
      "\n",
      "− E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "g(cid:4)(vT x)\n",
      "\n",
      "v\n",
      "\n",
      "(12.108)\n",
      "\n",
      "(In practice, the expectations can be replaced by Monte Carlo estimates from the training set, which gives an efficient online learning algorithm.) After performing this update, one should project back onto the constraint surface using\n",
      "\n",
      "vnew (cid:2)\n",
      "\n",
      "v∗ ||v∗||\n",
      "\n",
      "(12.109)\n",
      "\n",
      "412\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "0.5\n",
      "\n",
      "Gaussian\n",
      "\n",
      "0.45\n",
      "\n",
      "3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.35\n",
      "\n",
      "2\n",
      "\n",
      "0.3\n",
      "\n",
      "1\n",
      "\n",
      "0.25\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "−1\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "−2\n",
      "\n",
      "0.05\n",
      "\n",
      "−3\n",
      "\n",
      "0 −4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Laplace\n",
      "\n",
      "Uniform\n",
      "\n",
      "10\n",
      "\n",
      "8\n",
      "\n",
      "1.5\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      "4\n",
      "\n",
      "0.5\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−0.5\n",
      "\n",
      "−4\n",
      "\n",
      "−1\n",
      "\n",
      "−6\n",
      "\n",
      "−1.5\n",
      "\n",
      "−8\n",
      "\n",
      "−10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 12.22 Illustration of Gaussian, sub-Gaussian (uniform) and super-Gaussian (Laplace) distributions in 1d and 2d. Figure generated by subSuperGaussPlot, written by Kevin Swersky.\n",
      "\n",
      "(Due to the sign ambiguity of v, the values of v One iterates this algorithm until convergence. may not converge, but the direction deﬁned by this vector should converge, so one can assess convergence by monitoring |vT vnew|, which should approach 1.)\n",
      "\n",
      "Since the objective is not convex, there are multiple local optima. We can use this fact to learn multiple different weight vectors or features. We can either learn the features sequentially and then project out the part of vj that lies in the subspace deﬁned by earlier features, or we can learn them in parallel, and orthogonalize V in parallel. This latter approach is usually preferred, since, unlike PCA, the features are not ordered in any way. So the ﬁrst feature is not “more important” than the second, and hence it is better to treat them symmetrically.\n",
      "\n",
      "12.6.\n",
      "\n",
      "Independent Component Analysis (ICA)\n",
      "\n",
      "413\n",
      "\n",
      "12.6.2.1 Modeling the source densities\n",
      "\n",
      "So far, we have assumed that G(z) =− log p(z) is known. What kinds of models might be reasonable as signal priors? We know that using Gaussians (which correspond to quadratic functions for G) won’t work. So we want some kind of non-Gaussian distribution. In general, there are several kinds of non-Gaussian distributions, such as the following:\n",
      "\n",
      "Super-Gaussian distributions These are distributions which have a big spike at the mean, and hence (in order to ensure unit variance) have heavy tails. The Laplace distribution is a classic example. See Figure 12.22. Formally, we say a distribution is super-Gaussian or leptokurtic (“lepto” coming from the Greek for “thin”) if kurt(z) > 0, where kurt(z) is the kurtosis of the distribution, deﬁned by\n",
      "\n",
      "kurt(z) (cid:2)\n",
      "\n",
      "μ4 σ4\n",
      "\n",
      "− 3\n",
      "\n",
      "(12.110)\n",
      "\n",
      "where σ is the standard deviation, and μk is the k’th central moment, or moment about the mean:\n",
      "\n",
      "(12.111) (So μ1 = μ is the mean, and μ2 = σ2 is the variance.) It is conventional to subtract 3 in the deﬁnition of kurtosis to make the kurtosis of a Gaussian variable equal to zero.\n",
      "\n",
      "μk (cid:2) E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(X − E [X])k\n",
      "\n",
      "Sub-Gaussian distributions A sub-Gaussian or platykurtic (“platy” coming from the Greek for “broad”) distribution has negative kurtosis. These are distributions which are much ﬂatter than a Gaussian. The uniform distribution is a classic example. See Figure 12.22.\n",
      "\n",
      "Skewed distributions Another way to “be non-Gaussian” is to be asymmetric. One measure\n",
      "\n",
      "of this is skewness, deﬁned by μ3 σ3\n",
      "\n",
      "skew(z) (cid:2)\n",
      "\n",
      "(12.112)\n",
      "\n",
      "An example of a (right) skewed distribution is the gamma distribution (see Figure 2.9).\n",
      "\n",
      "When one looks at the empirical distribution of many natural signals, such as images and speech, when passed through certain linear ﬁlters, they tend to be very super-Gaussian. This result holds both for the kind of linear ﬁlters found in certain parts of the brain, such as the simple cells found in the primary visual cortex, as well as for the kinds of linear ﬁlters used in signal processing, such as wavelet transforms. One obvious choice for modeling natural signals with ICA is therefore the Laplace distribution. For mean zero and variance 1, this has a log pdf given by\n",
      "\n",
      "log p(z) = −\n",
      "\n",
      "√\n",
      "\n",
      "2|z| −log(\n",
      "\n",
      "√\n",
      "\n",
      "2)\n",
      "\n",
      "(12.113)\n",
      "\n",
      "Since the Laplace prior is not differentiable at the origin, it is more common to use other, smoother super-Gaussian distributions. One example is the logistic distribution. The corre- sponding log pdf, for the case where the mean is zero and the variance is 1 (so μ = 0 and s =\n",
      "\n",
      "log p(z) = −2 log cosh(\n",
      "\n",
      "√ 3 π ), is given by the following:\n",
      "\n",
      "2\n",
      "\n",
      "π √\n",
      "\n",
      "3\n",
      "\n",
      "z) − log\n",
      "\n",
      "4\n",
      "\n",
      "√\n",
      "\n",
      "π\n",
      "\n",
      "3\n",
      "\n",
      "(12.114)\n",
      "\n",
      "414\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "μ1k\n",
      "\n",
      "μDk\n",
      "\n",
      "π\n",
      "\n",
      "σ1k\n",
      "\n",
      "σDk\n",
      "\n",
      "qt1\n",
      "\n",
      "qtD\n",
      "\n",
      "zt1\n",
      "\n",
      "ztD\n",
      "\n",
      "W\n",
      "\n",
      "xt1\n",
      "\n",
      "xtD\n",
      "\n",
      "T\n",
      "\n",
      "ψ\n",
      "\n",
      "Figure 12.23 Modeling the source distributions using a mixture of univariate Gaussians (the independent factor analysis model of (Moulines et al. 1997; Attias 1999)).\n",
      "\n",
      "Various ways of estimating G(Z) = − log p(z) are discussed in the seminal paper (Pham and Garrat 1997). However, when ﬁtting ICA by maximum likelihood, it is not critical that the exact shape of the source distribution be known (although it is important to know whether it is sub z or G(z) = log cosh(z) or super Gaussian). Consequently, it is common to just use G(z) = instead of the more complex expressions above.\n",
      "\n",
      "√\n",
      "\n",
      "12.6.3\n",
      "\n",
      "Using EM\n",
      "\n",
      "An alternative to assuming a particular form for G(z), or equivalently for p(z), is to use a ﬂexible non-parametric density estimator, such as a mixture of (uni-variate) Gaussians:\n",
      "\n",
      "p(qj = k) =π k\n",
      "\n",
      "(12.115)\n",
      "\n",
      "p(zj|qj = k) =N (μj,k, σ2 p(x|z) =N (Wz, Ψ)\n",
      "\n",
      "j,k)\n",
      "\n",
      "(12.116) (12.117)\n",
      "\n",
      "This approach was proposed in (Moulines et al. 1997; Attias 1999), and the corresponding graph- ical model is shown in Figure 12.23.\n",
      "\n",
      "It is possible to derive an exact EM algorithm for this model. The key observation is that it is possible to compute E [zt|xt, θ] exactly by summing over all K L combinations of the qt variables, where K is the number of mixture components per source. (If this is too expensive, one can use a variational mean ﬁeld approximation (Attias 1999).) We can then estimate all the source distributions in parallel by ﬁtting a standard GMM to E [zt]. When the source GMMs are\n",
      "\n",
      "12.6.\n",
      "\n",
      "Independent Component Analysis (ICA)\n",
      "\n",
      "415\n",
      "\n",
      "known, we can compute the marginals pj(zj) very easily, using\n",
      "\n",
      "pj(zj) =\n",
      "\n",
      "K(cid:2)\n",
      "\n",
      "πj,kN (zj|μj,k, σ2\n",
      "\n",
      "j,k)\n",
      "\n",
      "(12.118)\n",
      "\n",
      "k=1\n",
      "\n",
      "Given the pj’s, we can then use an ICA algorithm to estimate W. Of course, these steps should be interleaved. The details can be found in (Attias 1999).\n",
      "\n",
      "12.6.4\n",
      "\n",
      "Other estimation principles *\n",
      "\n",
      "It is quite common to estimate the parameters of ICA models using methods that seem different to maximum likelihood. We will review some of these methods below, because they give additional insight into ICA. However, we will also see that these methods in fact are equivalent to maximum likelihood after all. Our presentation is based on (Hyvarinen and Oja 2000).\n",
      "\n",
      "12.6.4.1 Maximizing non-Gaussianity\n",
      "\n",
      "An early approach to ICA was to ﬁnd a matrix V such that the distribution z = Vx is as far from Gaussian as possible. (There is a related approach in statistics called projection pursuit.) One measure of non-Gaussianity is kurtosis, but this can be sensitive to outliers. Another measure is the negentropy, deﬁned as\n",
      "\n",
      "negentropy(z) (cid:2) H\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "N (μ, σ2)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "− H (z)\n",
      "\n",
      "(12.119)\n",
      "\n",
      "where μ = E [z] and σ2 = var [z]. Since the Gaussian is the maximum entropy distribution, this measure is always non-negative and becomes large for distributions that are highly non- Gaussian.\n",
      "\n",
      "We can deﬁne our objective as maximizing\n",
      "\n",
      "J(V) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "negentropy(zj) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "H\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "N (μj, σ2 j )\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "− H (zj)\n",
      "\n",
      "(12.120)\n",
      "\n",
      "j\n",
      "\n",
      "j\n",
      "\n",
      "where z = Vx. will be I independently of V, so the ﬁrst term is a constant. Hence\n",
      "\n",
      "J(V) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "−H (zj) + const =\n",
      "\n",
      "If we ﬁx V to be orthogonal, and if we whiten the data, the covariance of z\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "E [log p(zj)] + const\n",
      "\n",
      "(12.121)\n",
      "\n",
      "j\n",
      "\n",
      "j\n",
      "\n",
      "which we see is equal (up to a sign change, and irrelevant constants) to the log-likelihood in Equation 12.102.\n",
      "\n",
      "12.6.4.2 Minimizing mutual information\n",
      "\n",
      "One measure of dependence of a set of random variables is the multi-information:\n",
      "\n",
      "⎛\n",
      "\n",
      "⎞\n",
      "\n",
      "I(z) (cid:2) KL\n",
      "\n",
      "⎝p(z)||\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "p(zj)\n",
      "\n",
      "⎠ =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "H(zj) − H(z)\n",
      "\n",
      "(12.122)\n",
      "\n",
      "j\n",
      "\n",
      "j\n",
      "\n",
      "416\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "We would like to minimize this, since we are trying to ﬁnd independent components. Put another way, we want the best possible factored approximation to the joint distribution.\n",
      "\n",
      "Now since z = Vx, we have\n",
      "\n",
      "I(z) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "H(zj) − H(Vx)\n",
      "\n",
      "(12.123)\n",
      "\n",
      "j\n",
      "\n",
      "If we constrain V to be orthogonal, we can drop the last term, since then H(Vx) = H(x) (since multiplying by V does not change the shape of the distribution), and H(x) is a constant which is is solely determined by the empirical distribution. Hence we have I(z) = j H(zj). Minimizing this is equivalent to maximizing the negentropy, which is equivalent to maximum likelihood.\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "12.6.4.3 Maximizing mutual information (infomax)\n",
      "\n",
      "Instead of trying to minimize the mutual information between the components of z, let us imagine a neural network where x is the input and yj = φ(vT j x) + (cid:8) is the noisy output, where φ is some nonlinear scalar function, and (cid:8) ∼ N (0, 1). It seems reasonable to try to maximize the information ﬂow through this system, a principle known as infomax. (Bell and Sejnowski 1995). That is, we want to maximize the mutual information between y (the internal neural representation) and x (the observed input signal). We have I(x; y) = H(y) − H(y|x), where the latter term is constant if we assume the noise has constant variance. One can show that we can approximate the former term as follows\n",
      "\n",
      "H(y) =\n",
      "\n",
      "L(cid:2)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "log φ(cid:4)(vT\n",
      "\n",
      "j x)\n",
      "\n",
      "+ log | det(V)|\n",
      "\n",
      "(12.124)\n",
      "\n",
      "j=1\n",
      "\n",
      "where, as usual, we can drop the last term if V is orthogonal. If we deﬁne φ(z) to be a cdf, then φ(cid:4)(z) is its pdf, and the above expression is equivalent to the log likelihood. In particular, if we use a logistic nonlinearity, φ(z) = sigm(z), then the corresponding pdf is the logistic distribution, and log φ(cid:4)(z) = log cosh(z) (ignoring irrelevant constants). Thus we see that infomax is equivalent to maximum likelihood.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 12.1 M step for FA For the FA model, show that the MLE in the M step for W is given by Equation 12.23.\n",
      "\n",
      "Exercise 12.2 MAP estimation for the FA model Derive the M step for the FA model using conjugate priors for the parameters.\n",
      "\n",
      "Exercise 12.3 Heuristic for assessing applicability of PCA (Source: λd > 0. Explain why the variance of the evalues, σ2 = 1 d or not PCA would be useful for analysing the data (the higher the value of σ2 the more useful PCA).\n",
      "\n",
      "(Press 2005, Q9.8).). Let the empirical covariance matrix Σ have eigenvalues λ1 ≥ λ2 ≥ · · · ≥ i=1(λi − λ)2 is a good measure of whether\n",
      "\n",
      "(cid:2)d\n",
      "\n",
      "12.6.\n",
      "\n",
      "Independent Component Analysis (ICA)\n",
      "\n",
      "417\n",
      "\n",
      "Exercise 12.4 Deriving the second principal component a. Let\n",
      "\n",
      "J(v2, z2) =\n",
      "\n",
      "1 n\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "(xi − zi1v1 − zi2v2)T (xi − zi1v1 − zi2v2)\n",
      "\n",
      "i=1\n",
      "\n",
      "(12.125)\n",
      "\n",
      "Show that ∂J ∂z2\n",
      "\n",
      "= 0 yields zi2 = vT\n",
      "\n",
      "2 xi.\n",
      "\n",
      "b. Show that the value of v2 that minimizes\n",
      "\n",
      "(12.126) is given by the eigenvector of C with the second largest eigenvalue. Hint: recall that Cv1 = λ1v1 and ∂xT Ax\n",
      "\n",
      "∂x = (A + AT )x.\n",
      "\n",
      "˜J(v2) = −vT\n",
      "\n",
      "2 Cv2 + λ2(vT\n",
      "\n",
      "2 v2 − 1) + λ12(vT\n",
      "\n",
      "2 v1 − 0)\n",
      "\n",
      "Exercise 12.5 Deriving the residual error for PCA a. Prove that\n",
      "\n",
      "||xi −\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "zijvj||2 = xT\n",
      "\n",
      "i xi −\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "j xixT vT\n",
      "\n",
      "i vj\n",
      "\n",
      "(12.127)\n",
      "\n",
      "j=1\n",
      "\n",
      "j=1\n",
      "\n",
      "Hint: ﬁrst consider the case K = 2. Use the fact that vT recall zij = xT b. Now show that\n",
      "\n",
      "JK (cid:2) 1 n\n",
      "\n",
      "i vj.\n",
      "\n",
      "i=1\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "%\n",
      "\n",
      "xT\n",
      "\n",
      "i xi −\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "j=1\n",
      "\n",
      "j xixT vT\n",
      "\n",
      "i vj\n",
      "\n",
      "&\n",
      "\n",
      "=\n",
      "\n",
      "1 n\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "i=1\n",
      "\n",
      "j vj = 1 and vT\n",
      "\n",
      "xT\n",
      "\n",
      "i xi −\n",
      "\n",
      "j=1\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "λj\n",
      "\n",
      "j vk = 0 for k (cid:8)= j. Also,\n",
      "\n",
      "(12.128)\n",
      "\n",
      "c.\n",
      "\n",
      "Hint: recall vT If K = d there is no truncation, so Jd = 0. Use this to show that the error from only using K < d terms is given by\n",
      "\n",
      "j Cvj = λjvT\n",
      "\n",
      "j vj = λj.\n",
      "\n",
      "JK =\n",
      "\n",
      "d(cid:12)\n",
      "\n",
      "λj\n",
      "\n",
      "(12.129)\n",
      "\n",
      "j=K+1\n",
      "\n",
      "Hint: partition the sum\n",
      "\n",
      "(cid:2)d\n",
      "\n",
      "j=1 λj into\n",
      "\n",
      "(cid:2)K\n",
      "\n",
      "j=1 λj and\n",
      "\n",
      "(cid:2)d\n",
      "\n",
      "j=K+1 λj.\n",
      "\n",
      "Exercise 12.6 Derivation of Fisher’s linear discriminant Show that the maximum of J(w) = wT SB w where λ = wT SB w wT SW w where f (cid:2) = d dx\n",
      "\n",
      "f (x) and g(cid:2) = d dx\n",
      "\n",
      ". Hint: recall that the derivative of a ratio of two scalars is given by d dx\n",
      "\n",
      "g(x). Also, recall that d dx\n",
      "\n",
      "wT SW w is given by SBw = λSW w\n",
      "\n",
      "xT Ax = (A + AT )x.\n",
      "\n",
      "g(x) = f (cid:2)g−f g(cid:2)\n",
      "\n",
      "f (x)\n",
      "\n",
      "g2\n",
      "\n",
      ",\n",
      "\n",
      "Exercise 12.7 PCA via successive deﬂation Let v1, v2, . . . ,v k be the ﬁrst k eigenvectors with largest eigenvalues of C = 1 n basis vectors. These satisfy\n",
      "\n",
      "XT X, i.e., the principal\n",
      "\n",
      "vT\n",
      "\n",
      "j vk =\n",
      "\n",
      "0 1\n",
      "\n",
      "if j (cid:8)= k if j = k\n",
      "\n",
      "(12.130)\n",
      "\n",
      "We will construct a method for ﬁnding the vj sequentially.\n",
      "\n",
      "418\n",
      "\n",
      "Chapter 12. Latent linear models\n",
      "\n",
      "As we showed in class, v1 is the ﬁrst principal eigenvector of C, and satisﬁes Cv1 = λ1v1. Now deﬁne ˜xi as the orthogonal projection of xi onto the space orthogonal to v1:\n",
      "\n",
      "˜xi = P⊥v1 xi = (I − v1vT\n",
      "\n",
      "1 )xi\n",
      "\n",
      "(12.131)\n",
      "\n",
      "Deﬁne ˜X = [˜x1; ...; ˜xn] as the deﬂated matrix of rank d − 1, which is obtained by removing from the d dimensional data the component that lies in the direction of the ﬁrst principal direction:\n",
      "\n",
      "˜X = (I − v1vT\n",
      "\n",
      "1 )T X = (I − v1vT\n",
      "\n",
      "1 )X\n",
      "\n",
      "(12.132)\n",
      "\n",
      "a. Using the facts that XT Xv1 = nλ1v1 (and hence vT\n",
      "\n",
      "the covariance of the deﬂated matrix is given by\n",
      "\n",
      "1 XT X = nλ1vT\n",
      "\n",
      "1 ) and vT\n",
      "\n",
      "1 v1 = 1, show that\n",
      "\n",
      "˜C (cid:2) 1 n\n",
      "\n",
      "˜XT ˜X =\n",
      "\n",
      "1 n\n",
      "\n",
      "XT X − λ1v1vT 1\n",
      "\n",
      "(12.133)\n",
      "\n",
      "b. Let u be the principal eigenvector of ˜C. Explain why u = v2. (You may assume u is unit norm.) c. Suppose we have a simple method for ﬁnding the leading eigenvector and eigenvalue of a pd matrix, denoted by [λ, u] = f (C). Write some pseudo code for ﬁnding the ﬁrst K principal basis vectors of X that only uses the special f function and simple vector arithmetic, i.e., your code should not use SVD or theeig function. Hint: this should be a simple iterative routine that takes 2–3 lines to write. The input is C, K and the function f , the output should be vj and λj for j = 1 : K. Do not worry about being syntactically correct.\n",
      "\n",
      "Exercise 12.8 Latent semantic indexing (Source: de Freitas.). In this exercise, we study a technique called latent semantic indexing, which applies SVD to a document by term matrix, to create a low-dimensional embedding of the data that is designed to capture semantic similarity of words. The ﬁle lsiDocuments.pdf contains 9 documents on various topics. A list of all the 460 unique words/terms that occur in these documents is in lsiWords.txt. A document by term matrix is in lsiMatrix.txt.\n",
      "\n",
      "a. Let X be the transpose of lsiMatrix, so each column represents a document. Compute the SVD of X and make an approximation to it ˆX using the ﬁrst 2 singular values/ vectors. Plot the low dimensional representation of the 9 documents in 2D. You should get something like Figure 12.24.\n",
      "\n",
      "b. Consider ﬁnding documents that are about alien abductions.\n",
      "\n",
      "If If you look at lsiWords.txt, there are 3 versions of this word, term 23 (“abducted”), term 24 (“abduction”) and term 25 (“abductions”). Suppose we want to ﬁnd documents containing the word “abducted”. Documents 2 and 3 contain it, but document 1 does not. However, document 1 is clearly related to this topic. Thus LSI should also ﬁnd document 1. Create a test document q containing the one word “abducted”, and project it into the 2D subspace to make ˆq. Now compute the cosine similarity between ˆq and the low dimensional representation of all the documents. What are the top 3 closest matches?\n",
      "\n",
      "Exercise 12.9 Imputation in a FA model Derive an expression for p(xh|xv, θ) for a FA model.\n",
      "\n",
      "Exercise 12.10 Efficiently evaluating the PPCA density Derive an expression for p(x| ˆW, ˆσ2) for the PPCA model based on plugging in the MLEs and using the matrix inversion lemma.\n",
      "\n",
      "12.6.\n",
      "\n",
      "Independent Component Analysis (ICA)\n",
      "\n",
      "419\n",
      "\n",
      "0.6\n",
      "\n",
      "4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "6\n",
      "\n",
      "7 8\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−0.2\n",
      "\n",
      "9\n",
      "\n",
      "2\n",
      "\n",
      "−0.4\n",
      "\n",
      "1\n",
      "\n",
      "3\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.45\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.35\n",
      "\n",
      "−0.3\n",
      "\n",
      "−0.25\n",
      "\n",
      "−0.2\n",
      "\n",
      "Figure 12.24\n",
      "\n",
      "Projection of 9 documents into 2 dimensions. Figure generated by lsiCode.\n",
      "\n",
      "Exercise 12.11 PPCA vs FA (Source: Exercise 14.15 of (Hastie et al. 2009), due to Hinton.). Generate 200 observations from the following model, where zi ∼ N (0, I): xi1 = zi1, xi2 = zi1 + 0.001zi2, xi3 = 10zi3. Fit a FA and PCA model with 1 latent factor. Hence show that the corresponding weight vector w aligns with the maximal variance direction (dimension 3) in the PCA case, but with the maximal correlation direction (dimensions 1+2) in the case of FA.\n",
      "\n",
      "13 Sparse linear models\n",
      "\n",
      "13.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "We introduced the topic of feature selection in Section 3.5.4, where we discussed methods for ﬁnding input variables which had high mutual information with the output. The trouble with this approach is that it is based on a myopic strategy that only looks at one variable at a time. This can fail if there are interaction effects. For example, if y = xor(x1, x2), then neither x1 nor x2 on its own can predict the response, but together they perfectly predict the response. For a real-world example of this, consider genetic association studies: sometimes two genes on their own may be harmless, but when present together they cause a recessive disease (Balding 2006). In this chapter, we focus on selecting sets of variables at a time using a model-based approach. If the model is a generalized linear model, of the form p(y|x) =p( y|f (wT x)) for some link function f , then we can perform feature selection by encouraging the weight vector w to be sparse, i.e., to have lots of zeros. This approach turns out to offer signiﬁcant computational advantages, as we will see below.\n",
      "\n",
      "Here are some applications where feature selection/ sparsity is useful:\n",
      "\n",
      "\n",
      "\n",
      "In many problems, we have many more dimensions D than training cases N . The cor- responding design matrix is short and fat, rather than tall and skinny. This is called the small N , large D problem. This is becoming increasingly prevalent as we develop more high throughput measurement devices, For example, with gene microarrays, it is common to measure the expression levels of D ∼ 10, 000 genes, but to only get N ∼ 100 such examples. (It is perhaps a sign of the times that even our data seems to be getting fatter...) We may want to ﬁnd the smallest set of features that can accurately predict the response (e.g., growth rate of the cell) in order to prevent overﬁtting, to reduce the cost of building a diagnostic device, or to help with scientiﬁc insight into the problem.\n",
      "\n",
      "\n",
      "\n",
      "In Chapter 14, we will use basis functions centered on the training examples, so φ(x) = [κ(x, x1), . . . , κ(x, xN )], where κ is a kernel function. The resulting design matrix has size N × N . Feature selection in this context is equivalent to selecting a subset of the training examples, which can help reduce overﬁtting and computational cost. This is known as a sparse kernel machine.\n",
      "\n",
      "\n",
      "\n",
      "in terms of In signal processing, it is common to represent signals (images, speech, etc.) wavelet basis functions. To save time and space, it is useful to ﬁnd a sparse representation\n",
      "\n",
      "422\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "of the signals, in terms of a small number of such basis functions. This allows us to estimate signals from a small number of measurements, as well as to compress the signal. See Section 13.8.3 for more information.\n",
      "\n",
      "of machine learning/ statistics. main results.\n",
      "\n",
      "Note that the topic of feature selection and sparsity is currently one of the most active areas In this chapter, we only have space to give an overview of the\n",
      "\n",
      "13.2\n",
      "\n",
      "Bayesian variable selection\n",
      "\n",
      "A natural way to pose the variable selection problem is as follows. Let γj = 1 if feature j is “relevant”, and let γj = 0 otherwise. Our goal is to compute the posterior over models\n",
      "\n",
      "p(γ|D) =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "e−f (γ) γ (cid:2) e−f (γ (cid:2))\n",
      "\n",
      "(13.1)\n",
      "\n",
      "where f (γ) is the cost function:\n",
      "\n",
      "f (γ) (cid:2) −[log p(D|γ) + log p(γ)] (13.2) For example, suppose we generate N = 20 samples from a D = 10 dimensional linear regression model, yi ∼ N (wT xi, σ2), In particular, we use w = (0.00, −1.67, 0.13, 0.00, 0.00, 1.19, 0.00, −0.04, 0.33, 0.00) and σ2 = 1. We enumerate all 210 = 1024 models and compute p(γ|D) for each one (we give the equations for this below). We order the models in Gray code order, which ensures consecutive vectors differ by exactly 1 bit (the reasons for this are computational, and are discussed in Section 13.2.3).\n",
      "\n",
      "The resulting set of bit patterns is shown in Figure 13.1(a). The cost of each model, f (γ), is shown in Figure 13.1(b). We see that this objective function is extremely “bumpy”. The results are easier to interpret if we compute the posterior distribution over models, p(γ|D). This is shown in Figure 13.1(c). The top 8 models are listed below:\n",
      "\n",
      "in which K = 5 elements of w are non-zero.\n",
      "\n",
      "The “true” model is {2, 3, 6, 8, 9}. However, the coefficients associated with features 3 and 8 are very small (relative to σ2). so these variables are harder to detect. Given enough data, the method will converge on the true model (assuming the data is generated from a linear model), but for ﬁnite data sets, there will usually be considerable posterior uncertainty.\n",
      "\n",
      "model 4 61 452 60 29 68 36 5\n",
      "\n",
      "prob members 2, 0.447 2, 6, 0.241 2, 6, 9, 0.103 2, 3, 6, 0.091 2, 5, 0.041 2, 6, 7, 0.021 2, 5, 6, 0.015 2, 3, 0.010\n",
      "\n",
      "Interpreting the posterior over a large number of models is quite difficult, so we will seek\n",
      "\n",
      "various summary statistics. A natural one is the posterior mode, or MAP estimate\n",
      "\n",
      "ˆγ = argmax p(γ|D) = argmin f (γ)\n",
      "\n",
      "(13.3)\n",
      "\n",
      "13.2. Bayesian variable selection\n",
      "\n",
      "423\n",
      "\n",
      "1\n",
      "\n",
      "log p(model, data)\n",
      "\n",
      "2\n",
      "\n",
      "−40\n",
      "\n",
      "3\n",
      "\n",
      "−60\n",
      "\n",
      "4\n",
      "\n",
      "−80\n",
      "\n",
      "5\n",
      "\n",
      "−100\n",
      "\n",
      "6\n",
      "\n",
      "−120\n",
      "\n",
      "7\n",
      "\n",
      "−140\n",
      "\n",
      "8\n",
      "\n",
      "−160\n",
      "\n",
      "9\n",
      "\n",
      "−180\n",
      "\n",
      "−200\n",
      "\n",
      "10\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "600\n",
      "\n",
      "700\n",
      "\n",
      "800\n",
      "\n",
      "900\n",
      "\n",
      "1000\n",
      "\n",
      "−220\n",
      "\n",
      "0\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "600\n",
      "\n",
      "800\n",
      "\n",
      "1000\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "p(model|data)\n",
      "\n",
      "p(gamma(j)|data\n",
      "\n",
      "0.1\n",
      "\n",
      "1\n",
      "\n",
      "0.09\n",
      "\n",
      "0.9\n",
      "\n",
      "0.08\n",
      "\n",
      "0.8\n",
      "\n",
      "0.07\n",
      "\n",
      "0.7\n",
      "\n",
      "0.06\n",
      "\n",
      "0.6\n",
      "\n",
      "0.05\n",
      "\n",
      "0.5\n",
      "\n",
      "0.04\n",
      "\n",
      "0.4\n",
      "\n",
      "0.03\n",
      "\n",
      "0.3\n",
      "\n",
      "0.02\n",
      "\n",
      "0.2\n",
      "\n",
      "0.01\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "600\n",
      "\n",
      "800\n",
      "\n",
      "1000\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 13.1 all possible models. (d) Marginal inclusion probabilities. Figure generated by linregAllsubsetsGraycodeDemo.\n",
      "\n",
      "(a) All possible bit vectors of length 10 enumerated in Gray code order. (b) Score function for (c) Posterior over all 1024 models. Vertical scale has been truncated at 0.1 for clarity.\n",
      "\n",
      "However, the mode is often not representative of the full posterior mass (see Section 5.2.1.3). A better summary is the median model (Barbieri and Berger 2004; Carvahlo and Lawrence 2007), computed using\n",
      "\n",
      "ˆγ = {j : p(γj = 1|D) > 0.5}\n",
      "\n",
      "(13.4)\n",
      "\n",
      "This requires computing the posterior marginal inclusion probabilities, p(γj = 1|D). These are shown in Figure 13.1(d). We see that the model is conﬁdent that variables 2 and 6 are included; if we lower the decision threshold to 0.1, we would add 3 and 9 as well. However, if we wanted to “capture” variable 8, we would incur two false positives (5 and 7). This tradeoff between false positives and false negatives is discussed in more detail in Section 5.7.2.1.\n",
      "\n",
      "the problem was sufficiently small (only 10 variables) that we were able to compute the full posterior exactly. Of course, variable selection is most useful in the cases where the number of dimensions is large. Since there are 2D possible models (bit vectors), it will be impossible to compute the full posterior in general, and even ﬁnding summaries, such as the MAP estimate or marginal\n",
      "\n",
      "The above example illustrates the “gold standard” for variable selection:\n",
      "\n",
      "424\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "inclusion probabilities, will be intractable. We will therefore spend most of this chapter focussing on algorithmic speedups. But before we do that, we will explain how we computed p(γ|D) in the above example.\n",
      "\n",
      "13.2.1\n",
      "\n",
      "The spike and slab model\n",
      "\n",
      "The posterior is given by\n",
      "\n",
      "p(γ|D) ∝ p(γ)p(D|γ)\n",
      "\n",
      "(13.5)\n",
      "\n",
      "We ﬁrst consider the prior, then the likelihood.\n",
      "\n",
      "It is common to use the following prior on the bit vector:\n",
      "\n",
      "p(γ) =\n",
      "\n",
      "D(cid:27)\n",
      "\n",
      "Ber(γj|π0) = π||γ||0\n",
      "\n",
      "0\n",
      "\n",
      "(1 − π0)D−||γ||0\n",
      "\n",
      "(13.6)\n",
      "\n",
      "j=1\n",
      "\n",
      "where π0 is the probability a feature is relevant, and ||γ||0 = j=1 γj is the (cid:6)0 pseudo-norm, that is, the number of non-zero elements of the vector. For comparison with later models, it is useful to write the log prior as follows:\n",
      "\n",
      "(cid:10)D\n",
      "\n",
      "log p(γ|π0) =||γ ||0 log π0 + (D − ||γ||0) log(1 − π0)\n",
      "\n",
      "(13.7)\n",
      "\n",
      "= ||γ||0(log π0 − log(1 − π0)) + const = −λ||γ||0 + const\n",
      "\n",
      "(13.8)\n",
      "\n",
      "(13.9)\n",
      "\n",
      "where λ (cid:2) log 1−π0 π0\n",
      "\n",
      "We can write the likelihood as follows:\n",
      "\n",
      "controls the sparsity of the model.\n",
      "\n",
      "(cid:28) (cid:28)\n",
      "\n",
      "p(D|γ) = p(y|X, γ) =\n",
      "\n",
      "p(y|X, w, γ)p(w|γ, σ2)p(σ2)dwdσ2\n",
      "\n",
      "(13.10)\n",
      "\n",
      "For notational simplicity, we have assumed the response is centered, (i.e., y = 0), so we can ignore any offset term μ.\n",
      "\n",
      "We now discuss the prior p(w|γ, σ2). If γj = 0, feature j is irrelevant, so we expect wj = 0. If γj = 1, we expect wj to be non-zero. If we standardize the inputs, a reasonable prior is N (0, σ2σ2 w controls how big we expect the coefficients associated with the relevant variables to be (which is scaled by the overall noise level σ2). We can summarize this prior as follows:\n",
      "\n",
      "p(wj|σ2, γj) =\n",
      "\n",
      "w), where σ2\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "δ0(wj) N (wj|0, σ2σ2 w)\n",
      "\n",
      "if γj = 0 if γj = 1\n",
      "\n",
      "(13.11)\n",
      "\n",
      "The ﬁrst term is a “spike” at the origin. As σ2 w → ∞, the distribution p(wj|γj = 1) approaches a uniform distribution, which can be thought of as a “slab” of constant height. Hence this is called the spike and slab model (Mitchell and Beauchamp 1988).\n",
      "\n",
      "We can drop the coefficients wj for which wj = 0 from the model, since they are clamped to zero under the prior. Hence Equation 13.10 becomes the following (assuming a Gaussian likelihood):\n",
      "\n",
      "(cid:28) (cid:28)\n",
      "\n",
      "p(D|γ) =\n",
      "\n",
      "N (y|Xγ wγ, σ2IN )N (wγ|0Dγ , σ2σ2\n",
      "\n",
      "wIDγ )p(σ2)dwγdσ2\n",
      "\n",
      "(13.12)\n",
      "\n",
      "13.2. Bayesian variable selection\n",
      "\n",
      "425\n",
      "\n",
      "where Dγ = ||γ||0 is the number of non-zero elements in γ. In what follows, we will generalize this slightly by deﬁning a prior of the form p(w|γ, σ2) =N (wγ|0Dγ , σ2Σγ) for any positive deﬁnite matrix Σγ.1\n",
      "\n",
      "Given these priors, we can now compute the marginal likelihood.\n",
      "\n",
      "If the noise variance is\n",
      "\n",
      "known, we can write down the marginal likelihood (using Equation 13.151) as follows:\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(D|γ, σ2) =\n",
      "\n",
      "N (y|Xγwγ, σ2I)N (wγ|0, σ2Σγ)dwγ = N (y|0, Cγ)\n",
      "\n",
      "(13.13)\n",
      "\n",
      "Cγ (cid:2) σ2XγΣγXT\n",
      "\n",
      "γ + σ2IN\n",
      "\n",
      "(13.14)\n",
      "\n",
      "If the noise is unknown, we can put a prior on it and integrate it out. It is common to use p(σ2) = IG(σ2|aσ, bσ). Some guidelines on setting a, b can be found in (Kohn et al. 2001). If we use a = b = 0, we recover the Jeffrey’s prior, p(σ2) ∝ σ−2. When we integrate out the noise, we get the following more complicated expression for the marginal likelihood (Brown et al. 1998):\n",
      "\n",
      "(cid:28) (cid:28)\n",
      "\n",
      "p(D|γ) =\n",
      "\n",
      "p(y|γ, wγ, σ2)p(wγ|γ, σ2)p(σ2)dwγdσ2\n",
      "\n",
      "(13.15)\n",
      "\n",
      "∝ |XT\n",
      "\n",
      "γ Xγ + Σ−1\n",
      "\n",
      "γ |− 1\n",
      "\n",
      "2 |Σγ|− 1\n",
      "\n",
      "2 (2bσ + S(γ))−(2aσ+N −1)/2\n",
      "\n",
      "(13.16)\n",
      "\n",
      "where S(γ) is the RSS:\n",
      "\n",
      "S(γ) (cid:2) yT y − yT Xγ(XT\n",
      "\n",
      "γ Xγ + Σ−1\n",
      "\n",
      "γ )−1XT\n",
      "\n",
      "γ y\n",
      "\n",
      "(13.17)\n",
      "\n",
      "See also Exercise 13.4.\n",
      "\n",
      "When the marginal likelihood cannot be computed in closed form (e.g., if we are using logistic\n",
      "\n",
      "regression or a nonlinear model), we can approximate it using BIC, which has the form ||γ||0 2\n",
      "\n",
      "log p(D|γ) ≈ log p(y|X, ˆwγ, ˆσ2) −\n",
      "\n",
      "log N\n",
      "\n",
      "(13.18)\n",
      "\n",
      "where ˆwγ is the ML or MAP estimate based on Xγ , and ||γ||0 is the “degrees of freedom” of the model (Zou et al. 2007). Adding the log prior, the overall objective becomes\n",
      "\n",
      "log p(γ|D) ≈ log p(y|X, ˆwγ, ˆσ2) −\n",
      "\n",
      "||γ||0 2\n",
      "\n",
      "log N − λ||γ||0 + const\n",
      "\n",
      "(13.19)\n",
      "\n",
      "We see that there are two complexity penalties: one arising from the BIC approximation to the marginal likelihood, and the other arising from the prior on p(γ). Obviously these can be combined into one overall complexity parameter, which we will denote by λ.\n",
      "\n",
      "13.2.2\n",
      "\n",
      "From the Bernoulli-Gaussian model to (cid:9)0 regularization\n",
      "\n",
      "Another model that is sometimes used (e.g., (Kuo and Mallick 1998; Zhou et al. 2009; Soussen et al. 2010)) is the following: yi|xi, w, γ, σ2 ∼ N (\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "γjwjxij, σ2)\n",
      "\n",
      "(13.20)\n",
      "\n",
      "j\n",
      "\n",
      "γj ∼ Ber(π0) wj ∼ N (0, σ2 w)\n",
      "\n",
      "(13.21)\n",
      "\n",
      "(13.22)\n",
      "\n",
      "1. It is common to use a g-prior of the form Σγ = g(XT γ Xγ )−1 for reasons explained in Section 7.6.3.1 (see also Exercise 13.4). Various approaches have been proposed for setting g, including cross validation, empirical Bayes (Minka 2000b; George and Foster 2000), hierarchical Bayes (Liang et al. 2008), etc.\n",
      "\n",
      "426\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "(Soussen et al. 2010)), this is called the Bernoulli- In the signal processing literature (e.g., Gaussian model, although we could also call it the binary mask model, since we can think of the γj variables as “masking out” the weights wj.\n",
      "\n",
      "Unlike the spike and slab model, we do not integrate out the “irrelevant” coefficients; they always exist. In addition, the binary mask model has the form γj → y ← wj, whereas the spike and slab model has the form γj → wj → y. In the binary mask model, only the product γjwj can be identiﬁed from the likelihood.\n",
      "\n",
      "One interesting aspect of this model is that it can be used to derive an objective function that is widely used in the (non-Bayesian) subset selection literature. First, note that the joint prior has the form\n",
      "\n",
      "p(γ, w) ∝ N (w|0, σ2\n",
      "\n",
      "wI)π||γ||0\n",
      "\n",
      "0\n",
      "\n",
      "(1 − π0)D−||γ||0\n",
      "\n",
      "(13.23)\n",
      "\n",
      "Hence the scaled unnormalized negative log posterior has the form\n",
      "\n",
      "f (γ, w) (cid:2) −2σ2 log p(γ, w, y|X) =||y − X(γ. ∗ w)||2\n",
      "\n",
      "+\n",
      "\n",
      "σ2 σ2 w\n",
      "\n",
      "||w||2 + λ||γ||0 + const\n",
      "\n",
      "(13.24)\n",
      "\n",
      "where\n",
      "\n",
      "λ (cid:2) 2σ2 log(\n",
      "\n",
      "1 − π0 π0\n",
      "\n",
      ")\n",
      "\n",
      "(13.25)\n",
      "\n",
      "Let us split w into two subvectors, w−γ and wγ, indexed by the zero and non-zero entries of γ respectively. Since X(γ. ∗ w) = Xγwγ, we can just set w−γ = 0.\n",
      "\n",
      "w → ∞, so we do not regularize the non-zero weights (so there is no complexity penalty coming from the marginal likelihood or its BIC approximation). In this case, the objective becomes\n",
      "\n",
      "Now consider the case where σ2\n",
      "\n",
      "f (γ, w) = ||y − Xγ wγ||2\n",
      "\n",
      "2 + λ||γ||0\n",
      "\n",
      "(13.26)\n",
      "\n",
      "This is similar to the BIC objective above.\n",
      "\n",
      "Instead of keeping track of the bit vector γ, we can deﬁne the set of relevant variables to be the support, or set of non-zero entries, of w. Then we can rewrite the above equation as follows:\n",
      "\n",
      "f (w) = ||y − Xw||2\n",
      "\n",
      "2 + λ||w||0\n",
      "\n",
      "(13.27)\n",
      "\n",
      "This is called (cid:6)0 regularization. We have converted the discrete optimization problem (over γ ∈ {0, 1}D) into a continuous one (over w ∈ RD); however, the (cid:6)0 pseudo-norm makes the objective very non smooth, so this is still hard to optimize. We will discuss different solutions to this in the rest of this chapter.\n",
      "\n",
      "13.2.3\n",
      "\n",
      "Algorithms Since there are 2D models, we cannot explore the full posterior, or ﬁnd the globally optimal model. Instead we will have to resort to heuristics of one form or another. All of the methods we will discuss involve searching through the space of models, and evaluating the cost f (γ) at\n",
      "\n",
      "13.2. Bayesian variable selection\n",
      "\n",
      "427\n",
      "\n",
      "{1, 2, 3, 4}\n",
      "\n",
      "1.4\n",
      "\n",
      "all subsets on prostate cancer\n",
      "\n",
      "{1, 2, 3} {2, 3, 4} {1, 3, 4} {1, 2, 4}\n",
      "\n",
      "1.2\n",
      "\n",
      "r o r r e\n",
      "\n",
      "1\n",
      "\n",
      "{1, 2}\n",
      "\n",
      "{1, 3}\n",
      "\n",
      "{1, 4}\n",
      "\n",
      "{2, 3}\n",
      "\n",
      "{2, 4}\n",
      "\n",
      "{3, 4}\n",
      "\n",
      "t e s\n",
      "\n",
      "g n n a r t\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "0.8\n",
      "\n",
      "{1}\n",
      "\n",
      "{2}\n",
      "\n",
      "{3}\n",
      "\n",
      "{4}\n",
      "\n",
      "0.6\n",
      "\n",
      "{}\n",
      "\n",
      "0.4 0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4 subset size\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.2 (a) A lattice of subsets of {1, 2, 3, 4}. (b) Residual sum of squares versus subset size, on the prostate cancer data set. The lower envelope is the best RSS achievable for any set of a given size. Based on Figure 3.5 of (Hastie et al. 2001). Figure generated by prostateSubsets.\n",
      "\n",
      "each point. This requires ﬁtting the model (i.e., computing argmax p(D|w)), or evaluating its p(D|w)p(w)dw) at each step. This is sometimes called marginal likelihood (i.e., computing the wrapper method, since we “wrap” our search for the best model (or set of good models) around a generic model-ﬁtting procedure.\n",
      "\n",
      ")\n",
      "\n",
      "In order to make wrapper methods efficient, it is important that we can quickly evaluate the score function for some new model, γ(cid:4), given the score of a previous model, γ. This can be done provided we can efficiently update the sufficient statistics needed to compute f (γ). This is possible provided γ(cid:4) only differs from γ in one bit (corresponding to adding or removing a single variable), and provided f (γ) only depends on the data via Xγ . In this case, we can use rank-one matrix updates/ downdates to efficiently compute XT γ Xγ. These updates are usually applied to the QR decomposition of X. See e.g., (Miller 2002; Schniter et al. 2008) for details.\n",
      "\n",
      "γ(cid:2) Xγ(cid:2) from XT\n",
      "\n",
      "13.2.3.1\n",
      "\n",
      "Greedy search\n",
      "\n",
      "Suppose we want to ﬁnd the MAP model. If we use the (cid:6)0-regularized objective in Equation 13.27, we can exploit properties of least squares to derive various efficient greedy forwards search methods, some of which we summarize below. For further details, see (Miller 2002; Soussen et al. 2010).\n",
      "\n",
      "Single best replacement The simplest method is to use greedy hill climbing, where at each step, we deﬁne the neighborhood of the current model to be all models than can be reached by ﬂipping a single bit of γ, i.e., for each variable, if it is currently out of the model, we consider adding it, and if it is currently in the model, we consider removing it. In (Soussen et al. 2010), they call this the single best replacement (SBR). Since we are expecting a sparse solution, we can start with the empty set, γ = 0. We are essentially moving through the lattice of subsets, shown in Figure 13.2(a). We continue adding or removing until no improvement is possible.\n",
      "\n",
      "Orthogonal least squares If we set λ = 0 in Equation 13.27, so there is no complexity penalty, there will be no reason to perform deletion steps. In this case, the SBR algorithm is equivalent to orthogonal least squares (Chen and Wigger 1995), which in turn is equivalent\n",
      "\n",
      "428\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "to greedy forwards selection. In this algorithm, we start with the empty set and add the best feature at each step. The error will go down monotonically with ||γ||0, as shown in Figure 13.2(b). We can pick the next best feature j∗ to add to the current set γt by solving\n",
      "\n",
      "j∗ = arg min j(cid:7)∈γt\n",
      "\n",
      "min w\n",
      "\n",
      "||y − (Xγ t∪j)w||2\n",
      "\n",
      "(13.28)\n",
      "\n",
      "We then update the active set by setting γ(t+1) = γ(t) ∪ {j∗}. To choose the next feature to add at step t, we need to solve D − Dt least squares problems at step t, where Dt = |γt| is the cardinality of the current active set. Having chosen the best feature to add, we need to solve an additional least squares problem to compute wt+1).\n",
      "\n",
      "Orthogonal matching pursuits Orthogonal least squares is somewhat expensive. A simpli- ﬁcation is to “freeze” the current weights at their current value, and then to pick the next feature to add by solving\n",
      "\n",
      "j∗ = arg min j(cid:7)∈γt\n",
      "\n",
      "min β\n",
      "\n",
      "||y − Xwt − βx:,j||2\n",
      "\n",
      "(13.29)\n",
      "\n",
      "This inner optimization is easy to solve: we simply set β = xT y − Xwt is the current residual vector. If the columns are unit norm, we have\n",
      "\n",
      ":,jrt/||x:,j||2, where rt =\n",
      "\n",
      "j∗ = arg max xT\n",
      "\n",
      ":,jrt\n",
      "\n",
      "(13.30)\n",
      "\n",
      "so we are just looking for the column that is most correlated with the current residual. We then update the active set, and compute the new least squares estimate wt+1 using Xγ t+1 . This method is called orthogonal matching pursuits or OMP (Mallat et al. 1994). This only requires one least squares calculation per iteration and so is faster than orthogonal least squares, but is not quite as accurate (Blumensath and Davies 2007).\n",
      "\n",
      "Matching pursuits An even more aggressive approximation is to just greedily add the feature that is most correlated with the current residual. This is called matching pursuits (Mallat and Zhang 1993). This is also equivalent to a method known as least squares boosting (Section 16.4.6).\n",
      "\n",
      "Backwards selection Backwards selection starts with all variables in the model (the so- called saturated model), and then deletes the worst one at each step. This is equivalent to performing a greedy search from the top of the lattice downwards. This can give better results than a bottom-up search, since the decision about whether to keep a variable or not is made in the context of all the other variables that might depende on it. However, this method is typically infeasible for large problems, since the saturated model will be too expensive to ﬁt.\n",
      "\n",
      "FoBa The forwards-backwards algorithm of (Zhang 2008) is similar to the single best replacement algorithm presented above, except it uses an OMP-like approximation when choosing the next move to make. A similar “dual-pass” algorithm was described in (Moghad- dam et al. 2008).\n",
      "\n",
      "Bayesian Matching pursuit The algorithm of (Schniter et al. 2008) is similiar to OMP except it uses a Bayesian marginal likelihood scoring criterion (under a spike and slab model) instead of a least squares objective. In addition, it uses a form of beam search to explore multiple paths through the lattice at once.\n",
      "\n",
      "13.3. (cid:6)1 regularization: basics\n",
      "\n",
      "429\n",
      "\n",
      "13.2.3.2\n",
      "\n",
      "Stochastic search\n",
      "\n",
      "If we want to approximate the posterior, rather than just computing a mode (e.g. because we want to compute marginal inclusion probabilities), one option is to use MCMC. The standard approach is to use Metropolis Hastings, where the proposal distribution just ﬂips single bits. This enables us to efficiently compute p(γ(cid:4)|D) given p(γ|D). The probability of a state (bit conﬁguration) is estimated by counting how many times the random walk visits this state. See (O’Hara and Sillanpaa 2009) for a review of such methods, and (Bottolo and Richardson 2010) for a very recent method based on evolutionary MCMC.\n",
      "\n",
      "However, in a discrete state space, MCMC is needlessly inefficient, since we can compute the (unnormalized) probability of a state directly using p(γ, D) = exp(−f (γ)); thus there is no need to ever revisit a state. A much more efficient alternative is to use some kind of stochastic search algorithm, to generate a set S of high scoring models, and then to make the following approximation\n",
      "\n",
      "p(γ|D) ≈\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "e−f (γ) γ (cid:2)∈S e−f (γ (cid:2))\n",
      "\n",
      "(13.31)\n",
      "\n",
      "See (Heaton and Scott 2009) for a review of recent methods of this kind.\n",
      "\n",
      "13.2.3.3\n",
      "\n",
      "EM and variational inference *\n",
      "\n",
      "It is tempting to apply EM to the spike and slab model, which has the form γj → wj → y. We can compute p(γj = 1|wj) in the E step, and optimize w in the M step. However, this will not work, because when we compute p(γj = 1|wj), we are comparing a delta-function, δ0(wj), with a Gaussian pdf, N (wj|0, σ2 w). We can replace the delta function with a narrow Gaussian, and then the E step amounts to classifying wj under the two possible Gaussian models. However, this is likely to suffer from severe local minima.\n",
      "\n",
      "An alternative is to apply EM to the Bernoulli-Gaussian model, which has the form γj → y ← wj. In this case, the posterior p(γ|D, w) is intractable to compute because all the bits become correlated due to explaining away. However, it is possible to derive a mean ﬁeld approximation of the form\n",
      "\n",
      "’\n",
      "\n",
      "j q(γj)q(wj) (Huang et al. 2007; Rattray et al. 2009).\n",
      "\n",
      "13.3\n",
      "\n",
      "(cid:9)1 regularization: basics\n",
      "\n",
      "When we have many variables, it is computationally difficult to ﬁnd the posterior mode of p(γ|D). And although greedy algorithms often work well (see e.g., (Zhang 2008) for a theoretical analysis), they can of course get stuck in local optima.\n",
      "\n",
      "Part of the problem is due to the fact that the γj variables are discrete, γj ∈ {0, 1}. In the optimization community, it is common to relax hard constraints of this form by replacing discrete variables with continuous variables. We can do this by replacing the spike-and-slab style prior, that assigns ﬁnite probability mass to the event that wj = 0, to continuous priors that “encourage” wj = 0 by putting a lot of probability density near the origin, such as a zero-mean Laplace distribution. This was ﬁrst introduced in Section 7.4 in the context of robust linear regression. There we exploited the fact that the Laplace has heavy tails. Here we exploit the fact\n",
      "\n",
      "430\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "Figure 13.3 Illustration of (cid:7)1 (left) vs (cid:7)2 (right) regularization of a least squares problem. Based on Figure 3.12 of (Hastie et al. 2001).\n",
      "\n",
      "that it has a spike near μ = 0. More precisely, consider a prior of the form\n",
      "\n",
      "p(w|λ) =\n",
      "\n",
      "D(cid:27)\n",
      "\n",
      "Lap(wj|0, 1/λ) ∝\n",
      "\n",
      "D(cid:27)\n",
      "\n",
      "e−λ|wj |\n",
      "\n",
      "(13.32)\n",
      "\n",
      "j=1\n",
      "\n",
      "j=1\n",
      "\n",
      "We will use a uniform prior on the offset term, p(w0) ∝ 1. Let us perform MAP estimation with this prior. The penalized negative log likelihood has the form\n",
      "\n",
      "f (w) = − log p(D|w) − log p(w|λ) = NLL(w) + λ||w||1\n",
      "\n",
      "(13.33)\n",
      "\n",
      "j=1 |wj| is the (cid:6)1 norm of w. For suitably large λ, the estimate ˆw will be where ||w||1 = sparse, for reasons we explain below. Indeed, this can be thought of as a convex approximation to the non-convex (cid:6)0 objective\n",
      "\n",
      "(cid:10)D\n",
      "\n",
      "argmin w\n",
      "\n",
      "NLL(w) + λ||w||0\n",
      "\n",
      "(13.34)\n",
      "\n",
      "In the case of linear regression, the (cid:6)1 objective becomes\n",
      "\n",
      "f (w) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "− 1 2σ2 (yi − (w0 + wT xi))2 + λ||w||1\n",
      "\n",
      "(13.35)\n",
      "\n",
      "= RSS(w) + λ(cid:4)||w||1\n",
      "\n",
      "(13.36)\n",
      "\n",
      "where λ(cid:4) = 2λσ2. This method is known as basis pursuit denoising or BPDN (Chen et al. 1998). The reason for this term will become clear later. In general, the technique of putting a zero-mean Laplace prior on the parameters and performing MAP estimation is called (cid:6)1 regularization. It can be combined with any convex or non-convex NLL term. Many different algorithms have been devised for solving such problems, some of which we review in Section 13.4.\n",
      "\n",
      "13.3.1 Why does (cid:9)1 regularization yield sparse solutions?\n",
      "\n",
      "We now explain why (cid:6)1 regularization results in sparse solutions, whereas (cid:6)2 regularization does not. We focus on the case of linear regression, although similar arguments hold for logistic regression and other GLMs.\n",
      "\n",
      "13.3. (cid:6)1 regularization: basics\n",
      "\n",
      "431\n",
      "\n",
      "The objective is the following non-smooth objective function:\n",
      "\n",
      "min w\n",
      "\n",
      "RSS(w) +λ||w ||1\n",
      "\n",
      "(13.37)\n",
      "\n",
      "We can rewrite this as a constrained but smooth objective (a quadratic function with linear constraints):\n",
      "\n",
      "min w\n",
      "\n",
      "RSS(w)\n",
      "\n",
      "s.t.\n",
      "\n",
      "||w||1 ≤ B\n",
      "\n",
      "(13.38)\n",
      "\n",
      "where B is an upper bound on the (cid:6)1-norm of the weights: a small (tight) bound B corresponds to a large penalty λ, and vice versa.2 Equation 13.38 is known as lasso, which stands for “least absolute shrinkage and selection operator” (Tibshirani 1996). We will see why it has this name later.\n",
      "\n",
      "Similarly, we can write ridge regression\n",
      "\n",
      "min w\n",
      "\n",
      "RSS(w) +λ||w ||2 2\n",
      "\n",
      "(13.39)\n",
      "\n",
      "or as a bound constrained form: ||w||2\n",
      "\n",
      "min w\n",
      "\n",
      "RSS(w)\n",
      "\n",
      "s.t.\n",
      "\n",
      "2 ≤ B\n",
      "\n",
      "(13.40)\n",
      "\n",
      "In Figure 13.3, we plot the contours of the RSS objective function, as well as the contours of the (cid:6)2 and (cid:6)1 constraint surfaces. From the theory of constrained optimization, we know that the optimal solution occurs at the point where the lowest level set of the objective function intersects the constraint surface (assuming the constraint is active). It should be geometrically clear that as we relax the constraint B, we “grow” the(cid:6) 1 “ball” until it meets the objective; the corners of the ball are more likely to intersect the ellipse than one of the sides, especially in high dimensions, because the corners “stick out” more. The corners correspond to sparse solutions, which lie on the coordinate axes. By contrast, when we grow the (cid:6)2 ball, it can intersect the objective at any point; there are no “corners”, so there is no preference for sparsity.\n",
      "\n",
      "such as w = (1, 0), is the same as the cost of a dense solution, such as w = (1/ as long as they have the same (cid:6)2 norm:\n",
      "\n",
      "However, for lasso, setting w = (1, 0) is cheaper than setting w = (1/\n",
      "\n",
      "To see this another away, notice that, with ridge regression, the prior cost of a sparse solution, 2),\n",
      "\n",
      "||(1, 0)||1 = 1 < ||(1/\n",
      "\n",
      "||(1, 0)||2 = ||(1/\n",
      "\n",
      "√\n",
      "\n",
      "2, 1/\n",
      "\n",
      "√\n",
      "\n",
      "2, 1/\n",
      "\n",
      "√\n",
      "\n",
      "2||2 = 1\n",
      "\n",
      "√\n",
      "\n",
      "2||1 =\n",
      "\n",
      "√ 2\n",
      "\n",
      "√\n",
      "\n",
      "2, 1/\n",
      "\n",
      "√\n",
      "\n",
      "2), since\n",
      "\n",
      "√\n",
      "\n",
      "2, 1/\n",
      "\n",
      "(13.42)\n",
      "\n",
      "(13.41)\n",
      "\n",
      "√\n",
      "\n",
      "The most rigorous way to see that (cid:6)1 regularization results in sparse solutions is to examine\n",
      "\n",
      "conditions that hold at the optimum. We do this in Section 13.3.2.\n",
      "\n",
      "13.3.2\n",
      "\n",
      "Optimality conditions for lasso\n",
      "\n",
      "The lasso objective has the form\n",
      "\n",
      "f (θ) = RSS(θ) +λ||w ||1\n",
      "\n",
      "(13.43)\n",
      "\n",
      "2. Equation 13.38 is an example of a quadratic program or QP, since we have a quadratic objective subject to linear inequality constraints. Its Lagrangian is given by Equation 13.37.\n",
      "\n",
      "432\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "cʼ\n",
      "\n",
      "c\n",
      "\n",
      "f(x) − f(x ) 0\n",
      "\n",
      ") c(x − x 0\n",
      "\n",
      "X 0\n",
      "\n",
      "X\n",
      "\n",
      "Figure 13.4 //en.wikipedia.org/wiki/Subderivative. Figure generated by subgradientPlot.\n",
      "\n",
      "Illustration of some sub-derivatives of a function at point x0. Based on a ﬁgure at http:\n",
      "\n",
      "Unfortunately, the ||w||1 term is not differentiable whenever wj = 0. This is an example of a non-smooth optimization problem.\n",
      "\n",
      "To handle non-smooth functions, we need to extend the notion of a derivative. We deﬁne a subderivative or subgradient of a (convex) function f : I → R at a point θ0 to be a scalar g such that\n",
      "\n",
      "f (θ) − f (θ0) ≥ g(θ − θ0) ∀θ ∈ I\n",
      "\n",
      "(13.44)\n",
      "\n",
      "where I is some interval containing θ0. See Figure 13.4 for an illustration.3 We deﬁne the set of subderivatives as the interval [a, b] where a and b are the one-sided limits\n",
      "\n",
      "a = lim θ→θ− 0\n",
      "\n",
      "f (θ) − f (θ0) θ − θ0\n",
      "\n",
      ", b = lim θ→θ+ 0\n",
      "\n",
      "f (θ) − f (θ0) θ − θ0\n",
      "\n",
      "(13.46)\n",
      "\n",
      "The set [a, b] of all subderivatives is called the subdifferential of the function f at θ0 and is denoted ∂f (θ)|θ0 . For example, in the case of the absolute value function f (θ) =|θ |, the subderivative is given by\n",
      "\n",
      "∂f (θ) =\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "{−1} [−1, 1] {+1}\n",
      "\n",
      "if θ < 0 if θ = 0 if θ > 0\n",
      "\n",
      "(13.47)\n",
      "\n",
      "If the function is everywhere differentiable, then ∂f (θ) = { df (θ) calculus result, one can show that the point ˆθ is a local minimum of f iff 0 ∈ ∂f (θ)|ˆθ.\n",
      "\n",
      "dθ }. By analogy to the standard\n",
      "\n",
      "3. In general, for a vector valued function, we say that g is a subgradient of f at θ0 if for all vectors θ,\n",
      "\n",
      "f (θ) − f (θ0) ≥ (θ − θ0)T g\n",
      "\n",
      "(13.45)\n",
      "\n",
      "so g is a linear lower bound to the function at θ0.\n",
      "\n",
      "13.3. (cid:6)1 regularization: basics\n",
      "\n",
      "433\n",
      "\n",
      "(cid:70)(cid:78)\n",
      "\n",
      "(cid:70)(cid:78)\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.5\n",
      "\n",
      "Left: soft thresholding. The ﬂat region is the interval [−λ, +λ]. Right: hard thresholding.\n",
      "\n",
      "Let us apply these concepts to the lasso problem. Let us initially ignore the non-smooth\n",
      "\n",
      "penalty term. One can show (Exercise 13.1) that\n",
      "\n",
      "∂ ∂wj\n",
      "\n",
      "RSS(w) =a jwj − cj\n",
      "\n",
      "(13.48)\n",
      "\n",
      "aj = 2\n",
      "\n",
      "n(cid:2)\n",
      "\n",
      "x2 ij\n",
      "\n",
      "(13.49)\n",
      "\n",
      "cj = 2\n",
      "\n",
      "i=1 n(cid:2)\n",
      "\n",
      "xij(yi − wT\n",
      "\n",
      "−jxi,−j)\n",
      "\n",
      "(13.50)\n",
      "\n",
      "i=1\n",
      "\n",
      "where w−j is w without component j, and similarly for xi,−j. We see that cj is (proportional to) the correlation between the j’th feature x:,j and the residual due to the other features, r−j = y − X:,−jw−j. Hence the magnitude of cj is an indication of how relevant feature j is for predicting y (relative to the other features and the current parameters). Adding in the penalty term, we ﬁnd that the subderivative is given by\n",
      "\n",
      "∂wj\n",
      "\n",
      "f (w) = (ajwj − cj) +λ∂ wj\n",
      "\n",
      "=\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "{ajwj − cj − λ} [−cj − λ, −cj + λ] {ajwj − cj + λ}\n",
      "\n",
      "||w||1\n",
      "\n",
      "if wj < 0 if wj = 0 if wj > 0\n",
      "\n",
      "(13.51)\n",
      "\n",
      "(13.52)\n",
      "\n",
      "We can write this in a more compact fashion as follows:\n",
      "\n",
      "XT (Xw − y)j ∈\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "{−λ} [−λ, λ] {λ}\n",
      "\n",
      "if wj < 0 if wj = 0 if wj > 0\n",
      "\n",
      "(13.53)\n",
      "\n",
      "of wj, as follows:\n",
      "\n",
      "Depending on the value of cj, the solution to ∂wj\n",
      "\n",
      "f (w) = 0 can occur at 3 different values\n",
      "\n",
      "434\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "1. If cj < −λ, so the feature is strongly negatively correlated with the residual, then the\n",
      "\n",
      "subgradient is zero at ˆwj =\n",
      "\n",
      "cj +λ aj\n",
      "\n",
      "< 0.\n",
      "\n",
      "2. If cj ∈ [−λ, λ], so the feature is only weakly correlated with the residual, then the subgradient\n",
      "\n",
      "is zero at ˆwj = 0.\n",
      "\n",
      "3. If cj > λ, so the feature is strongly positively correlated with the residual, then the subgra-\n",
      "\n",
      "dient is zero at ˆwj =\n",
      "\n",
      "cj −λ aj\n",
      "\n",
      "> 0.\n",
      "\n",
      "In summary, we have\n",
      "\n",
      "ˆwj(cj) =\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "(cj + λ)/aj 0 (cj − λ)/aj\n",
      "\n",
      "if cj < −λ if cj ∈ [−λ, λ] if cj > λ\n",
      "\n",
      "(13.54)\n",
      "\n",
      "We can write this as follows:\n",
      "\n",
      "ˆwj = soft(\n",
      "\n",
      "cj aj\n",
      "\n",
      ";\n",
      "\n",
      "λ aj\n",
      "\n",
      ")\n",
      "\n",
      "(13.55)\n",
      "\n",
      "where\n",
      "\n",
      "soft (a; δ) (cid:2) sign(a) (|a| −δ )+\n",
      "\n",
      "(13.56)\n",
      "\n",
      "and x+ = max(x, 0) is the positive part of x. This is called soft thresholding. This is illustrated in Figure 13.5(a), where we plot ˆwj vs cj. The dotted line is the line wj = cj/aj corresponding to the least squares ﬁt. The solid line, which represents the regularized estimate ˆwj(cj), shifts the dotted line down (or up) by λ, except when −λ ≤ cj ≤ λ, in which case it sets wj = 0.\n",
      "\n",
      "By contrast, in Figure 13.5(b), we illustrate hard thresholding. This sets values of wj to 0 if −λ ≤ cj ≤ λ, but it does not shrink the values of wj outside of this interval. The slope of the soft thresholding line does not coincide with the diagonal, which means that even large coefficients are shrunk towards zero; consequently lasso is a biased estimator. This is undesirable, since if the likelihood indicates (via cj) that the coefficient wj should be large, we do not want to shrink it. We will discuss this issue in more detail in Section 13.6.2.\n",
      "\n",
      "Now we ﬁnally can understand why Tibshirani invented the term “lasso” in (Tibshirani 1996): it stands for “least absolute selection and shrinkage operator”, since it selects a subset of the variables, and shrinks all the coefficients by penalizing the absolute values. If λ = 0, we get the OLS solution (of minimal (cid:6)1 norm). If λ ≥ λmax, we get ˆw = 0, where\n",
      "\n",
      "λmax = ||XT y||∞ = max\n",
      "\n",
      "j\n",
      "\n",
      "|yT x:,j|\n",
      "\n",
      "(13.57)\n",
      "\n",
      "This value is computed using the fact that 0 is optimal if (XT y)j ∈ [−λ, λ] for all j. In general, the maximum penalty for an (cid:6)1 regularized objective is\n",
      "\n",
      "λmax = max\n",
      "\n",
      "j\n",
      "\n",
      "|∇jN LL(0)|\n",
      "\n",
      "(13.58)\n",
      "\n",
      "13.3. (cid:6)1 regularization: basics\n",
      "\n",
      "435\n",
      "\n",
      "13.3.3\n",
      "\n",
      "Comparison of least squares, lasso, ridge and subset selection\n",
      "\n",
      "We can gain further insight into (cid:6)1 regularization by comparing it to least squares, and (cid:6)2 and (cid:6)0 regularized least squares. For simplicity, assume all the features of X are orthonormal, so XT X = I. In this case, the RSS is given by\n",
      "\n",
      "RSS(w) =||y − Xw||2 = yT y + wT XT Xw − 2wT XT y k − 2\n",
      "\n",
      "= const +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "w2\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "wkxikyi\n",
      "\n",
      "(13.59)\n",
      "\n",
      "(13.60)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "i\n",
      "\n",
      "so we see this factorizes into a sum of terms, one per dimension. Hence we can write down the MAP and ML estimates analytically, as follows:\n",
      "\n",
      "MLE The OLS solution is given by :ky\n",
      "\n",
      "(13.61) where x:k is the k’th column of X. This follows trivially from Equation 13.60. We see that ˆwOLS is just the orthogonal projection of feature k onto the response vector (see k Section 7.3.2).\n",
      "\n",
      "ˆwOLS k\n",
      "\n",
      "= xT\n",
      "\n",
      "Ridge One can show that the ridge estimate is given by\n",
      "\n",
      "ˆwridge k\n",
      "\n",
      "=\n",
      "\n",
      "ˆwOLS k 1 + λ\n",
      "\n",
      "(13.62)\n",
      "\n",
      "Lasso From Equation 13.55, and using the fact that ak = 2 and ˆwOLS\n",
      "\n",
      "ˆwlasso k\n",
      "\n",
      "= sign( ˆwOLS\n",
      "\n",
      "k\n",
      "\n",
      ")\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "| ˆwOLS k\n",
      "\n",
      "| −\n",
      "\n",
      "λ\n",
      "\n",
      "2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "+\n",
      "\n",
      "k\n",
      "\n",
      "= ck/2, we have\n",
      "\n",
      "(13.63)\n",
      "\n",
      "This corresponds to soft thresholding, shown in Figure 13.5(a).\n",
      "\n",
      "Subset selection If we pick the best K features using subset selection, the parameter\n",
      "\n",
      "estimate is as follows\n",
      "\n",
      "ˆwSS k\n",
      "\n",
      "=\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "ˆwOLS k 0\n",
      "\n",
      "if rank(|wOLS k otherwise\n",
      "\n",
      "|) ≤ K\n",
      "\n",
      "(13.64)\n",
      "\n",
      "where rank refers to the location in the sorted list of weight magnitudes. This corresponds to hard thresholding, shown in Figure 13.5(b).\n",
      "\n",
      "Figure 13.6(a) plots the MSE vs λ for lasso for a degree 14 polynomial, and Figure 13.6(b) plots the MSE vs polynomial order. We see that lasso gives similar results to the subset selection method.\n",
      "\n",
      "As another example, consider a data set concerning prostate cancer. We have D = 8 features and N = 67 training cases; the goal is to predict the log prostate-speciﬁc antigen levels (see (Hastie et al. 2009, p4) for more biological details). Table 13.1 shows that lasso gives better prediction accuracy (at least on this particular data set) than least squares, ridge, and best subset regression. (In each case, the strength of the regularizer was chosen by cross validation.) Lasso also gives rise to a sparse solution. Of course, for other problems, ridge may give better predictive accuracy. In practice, a combination of lasso and ridge, known as the elastic net, often performs best, since it provides a good combination of sparsity and regularization (see Section 13.5.3).\n",
      "\n",
      "436\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "35\n",
      "\n",
      "train test\n",
      "\n",
      "35\n",
      "\n",
      "performance of MLE\n",
      "\n",
      "train test\n",
      "\n",
      "30\n",
      "\n",
      "30\n",
      "\n",
      "25\n",
      "\n",
      "25\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "e s m\n",
      "\n",
      "15\n",
      "\n",
      "e s m\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "103.249\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "lambda\n",
      "\n",
      "0.1\n",
      "\n",
      "0.01\n",
      "\n",
      "0.0001\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8 degree\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.6 (a) MSE vs λ for lasso for a degree 14 polynomial. Note that λ decreases as we move to the right. Figure generated by linregPolyLassoDemo. (b) MSE versus polynomial degree. Note that the model order increases as we move to the right. See Figure 1.18 for a plot of some of these polynomial regression models. Figure generated by linregPolyVsDegree.\n",
      "\n",
      "Term Intercept lcavol lweight age lbph svi lcp gleason pgg45 Test Error\n",
      "\n",
      "LS 2.452 0.716 0.293 -0.143 0.212 0.310 -0.289 -0.021 0.277 0.586\n",
      "\n",
      "Best Subset 2.481 0.651 0.380 -0.000 -0.000 -0.000 -0.000 -0.000 0.178 0.572\n",
      "\n",
      "Ridge 2.479 0.656 0.300 -0.129 0.208 0.301 -0.260 -0.019 0.256 0.580\n",
      "\n",
      "Lasso 2.480 0.653 0.297 -0.119 0.200 0.289 -0.236 0.000 0.226 0.564\n",
      "\n",
      "Table 13.1 Results of different methods on the prostate cancer data, which has 8 features and 67 training cases. Methods are: LS = least squares, Subset = best subset regression, Ridge, Lasso. Rows represent the coefficients; we see that subset regression and lasso give sparse solutions. Bottom row is the mean squared error on the test set (30 cases). Based on Table 3.3. of (Hastie et al. 2009). Figure generated by prostateComparison.\n",
      "\n",
      "13.3.4\n",
      "\n",
      "Regularization path\n",
      "\n",
      "As we increase λ, the solution vector ˆw(λ) will tend to get sparser, although not necessarily monotonically. We can plot the values ˆwj(λ) vs λ for each feature j; this is known as the regularization path.\n",
      "\n",
      "This is illustrated for ridge regression in Figure 13.7(a), where we plot ˆwj(λ) as the regularizer λ decreases. We see that when λ = ∞, all the coefficients are zero. But for any ﬁnite value of λ, all coefficients are non-zero; furthermore, they increase in magnitude as λ is decreased.\n",
      "\n",
      "In Figure 13.7(b), we plot the analogous result for lasso. As we move to the right, the upper bound on the (cid:6)1 penalty, B, increases. When B = 0, all the coefficients are zero. As we increase\n",
      "\n",
      "13.3. (cid:6)1 regularization: basics\n",
      "\n",
      "437\n",
      "\n",
      "0.6\n",
      "\n",
      "0.7\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "lcavol lweight age lbph svi lcp gleason pgg45\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "lcavol lweight age lbph svi lcp gleason pgg45\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.1\n",
      "\n",
      "−0.1\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(a) Proﬁles of ridge coefficients for the prostate cancer example vs bound on (cid:7)2 norm of w, Figure 13.7 so small t (large λ) is on the left. The vertical line is the value chosen by 5-fold CV using the 1SE rule. Based on Figure 3.8 of (Hastie et al. 2009). Figure generated by ridgePathProstate. (b) Proﬁles of lasso coefficients for the prostate cancer example vs bound on (cid:7)1 norm of w, so small t (large λ) is on the left. Based on Figure 3.10 of (Hastie et al. 2009). Figure generated by lassoPathProstate.\n",
      "\n",
      "0.7\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "lcavol lweight age lbph svi lcp gleason pgg45\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "lcavol lweight age lbph svi lcp gleason pgg45\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.1\n",
      "\n",
      "−0.1\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1 τ\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "−0.2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5 lars step\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.8 Illustration of piecewise linearity of regularization path for lasso on the prostate cancer example. (a) We plot ˆwj(B) vs B for the critical values of B. (b) We plot vs steps of the LARS algorithm. Figure generated by lassoPathProstate.\n",
      "\n",
      "B, the coefficients gradually “turn on”. But for any value between 0 and Bmax = || ˆwOLS||1, the solution is sparse.4\n",
      "\n",
      "Remarkably, it can be shown that the solution path is a piecewise linear function of B (Efron et al. 2004). That is, there are a set of critical values of B where the active set of non-zero coefficients changes. For values of B between these critical values, each non-zero coefficient increases or decreases in a linear fashion. This is illustrated in Figure 13.8(a). Furthermore, one can solve for these critical values analytically. This is the basis of the LARS algorithm (Efron et al. 2004), which stands for “least angle regression and shrinkage” (see Section 13.4.2 for details). Remarkably, LARS can compute the entire regularization path for roughly the same\n",
      "\n",
      "4. It is common to plot the solution versus the shrinkage factor, deﬁned as s(B) =B/B max, rather than against B. This merely affects the scale of the horizontal axis, not the shape of the curves.\n",
      "\n",
      "438\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "Original (D = 4096, number of nonzeros = 160)\n",
      "\n",
      "1 0 −1 0\n",
      "\n",
      "2000 L1 reconstruction (K0 = 1024, lambda = 0.0516, MSE = 0.0027)\n",
      "\n",
      "1000\n",
      "\n",
      "3000\n",
      "\n",
      "4000\n",
      "\n",
      "1 0 −1 0\n",
      "\n",
      "1000\n",
      "\n",
      "3000 2000 Debiased (MSE = 3.26e−005)\n",
      "\n",
      "4000\n",
      "\n",
      "1 0 −1 0\n",
      "\n",
      "2000 Minimum norm solution (MSE = 0.0292)\n",
      "\n",
      "1000\n",
      "\n",
      "3000\n",
      "\n",
      "4000\n",
      "\n",
      "0.5 0 −0.5 0\n",
      "\n",
      "1000\n",
      "\n",
      "2000\n",
      "\n",
      "3000\n",
      "\n",
      "4000\n",
      "\n",
      "Figure 13.9 Example of recovering a sparse signal using lasso. See text for details. Based on Figure 1 of (Figueiredo et al. 2007). Figure generated by sparseSensingDemo, written by Mario Figueiredo.\n",
      "\n",
      "computational cost as a single least squares ﬁt (namely O(min(N D2, DN 2)).\n",
      "\n",
      "In Figure 13.8(b), we plot the coefficients computed at each critical value of B. Now the piecewise linearity is more evident. Below we display the actual coefficient values at each step along the regularization path (the last line is the least squares solution):\n",
      "\n",
      "Listing 13.1 Output of lassoPathProstate\n",
      "\n",
      "0 0.4279 0.5015 0.5610 0.5622 0.5797 0.5864 0.6994 0.7164\n",
      "\n",
      "0 0 0.0735 0.1878 0.1890 0.2456 0.2572 0.2910 0.2926\n",
      "\n",
      "0 0 0 0 0 0 -0.0321 -0.1337 -0.1425\n",
      "\n",
      "0 0 0 0 0.0036 0.1435 0.1639 0.2062 0.2120\n",
      "\n",
      "0 0 0 0.0930 0.0963 0.2003 0.2082 0.3003 0.3096\n",
      "\n",
      "0 0 0 0 0 0 0 -0.2565 -0.2890\n",
      "\n",
      "0 0 0 0 0 0 0 0 -0.0209\n",
      "\n",
      "0 0 0 0 0 0.0901 0.1066 0.2452 0.2773\n",
      "\n",
      "By changing B from 0 to Bmax, we can go from a solution in which all the weights are zero to a solution in which all weights are non-zero. Unfortunately, not all subset sizes are achievable using lasso. One can show that, if D > N , the optimal solution can have at most N variables in it, before reaching the complete set corresponding to the OLS solution of minimal (cid:6)1 norm. In Section 13.5.3, we will see that by using an (cid:6)2 regularizer as well as an (cid:6)1 regularizer (a method known as the elastic net), we can achieve sparse solutions which contain more variables than training cases. This lets us explore model sizes between N and D.\n",
      "\n",
      "13.3. (cid:6)1 regularization: basics\n",
      "\n",
      "439\n",
      "\n",
      "13.3.5 Model selection\n",
      "\n",
      "It is tempting to use (cid:6)1 regularization to estimate the set of relevant variables. In some cases, we can recover the true sparsity pattern of w∗, the parameter vector that generated the data. A method that can recover the true model in the N → ∞ limit is called model selection consistent. The details on which methods enjoy this property, and when, are beyond the scope of this book; see e.g., (Buhlmann and van de Geer 2011) for details.\n",
      "\n",
      "Instead of going into a theoretical discussion, we will just show a small example. We ﬁrst generate a sparse signal w∗ of size D = 4096, consisting of 160 randomly placed ±1 spikes. Next we generate a random design matrix X of size N × D, where N = 1024. Finally we generate a noisy observation y = Xw∗ + (cid:7), where (cid:8)i ∼ N (0, 0.012). We then estimate w from y and X.\n",
      "\n",
      "The original w∗ is shown in the ﬁrst row of Figure 13.9. The second row is the (cid:6)1 estimate ˆwL1 using λ = 0.1λmax. We see that this has “spikes” in the right places, but they are too small. The third row is the least squares estimate of the coefficients which are estimated to be non-zero based on supp( ˆwL1). This is called debiasing, and is necessary because lasso shrinks the relevant coefficients as well as the irrelevant ones. The last row is the least squares estimate for all the coefficients jointly, ignoring sparsity. We see that the (debiased) sparse estimate is an excellent estimate of the original signal. By contrast, least squares without the sparsity assumption performs very poorly.\n",
      "\n",
      "Of course, to perform model selection, we have to pick λ. It is common to use cross validation. However, it is important to note that cross validation is picking a value of λ that results in good predictive accuracy. This is not usually the same value as the one that is likely to recover the “true” model. To see why, recall that (cid:6)1 regularization performs selection and shrinkage, that is, the chosen coefficients are brought closer to 0. In order to prevent relevant coefficients from being shrunk in this way, cross validation will tend to pick a value of λ that is not too large. Of course, this will result in a less sparse model which contains irrelevant variables (false positives). Indeed, it was proved in (Meinshausen and Buhlmann 2006) that the prediction-optimal value of λ does not result in model selection consistency. In Section 13.6.2, we will discuss some adaptive mechanisms for automatically tuning λ on a per-dimension basis that does result in model selection consistency.\n",
      "\n",
      "A downside of using (cid:6)1 regularization to select variables is that it can give quite different results if the data is perturbed slightly. The Bayesian approach, which estimates posterior marginal inclusion probabilities, p(γj = 1|D), is much more robust. A frequentist solution to this is to use bootstrap resampling (see Section 6.2.1), and to rerun the estimator on different versions of the data. By computing how often each variable is selected across different trials, we can approximate the posterior inclusion probabilities. This method is known as stability selection (Meinshausen and BÃijhlmann 2010).\n",
      "\n",
      "We can threshold the stability selection (bootstrap) inclusion probabilities at some level, say 90%, and thus derive a sparse estimator. This is known as bootstrap lasso or bolasso (Bach 2008). It will include a variable if it occurs in at least 90% of sets returned by lasso (for a ﬁxed λ). This process of intersecting the sets is a way of eliminating the false positives that vanilla lasso produces. The theoretical results in (Bach 2008) prove that bolasso is model selection consistent under a wider range of conditions than vanilla lasso.\n",
      "\n",
      "As an illustration, we reproduced the experiments in (Bach 2008).\n",
      "\n",
      "In particular, we created\n",
      "\n",
      "440\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "lasso on sign inconsistent data\n",
      "\n",
      "1\n",
      "\n",
      "bolasso on sign inconsistent data 128 bootstraps\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "lasso vs bolasso on sign inconsistent data nbootstraps = [0,2,4,8,16,32,64,128,256]\n",
      "\n",
      "2\n",
      "\n",
      "0.9\n",
      "\n",
      "2\n",
      "\n",
      "0.9\n",
      "\n",
      "lasso bolasso\n",
      "\n",
      "4\n",
      "\n",
      "0.8\n",
      "\n",
      "4\n",
      "\n",
      "0.8\n",
      "\n",
      "x e d n\n",
      "\n",
      "i\n",
      "\n",
      "e b a i r a v\n",
      "\n",
      "l\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "x e d n\n",
      "\n",
      "i\n",
      "\n",
      "e b a i r a v\n",
      "\n",
      "l\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      ") t r o p p u s\n",
      "\n",
      "t c e r r o c ( P\n",
      "\n",
      "0.5\n",
      "\n",
      "14\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "14\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "16\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−log(λ)\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "0\n",
      "\n",
      "16\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−log(λ)\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−log(λ)\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 13.10 (a) Probability of selection of each variable (white = large probabilities, black = small proba- bilities) vs. regularization parameter for Lasso. As we move from left to right, we decrease the amount of regularization, and therefore select more variables. (b) Same as (a) but for bolasso. (c) Probability of correct sign estimation vs. regularization parameter. Bolasso (red, dashed) and Lasso (black, plain): The number of bootstrap replications is in {2, 4, 8, 16, 32, 64, 128, 256}. Based on Figures 1-3 of (Bach 2008). Figure generated by bolassoDemo.\n",
      "\n",
      "256 datasets of size N = 1000 with D = 16 variables, of which 8 are relevant. See (Bach 2008) for more detail on the experimental setup. For dataset n, variable j, and sparsity level k, deﬁne S(j, k, n) = I( ˆwj(λk, Dn) (cid:22)= 0). Now deﬁneP (j, k) be the average of S(j, k, n) over the 256 In Figure 13.10(a-b), we plot P vs − log(λ) for lasso and bolasso. We see that for datasets. bolasso, there is a large range of λ where the true variables are selected, but this is not the case for lasso. This is emphasized in Figure 13.10(c), where we plot the empirical probability that the correct set of variables is recovered, for lasso and for bolasso with an increasing number of bootstrap samples. Of course, using more samples takes longer. In practice, 32 bootstraps seems to be a good compromise between speed and accuracy.\n",
      "\n",
      "With bolasso, there is the usual issue of picking λ. Obviously we could use cross validation, but plots such as Figure 13.10(b) suggest another heuristic: shuffle the rows to create a large black block, and then pick λ to be in the middle of this region. Of course, operationalizing this intuition may be tricky, and will require various ad-hoc thresholds (it is reminiscent of the “ﬁnd the knee in the curve” heuristic discussed in Section 11.5.2 when discussing how to pick K for mixture models). A Bayesian approach provides a more principled method for selecting λ.\n",
      "\n",
      "13.3.6\n",
      "\n",
      "Bayesian inference for linear models with Laplace priors\n",
      "\n",
      "We have been focusing on MAP estimation in sparse linear models. It is also possible to perform Bayesian inference (see e.g., (Park and Casella 2008; Seeger 2008)). However, the posterior mean and median, as well as samples from the posterior, are not sparse; only the mode is sparse. This is another example of the phenomenon discussed in Section 5.2.1, where we said that the MAP estimate is often untypical of the bulk of the posterior.\n",
      "\n",
      "Another argument in favor of using the posterior mean comes from Equation 5.108, which showed that that plugging in the posterior mean, rather than the posterior mode, is the optimal thing to do if we want to minimize squared prediction error. (Schniter et al. 2008) shows experimentally, and (Elad and Yavnch 2009) shows theoretically, that using the posterior mean with a spike-and-slab prior results in better prediction accuracy than using the posterior mode with a Laplace prior, albeit at slightly higher computational cost.\n",
      "\n",
      "13.4. (cid:6)1 regularization: algorithms\n",
      "\n",
      "441\n",
      "\n",
      "13.4\n",
      "\n",
      "(cid:9)1 regularization: algorithms\n",
      "\n",
      "In this section, we give a brief review of some algorithms that can be used to solve (cid:6)1 regularized estimation problems. We focus on the lasso case, where we have a quadratic loss. However, most of the algorithms can be extended to more general settings, such as logistic regression (see (Yaun et al. 2010) for a comprehensive review of (cid:6)1 regularized logistic regression). Note that this area of machine learning is advancing very rapidly, so the methods below may not be state of the art by the time you read this chapter. (See (Schmidt et al. 2009; Yaun et al. 2010; Yang et al. 2010) for some recent surveys.)\n",
      "\n",
      "13.4.1\n",
      "\n",
      "Coordinate descent\n",
      "\n",
      "Sometimes it is hard to optimize all the variables simultaneously, but it easy to optimize them one by one. In particular, we can solve for the j’th coefficient with all the others held ﬁxed:\n",
      "\n",
      "w∗\n",
      "\n",
      "j = argmin\n",
      "\n",
      "z\n",
      "\n",
      "f (w + zej) − f (w)\n",
      "\n",
      "(13.65)\n",
      "\n",
      "where ej is the j’th unit vector. We can either cycle through the coordinates in a deterministic fashion, or we can sample them at random, or we can choose to update the coordinate for which the gradient is steepest.\n",
      "\n",
      "The coordinate descent method is particularly appealing if each one-dimensional optimization problem can be solved analytically For example, the shooting algorithm (Fu 1998; Wu and Lange 2008) for lasso uses Equation 13.54 to compute the optimal value of wj given all the other coefficients. See Algorithm 7 for the pseudo code (and LassoShooting for some Matlab code). See (Yaun et al. 2010) for some extensions of this method to the logistic regression case. The resulting algorithm was the fastest method in their experimental comparison, which concerned document classiﬁcation with large sparse feature vectors (representing bags of words). Other types of data (e.g., dense features and/or regression problems) might call for different algorithms.\n",
      "\n",
      "Algorithm 13.1: Coordinate descent for lasso (aka shooting algorithm) 1 Initialize w = (XT X + λI)−1XT y; 2 repeat 3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "for j = 1, . . . , D do i=1 x2 ij; i=1 xij(yi − wT xi + wjxij) ; cj aj\n",
      "\n",
      "aj = 2 cj = 2 wj = soft(\n",
      "\n",
      "(cid:10)n (cid:10)n\n",
      "\n",
      ", λ aj\n",
      "\n",
      ");\n",
      "\n",
      "7 until converged;\n",
      "\n",
      "13.4.2\n",
      "\n",
      "LARS and other homotopy methods\n",
      "\n",
      "The problem with coordinate descent is that it only updates one variable at a time, so can be slow to converge. Active set methods update many variables at a time. Unfortunately, they are\n",
      "\n",
      "442\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "more complicated, because of the need to identify which variables are constrained to be zero, and which are free to be updated.\n",
      "\n",
      "Active set methods typically only add or remove a few variables at a time, so they can take a long if they are started far from the solution. But they are ideally suited for generating a set of solutions for different values of λ, starting with the empty set, i.e., for generating regularization path. These algorithms exploit the fact that one can quickly compute ˆw(λk) from ˆw(λk−1) if λk ≈ λk−1; this is known as warm starting. In fact, even if we only want the solution for a single value of λ, call it λ∗, it can sometimes be computationally more efficient to compute a set of solutions, from λmax down to λ∗, using warm-starting; this is called a continuation method or homotopy method. This is often much faster than directly “cold-starting” at λ∗; this is particularly true if λ∗ is small.\n",
      "\n",
      "Perhaps the most well-known example of a homotopy method in machine learning is the LARS algorithm, which stands for “least angle regression and shrinkage” (Efron et al. 2004) (a similar algorithm was independently invented in (Osborne et al. 2000b,a)). This can compute ˆw(λ) for all possible values of λ in an efficient manner.\n",
      "\n",
      "LARS works as follows. It starts with a large value of λ, such that only the variable that is most correlated with the response vector y is chosen. Then λ is decreased until a second variable is found which has the same correlation (in terms of magnitude) with the current residual as the ﬁrst variable, where the residual at step k is deﬁned as rk = y − X:,Fk wk, where Fk is the current active set (c.f., Equation 13.50). Remarkably, one can solve for this new value of λ analytically, by using a geometric argument (hence the term “least angle”). This allows the algorithm to quickly “jump” to the next point on the regularization path where the active set changes. This repeats until all the variables are added.\n",
      "\n",
      "It is necessary to allow variables to be removed from the active set if we want the sequence of solutions to correspond to the regularization path of lasso. If we disallow variable removal, we get a slightly different algorithm called LAR, which tends to be faster. In particular, LAR costs the same as a single ordinary least squares ﬁt, namely O(N D min(N, D)), which is O(N D2) if N > D, and O(N 2D) if D > N . LAR is very similar to greedy forward selection, and a method known as least squares boosting (see Section 16.4.6).\n",
      "\n",
      "There have been many attempts to extend the LARS algorithm to compute the full regulariza- tion path for (cid:6)1 regularized GLMs, such as logistic regression. In general, one cannot analytically solve for the critical values of λ. Instead, the standard approach is to start at λmax, and then slowly decrease λ, tracking the solution as we go; this is called a continuation method or homotopy method. These methods exploit the fact that we can quickly compute ˆw(λk) from ˆw(λk−1) if λk ≈ λk−1; this is known as warm starting. Even if we don’t want the full path, this method is often much faster than directly “cold-starting” at the desired value of λ (this is particularly true if λ is small).\n",
      "\n",
      "The method described in (Friedman et al. 2010) combines coordinate descent with this warm- starting strategy, and computes the full regularization path for any (cid:6)1 regularized GLM. This has been implemented in the glmnet package, which is bundled with PMTK.\n",
      "\n",
      "13.4.3\n",
      "\n",
      "Proximal and gradient projection methods\n",
      "\n",
      "In this section, we consider some methods that are suitable for very large scale problems, where homotopy methods made be too slow. These methods will also be easy to extend to other kinds\n",
      "\n",
      "13.4. (cid:6)1 regularization: algorithms\n",
      "\n",
      "443\n",
      "\n",
      "of regularizers, beyond (cid:6)1, as we will see later. Our presentation in this section is based on (Vandenberghe 2011; Yang et al. 2010).\n",
      "\n",
      "Consider a convex objective of the form\n",
      "\n",
      "f (θ) = L(θ) +R (θ)\n",
      "\n",
      "(13.66)\n",
      "\n",
      "where L(θ) (representing the loss) is convex and differentiable, and R(θ) (representing the regularizer) is convex but not necessarily differentiable. For example, L(θ) = RSS(θ) and R(θ) = λ||θ||1 corresponds to the BPDN problem. As another example, the lasso problem can be formulated as follows: L(θ) = RSS(θ) and R(θ) =I C(θ), where C = {θ : ||θ||1 ≤ B}, and IC(θ) is the indicator function of a convex set C, deﬁned as\n",
      "\n",
      "IC(θ) (cid:2)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "0 +∞ otherwise\n",
      "\n",
      "θ ∈ C\n",
      "\n",
      "(13.67)\n",
      "\n",
      "In some cases, it is easy to optimize functions of the form in Equation 13.66. For example, suppose L(θ) = RSS(θ), and the design matrix is simply X = I. Then the obective becomes f (θ) = R(θ) + 1 2. The minimizer of this is given by proxR(y), which is the proximal operator for the convex function R, deﬁned by (cid:9)\n",
      "\n",
      "proxR(y) = argmin\n",
      "\n",
      "2 ||θ − y|2\n",
      "\n",
      "z\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "R(z) +\n",
      "\n",
      "1 2\n",
      "\n",
      "||z − y||2 2\n",
      "\n",
      "(13.68)\n",
      "\n",
      "Intuitively, we are returning a point that minimizes R but which is also close (proximal) to y. In general, we will use this operator inside an iterative optimizer, in which case we want to stay close to the previous iterate. In this case, we use\n",
      "\n",
      "proxR(θk) = argmin\n",
      "\n",
      "z\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "R(z) +\n",
      "\n",
      "1 2\n",
      "\n",
      "||z − θk||2 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(13.69)\n",
      "\n",
      "The key issues are: how do we efficiently compute the proximal operator for different regu- larizers R, and how do we extend this technique to more general loss functions L? We discuss these issues below.\n",
      "\n",
      "13.4.3.1\n",
      "\n",
      "Proximal operators\n",
      "\n",
      "If R(θ) = λ||θ||1, the proximal operator is given by componentwise soft-thresholding:\n",
      "\n",
      "proxR(θ) = soft(θ, λ)\n",
      "\n",
      "(13.70)\n",
      "\n",
      "as we showed in Section 13.3.2. If R(θ) = λ||θ||0, the proximal operator is given by componen- twise hard-thresholding:\n",
      "\n",
      "proxR(θ) = hard(θ,\n",
      "\n",
      "√\n",
      "\n",
      "2λ)\n",
      "\n",
      "(13.71)\n",
      "\n",
      "where hard(u, a) (cid:2) uI(|u| > a).\n",
      "\n",
      "If R(θ) = IC(θ), the proximal operator is given by the projection onto the set C:\n",
      "\n",
      "proxR(θ) = argmin\n",
      "\n",
      "z∈C\n",
      "\n",
      "||z − θ||2\n",
      "\n",
      "2 = projC(θ)\n",
      "\n",
      "(13.72)\n",
      "\n",
      "444\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "Illustration of projected gradient descent. The step along the negative gradient, to θk − gk, Figure 13.11 If we project that point onto the closest point in the set we get takes us outside the feasible set. θk+1 = projΘ(θk − gk). We can then derive the implicit update direction using dk = θk+1 − θk. Used with kind permission of Mark Schmidt.\n",
      "\n",
      "onto the rectangular set deﬁned by the box constraints C = {θ : (cid:6)j ≤ θj ≤ uj} we can use\n",
      "\n",
      "For some convex sets, it is easy to compute the projection operator. For example, to project\n",
      "\n",
      "projC(θ)j =\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "(cid:6)j θj uj\n",
      "\n",
      "θj ≤ (cid:6)j (cid:6)j ≤ θj ≤ uj θj ≥ uj\n",
      "\n",
      "(13.73)\n",
      "\n",
      "To project onto the Euclidean ball C = {θ : ||θ||2 ≤ 1} we can use\n",
      "\n",
      "projC(θ) =\n",
      "\n",
      "(cid:26) θ ||θ||2 θ\n",
      "\n",
      "||θ||2 > 1 ||θ||2 ≤ 1\n",
      "\n",
      "(13.74)\n",
      "\n",
      "To project onto the 1-norm ball C = {θ : ||θ||1 ≤ 1} we can use\n",
      "\n",
      "projC(θ) = soft(θ, λ)\n",
      "\n",
      "(13.75)\n",
      "\n",
      "where λ = 0 if ||θ||1 ≤ 1, and otherwise λ is the solution to the equation\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "max(|θj| − λ, 0) = 1\n",
      "\n",
      "(13.76)\n",
      "\n",
      "j=1\n",
      "\n",
      "We can implement the whole procedure in O(D) time, as explained in (Duchi et al. 2008).\n",
      "\n",
      "We will see an application of these different projection methods in Section 13.5.1.2.\n",
      "\n",
      "13.4.3.2\n",
      "\n",
      "Proximal gradient method\n",
      "\n",
      "We now discuss how to use the proximal operator inside of a gradient descent routine. The basic idea is to minimize a simple quadratic approximation to the loss function, centered on the\n",
      "\n",
      "13.4. (cid:6)1 regularization: algorithms\n",
      "\n",
      "445\n",
      "\n",
      "θk:\n",
      "\n",
      "θk+1 = argmin\n",
      "\n",
      "z\n",
      "\n",
      "R(z) +L( θk) +g T\n",
      "\n",
      "k (z − θk) +\n",
      "\n",
      "1 2tk\n",
      "\n",
      "||z − θk||2 2\n",
      "\n",
      "(13.77)\n",
      "\n",
      "where gk = ∇L(θk) is the gradient of the loss, tk is a constant discussed below, and the last term arises from a simple approximation to the Hessian of the loss of the form ∇2L(θk) ≈ 1 I. tk Dropping terms that are independent of z, and multiplying by tk, we can rewrite the above\n",
      "\n",
      "expression in terms of a proximal operator as follows:\n",
      "\n",
      "θk+1 = argmin\n",
      "\n",
      "z\n",
      "\n",
      "(cid:29)\n",
      "\n",
      "tkR(z) +\n",
      "\n",
      "1 2\n",
      "\n",
      "||z − uk||2 2\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "= proxtkR(uk)\n",
      "\n",
      "(13.78)\n",
      "\n",
      "(13.80) If R(θ) = 0, this is equivalent to gradient descent. If R(θ) =I C(θ), the method is equivalent If R(θ) =λ||θ ||1, the method is to projected gradient descent, sketched in Figure 13.11. known as iterative soft thresholding.\n",
      "\n",
      "mation to the Hessian ∇2L, we require that\n",
      "\n",
      "There are several ways to pick tk, or equivalently, αk = 1/tk. Given that αkI is an approxi-\n",
      "\n",
      "uk = θk − tkgk gk = ∇L(θk)\n",
      "\n",
      "(13.79)\n",
      "\n",
      "αk(θk − θk−1) ≈ gk − gk−1\n",
      "\n",
      "(13.81)\n",
      "\n",
      "in the least squares sense. Hence\n",
      "\n",
      "αk = argmin\n",
      "\n",
      "α\n",
      "\n",
      "||α(θk − θk−1) − (gk − gk−1)||2\n",
      "\n",
      "2 =\n",
      "\n",
      "(θk − θk−1)T (gk − gk−1) (θk − θk−1)T (θk − θk−1)\n",
      "\n",
      "(13.82)\n",
      "\n",
      "This is known as the Barzilai-Borwein (BB) or spectral stepsize (Barzilai and Borwein 1988; Fletcher 2005; Raydan 1997). This stepsize can be used with any gradient method, whether proximal or not. It does not lead to monotonic decrease of the objective, but it is much faster (To ensure convergence, we require that the objective than standard line search techniques. decrease “on average”, where the average is computed over a sliding window of size M + 1.)\n",
      "\n",
      "When we combine the BB stepsize with the iterative soft thresholding technique (for R(θ) = λ||θ||1), plus a continuation method that gradually reduces λ, we get a fast method for the BPDN problem known as the SpaRSA algorithm, which stands for “sparse reconstruction by separable approximation” (Wright et al. 2009). However, we will call it the iterative shrinkage and thresholding algorithm. See Algorithm 12 for some pseudocode, and SpaRSA for some Matlab code. See also Exercise 13.11 for a related approach based on projected gradient descent.\n",
      "\n",
      "13.4.3.3\n",
      "\n",
      "Nesterov’s method\n",
      "\n",
      "A faster version of proximal gradient descent can be obtained by epxanding the quadratic approximation around a point other than the most recent parameter value. In particular, consider performing updates of the form\n",
      "\n",
      "θk+1 = proxtkR(φk − tkgk)\n",
      "\n",
      "(13.83)\n",
      "\n",
      "gk = ∇L(φk)\n",
      "\n",
      "(13.84)\n",
      "\n",
      "φk = θk +\n",
      "\n",
      "k − 1 k + 2\n",
      "\n",
      "(θk − θk−1)\n",
      "\n",
      "(13.85)\n",
      "\n",
      "446\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "10\n",
      "\n",
      "11 12 until λt = λ;\n",
      "\n",
      "Algorithm 13.2: Iterative Shrinkage-Thresholding Algorithm (ISTA) 1 Input: X ∈ RN ×D, y ∈ RN , parameters λ ≥ 0, M ≥ 1, 0 < s < 1 ; 2 Initialize θ0 = 0, α = 1, r = y, λ0 = ∞; 3 repeat 4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "λt = max(s||XT r||∞, λ) // Adapt the regularizer ; repeat\n",
      "\n",
      "g = ∇L(θ); u = θ − 1 α g; θ = soft(u, λt Update α using BB stepsize in Equation 13.82 ; until f (θ) increased too much within the past M steps; r = y − Xθ // Update residual ;\n",
      "\n",
      "α );\n",
      "\n",
      "γ\n",
      "\n",
      "τj\n",
      "\n",
      "wj\n",
      "\n",
      "D\n",
      "\n",
      "yi\n",
      "\n",
      "σ2\n",
      "\n",
      "xi\n",
      "\n",
      "N\n",
      "\n",
      "Figure 13.12 Representing lasso using a Gaussian scale mixture prior.\n",
      "\n",
      "This is known as Nesterov’s method (Nesterov 2004; Tseng 2008). As before, there are a variety of ways of setting tk; typically one uses line search.\n",
      "\n",
      "When this method is combined with the iterative soft thresholding technique (for R(θ) = λ||θ||1), plus a continuation method that gradually reduces λ, we get a fast method for the BPDN problem known as the fast iterative shrinkage thesholding algorithm or FISTA (Beck and Teboulle 2009).\n",
      "\n",
      "13.4. (cid:6)1 regularization: algorithms\n",
      "\n",
      "447\n",
      "\n",
      "13.4.4\n",
      "\n",
      "EM for lasso\n",
      "\n",
      "In this section, we show how to solve the lasso problem using lasso. At ﬁrst sight, this might seem odd, since there are no hidden variables. The key insight is that we can represent the Laplace distribution as a Gaussian scale mixture (GSM) (Andrews and Mallows 1974; West 1987) as follows:\n",
      "\n",
      "Lap(wj|0, 1/γ) =\n",
      "\n",
      "γ\n",
      "\n",
      "2\n",
      "\n",
      "e−γ|wj | =\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "N (wj|0, τ 2\n",
      "\n",
      "j )Ga(τ 2\n",
      "\n",
      "j |1,\n",
      "\n",
      "γ2\n",
      "\n",
      "2\n",
      "\n",
      ")dτ 2 j\n",
      "\n",
      "(13.86)\n",
      "\n",
      "Thus the Laplace is a GSM where the mixing distibution on the variances is the exponential distribution, Expon(τ 2 2 ). Using this decomposition, we can represent the lasso model as shown in Figure 13.12. The corresponding joint distribution has the form\n",
      "\n",
      "j | γ2\n",
      "\n",
      "2 = Ga(τ 2\n",
      "\n",
      "j |1, γ2\n",
      "\n",
      "p(y, w, τ , σ2|X) =N (y|Xw, σ2IN ) N (w|0, Dτ )\n",
      "\n",
      "⎡\n",
      "\n",
      "⎤\n",
      "\n",
      "IG(σ2|aσ, bσ)\n",
      "\n",
      "⎣\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "Ga(τ 2\n",
      "\n",
      "j |1, γ2/2)\n",
      "\n",
      "⎦\n",
      "\n",
      "(13.87)\n",
      "\n",
      "j\n",
      "\n",
      "where Dτ = diag(τ 2 j ), and where we have assumed for notational simplicity that X is stan- dardized and that y is centered (so we can ignore the offset term μ). Expanding out, we get\n",
      "\n",
      "p(y, w, τ , σ2|X) ∝\n",
      "\n",
      "exp(−bσ/σ2)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "σ2\n",
      "\n",
      "(cid:4)−N/2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "wT Dτ w\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "j\n",
      "\n",
      "− 1 2σ2 (cid:9)\n",
      "\n",
      "exp(−\n",
      "\n",
      "(σ2)−(aσ+1)\n",
      "\n",
      "||y − Xw||2 2\n",
      "\n",
      "γ2\n",
      "\n",
      "2\n",
      "\n",
      "τ 2 j )\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "|Dτ |− 1\n",
      "\n",
      "2\n",
      "\n",
      "(13.88)\n",
      "\n",
      "Below we describe how to apply the EM algorithm to the model in Figure 13.12.5 In brief, in the E step we infer τ 2 j and σ2, and in the M step we estimate w. The resulting estimate ˆw is the same as the lasso estimator. This approach was ﬁrst proposed in (Figueiredo 2003) (see also (Griffin and Brown 2007; Caron and Doucet 2008; Ding and Harrison 2010) for some extensions).\n",
      "\n",
      "13.4.4.1 Why EM?\n",
      "\n",
      "Before going into the details of EM, it is worthwhile asking why we are presenting this approach at all, given that there are a variety of other (often much faster) algorithms that directly solve the (cid:6)1 MAP estimation problem (see linregFitL1Test for an empirical comparison). The reason is that the latent variable perspective brings several advantages, such as the following:\n",
      "\n",
      "\n",
      "\n",
      "It provides an easy way to derive an algorithm to ﬁnd (cid:6)1-regularized parameter estimates for a variety of other models, such as robust linear regression (Exercise 11.12) or probit regression (Exercise 13.9).\n",
      "\n",
      "5. To ensure the posterior is unimodal, one can follow (Park and Casella 2008) and slightly modify the model by making the prior variance for the weights depend on the observation noise: p(wj |τ 2 j ). The EM algorithm is easy to modify.\n",
      "\n",
      "j , σ2) =N (wj |0, σ2τ 2\n",
      "\n",
      "448\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "\n",
      "\n",
      "It suggests trying other priors on the variances besides Ga(τ 2 various extensions below.\n",
      "\n",
      "j |1, γ2/2). We will consider\n",
      "\n",
      "\n",
      "\n",
      "It makes it clear how we can compute the full posterior, p(w|D), rather than just a MAP estimate. This technique is known as the Bayesian lasso (Park and Casella 2008; Hans 2009).\n",
      "\n",
      "13.4.4.2\n",
      "\n",
      "The objective function\n",
      "\n",
      "From Equation 13.88, the complete data penalized log likelihood is as follows (dropping terms that do not depend on w)\n",
      "\n",
      "(cid:6)c(w) =−\n",
      "\n",
      "1 2σ2\n",
      "\n",
      "||y − Xw||2\n",
      "\n",
      "2 − 1 2\n",
      "\n",
      "wT Λw + const\n",
      "\n",
      "(13.89)\n",
      "\n",
      "where Λ = diag( 1 τ 2 j\n",
      "\n",
      ") is the precision matrix for w.\n",
      "\n",
      "13.4.4.3\n",
      "\n",
      "The E step\n",
      "\n",
      "The key is to compute E we can derive the full posterior, which is given by the following (Park and Casella 2008):\n",
      "\n",
      "p(1/τ 2\n",
      "\n",
      "j |w, D) =InverseGaussian\n",
      "\n",
      "+\n",
      "\n",
      "1 τ 2 j\n",
      "\n",
      "|wj\n",
      "\n",
      ",\n",
      "\n",
      ". We can derive this directly (see Exercise 13.8). Alternatively,\n",
      "\n",
      "(cid:11)7\n",
      "\n",
      "γ2 w2 j\n",
      "\n",
      ", γ2\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(13.90)\n",
      "\n",
      "(Note that the inverse Gaussian distribution is also known as the Wald distribution.) Hence\n",
      "\n",
      "E\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "1 τ 2 j\n",
      "\n",
      "|wj\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "=\n",
      "\n",
      "γ |wj|\n",
      "\n",
      "(13.91)\n",
      "\n",
      "Let Λ = diag(E\n",
      "\n",
      "1/τ 2 ) denote the result of this E step. D We also need to infer σ2. It is easy to show that that the posterior is\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "1/τ 2 1\n",
      "\n",
      ", . . . , E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "p(σ2|D, w) = IG(aσ + (N )/2, bσ +\n",
      "\n",
      "1 2\n",
      "\n",
      "(y − X ˆw)T (y − X ˆw)) = IG(aN , bN )\n",
      "\n",
      "(13.92)\n",
      "\n",
      "Hence (cid:31)\n",
      "\n",
      "E\n",
      "\n",
      "1/σ2\n",
      "\n",
      "=\n",
      "\n",
      "aN bN\n",
      "\n",
      "(cid:2) ω\n",
      "\n",
      "(13.93)\n",
      "\n",
      "13.4.4.4\n",
      "\n",
      "The M step\n",
      "\n",
      "The M step consists of computing\n",
      "\n",
      "ˆw = argmax\n",
      "\n",
      "w\n",
      "\n",
      "− 1 2\n",
      "\n",
      "ω||y − Xw||2\n",
      "\n",
      "2 − 1 2\n",
      "\n",
      "wT Λw\n",
      "\n",
      "(13.94)\n",
      "\n",
      "This is just MAP estimation under a Gaussian prior:\n",
      "\n",
      "ˆw = (σ2Λ + XT X)−1XT y\n",
      "\n",
      "(13.95)\n",
      "\n",
      "13.5. (cid:6)1 regularization: extensions\n",
      "\n",
      "449\n",
      "\n",
      "However, since we expect many wj = 0, we will have τ 2 j = 0 for many j, making inverting Λ numerically unstable. Fortunately, we can use the SVD of X, given by X = UDVT , as follows:\n",
      "\n",
      "ˆw = ΨV(VT ΨV +\n",
      "\n",
      "1 ω\n",
      "\n",
      "D−2)−1D−1UT y\n",
      "\n",
      "(13.96)\n",
      "\n",
      "where\n",
      "\n",
      "Ψ = Λ−1\n",
      "\n",
      "= diag(\n",
      "\n",
      "E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "1 1/τ 2 j\n",
      "\n",
      ") = diag(\n",
      "\n",
      "|wj| π(cid:4)(wj)\n",
      "\n",
      ")\n",
      "\n",
      "(13.97)\n",
      "\n",
      "13.4.4.5\n",
      "\n",
      "Caveat\n",
      "\n",
      "Since the lasso objective is convex, this method should always ﬁnd the global optimum. Unfor- tunately, this sometimes does not happen, for numerical reasons. In particular, suppose that in the true solution, w∗ j (cid:22)= 0. Further, suppose that we set ˆwj = 0 in an M step. In the following E step we infer that τ 2 j = 0, so then we set ˆwj = 0 again; thus we can never “undo” our mistake. Fortunately, in practice, this situation seems to be rare. See (Hunter and Li 2005) for further discussion.\n",
      "\n",
      "13.5\n",
      "\n",
      "(cid:9)1 regularization: extensions\n",
      "\n",
      "In this section, we discuss various extensions of “vanilla” (cid:6)1 regularization.\n",
      "\n",
      "13.5.1\n",
      "\n",
      "Group Lasso\n",
      "\n",
      "In standard (cid:6)1 regularization, we assume that there is a 1:1 correspondence between parameters and variables, so that if ˆwj = 0, we interpret this to mean that variable j is excluded. But In in more complex models, there may be many parameters associated with a given variable. particular, we may have a vector of weights for each input, wj. Here are some examples:\n",
      "\n",
      "Multinomial logistic regression Each feature is associated with C different weights, one\n",
      "\n",
      "per class.\n",
      "\n",
      "Linear regression with categorical inputs Each scalar input is one-hot encoded into a\n",
      "\n",
      "vector of length C.\n",
      "\n",
      "Multi-task learning In multi-task learning, we have multiple related prediction problems. For example, we might have C separate regression or binary classiﬁcation problems. Thus each feature is associated with C different weights. We may want to use a feature for all of the tasks or none of the tasks, and thus select weights at the group level (Obozinski et al. 2007).\n",
      "\n",
      "c |wjc|, we may end up with with some elements of wj,: being zero and some not. To prevent this kind of situation, we partition the parameter vector into G groups. We now minimize the following objective\n",
      "\n",
      "If we use an (cid:6)1 regularizer of the form ||w|| =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "j\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "J(w) = NLL(w) +\n",
      "\n",
      "G(cid:2)\n",
      "\n",
      "λg||wg||2\n",
      "\n",
      "(13.98)\n",
      "\n",
      "g=1\n",
      "\n",
      "450\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "where\n",
      "\n",
      "||wg||2 =\n",
      "\n",
      "7(cid:2)\n",
      "\n",
      "w2 j\n",
      "\n",
      "(13.99)\n",
      "\n",
      "j∈g\n",
      "\n",
      "is the 2-norm of the group weight vector. group lasso (Yuan and Lin 2006).\n",
      "\n",
      "number of elements in group g. For example, objective becomes\n",
      "\n",
      "We often use a larger penalty for larger groups, by setting λg = λ\n",
      "\n",
      "J(w) = NLL(w) +λ\n",
      "\n",
      "(cid:29)\n",
      "\n",
      "√ 2\n",
      "\n",
      "2\n",
      "\n",
      "(w2\n",
      "\n",
      "1 + w2\n",
      "\n",
      "2|) +\n",
      "\n",
      "If the NLL is least squares, this method is called\n",
      "\n",
      "√\n",
      "\n",
      "3\n",
      "\n",
      "dg, where dg is the if we have groups {1, 2} and {3, 4, 5}, the\n",
      "\n",
      "2\n",
      "\n",
      "(w2\n",
      "\n",
      "3 + w2\n",
      "\n",
      "4 + w2 5)\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "(13.100)\n",
      "\n",
      "Note that if we had used the square of the 2-norms, the model would become equivalent to ridge regression, since\n",
      "\n",
      "G(cid:2)\n",
      "\n",
      "||wg||2\n",
      "\n",
      "2 =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "w2\n",
      "\n",
      "j = ||w||2 2\n",
      "\n",
      "(13.101)\n",
      "\n",
      "g=1\n",
      "\n",
      "g\n",
      "\n",
      "j∈g\n",
      "\n",
      "By using the square root, we are penalizing the radius of a ball containing the group’s weight vector: the only way for the radius to be small is if all elements are small. Thus the square root results in group sparsity.\n",
      "\n",
      "A variant of this technique replaces the 2-norm with the inﬁnity-norm (Turlach et al. 2005;\n",
      "\n",
      "Zhao et al. 2005):\n",
      "\n",
      "||wg||∞ = max j∈g\n",
      "\n",
      "|wj|\n",
      "\n",
      "(13.102)\n",
      "\n",
      "It is clear that this will also result in group sparsity.\n",
      "\n",
      "An illustration of the difference is shown in Figures 13.13 and 13.14. In both cases, we have a true signal w of size D = 212 = 4096, divided into 64 groups each of size 64. We randomly choose 8 groups of w and assign them non-zero values. In the ﬁrst example, the values are drawn from a N (0, 1). In the second example, the values are all set to 1. We then pick a random design matrix X of size N × D, where N = 210 = 1024. Finally, we generate y = Xw + (cid:7), where (cid:7) ∼ N (0, 10−4IN ). Given this data, we estimate the support of w using (cid:6)1 or group (cid:6)1, and then estimate the non-zero values using least squares. We see that group lasso does a much better job than vanilla lasso, since it respects the known group structure.6 We also see that the (cid:6)∞ norm has a tendency to make all the elements within a block to have similar magnitude. This is appropriate in the second example, but not the ﬁrst. (The value of λ was the same in all examples, and was chosen by hand.)\n",
      "\n",
      "13.5.1.1\n",
      "\n",
      "GSM interpretation of group lasso\n",
      "\n",
      "Group lasso is equivalent to MAP estimation using the following prior\n",
      "\n",
      "p(w|γ, σ2) ∝ exp\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "−\n",
      "\n",
      "γ σ\n",
      "\n",
      "G(cid:2)\n",
      "\n",
      "g=1\n",
      "\n",
      "||wg||2\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(13.103)\n",
      "\n",
      "6. The slight non-zero “noise” in the (cid:7)∞ group lasso results is presumably due to numerical errors.\n",
      "\n",
      "13.5. (cid:6)1 regularization: extensions\n",
      "\n",
      "451\n",
      "\n",
      "Original (D = 4096, number groups = 64, active groups = 8)\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "3000\n",
      "\n",
      "3500\n",
      "\n",
      "4000\n",
      "\n",
      "Standard L1 (debiased 1, tau = 0.385, MSE = 0.06929)\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "3000\n",
      "\n",
      "3500\n",
      "\n",
      "4000\n",
      "\n",
      "(a)\n",
      "\n",
      "Block−L2 (debiased 1, tau = 0.385, MSE = 0.000351)\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "3000\n",
      "\n",
      "3500\n",
      "\n",
      "4000\n",
      "\n",
      "Block−Linf (debiased 1, tau = 0.385, MSE = 0.053)\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "3000\n",
      "\n",
      "3500\n",
      "\n",
      "4000\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.13 Illustration of group lasso where the original signal is piecewise Gaussian. Top left: original signal. Bottom left:: vanilla lasso estimate. Top right: group lasso estimate using a (cid:7)2 norm on the blocks. Bottom right: group lasso estimate using an (cid:7)∞ norm on the blocks. Based on Figures 3-4 of (Wright et al. 2009). Figure generated by groupLassoDemo, based on code by Mario Figueiredo.\n",
      "\n",
      "Now one can show (Exercise 13.10) that this prior can be written as a GSM, as follows:\n",
      "\n",
      "wg|σ2, τ 2\n",
      "\n",
      "τ 2 g |γ ∼ Ga(\n",
      "\n",
      "g ∼ N (0, σ2τ 2 dg + 1 2\n",
      "\n",
      "g Idg ) γ\n",
      "\n",
      ",\n",
      "\n",
      "2\n",
      "\n",
      ")\n",
      "\n",
      "(13.104)\n",
      "\n",
      "(13.105)\n",
      "\n",
      "where dg is the size of group g. So we see that there is one variance term per group, each of which comes from a Gamma prior, whose shape parameter depends on the group size, and whose rate parameter is controlled by γ. Figure 13.15 gives an example, where we have 2 groups, one of size 2 and one of size 3.\n",
      "\n",
      "small; then τ 2 suppose w1,1 is large; then τ 2 large as well.\n",
      "\n",
      "This picture also makes it clearer why there should be a grouping effect. Suppose w1,1 is 1 will be estimated to be small, which will force w1,2 to be small. Converseley, 1 will be estimated to be large, which will allow w1,2 to be become\n",
      "\n",
      "452\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "Original (D = 4096, number groups = 64, active groups = 8)\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0 0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "3000\n",
      "\n",
      "3500\n",
      "\n",
      "4000\n",
      "\n",
      "Standard L1 (debiased 1, tau = 0.356, MSE = 0.1206)\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "3000\n",
      "\n",
      "3500\n",
      "\n",
      "4000\n",
      "\n",
      "(a)\n",
      "\n",
      "Block−L2 (debiased 1, tau = 0.356, MSE = 0.000342)\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "3000\n",
      "\n",
      "3500\n",
      "\n",
      "4000\n",
      "\n",
      "Block−Linf (debiased 1, tau = 0.356, MSE = 0.000425)\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "1500\n",
      "\n",
      "2000\n",
      "\n",
      "2500\n",
      "\n",
      "3000\n",
      "\n",
      "3500\n",
      "\n",
      "4000\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.14\n",
      "\n",
      "Same as Figure 13.13, except the original signal is piecewise constant.\n",
      "\n",
      "γ\n",
      "\n",
      "τ1\n",
      "\n",
      "τ2\n",
      "\n",
      "w11\n",
      "\n",
      "w12\n",
      "\n",
      "w21\n",
      "\n",
      "w22\n",
      "\n",
      "w23\n",
      "\n",
      "yi\n",
      "\n",
      "σ2\n",
      "\n",
      "xi\n",
      "\n",
      "Figure 13.15 Graphical model for group lasso with 2 groups, the ﬁrst has size G1 = 2, the second has size G2 = 3.\n",
      "\n",
      "13.5. (cid:6)1 regularization: extensions\n",
      "\n",
      "453\n",
      "\n",
      "13.5.1.2\n",
      "\n",
      "Algorithms for group lasso\n",
      "\n",
      "There are a variety of algorithms for group lasso. Here we brieﬂy mention two. The ﬁrst approach is based on proximal gradient descent, discussed in Section 13.4.3. Since the regularizer is separable, R(w) = g ||wg||p, the proximal operator decomposes into G separate operators of the form\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "proxR(b) = argmin z∈RDg\n",
      "\n",
      "||z − b||2\n",
      "\n",
      "2 + λ||z||p\n",
      "\n",
      "(13.106)\n",
      "\n",
      "where b = θkg − tkgkg. If p = 2, one can show (Combettes and Wajs 2005) that this can be implemented as follows\n",
      "\n",
      "proxR(b) = b − projλC(b)\n",
      "\n",
      "(13.107)\n",
      "\n",
      "where C = {z : ||z||2 ≤ 1} is the (cid:6)2 ball. Using Equation 13.74, if ||b||2 < λ, we have\n",
      "\n",
      "proxR(b) = b − b = 0\n",
      "\n",
      "(13.108)\n",
      "\n",
      "otherwise we have\n",
      "\n",
      "proxR(b) = b − λ\n",
      "\n",
      "b ||b||2\n",
      "\n",
      "= b\n",
      "\n",
      "||b||2 − λ ||b||2\n",
      "\n",
      "(13.109)\n",
      "\n",
      "We can combine these into a vectorial soft-threshold function as follows (Wright et al. 2009):\n",
      "\n",
      "proxR(b) = b max(||b||2 − λ, 0) max(||b||2 − λ, 0) + λ\n",
      "\n",
      "(13.110)\n",
      "\n",
      "If p = ∞, we use C = {z : ||z||1 ≤ 1}, which is the (cid:6)1 ball. We can project onto this in O(dg) time using an algorithm described in (Duchi et al. 2008).\n",
      "\n",
      "Another approach is to modify the EM algorithm. The method is almost the same as for vanilla lasso. If we deﬁne τ 2 g(j), where g(j) is the group to which dimension j belongs, we can use the same full conditionals for σ2 and w as before. The only changes are as follows:\n",
      "\n",
      "j = τ 2\n",
      "\n",
      "We must modify the full conditional for the weight precisions, which are estimated based on\n",
      "\n",
      "a shared set of weights:\n",
      "\n",
      "7\n",
      "\n",
      "1 τ 2 g\n",
      "\n",
      "|γ, w, σ2, y, X ∼ InverseGaussian(\n",
      "\n",
      "γ2σ2 ||wg||2 2\n",
      "\n",
      ", γ2)\n",
      "\n",
      "(13.111)\n",
      "\n",
      "where ||wg||2 (cid:29)\n",
      "\n",
      "E\n",
      "\n",
      "1 τ 2 g\n",
      "\n",
      "(cid:30)\n",
      "\n",
      "=\n",
      "\n",
      "2 =\n",
      "\n",
      "γσ ||wg||2\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "j∈g w2\n",
      "\n",
      "jg. For the E step, we can use\n",
      "\n",
      "(13.112)\n",
      "\n",
      "We must modify the full conditional for the tuning parameter, which is now only estimated\n",
      "\n",
      "based on G values of τ 2 g :\n",
      "\n",
      "p(γ2|τ ) = Ga(aγ + G/2, bγ +\n",
      "\n",
      "1 2\n",
      "\n",
      "G(cid:2)\n",
      "\n",
      "g\n",
      "\n",
      "τ 2 g )\n",
      "\n",
      "(13.113)\n",
      "\n",
      "454\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "H G C\n",
      "\n",
      "5 0\n",
      "\n",
      ".\n",
      "\n",
      "0\n",
      "\n",
      ".\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      ".\n",
      "\n",
      "0 −\n",
      "\n",
      "0\n",
      "\n",
      ".\n",
      "\n",
      "1 −\n",
      "\n",
      "\n",
      "\n",
      "● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "● ●\n",
      "\n",
      "\n",
      "\n",
      "● ●\n",
      "\n",
      "● ● ● ● ● ● ● ● ● ● ●● ● ●● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "\n",
      "\n",
      "●●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "● ● ● ● ● ● ● ● ● ● ● ● ● ●●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "●●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "●● ● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "● ●\n",
      "\n",
      "●● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "● ●\n",
      "\n",
      "●● ●\n",
      "\n",
      "\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●● ● ●\n",
      "\n",
      "●\n",
      "\n",
      "●● ● ●\n",
      "\n",
      "\n",
      "\n",
      "● ●\n",
      "\n",
      "● ●●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "● ● ● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "● ●●\n",
      "\n",
      "● ●● ●\n",
      "\n",
      "\n",
      "\n",
      "● ● ●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "\n",
      "\n",
      "● ● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "●●\n",
      "\n",
      "● ●\n",
      "\n",
      "●● ●● ●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "● ● ● ● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "● ● ● ● ●\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "●● ● ●\n",
      "\n",
      "● ● ● ● ● ● ● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "●● ●\n",
      "\n",
      "●●\n",
      "\n",
      "●● ●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "● ●\n",
      "\n",
      "●● ● ●\n",
      "\n",
      "\n",
      "\n",
      "●\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 . 1 −\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "Index\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 13.16 (a) Example of the fused lasso. The vertical axis represents array CGH (chromosomal genome Source: Figure 1 of hybridization) intensity, and the horizontal axis represents location along a genome. (Hoeﬂing 2010). Source: Figure 2 of (Hoeﬂing 2010). Used with kind permission of Holger Hoeﬂing.\n",
      "\n",
      "(b) Noisy image.\n",
      "\n",
      "(c) Fused lasso estimate using 2d lattice prior.\n",
      "\n",
      "13.5.2\n",
      "\n",
      "Fused lasso\n",
      "\n",
      "In some problem settings (e.g., functional data analysis), we want neighboring coefficients to be similar to each other, in addition to being sparse. An example is given in Figure 13.16(a), where we want to ﬁt a signal that is mostly “off”, but in addition has the property that neighboring locations are typically similar in value. We can model this by using a prior of the form\n",
      "\n",
      "⎛\n",
      "\n",
      "⎞\n",
      "\n",
      "p(w|σ2) ∝ exp\n",
      "\n",
      "⎝−\n",
      "\n",
      "λ1 σ\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "|wj| −\n",
      "\n",
      "λ2 σ\n",
      "\n",
      "D−1(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "|wj+1 − wj|\n",
      "\n",
      "⎠\n",
      "\n",
      "(13.114)\n",
      "\n",
      "This is known as the fused lasso penalty. In the context of functional data analysis, we often use X = I, so there is one coefficient for each location in the signal (see Section 4.4.2.3). In this case, the overall objective has the form\n",
      "\n",
      "J(w, λ1, λ2) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(yi − wi)2 + λ1\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "|wi| + λ2\n",
      "\n",
      "N −1(cid:2)\n",
      "\n",
      "|wi+1 − wi|\n",
      "\n",
      "(13.115)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "This is a sparse version of Equation 4.148.\n",
      "\n",
      "It is possible to generalize this idea beyond chains, and to consider other graph structures,\n",
      "\n",
      "using a penalty of the form\n",
      "\n",
      "J(w, λ1, λ2) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(ys − ws)2 + λ1\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "|ws| + λ2\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "|ws − wt|\n",
      "\n",
      "(13.116)\n",
      "\n",
      "s∈V\n",
      "\n",
      "s∈V\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "This is called graph-guided fused lasso (see e.g., (Chen et al. 2010)). The graph might come from some prior knowledge, e.g., from a database of known biological pathways. Another example is shown in Figure 13.16(b-c), where the graph structure is a 2d lattice.\n",
      "\n",
      "13.5. (cid:6)1 regularization: extensions\n",
      "\n",
      "455\n",
      "\n",
      "13.5.2.1\n",
      "\n",
      "GSM interpretation of fused lasso\n",
      "\n",
      "One can show (Kyung et al. 2010) that the fused lasso model is equivalent to the following hierarchical model\n",
      "\n",
      "w|σ2, τ , ω ∼ N (0, σ2Σ(τ , ω))\n",
      "\n",
      "(13.117)\n",
      "\n",
      "γ2 1 2 γ2 2 2 where Σ = Ω−1, and Ω is a tridiagonal precision matrix with\n",
      "\n",
      "ω2 j |γ2 ∼ Expon(\n",
      "\n",
      "τ 2 j |γ1 ∼ Expon(\n",
      "\n",
      "), j = 1 :D\n",
      "\n",
      "), j = 1 :D − 1\n",
      "\n",
      "(13.118)\n",
      "\n",
      "(13.119)\n",
      "\n",
      "main diagonal = { 1 τ 2 j off diagonal = {− 1 ω2 j\n",
      "\n",
      "+\n",
      "\n",
      "}\n",
      "\n",
      "1 ω2\n",
      "\n",
      "j−1\n",
      "\n",
      "+\n",
      "\n",
      "1 ω2 j\n",
      "\n",
      "}\n",
      "\n",
      "(13.120)\n",
      "\n",
      "(13.121)\n",
      "\n",
      "where we have deﬁned ω−2 D = 0. This is very similar to the model in Section 4.4.2.3, where we used a chain-structured Gaussian Markov random ﬁeld as the prior, with ﬁxed vari- ance. Here we just let the variance be random. In the case of graph-guided lasso, the structure of the graph is reﬂected in the zero pattern of the Gaussian precision matrix (see Section 19.4.4).\n",
      "\n",
      "0 = ω−2\n",
      "\n",
      "13.5.2.2\n",
      "\n",
      "Algorithms for fused lasso\n",
      "\n",
      "It is possible to generalize the EM algorithm to ﬁt the fused lasso model, by exploiting the Markov structure of the Gaussian prior for efficiency. Direct solvers (which don’t use the latent variable trick) can also be derived (see e.g., (Hoeﬂing 2010)). However, this model is undeniably more expensive to ﬁt than the other variants we have considered.\n",
      "\n",
      "13.5.3\n",
      "\n",
      "Elastic net (ridge and lasso combined)\n",
      "\n",
      "Although lasso has proved to be effective as a variable selection technique, problems (Zou and Hastie 2005), such as the following:\n",
      "\n",
      "it has several\n",
      "\n",
      "\n",
      "\n",
      "If there is a group of variables that are highly correlated (e.g., genes that are in the same pathway), then the lasso tends to select only one of them, chosen rather arbitrarily. (This is evident from the LARS algorithm: once one member of the group has been chosen, the remaining members of the group will not be very correlated with the new residual and hence will not be chosen.) It is usually better to select all the relevant variables in a group. If we know the grouping structure, we can use group lasso, but often we don’t know the grouping structure.\n",
      "\n",
      "\n",
      "\n",
      "In the D > N case, lasso can select at most N variables before it saturates.\n",
      "\n",
      "\n",
      "\n",
      "If N > D, but the variables are correlated, prediction performance of ridge is better than that of lasso.\n",
      "\n",
      "it has been empirically observed that the\n",
      "\n",
      "456\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "Zou and Hastie (Zou and Hastie 2005) proposed an approach called the elastic net, which is a hybrid between lasso and ridge regression, which solves all of these problems. It is apparently called the “elastic net” because it is “like a stretchable ﬁshing net that retains ’all the big ﬁsh”’ (Zou and Hastie 2005).\n",
      "\n",
      "13.5.3.1\n",
      "\n",
      "Vanilla version\n",
      "\n",
      "The vanilla version of the model deﬁnes the following objective function:\n",
      "\n",
      "J(w, λ1, λ2) = ||y − Xw||2 + λ2||w||2\n",
      "\n",
      "2 + λ1||w||1\n",
      "\n",
      "(13.122)\n",
      "\n",
      "Notice that this penalty function is strictly convex (assuming λ2 > 0) so there is a unique global minimum, even if X is not full rank.\n",
      "\n",
      "It can be shown (Zou and Hastie 2005) that any strictly convex penalty on w will exhibit a grouping effect, which means that the regression coefficients of highly correlated variables tend to be equal (up to a change of sign if they are negatively correlated). For example, if two features are equal, so X:j = X:k, one can show that their estimates are also equal, ˆwj = ˆwk. By contrast, with lasso, we may have that ˆwj = 0 and ˆwk (cid:22)= 0 or vice versa.\n",
      "\n",
      "13.5.3.2\n",
      "\n",
      "Algorithms for vanilla elastic net\n",
      "\n",
      "It is simple to show (Exercise 13.5) that the elastic net problem can be reduced to a lasso problem on modiﬁed data. In particular, deﬁne (cid:9)\n",
      "\n",
      "˜X = c\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "X√\n",
      "\n",
      "λ2ID\n",
      "\n",
      "(cid:9)\n",
      "\n",
      ",\n",
      "\n",
      "˜y =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "y 0D×1\n",
      "\n",
      "(13.123)\n",
      "\n",
      "where c = (1 + λ2)− 1\n",
      "\n",
      "2 . Then we solve\n",
      "\n",
      "˜w = arg min ˜w\n",
      "\n",
      "||˜y − ˜X ˜w||2 + cλ1|| ˜w||1\n",
      "\n",
      "(13.124)\n",
      "\n",
      "and set w = c ˜w.\n",
      "\n",
      "If we stop the algorithm after m variables have been included, the cost is O(m3 + Dm2). Note that we can use m = D if we wish, since ˜X has rank D. This is in contrast to lasso, which cannot select more than N variables (before jumping to the OLS solution) if N < D.\n",
      "\n",
      "λ2.\n",
      "\n",
      "We can use LARS to solve this subproblem; this is known as the LARS-EN algorithm.\n",
      "\n",
      "When using LARS-EN (or other (cid:6)1 solvers), one typically uses cross-validation to select λ1 and\n",
      "\n",
      "13.5.3.3\n",
      "\n",
      "Improved version\n",
      "\n",
      "Unfortunately it turns out that the “vanilla” elastic net does not produce functions that predict very accurately, unless it is very close to either pure ridge or pure lasso. Intuitively the reason is that it performs shrinkage twice: once due to the (cid:6)2 penalty and again due to the (cid:6)1 penalty. The solution is simple: undo the (cid:6)2 shrinkage by scaling up the estimates from the vanilla version. In other words, if w∗ is the solution of Equation 13.124, then a better estimate is\n",
      "\n",
      "ˆw =\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "1 + λ2 ˜w\n",
      "\n",
      "(13.125)\n",
      "\n",
      "13.6. Non-convex regularizers\n",
      "\n",
      "457\n",
      "\n",
      "We will call this a corrected estimate.\n",
      "\n",
      "One can show that the corrected estimates are given by\n",
      "\n",
      "ˆw = arg min w\n",
      "\n",
      "wT\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "XT X + λ2I 1 + λ2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "w − 2yT Xw + λ1||w||1\n",
      "\n",
      "(13.126)\n",
      "\n",
      "Now\n",
      "\n",
      "XT X + λ2I 1 + λ2\n",
      "\n",
      "= (1 − ρ) ˆΣ + ρI\n",
      "\n",
      "(13.127)\n",
      "\n",
      "where ρ = λ2/(1 + λ2). So the the elastic net is like lasso but where we use a version of ˆΣ that is shrunk towards I. (See Section 4.2.6 for more discussion of regularized estimates of covariance matrices.)\n",
      "\n",
      "13.5.3.4\n",
      "\n",
      "GSM interpretation of elastic net\n",
      "\n",
      "The implicit prior being used by the elastic net obviously has the form\n",
      "\n",
      "⎛\n",
      "\n",
      "⎞\n",
      "\n",
      "p(w|σ2) ∝ exp\n",
      "\n",
      "⎝−\n",
      "\n",
      "γ1 σ\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "|wj| −\n",
      "\n",
      "γ2 2σ2\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "w2 j\n",
      "\n",
      "⎠\n",
      "\n",
      "(13.128)\n",
      "\n",
      "which is just a product of Gaussian and Laplace distributions.\n",
      "\n",
      "This can be written as a hierarchical prior as follows (Kyung et al. 2010; Chen et al. 2011):\n",
      "\n",
      "wj|σ2, τ 2\n",
      "\n",
      "j ∼ N (0, σ2(τ −2\n",
      "\n",
      "j + γ2)−1)\n",
      "\n",
      "(13.129)\n",
      "\n",
      "τ 2 j |γ1 ∼ Expon(\n",
      "\n",
      "γ2 1 2\n",
      "\n",
      ")\n",
      "\n",
      "(13.130)\n",
      "\n",
      "Clearly if γ2 = 0, this reduces to the regular lasso.\n",
      "\n",
      "It is possible to perform MAP estimation in this model using EM, or Bayesian inference using\n",
      "\n",
      "MCMC (Kyung et al. 2010) or variational Bayes (Chen et al. 2011).\n",
      "\n",
      "13.6 Non-convex regularizers\n",
      "\n",
      "Although the Laplace prior results in a convex optimization problem, from a statistical point of view this prior is not ideal. There are two main problems with it. First, it does not put enough probability mass near 0, so it does not sufficiently suppress noise. Second, it does not put enough probability mass on large values, so it causes shrinkage of relevant coefficients, corresponding to “signal”. (This can be seen in Figure 13.5(a): we see that (cid:6)1 estimates of large coefficients are signiﬁcantly smaller than their ML estimates, a phenomenon known as bias.)\n",
      "\n",
      "Both problems can be solved by going to more ﬂexible kinds of priors which have a larger spike at 0 and heavier tails. Even though we cannot ﬁnd the global optimum anymore, these non-convex methods often outperform (cid:6)1 regularization, both in terms of predictive accuracy and in detecting relevant variables (Fan and Li 2001; Schniter et al. 2008). We give some examples below.\n",
      "\n",
      "458\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "13.6.1\n",
      "\n",
      "Bridge regression\n",
      "\n",
      "A natural generalization of (cid:6)1 regularization, known as bridge regression (Frank and Friedman 1993), has the form\n",
      "\n",
      "ˆw = NLL(w) +λ\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "|wj|b\n",
      "\n",
      "(13.131)\n",
      "\n",
      "j\n",
      "\n",
      "for b ≥ 0. This corresponds to MAP estimation using a exponential power distribution given by\n",
      "\n",
      "ExpPower(w|μ, a, b) (cid:2)\n",
      "\n",
      "b 2aΓ(1 + 1/b)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "−\n",
      "\n",
      "|x − μ| a\n",
      "\n",
      "b\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(13.132)\n",
      "\n",
      "If b = 2, we get the Gaussian distribution (with a = σ 2), corresonding to ridge regression; if we set b = 1, we get the Laplace distribution, corresponding to lasso; if we set b = 0, we get (cid:6)0 regression, which is equivalent to best subset selection. Unfortunately, the objective is not convex for b <1, and is not sparsity promoting for b >1. So the (cid:6)1 norm is the tightest convex approximation to the (cid:6)0 norm.\n",
      "\n",
      "The effect of changing b is illustrated in Figure 13.17, where we plot the prior for b = 2, b = 1 and b = 0.4; we assume p(w) =p( w1)p(w2). We also plot the posterior after seeing a single observation, (x, y), which imposes a single linear constraint of the form, y = wT x, with a certain tolerance controlled by the observation noise (compare to Figure 7.11). We see see that the mode of the Laplace is on the vertical axis, corresponding to w1 = 0. By contrast, there are two modes when using b = 0.4, corresponding to two different sparse solutions. When using the Gaussian, the MAP estimate is not sparse (the mode does not lie on either of the coordinate axes).\n",
      "\n",
      "√\n",
      "\n",
      "13.6.2\n",
      "\n",
      "Hierarchical adaptive lasso\n",
      "\n",
      "Recall that one of the principal problems with lasso is that it results in biased estimates. This is because it needs to use a large value of λ to “squash” the irrelevant parameters, but this then over-penalizes the relevant parameters. It would be better if we could associate a different penalty parameter with each parameter. Of course, it is completely infeasible to tune D parameters by cross validation, but this poses no problem to the Bayesian: we simply make each τ 2 j have its own private tuning parameter, γj, which are now treated as random variables coming from the conjugate prior γj ∼ IG(a, b). The full model is as follows:\n",
      "\n",
      "γj ∼ IG(a, b) j |γj ∼ Ga(1, γ2 τ 2 j /2) j ∼ N (0, τ 2 wj|τ 2 j )\n",
      "\n",
      "(13.133)\n",
      "\n",
      "(13.134)\n",
      "\n",
      "(13.135)\n",
      "\n",
      "See Figure 13.18(a). This has been called the hierarchical adaptive lasso (HAL) (Lee et al. 2010) (see also (Lee et al. 2011; Cevher 2009; Armagan et al. 2011)). We can integrate out τ 2 j , which induces a Lap(wj|0, 1/γj) distribution on wj as before. The result is that p(wj) is now a It turns out that we can ﬁt this model (i.e., compute a local scaled mixture of Laplacians. posterior mode) using EM, as we explain below. The resulting estimate, ˆwHAL, often works\n",
      "\n",
      "13.6. Non-convex regularizers\n",
      "\n",
      "459\n",
      "\n",
      "Figure 13.17 Top: plot of log prior for three different distributions with unit variance: Gaussian, Laplace and exponential power. Bottom: plot of log posterior after observing a single observation, corresponding to a single linear constraint. The precision of this observation is shown by the diagonal lines in the top ﬁgure. In the case of the Laplace prior, the posterior is unimodal and asymmetric (skewed). In the case of the exponential prior, the posterior is bimodal. Based on Figure 1 of (Seeger 2008). Figure generated by sparsePostPlot, written by Florian Steinke.\n",
      "\n",
      "In the case of the Gaussian prior, the posterior is unimodal and symmetric.\n",
      "\n",
      "much better than the estimate returned by lasso, ˆwL1, in the sense that it is more likely to contain zeros in the right places (model selection consistency) and more likely to result in good predictions (prediction consistency) (Lee et al. 2010). We give an explanation for this behavior in Section 13.6.2.2.\n",
      "\n",
      "13.6.2.1\n",
      "\n",
      "EM for HAL\n",
      "\n",
      "Since the inverse Gamma is conjugate to the Laplace, we ﬁnd that the E step for γj is given by\n",
      "\n",
      "p(γj|wj) = IG(a + 1, b + |wj|)\n",
      "\n",
      "(13.136)\n",
      "\n",
      "The E step for σ2 is the same as for vanilla lasso.\n",
      "\n",
      "The prior for w has the following form:\n",
      "\n",
      "p(w|γ) =\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "j\n",
      "\n",
      "1 2γj\n",
      "\n",
      "exp(−|wj|/γj)\n",
      "\n",
      "(13.137)\n",
      "\n",
      "Hence the M step must optimize\n",
      "\n",
      "ˆw(t+1) = argmax\n",
      "\n",
      "w\n",
      "\n",
      "log N (y|Xw, σ2) −\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "j\n",
      "\n",
      "|wj|E [1/γj]\n",
      "\n",
      "(13.138)\n",
      "\n",
      "460\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "a\n",
      "\n",
      "b\n",
      "\n",
      "1\n",
      "\n",
      "HAL\n",
      "\n",
      "0.8\n",
      "\n",
      "a=1, b=0.01 a=1, b=0.10 a=1, b=1.00\n",
      "\n",
      "γj\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "τ 2 j\n",
      "\n",
      "0.2\n",
      "\n",
      "wj D\n",
      "\n",
      "0\n",
      "\n",
      "aσ\n",
      "\n",
      "σ2\n",
      "\n",
      "yi\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "bσ\n",
      "\n",
      "−0.6\n",
      "\n",
      "xi\n",
      "\n",
      "N\n",
      "\n",
      "−0.8\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.18 (a) DGM for hierarchical adaptive lasso. (b) Contours of Hierarchical adpative Laplace. Based on Figure 1 of (Lee et al. 2010). Figure generated by normalGammaPenaltyPlotDemo.\n",
      "\n",
      "The expectation is given by\n",
      "\n",
      "E [1/γj] =\n",
      "\n",
      "a + 1 b + |w(t) j\n",
      "\n",
      "|\n",
      "\n",
      "(cid:2) s(t) j\n",
      "\n",
      "(13.139)\n",
      "\n",
      "Thus the M step becomes a weighted lasso problem:\n",
      "\n",
      "ˆw(t+1) = argmin\n",
      "\n",
      "w\n",
      "\n",
      "||y − Xw||2\n",
      "\n",
      "2 +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "j\n",
      "\n",
      "s(t) j |wj|\n",
      "\n",
      "(13.140)\n",
      "\n",
      "This is easily solved using standard methods (e.g., LARS). Note that if the coefficient was esti- mated to be large in the previous iteration (so w(t) j will be small, so large coefficients are not penalized heavily. Conversely, small coefficients do get penalized heavily. This is the way that the algorithm adapts the penalization strength of each coefficient. The result is an estimate that is often much sparser than returned by lasso, but also less biased.\n",
      "\n",
      "Note that if we seta = b = 0, and we only perform 1 iteration of EM, we get a method that is closely related to the adaptive lasso of (Zou 2006; Zou and Li 2008). This EM algorithm is also closely related to some iteratively reweighted (cid:6)1 methods proposed in the signal processing community (Chartrand and Yin 2008; Candes et al. 2008).\n",
      "\n",
      "j\n",
      "\n",
      "is large), then the scaling factor s(t)\n",
      "\n",
      "13.6.2.2\n",
      "\n",
      "Understanding the behavior of HAL\n",
      "\n",
      "We can get a better understanding of HAL by integrating out γj to get the following marginal distribution,\n",
      "\n",
      "p(wj|a, b) =\n",
      "\n",
      "a 2b\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "|wj| b\n",
      "\n",
      "(cid:9)−(a+1)\n",
      "\n",
      "+ 1\n",
      "\n",
      "(13.141)\n",
      "\n",
      "13.6. Non-convex regularizers\n",
      "\n",
      "461\n",
      "\n",
      "Lasso\n",
      "\n",
      "HAL\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "P A M w\n",
      "\n",
      "0\n",
      "\n",
      "P A M w\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−8\n",
      "\n",
      "−8\n",
      "\n",
      "b = 0.010, a=1 b = 0.100, a=1 b = 1.000, a=1\n",
      "\n",
      "−10\n",
      "\n",
      "−10\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0 wMLE\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "−10\n",
      "\n",
      "−10\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0 wMLE\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.19 Thresholding behavior of (b) Hierarchical adaptive Laplace. normalGammaThresholdPlotDemo.\n",
      "\n",
      "Based on Figure 2 of\n",
      "\n",
      "two penalty functions (negative log priors).\n",
      "\n",
      "(Lee et al. 2010).\n",
      "\n",
      "(a) Laplace. Figure generated by\n",
      "\n",
      "This is an instance of the generalized t distribution (McDonald and Newey 1988) (in (Cevher 2009; Armagan et al. 2011), this is called the double Pareto distribution) deﬁned as (cid:8)\n",
      "\n",
      "GT(w|μ, a, c, q) (cid:2)\n",
      "\n",
      "q 2ca1/qB(1/q, a)\n",
      "\n",
      "1 +\n",
      "\n",
      "|w − μ|q acq\n",
      "\n",
      "(cid:9)−(a+1/q)\n",
      "\n",
      "(13.142)\n",
      "\n",
      "where c is the scale parameter (which controls the degree of sparsity), and a is related to the √ degrees of freedom. When q = 2 and c = 2 we recover the standard t distribution; when a → ∞, we recover the exponential power distribution; and when q = 1 and a = ∞ we In the context of the current model, we see that p(wj|a, b) = get the Laplace distribution. GT(wj|0, a, b/a, 1).\n",
      "\n",
      "The resulting penalty term has the form\n",
      "\n",
      "πλ(wj) (cid:2) − log p(wj) = (a + 1) log(1 +\n",
      "\n",
      "|wj| b\n",
      "\n",
      ") + const\n",
      "\n",
      "(13.143)\n",
      "\n",
      "where λ = (a, b) are the tuning parameters. We plot this penalty in 2d (i.e., we plot πλ(w1) + πλ(w2)) in Figure 13.18(b) for various values of b. Compared to the diamond-shaped Laplace it penalty, shown in Figure 13.3(a), we see that the HAL penalty looks more like a “star ﬁsh”: puts much more density along the “spines”, thus enforcing sparsity more aggressively. Note that this penalty is clearly not convex.\n",
      "\n",
      "We can gain further understanding into the behavior of this penalty function by considering applying it to the problem of linear regression with an orthogonal design matrix. In this case,\n",
      "\n",
      "462\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "p(τ 2 j ) Ga(1, γ2 2 ) Ga(1, γ2 2 ) Ga(1, γ2 2 ) Ga(δ, γ2 2 ) Ga(τ 2 j |0, 0) 2 , δγ2 IG( δ 2 ) C +(0, γ)\n",
      "\n",
      "p(γj) Fixed IG(a, b) GT(0, a, b/a, 1) Ga(a, b) Fixed - Fixed C +(0, b)\n",
      "\n",
      "p(wj) Lap(0, 1/γ)\n",
      "\n",
      "NEG(a, b) NG(δ, γ) NJ(wj) T (0, δ, γ) horseshoe(b)\n",
      "\n",
      "Ref\n",
      "\n",
      "(Andrews and Mallows 1974; West 1987) (Lee et al. 2010, 2011; Cevher 2009; Armagan et al. 2011) (Griffin and Brown 2007, 2010; Chen et al. 2011) (Griffin and Brown 2007, 2010) (Figueiredo 2003) (Andrews and Mallows 1974; West 1987) (Carvahlo et al. 2010)\n",
      "\n",
      "Table 13.2 Some scale mixtures of Gaussians. Abbreviations: C + = half-rectiﬁed Cauchy; Ga = Gamma (shape and rate parameterization); GT = generalized t; IG = inverse Gamma; NEG = Normal-Exponential- Gamma; NG = Normal-Gamma; NJ = Normal-Jeffreys. The horseshoe distribution is the name we give to the distribution induced on wj by the prior described in (Carvahlo et al. 2010); this has no simple analytic form. The deﬁnitions of the NEG and NG densities are a bit complicated, but can be found in the references. The other distributions are deﬁned in the text.\n",
      "\n",
      "one can show that the objective becomes\n",
      "\n",
      "J(w) =\n",
      "\n",
      "1 2\n",
      "\n",
      "||y − Xw||2\n",
      "\n",
      "2 +\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "πλ(|wj|)\n",
      "\n",
      "(13.144)\n",
      "\n",
      "=\n",
      "\n",
      "1 2\n",
      "\n",
      "||y − ˆy||2 +\n",
      "\n",
      "1 2\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "( ˆwmle\n",
      "\n",
      "j − wj)2 +\n",
      "\n",
      "j=1\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "πλ(|wj|)\n",
      "\n",
      "(13.145)\n",
      "\n",
      "where ˆwmle = XT y is the MLE and ˆy = X ˆwmle. Thus we can compute the MAP estimate one dimension at a time by solving the following 1d optimization problem:\n",
      "\n",
      "ˆwj = argmin\n",
      "\n",
      "wj\n",
      "\n",
      "1 2\n",
      "\n",
      "( ˆwmle\n",
      "\n",
      "j − wj)2 + πλ(wj)\n",
      "\n",
      "(13.146)\n",
      "\n",
      "In Figure 13.19(a) we plot the lasso estimate, ˆwL1, vs the ML estimate, ˆwmle. We see that the (cid:6)1 estimator has the usual soft-thresholding behavior seen earlier in Figure 13.5(a). However, this behavior is undesirable since the large magnitude coefficients are also shrunk towards 0, whereas we would like them to be equal to their unshrunken ML estimates.\n",
      "\n",
      "In Figure 13.19(b) we plot the HAL estimate, ˆwHAL, vs the ML estimate ˆwmle. We see that this approximates the more desirable hard thresholding behavior seen earlier in Figure 13.5(b) much more closely.\n",
      "\n",
      "13.6.3\n",
      "\n",
      "Other hierarchical priors\n",
      "\n",
      "Many other hierarchical sparsity-promoting priors have been proposed; see Table 13.2 for a brief In some cases, we can analytically derive the form of the marginal prior for wj. summary. Generally speaking, this prior is not concave.\n",
      "\n",
      "in (Figueiredo 2003). This puts a non-informative Jeffreys prior on the variance, Ga(τ 2\n",
      "\n",
      "A particularly interesting prior is the improper Normal-Jeffreys prior, which has been used j |0, 0) ∝\n",
      "\n",
      "13.7. Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)\n",
      "\n",
      "463\n",
      "\n",
      "1/τ 2 j ; the resulting marginal has the form p(wj) =NJ( wj) ∝ 1/|wj|. This gives rise to a thresholding rule that looks very similar to HAL in Figure 13.19(b), which in turn is very similar to hard thresholding. However, this prior has no free parameters, which is both a good thing (nothing to tune) and a bad thing (no ability to adapt the level of sparsity).\n",
      "\n",
      "13.7\n",
      "\n",
      "Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)\n",
      "\n",
      "All the methods we have considered so far (except for the spike-and-slab methods in Sec- tion 13.2.1) have used a factorial prior of the form p(w) = j p(wj). We have seen how these priors can be represented in terms of Gaussian scale mixtures of the form wj ∼ N (0, τ 2 j ), where τ 2 j has one of the priors listed in Table 13.2. Using these latent variances, we can represent the model in the form τ 2 j → wj → y ← X. We can then use EM to perform MAP estimation, where in the E step we inferp( τ 2 j |wj), and in the M step we estimate w from y, X and τ . This M step either involves a closed-form weighted (cid:6)2 optimization (in the case of Gaussian scale mixtures), or a weighted (cid:6)1 optimization (in the case of Laplacian scale mixtures). We also discussed how to perform Bayesian inference in such models, rather than just computing MAP estimates.\n",
      "\n",
      "’\n",
      "\n",
      "In this section, we discuss an alternative approach based on type II ML estimation (empirical Bayes), whereby we integrate out w and maximize the marginal likelihood wrt τ . This EB procedure can be implemented via EM, or via a reweighted (cid:6)1 scheme, as we will explain below. Having estimated the variances, we plug them in to compute the posterior mean of the weights, E [w|ˆτ , D]; rather surprisingly (in view of the Gaussian prior), the result is an (approximately) sparse estimate, for reasons we explain below.\n",
      "\n",
      "In the context of neural networks, this this method is called called automatic relevance determination or ARD (MacKay 1995b; Neal 1996): see Section 16.5.7.5. In the context of the linear models we are considering in this chapter, this method is called sparse Bayesian learning or SBL (Tipping 2001). Combining ARD/SBL with basis function expansion in a linear model gives rise to a technique called the relevance vector machine (RVM), which we will discuss in Section 14.3.2.\n",
      "\n",
      "13.7.1\n",
      "\n",
      "ARD for linear regression\n",
      "\n",
      "We will explain the procedure in the context of linear regression; ARD for GLMs requires the use of the Laplace (or some other) approximation. case can be It is conventional, when discussing ARD / SBL, to denote the weight precisions by αj = 1/τ 2 j , and the measurement precision by β = 1/σ2 (do not confuse this with the use of β in statistics to represent the regression coefficients!). In particular, we will assume the following model:\n",
      "\n",
      "p(y|x, w, β) =N (y|wT x, 1/β)\n",
      "\n",
      "(13.147)\n",
      "\n",
      "p(w) =N (w|0, A−1)\n",
      "\n",
      "(13.148)\n",
      "\n",
      "464\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "where A = diag(α). The marginal likelihood can be computed analytically as follows:\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(y|X, α, β) =\n",
      "\n",
      "N (y|Xw, βIN )N (w|0, A)dw\n",
      "\n",
      "(13.149)\n",
      "\n",
      "= N (y|0, βIN + XA−1XT ) 2 exp(− 1 = (2π)−N/2|Cα|− 1 2\n",
      "\n",
      "yT C−1\n",
      "\n",
      "α y)\n",
      "\n",
      "(13.150)\n",
      "\n",
      "(13.151)\n",
      "\n",
      "where\n",
      "\n",
      "Cα (cid:2) β−1IN + XA−1XT\n",
      "\n",
      "(13.152)\n",
      "\n",
      "Compare this to the marginal likelihood in Equation 13.13 in the spike and slab model; modulo the β = 1/σ2 factor missing from the second term, the equations are the same, except we have replaced the binary γj ∈ {0, 1} with continuous αj ∈ R+. In log form, the objective becomes\n",
      "\n",
      "and β ∼ Ga(c, d). The modiﬁed objective becomes\n",
      "\n",
      "(cid:6)(α, β) (cid:2) − 1 2 To regularize the problem, we may put a conjugate prior on each precision, αj ∼ Ga(a, b)\n",
      "\n",
      "(cid:6)(α, β) (cid:2) − 1 2\n",
      "\n",
      "= log |Cα| + yT C−1\n",
      "\n",
      "log p(y|X, α, β) = log |Cα| + yT C−1\n",
      "\n",
      "log p(y|X, α, β) +\n",
      "\n",
      "α y +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "j (cid:2)\n",
      "\n",
      "(a log αj − bαj) +c log β − dβ\n",
      "\n",
      "log Ga(αj|a, b) + log Ga(β|c, d)\n",
      "\n",
      "α y\n",
      "\n",
      "(13.153)\n",
      "\n",
      "(13.154)\n",
      "\n",
      "(13.155)\n",
      "\n",
      "j\n",
      "\n",
      "This is useful when performing Bayesian inference for α and β (Bishop and Tipping 2000). However, when performing (type II) point estimation, we will use the improper prior a = b = c = d = 0, which results in maximal sparsity.\n",
      "\n",
      "Below we describe how to optimize (cid:6)(α, β) wrt the precision terms α and β.7 This is a proxy for ﬁnding the most probable model setting of γ in the spike and slab model, which in turn is closely related to (cid:6)0 regularization. In particular, it can be shown (Wipf et al. 2010) that the objective in Equation 13.153 has many fewer local optima than the (cid:6)0 objective, and hence is much easier to optimize.\n",
      "\n",
      "Once we have estimated α and β, we can compute the posterior over the parameters using p(w|D, ˆα, ˆβ) = N (μ, Σ)\n",
      "\n",
      "(13.156)\n",
      "\n",
      "(13.158) The fact that we compute a posterior over w, while simultaneously encouraging sparsity, is why the method is called “sparse Bayesian learning”. Nevertheless, since there are many ways to be sparse and Bayesian, we will use the “ARD” term instead, even in the linear model context. (In addition, SBL is only “being Bayesian” about the values of the coefficients, rather than reﬂecting uncertainty about the set of relevant variables, which is typically of more interest.)\n",
      "\n",
      "Σ−1 = ˆβXT X + A μ = ˆβΣXT y\n",
      "\n",
      "(13.157)\n",
      "\n",
      "7. An alternative approach to optimizing β is to put a Gamma prior on β and to integrate it out to get a Student posterior for w (Buntine and Weigend 1991). However, it turns out that this results in a less accurate estimate for α (MacKay 1999). In addition, working with Gaussians is easier than working with the Student distribution, and the Gaussian case generalizes more easily to other cases such as logistic regression.\n",
      "\n",
      "13.7. Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)\n",
      "\n",
      "465\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "C\n",
      "\n",
      "y\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.20 Illustration of why ARD results in sparsity. The vector of inputs x does not point towards the vector of outputs y, so the feature should be removed. (a) For ﬁnite α, the probability density is spread in directions away from y. (b) When α = ∞, the probability density at y is maximized. Based on Figure 8 of (Tipping 2001).\n",
      "\n",
      "13.7.2 Whence sparsity?\n",
      "\n",
      "If ˆαj ≈ 0, we ﬁnd ˆwj ≈ ˆwmle , since the Gaussian prior shrinking wj towards 0 has zero precision. However, if we ﬁnd that ˆαj ≈ ∞, then the prior is very conﬁdent that wj = 0, and hence that feature j is “irrelevant”. Hence the posterior mean will have ˆwj ≈ 0. Thus irrelevant features automatically have their weights “turned off” or “pruned out”.\n",
      "\n",
      "j\n",
      "\n",
      "We now give an intuitive argument, based on (Tipping 2001), about why ML-II should encour- age αj → ∞ for irrelevant features. Consider a 1d linear regression with 2 training examples, so X = x = (x1, x2), and y = (y1, y2). We can plot x and y as vectors in the plane, as shown in Figure 13.20. Suppose the feature is irrelevant for predicting the response, so x points in a nearly orthogonal direction to y. Let us see what happens to the marginal likelihood as we change α. The marginal likelihood is given by p(y|x, α, β) = N (y|0, C), where\n",
      "\n",
      "C =\n",
      "\n",
      "1 β\n",
      "\n",
      "I +\n",
      "\n",
      "1 α\n",
      "\n",
      "xxT\n",
      "\n",
      "(13.159)\n",
      "\n",
      "If α is ﬁnite, the posterior will be elongated along the direction of x, as in Figure 13.20(a). However, if α = ∞, we ﬁnd C = 1 If |C| is held constant, the latter assigns higher probability density to the observed response vector y, so this is the preferred solution. In other words, the marginal likelihood “punishes” solutions where αj is small but X:,j is irrelevant, since these waste probability mass. It is more parsimonious (from the point of view of Bayesian Occam’s razor) to eliminate redundant dimensions.\n",
      "\n",
      "β I, so C is spherical, as in Figure 13.20(b).\n",
      "\n",
      "13.7.3\n",
      "\n",
      "Connection to MAP estimation\n",
      "\n",
      "ARD seems quite different from the MAP estimation methods we have been considering earlier in this chapter. In particular, in ARD, we are not integrating out α and optimizing w, but vice\n",
      "\n",
      "466\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "versa. Because the parameters wj become correlated in the posterior (due to explaining away), when we estimate αj we are borrowing information from all the features, not just feature j. Consequently, the effective prior p(w| ˆα) is non-factorial, and furthermore it depends on the data D (and σ2). However, in (Wipf and Nagarajan 2007), it was shown that ARD can be viewed as the following MAP estimation problem: β||y − Xw||2\n",
      "\n",
      "gARD(w) (cid:2) min α≥0\n",
      "\n",
      "ˆwARD = arg min w (cid:2)\n",
      "\n",
      "j\n",
      "\n",
      "αjw2\n",
      "\n",
      "j + log |Cα|\n",
      "\n",
      "2 + gARD(w)\n",
      "\n",
      "(13.160)\n",
      "\n",
      "(13.161)\n",
      "\n",
      "The proof, which is based on convex analysis, is a little complicated and hence is omitted.\n",
      "\n",
      "Furthermore, (Wipf and Nagarajan 2007; Wipf et al. 2010) prove that MAP estimation with non-factorial priors is strictly better than MAP estimation with any possible factorial prior in the following sense: the non-factorial objective always has fewer local minima than factorial objectives, while still satisfying the property that the global optimum of the non-factorial objec- tive corresponds to the global optimum of the (cid:6)0 objective — a property that (cid:6)1 regularization, which has no local minima, does not enjoy.\n",
      "\n",
      "13.7.4\n",
      "\n",
      "Algorithms for ARD *\n",
      "\n",
      "In this section, we review several different algorithms for implementing ARD.\n",
      "\n",
      "13.7.4.1\n",
      "\n",
      "EM algorithm\n",
      "\n",
      "The easiest way to implement SBL/ARD is to use EM. The expected complete data log likelihood is given by\n",
      "\n",
      "Q(α, β) =E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "log N (y|Xw, σ2I) + log N (w|0, A−1) ⎡\n",
      "\n",
      "⎤\n",
      "\n",
      "(13.162)\n",
      "\n",
      "=\n",
      "\n",
      "1 2\n",
      "\n",
      "E\n",
      "\n",
      "⎣N log β − β||y − Xw||2 +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "j\n",
      "\n",
      "log αj − tr(AwwT )\n",
      "\n",
      "⎦ + const (13.163)\n",
      "\n",
      "=\n",
      "\n",
      "+\n",
      "\n",
      "1 2\n",
      "\n",
      "N log β − (cid:2)\n",
      "\n",
      "1 2\n",
      "\n",
      "j\n",
      "\n",
      "2 log αj − 1 2\n",
      "\n",
      "β\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "||y − Xμ||2 + tr(XT XΣ)\n",
      "\n",
      "tr[A(μμT + Σ)] + const\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(13.164)\n",
      "\n",
      "where μ and Σ are computed in the E step using Equation 13.158.\n",
      "\n",
      "Suppose we put a Ga(a, b) prior on αj and a Ga(c, d) prior on β. The penalized objective\n",
      "\n",
      "becomes\n",
      "\n",
      "Q(cid:4)(α, β) = Q(α, β) +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(a log αj − bαj) +c log β − dβ\n",
      "\n",
      "(13.165)\n",
      "\n",
      "j\n",
      "\n",
      "Setting\n",
      "\n",
      "dQ(cid:2) dαj\n",
      "\n",
      "= 0 we get the following M step:\n",
      "\n",
      "αj =\n",
      "\n",
      "E\n",
      "\n",
      "1 + 2a (cid:31) w2 j\n",
      "\n",
      "+ 2b\n",
      "\n",
      "=\n",
      "\n",
      "m2\n",
      "\n",
      "1 + 2a j + Σjj + 2b\n",
      "\n",
      "(13.166)\n",
      "\n",
      "13.7. Automatic relevance determination (ARD)/sparse Bayesian learning (SBL)\n",
      "\n",
      "467\n",
      "\n",
      "If αj = α, and a = b = 0, the update becomes\n",
      "\n",
      "α =\n",
      "\n",
      "D E [wT w]\n",
      "\n",
      "=\n",
      "\n",
      "D μT μ + tr(Σ)\n",
      "\n",
      "(13.167)\n",
      "\n",
      "The update for β is given by\n",
      "\n",
      "β−1\n",
      "\n",
      "new =\n",
      "\n",
      "||y − Xμ||2 + β−1\n",
      "\n",
      "N + 2c\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "j(1 − αjΣjj) + 2d\n",
      "\n",
      "(13.168)\n",
      "\n",
      "(Deriving this is Exercise 13.2.)\n",
      "\n",
      "13.7.4.2\n",
      "\n",
      "Fixed-point algorithm\n",
      "\n",
      "A faster and more direct approach is to directly optimize the objective in Equation 13.155. One can show (Exercise 13.3) that the equations d(cid:14) dβ = 0 lead to the following ﬁxed dαj point updates:\n",
      "\n",
      "= 0 and d(cid:14)\n",
      "\n",
      "αj ←\n",
      "\n",
      "β−1 ←\n",
      "\n",
      "γj + 2a m2 j + 2b ||y − Xμ||2 + 2d j γj + 2c N −\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(13.169)\n",
      "\n",
      "(13.170)\n",
      "\n",
      "γj (cid:2) 1 − αjΣjj\n",
      "\n",
      "(13.171)\n",
      "\n",
      "The quantity γj is a measure of how well-determined wj is by the data (MacKay 1992). Hence j γj is the effective degrees of freedom of the model. See Section 7.5.3 for further γ = discussion.\n",
      "\n",
      "Since α and β both depend on μ and Σ (which can be computed using Equation 13.158 or the Laplace approximation), we need to re-estimate these equations until convergence. (Convergence properties of this algorithm have been studied in (Wipf and Nagarajan 2007).) At convergence, the results are formally identical to those obtained by EM, but since the objective is non-convex, the results can depend on the initial values.\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "13.7.4.3\n",
      "\n",
      "Iteratively reweighted (cid:9)1 algorithm\n",
      "\n",
      "Another approach to solving the ARD problem is based on the view that it is a MAP estimation problem. Although the log prior g(w) is rather complex in form, it can be shown to be a non-decreasing, concave function of |wj|. This means that it can be solved by an iteratively reweighted (cid:6)1 problem of the form (cid:2)\n",
      "\n",
      "wt+1 = arg min w\n",
      "\n",
      "NLL(w) +\n",
      "\n",
      "j\n",
      "\n",
      "λ(t) j |wj|\n",
      "\n",
      "(13.172)\n",
      "\n",
      "In (Wipf and Nagarajan 2010), the following procedure for setting the penalty terms is suggested (based on a convex bound to the penalty function). We initialize with λ(0) j = 1, and then at\n",
      "\n",
      "468\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "iteration t + 1, compute λ(t+1)\n",
      "\n",
      "j\n",
      "\n",
      "by iterating the following equation a few times:8\n",
      "\n",
      "λj ←\n",
      "\n",
      "(cid:29)\n",
      "\n",
      "X:,j\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "σ2I + Xdiag(1/λj)diag(|w(t+1)\n",
      "\n",
      "j\n",
      "\n",
      "|)\n",
      "\n",
      "(cid:6)−1\n",
      "\n",
      "XT )−1X:,j\n",
      "\n",
      "(cid:30) 1 2\n",
      "\n",
      "(13.173)\n",
      "\n",
      "We see that the new penalty λj depends on all the old weights. This is quite different from the adaptive lasso method of Section 13.6.2.\n",
      "\n",
      "To understand this difference, consider the noiseless case where σ2 = 0, and assume D (cid:18) N . solutions which perfectly reconstruct the data, Xw = y, and which In this case, there are have sparsity ||w||0 = N ; these are called basic feasible solutions or BFS. What we want are solutions that satsify Xw = y but which are much sparser than this. Suppose the method has found a BFS. We do not want to increase the penalty on a weight just because it is small (as in adaptive lasso), since that will just reinforce our current local optimum. Instead, we want to increase the penalty on a weight if it is small and if we have ||w(t+1)|| < N . The covariance term (Xdiag(1/λj)diag(|w(t+1) |))−1 has this effect: if w is a BFS, this matrix will be full rank, so the penalty will not increase much, but if w is sparser than N , the matrix will not be full rank, so the penalties associated with zero-valued coefficients will increase, thus reinforcing this solution (Wipf and Nagarajan 2010).\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "D N\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "j\n",
      "\n",
      "13.7.5\n",
      "\n",
      "ARD for logistic regression Now consider binary logistic regression, p(y|x, w) = Ber(y|sigm(wT x)), using the same Gaussian prior, p(w) =N (w|0, A−1). We can no longer use EM to estimate α, since the Gaussian prior is not conjugate to the logistic likelihood, so the E step cannot be done exactly. One approach is to use a variational approximation to the E step, as discussed in Section 21.8.1.1. A simpler approach is to use a Laplace approximation (see Section 8.4.1) in the E step. We can then use this approximation inside the same EM procedure as before, except we no longer need to update β. Note, however, that this is not guaranteed to converge. An alternative is to use the techniques from Section 13.7.4.3.\n",
      "\n",
      "In this case, we can use exact methods to compute the inner weighted (cid:6)1 regularized logistic regression problem, and no approximations are required.\n",
      "\n",
      "13.8\n",
      "\n",
      "Sparse coding *\n",
      "\n",
      "So far, we have been concentrating on sparse priors for supervised learning. In this section, we discuss how to use them for unsupervised learning.\n",
      "\n",
      "In Section 12.6, we discussed ICA, which is like PCA except it uses a non-Gaussian prior for the latent factors zi. If we make the non-Gaussian prior be sparsity promoting, such as a Laplace distribution, we will be approximating each observed vector xi as a sparse combination of basis vectors (columns of W); note that the sparsity pattern (controlled by zi) changes from data case to data case. If we relax the constraint that W is orthogonal, we get a method called\n",
      "\n",
      "8. The algorithm in (Wipf and Nagarajan 2007) is equivalent to a single iteration of Equation 13.173. However, since the equation is cheap to compute (only O(N D||w(t+1)||0) time), it is worth iterating a few times before solving the more expensive (cid:7)1 problem.\n",
      "\n",
      "13.8. Sparse coding *\n",
      "\n",
      "469\n",
      "\n",
      "Method PCA FA ICA Sparse coding Sparse PCA Sparse MF\n",
      "\n",
      "p(zi) Gauss Gauss Non-Gauss Laplace Gauss Laplace\n",
      "\n",
      "p(W) W orthogonal yes - no - yes - - no Laplace maybe Laplace\n",
      "\n",
      "no\n",
      "\n",
      "Table 13.3 Summary of various latent factor models. A dash “-” in the p(W) column means we are performing ML parameter estimation rather than MAP parameter estimation. Summary of abbreviations: PCA = principal components analysis; FA = factor analysis; ICA = independent components analysis; MF = matrix factorization.\n",
      "\n",
      "sparse coding. In this context, we call the factor loading matrix W a dictionary; each column is referred to as an atom.9 In view of the sparse representation, it is common for L > D, in which case we call the representation overcomplete.\n",
      "\n",
      "In sparse coding, the dictionary can be ﬁxed or learned. If it is ﬁxed, it is common to use a wavelet or DCT basis, since many natural signals can be well approximated by a small number of such basis functions. However, it is also possible to learn the dictionary, by maximizing the likelihood\n",
      "\n",
      "log p(D|W) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "log\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "zi\n",
      "\n",
      "N (xi|Wzi, σ2I)p(zi)dzi\n",
      "\n",
      "(13.174)\n",
      "\n",
      "We discuss ways to optimize this below, and then we present several interesting applications.\n",
      "\n",
      "Do not confuse sparse coding with sparse PCA (see e.g., (Witten et al. 2009; Journee et al. this puts a sparsity promoting prior on the regression weights W, whereas in sparse 2010)): coding, we put a sparsity promoting prior on the latent factors zi. Of course, the two techniques can be combined; we call the result sparse matrix factorization, although this term is non- standard. See Table 13.3 for a summary of our terminology.\n",
      "\n",
      "13.8.1\n",
      "\n",
      "Learning a sparse coding dictionary\n",
      "\n",
      "Since Equation 13.174 is a hard objective to maximize, approximation:\n",
      "\n",
      "it is common to make the following\n",
      "\n",
      "log p(D|W) ≈\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "max zi\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "log N (xi|Wzi, σ2I) + log p(zi)\n",
      "\n",
      "(13.175)\n",
      "\n",
      "If p(zi) is Laplace, we can rewrite the NLL as\n",
      "\n",
      "NLL(W, Z) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "1 2\n",
      "\n",
      "||xi − Wzi||2\n",
      "\n",
      "2 + λ||zi||1\n",
      "\n",
      "(13.176)\n",
      "\n",
      "9. It is common to denote the dictionary by D, and to denote the latent factors by αi. However, we will stick with the W and zi notation.\n",
      "\n",
      "470\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "To prevent W from becoming arbitrarily large, it is common to constrain the (cid:6)2 norm of its\n",
      "\n",
      "columns to be less than or equal to 1. Let us denote this constraint set by\n",
      "\n",
      "C = {W ∈ RD×L\n",
      "\n",
      "s.t. wT\n",
      "\n",
      "j wj ≤ 1}\n",
      "\n",
      "(13.177)\n",
      "\n",
      "Then we want to solve minW∈C,Z∈RN ×L NLL(W, Z). For a ﬁxed zi, the optimization over W is a simple least squares problem. And for a ﬁxed dictionary W, the optimization problem over Z is identical to the lasso problem, for which many fast algorithms exist. This suggests an obvious iterative optimization scheme, in which we alternate between optimizing W and Z. (Mumford 1994) called this kind of approach an analysis-synthesis loop, where estimating the basis W is the analysis phase, and estimating the coefficients Z is the synthesis phase. In cases where this is too slow, more sophisticated algorithms can be used, see e.g., (Mairal et al. 2010). A variety of other models result in an optimization problem that is similar to Equation 13.176. For example, non-negative matrix factorization or NMF (Paatero and Tapper 1994; Lee and Seung 2001) requires solving an objective of the form\n",
      "\n",
      "min W∈C,Z∈RL×N\n",
      "\n",
      "1 2\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "||xi − Wzi||2 2\n",
      "\n",
      "s.t. W ≥ 0, zi ≥ 0\n",
      "\n",
      "(13.178)\n",
      "\n",
      "(Note that this has no hyper-parameters to tune.) The intuition behind this constraint is that the learned dictionary may be more interpretable if it is a positive sum of positive “parts”, rather than a sparse sum of atoms that may be positive or negative. Of course, we can combine NMF with a sparsity promoting prior on the latent factors. This is called non-negative sparse coding (Hoyer 2004).\n",
      "\n",
      "Alternatively, we can drop the positivity constraint, but impose a sparsity constraint on both the factors zi and the dictionary W. We call this sparse matrix factorization. To ensure strict convexity, we can use an elastic net type penalty on the weights (Mairal et al. 2010) resulting in\n",
      "\n",
      "min W,Z\n",
      "\n",
      "1 2\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "||xi − Wzi||2\n",
      "\n",
      "2 + λ||zi||1\n",
      "\n",
      "s.t.\n",
      "\n",
      "||wj||2\n",
      "\n",
      "2 + γ||wj||1 ≤ 1\n",
      "\n",
      "(13.179)\n",
      "\n",
      "There are several related objectives one can write down. For example, we can replace the lasso NLL with group lasso or fused lasso (Witten et al. 2009).\n",
      "\n",
      "We can also use other sparsity-promoting priors besides the Laplace. For example, (Zhou et al. 2009) propose a model in which the latent factors zi are made sparse using the binary mask model of Section 13.2.2. Each bit of the mask can be generated from a Bernoulli distribution with parameter π, which can be drawn from a beta distribution. Alternatively, we can use a non-parametric prior, such as the beta process. This allows the model to use dictionaries of unbounded size, rather than having to specify L in advance. One can perform Bayesian inference in this model using e.g., Gibbs sampling or variational Bayes. One ﬁnds that the effective size of the dictionary goes down as the noise level goes up, due to the Bayesian Occam’s razor. This can prevent overﬁtting. See (Zhou et al. 2009) for details.\n",
      "\n",
      "13.8.2\n",
      "\n",
      "Results of dictionary learning from image patches\n",
      "\n",
      "One reason that sparse coding has generated so much interest recently is because it explains an interesting phenomenon in neuroscience. In particular, the dictionary that is learned by applying\n",
      "\n",
      "13.8. Sparse coding *\n",
      "\n",
      "471\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(e)\n",
      "\n",
      "(f)\n",
      "\n",
      "Figure 13.21 Illustration of the ﬁlters learned by various methods when applied to natural image patches. (a) ICA. Figure generated by icaBasisDemo, (Each patch is ﬁrst centered and normalized to unit norm.) (e) kindly provided by Aapo Hyvarinen. sparse PCA with low sparsity on weight matrix. (f) sparse PCA with high sparsity on weight matrix. Figure generated by sparseDictDemo, written by Julien Mairal.\n",
      "\n",
      "(b) sparse coding.\n",
      "\n",
      "(c) PCA. (d) non-negative matrix factorization.\n",
      "\n",
      "472\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "sparse coding to patches of natural images consists of basis vectors that look like the ﬁlters that are found in simple cells in the primary visual cortex of the mammalian brain (Olshausen and Field 1996). In particular, the ﬁlters look like bar and edge detectors, as shown in Figure 13.21(b). (In this example, the parameter λ was chosen so that the number of active basis functions (non-zero components of zi) is about 10.) Interestingly, using ICA gives visually similar results, as shown in Figure 13.21(a). By contrast, applying PCA to the same data results in sinusoidal gratings, as shown in Figure 13.21(c); these do not look like cortical cell response patterns.10 It has therefore been conjectured that parts of the cortex may be performing sparse coding of the sensory input; the resulting latent representation is then further processed by higher levels of the brain.\n",
      "\n",
      "Figure 13.21(d) shows the result of using NMF, and Figure 13.21(e-f) show the results of sparse\n",
      "\n",
      "PCA, as we increase the sparsity of the basis vectors.\n",
      "\n",
      "13.8.3\n",
      "\n",
      "Compressed sensing\n",
      "\n",
      "Although it is interesting to look at the dictionaries learned by sparse coding, it is not necessarily very useful. However, there are some practical applications of sparse coding, which we discuss below.\n",
      "\n",
      "Imagine that, instead of observing the data x ∈ RD, we observe a low-dimensional projection of it, y = Rx + (cid:7) where y ∈ RM , R is a M × D matrix, M (cid:23) D, and (cid:7) is a noise term (usually Gaussian). We assume R is a known sensing matrix, corresponding to different linear projections of x. For example, consider an MRI scanner: each beam direction corresponds to a vector, encoded as a row in R. Figure 13.22 illustrates the modeling assumptions.\n",
      "\n",
      "Our goal is to infer p(x|y, R). How can we hope to recover all of x if we do not measure all of x? The answer is: we can use Bayesian inference with an appropriate prior, that exploits the fact that natural signals can be expressed as a weighted combination of a small number of suitably chosen basis functions. That is, we assume x = Wz, where z has a sparse prior, and W is suitable dictionary. This is called compressed sensing or compressive sensing (Candes et al. 2006; Baruniak 2007; Candes and Wakin 2008; Bruckstein et al. 2009).\n",
      "\n",
      "For CS to work, it is important to represent the signal in the right basis, otherwise it will not be sparse. In traditional CS applications, the dictionary is ﬁxed to be a standard form, such as wavelets. However, one can get much better performance by learning a domain-speciﬁc dictionary using sparse coding (Zhou et al. 2009). As for the sensing matrix R, it is often chosen to be a random matrix, for reasons explained in (Candes and Wakin 2008). However, one can get better performance by adapting the projection matrix to the dictionary (Seeger and Nickish 2008; Chang et al. 2009).\n",
      "\n",
      "13.8.4\n",
      "\n",
      "Image inpainting and denoising\n",
      "\n",
      "Suppose we have an image which is corrupted in some way, e.g., by having text or scratches sparsely superimposed on top of it, as in Figure 13.23. We might want to estimate the underlying\n",
      "\n",
      "10. The reason PCA discovers sinusoidal grating patterns is because it is trying to model the covariance of the data, which, (cid:6) in the case of image patches, is translation invariant. This means cov [I(x, y), I(x(cid:2), y(cid:2))] = f (x − x(cid:2))2 + (y − y(cid:2))2 for some function f , where I(x, y) is the image intensity at location (x, y). One can show (Hyvarinen et al. 2009, p125) that the eigenvectors of a matrix of this kind are always sinusoids of different phases, i.e., PCA discovers a Fourier basis.\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "13.8. Sparse coding *\n",
      "\n",
      "473\n",
      "\n",
      "z\n",
      "\n",
      "λ\n",
      "\n",
      "x\n",
      "\n",
      "W\n",
      "\n",
      "y\n",
      "\n",
      "R\n",
      "\n",
      "Figure 13.22 Schematic DGM for compressed sensing. We observe a low dimensional measurement y generated by passing x through a measurement matrix R, and possibly subject to observation noise with variance σ2. We assume that x has a sparse decomposition in terms of the dictionary W and the latent variables z. the parameter λ controlls the sparsity level.\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 13.23 An example of image inpainting using sparse coding. Left: original image. Right: recon- struction. Source: Figure 13 of (Mairal et al. 2008). Used with kind permission of Julien Mairal.\n",
      "\n",
      "“clean” image. This is called image inpainting. One can use similar techniques for image denoising.\n",
      "\n",
      "We can model this as a special kind of compressed sensing problem. The basic idea is as follows. We partition the image into overlapping patches, yi, and concatenate them to form y. We deﬁne R so that the i’th row selects out patch i. Now deﬁne V to be the visible (uncorrupted) components of y, and H to be the hidden components. To perform image inpainting, we just compute p(yH|yV , θ), where θ are the model parameters, which specify the dictionary W and the sparsity level λ of z. We can either learn a dictionary offline from a database of images, or we can learn a dictionary just for this image, based on the non-corrupted patches.\n",
      "\n",
      "from 7 × 106 undamaged 12 × 12 color patches in the 12 mega-pixel image.\n",
      "\n",
      "Figure 13.23 shows this technique in action. The dictionary (of size 256 atoms) was learned\n",
      "\n",
      "An alternative approach is to use a graphical model (e.g., the ﬁelds of experts model (S.\n",
      "\n",
      "474\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "and Black 2009)) which directly encodes correlations between neighboring image patches, rather than using a latent variable model. Unfortunately such models tend to be computationally more expensive.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 13.1 Partial derivative of the RSS Deﬁne\n",
      "\n",
      "RSS(w) = ||Xw − y||2 2\n",
      "\n",
      "(13.180)\n",
      "\n",
      "a. Show that\n",
      "\n",
      "∂ ∂wk\n",
      "\n",
      "RSS(w) =a\n",
      "\n",
      "kwk − ck\n",
      "\n",
      "(13.181)\n",
      "\n",
      "ak = 2\n",
      "\n",
      "n(cid:12)\n",
      "\n",
      "ik = 2||x:,k||2 x2\n",
      "\n",
      "(13.182)\n",
      "\n",
      "ck = 2\n",
      "\n",
      "i=1 n(cid:12)\n",
      "\n",
      "xik(yi − wT\n",
      "\n",
      "−kxi,−k) = 2xT\n",
      "\n",
      ":,krk\n",
      "\n",
      "(13.183)\n",
      "\n",
      "i=1\n",
      "\n",
      "b. Show that if\n",
      "\n",
      "where w−k = w without component k, xi,−k is xi without component k, and rk = y − wT −kx:,−k is the residual due to using all the features except feature k. Hint: Partition the weights into those involving k and those not involving k. RSS(w) = 0, then\n",
      "\n",
      "∂ ∂wk\n",
      "\n",
      "ˆwk =\n",
      "\n",
      "xT :,krk ||x:,k||2\n",
      "\n",
      "(13.184)\n",
      "\n",
      "Hence when we sequentially add features, the optimal weight for feature k is computed by computing orthogonally projecting x:,k onto the current residual.\n",
      "\n",
      "Exercise 13.2 Derivation of M step for EB for linear regression Derive Equations 13.166 and 13.168. Hint: the following identity should be useful\n",
      "\n",
      "ΣXT X = ΣXT X + β−1ΣA − β−1ΣA = Σ(XT Xβ + A)β−1 − β−1ΣA = (A + βXT X)−1(XT Xβ + A)β−1 − β−1ΣA = (I − AΣ)β−1\n",
      "\n",
      "(13.185)\n",
      "\n",
      "(13.186)\n",
      "\n",
      "(13.187)\n",
      "\n",
      "(13.188)\n",
      "\n",
      "Exercise 13.3 Derivation of ﬁxed point updates for EB for linear regression Derive Equations 13.169 and 13.170. Hint: The easiest way to derive this result is to rewrite log p(D|α, β) as in Equation 8.54. This is exactly equivalent, since in the case of a Gaussian prior and likelihood, the posterior is also Gaussian, so the Laplace “approximation” is exact. In this case, we get\n",
      "\n",
      "log p(D|α, β) =\n",
      "\n",
      "+\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "1 2\n",
      "\n",
      "log β − (cid:12)\n",
      "\n",
      "j\n",
      "\n",
      "2 log αj − 1 2\n",
      "\n",
      "β\n",
      "\n",
      "||y − Xw||2\n",
      "\n",
      "mT Am +\n",
      "\n",
      "1 2\n",
      "\n",
      "log |Σ| −\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "log(2π)\n",
      "\n",
      "(13.189)\n",
      "\n",
      "The rest is straightforward algebra.\n",
      "\n",
      "13.8. Sparse coding *\n",
      "\n",
      "475\n",
      "\n",
      "Exercise 13.4 Marginal likelihood for linear regression Suppose we use a g-prior of the form Σγ = g(XT\n",
      "\n",
      "γ Xγ)−1. Show that Equation 13.16 simpliﬁes to\n",
      "\n",
      "p(D|γ) ∝ (1 + g)−Dγ /2(2bσ + S(γ))−(2aσ +N −1)/2\n",
      "\n",
      "S(γ) =y T y −\n",
      "\n",
      "g 1 +g\n",
      "\n",
      "yT Xγ(XT\n",
      "\n",
      "γ Xγ)−1XT\n",
      "\n",
      "γ y\n",
      "\n",
      "(13.190)\n",
      "\n",
      "(13.191)\n",
      "\n",
      "Exercise 13.5 Reducing elastic net to lasso Deﬁne\n",
      "\n",
      "J1(w) = |y − Xw|2 + λ2|w|2 + λ1|w|1\n",
      "\n",
      "(13.192)\n",
      "\n",
      "and\n",
      "\n",
      "J2(w) = |˜y − ˜X ˜w|2 + cλ1|w|1\n",
      "\n",
      "(13.193)\n",
      "\n",
      "where c = (1 + λ2)− 1 (cid:3) (cid:4)\n",
      "\n",
      "˜X = c\n",
      "\n",
      "X√\n",
      "\n",
      "λ2Id\n",
      "\n",
      "2 and\n",
      "\n",
      ",\n",
      "\n",
      "˜y =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "y 0d×1\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(13.194)\n",
      "\n",
      "Show\n",
      "\n",
      "arg min J1(w) = c(arg min J2(w))\n",
      "\n",
      "(13.195)\n",
      "\n",
      "i.e.\n",
      "\n",
      "J1(cw) = J2(w)\n",
      "\n",
      "(13.196)\n",
      "\n",
      "and hence that one can solve an elastic net problem using a lasso solver on modiﬁed data.\n",
      "\n",
      "Exercise 13.6 Shrinkage in linear regression (Source: Jaakkola.) Consider performing linear regression with an orthonormal design matrix, so ||x:,k||2 1 for each column (feature) k, and xT Figure 13.24 plots ˆwk vs ck = 2yT x:,k, the correlation of feature k with the response, for 3 different esimation methods: ordinary least squares (OLS), ridge regression with parameter λ2, and lasso with parameter λ1.\n",
      "\n",
      ":,kx:,j = 0, so we can estimate each parameter wk separately.\n",
      "\n",
      "2 =\n",
      "\n",
      "a. Unfortunately we forgot to label the plots. Which method does the solid (1), dotted (2) and dashed (3)\n",
      "\n",
      "line correspond to? Hint: see Section 13.3.3.\n",
      "\n",
      "b. What is the value of λ1? c. What is the value of λ2?\n",
      "\n",
      "Exercise 13.7 Prior for the Bernoulli rate parameter in the spike and slab model Consider the model in Section 13.2.1. Suppose we put a prior on the sparsity rates, πj ∼ Beta(α1, α2). Derive an expression for p(γ|α) after integrating out the πj’s. Discuss some advantages and disadvantages of this approach compared to assuming πj = π0 for ﬁxed π0.\n",
      "\n",
      "476\n",
      "\n",
      "Chapter 13. Sparse linear models\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "1 2 3\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "w\n",
      "\n",
      "k\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.8\n",
      "\n",
      "−1 −2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0 c\n",
      "\n",
      "k\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "Figure 13.24\n",
      "\n",
      "Plot of ˆwk vs amount of correlation ck for three different estimators.\n",
      "\n",
      "Exercise 13.8 Deriving E step for GSM prior Show that\n",
      "\n",
      "E\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "1 τ 2 j\n",
      "\n",
      "|wj\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "=\n",
      "\n",
      "π(cid:2)(wj) |wj|\n",
      "\n",
      "(13.197)\n",
      "\n",
      "where π(wj) = − log p(wj) and p(wj) = intN (wj|0, τ 2\n",
      "\n",
      "j )p(τ 2\n",
      "\n",
      "j )dτ 2\n",
      "\n",
      "j . Hint 1:\n",
      "\n",
      "1 τ 2 j\n",
      "\n",
      "N (wj|0, τ 2\n",
      "\n",
      "j ) ∝ 1 τ 2 j −1 |wj| −1 |wj|\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "exp(−\n",
      "\n",
      "−2wj 2τ 2 j d d|wj|\n",
      "\n",
      "w2 j 2τ 2 j\n",
      "\n",
      "N (wj|0, τ 2 j )\n",
      "\n",
      "exp(−\n",
      "\n",
      ")\n",
      "\n",
      "w2 j 2τ 2 j\n",
      "\n",
      ")\n",
      "\n",
      "(13.198)\n",
      "\n",
      "(13.199)\n",
      "\n",
      "(13.200)\n",
      "\n",
      "Hint 2: d d|wj|\n",
      "\n",
      "p(wj) =\n",
      "\n",
      "1 p(wj)\n",
      "\n",
      "d d|wj|\n",
      "\n",
      "log p(wj)\n",
      "\n",
      "(13.201)\n",
      "\n",
      "Exercise 13.9 EM for sparse probit regression with Laplace prior Derive an EM algorithm for ﬁtting a binary probit classiﬁer (Section 9.4) using a Laplace prior on the weights. (If you get stuck, see (Figueiredo 2003; Ding and Harrison 2010).)\n",
      "\n",
      "Exercise 13.10 GSM representation of group lasso Consider the prior τ 2 j ∼ Ga(δ, ρ2/2), ignoring the grouping issue for now. The marginal distribution induced on the weights by a Gamma mixing distribution is called the normal Gamma distribution and is\n",
      "\n",
      "13.8. Sparse coding *\n",
      "\n",
      "477\n",
      "\n",
      "given by\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "NG(wj|δ, ρ) =\n",
      "\n",
      "N (wj|0, τ 2\n",
      "\n",
      "j )Ga(τ 2\n",
      "\n",
      "j |δ, ρ2/2)dτ 2 j\n",
      "\n",
      "(13.202)\n",
      "\n",
      "=\n",
      "\n",
      "1 Z\n",
      "\n",
      "|wj|δ−1/2 Kδ− 1\n",
      "\n",
      "2\n",
      "\n",
      "(ρ|wj|)\n",
      "\n",
      "(13.203)\n",
      "\n",
      "1/Z =\n",
      "\n",
      "√\n",
      "\n",
      "ρδ+ 1 π 2δ−1/2 ρ(δ)\n",
      "\n",
      "2\n",
      "\n",
      "(13.204)\n",
      "\n",
      "where Kα(x) is the modiﬁed Bessel function of the second kind (the besselk function in Matlab). Now suppose we have the following prior on the variances\n",
      "\n",
      "p(σ2\n",
      "\n",
      "1:D) =\n",
      "\n",
      "G(cid:13)\n",
      "\n",
      "p(σ2\n",
      "\n",
      "1:dg ), p(σ2\n",
      "\n",
      "1:dg ) =\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "Ga(τ 2\n",
      "\n",
      "j |δg, ρ2/2)\n",
      "\n",
      "(13.205)\n",
      "\n",
      "g=1\n",
      "\n",
      "j∈g\n",
      "\n",
      "The corresponding marginal for each group of weights has the form\n",
      "\n",
      "p(wg) ∝ |ug|δg −dg /2 Kδg −dg /2(ρug)\n",
      "\n",
      "(13.206)\n",
      "\n",
      "where\n",
      "\n",
      "ug (cid:2)\n",
      "\n",
      "’(cid:12)\n",
      "\n",
      "w2\n",
      "\n",
      "g,j = ||wg||2\n",
      "\n",
      "(13.207)\n",
      "\n",
      "j∈g\n",
      "\n",
      "Now suppose δg = (dg + 1)/2, so δg − dg/2 = 1 that the resulting MAP estimate is equivalent to group lasso.\n",
      "\n",
      "2 . Conveniently, we have K 1\n",
      "\n",
      "2\n",
      "\n",
      "(z) =\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "2z exp(−z). Show\n",
      "\n",
      "π\n",
      "\n",
      "Exercise 13.11 Projected gradient descent for (cid:7)1 regularized least squares Consider the BPDN problem argminθ RSS(θ) +λ ||θ||1. By using the split variable trick introducted in Section 7.4 (i.e., by deﬁning θ = [θ+, θ−]), rewrite this as a quadratic program with a simple bound (If you get stuck, constraint. Then sketch how to use projected gradient descent to solve this problem. consult (Figueiredo et al. 2007).)\n",
      "\n",
      "Exercise 13.12 Subderivative of the hinge loss function Let f (x) = (1 − x)+ be the hinge loss function, where (z)+ = max(0, z). What are ∂f (0), ∂f (1), and ∂f (2)?\n",
      "\n",
      "Exercise 13.13 Lower bounds to convex functions Let f be a convex function. Explain how to ﬁnd a global affine lower bound to f at an arbitrary point x ∈ dom(f ).\n",
      "\n",
      "14 Kernels\n",
      "\n",
      "14.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "So far in this book, we have been assuming that each object that we wish to classify or cluster or process in anyway can be represented as a ﬁxed-size feature vector, typically of the form xi ∈ RD. However, for certain kinds of objects, it is not clear how to best represent them as ﬁxed-sized feature vectors. For example, how do we represent a text document or protein sequence, which can be of variable length? or a molecular structure, which has complex 3d geometry? or an evolutionary tree, which has variable size and shape?\n",
      "\n",
      "One approach to such problems is to deﬁne a generative model for the data, and use the inferred latent representation and/or the parameters of the model as features, and then to plug these features in to standard methods. For example, in Chapter 28, we discuss deep learning, which is essentially an unsupervised way to learn good feature representations.\n",
      "\n",
      "Another approach is to assume that we have some way of measuring the similarity between objects, that doesn’t require preprocessing them into feature vector format. For example, when comparing strings, we can compute the edit distance between them. Let κ(x, x(cid:4)) ≥ 0 be some measure of similarity between objects x, x(cid:4) ∈ X , where X is some abstract space; we will call κ a kernel function. Note that the word “kernel” has several meanings; we will discuss a different interpretation in Section 14.7.1.\n",
      "\n",
      "In this chapter, we will discuss several kinds of kernel functions. We then describe some algorithms that can be written purely in terms of kernel function computations. Such methods can be used when we don’t have access to (or choose not to look at) the “inside” of the objects x that we are processing.\n",
      "\n",
      "14.2\n",
      "\n",
      "Kernel functions\n",
      "\n",
      "We deﬁne a kernel function to be a real-valued function of two arguments, κ(x, x(cid:4)) ∈ R, for x, x(cid:4) ∈ X . Typically the function is symmetric (i.e., κ(x, x(cid:4)) = κ(x(cid:4), x)), and non-negative (i.e., κ(x, x(cid:4)) ≥ 0), so it can be interpreted as a measure of similarity, but this is not required. We give several examples below.\n",
      "\n",
      "480\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "14.2.1\n",
      "\n",
      "RBF kernels\n",
      "\n",
      "The squared exponential kernel (SE kernel) or Gaussian kernel is deﬁned by\n",
      "\n",
      "κ(x, x(cid:4)) = exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(x − x(cid:4))T Σ−1(x − x(cid:4))\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(14.1)\n",
      "\n",
      "If Σ is diagonal, this can be written as ⎛ ⎝− 1 2\n",
      "\n",
      "κ(x, x(cid:4)) = exp\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "1 σ2 j\n",
      "\n",
      "(xj − x(cid:4)\n",
      "\n",
      "j)2\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "(14.2)\n",
      "\n",
      "We can interpret the σj as deﬁning the characteristic length scale of dimension j. If σj = ∞, If Σ is the corresponding dimension is ignored; hence this is known as the ARD kernel. spherical, we get the isotropic kernel (cid:9)\n",
      "\n",
      "κ(x, x(cid:4)) = exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "−\n",
      "\n",
      "||x − x(cid:4)||2 2σ2\n",
      "\n",
      "(14.3)\n",
      "\n",
      "Here σ2 is known as the bandwidth. Equation 14.3 is an example of a a radial basis function or RBF kernel, since it is only a function of ||x − x(cid:4)||.\n",
      "\n",
      "14.2.2\n",
      "\n",
      "Kernels for comparing documents\n",
      "\n",
      "When performing document classiﬁcation or retrieval, it is useful to have a way of comparing two documents, xi and xi(cid:2) . If we use a bag of words representation, where xij is the number of times words j occurs in document i, we can use the cosine similarity, which is deﬁned by\n",
      "\n",
      "κ(xi, xi(cid:2) ) =\n",
      "\n",
      "xT i xi(cid:2) ||xi||2||xi(cid:2) ||2\n",
      "\n",
      "(14.4)\n",
      "\n",
      "This quantity measures the cosine of the angle between xi and xi(cid:2) when interpreted as vectors. Since xi is a count vector (and hence non-negative), the cosine similarity is between 0 and 1, where 0 means the vectors are orthogonal and therefore have no words in common.\n",
      "\n",
      "Unfortunately, this simple method does not work very well, for two main reasons. First, if xi has any word in common with xi(cid:2) , it is deemed similar, even though some popular words, such (These are as “the” or “and” occur in many documents, and are therefore not discriminative. known as stop words.) Second, if a discriminative word occurs many times in a document, the similarity is artiﬁcially boosted, even though word usage tends to be bursty, meaning that once a word is used in a document it is very likely to be used again (see Section 3.5.5).\n",
      "\n",
      "Fortunately, we can signiﬁcantly improve performance using some simple preprocessing. The idea is to replace the word count vector with a new feature vector called the TF-IDF representa- tion, which stands for “term frequency inverse document frequency”. We deﬁne this as follows. First, the term frequency is deﬁned as a log-transform of the count:\n",
      "\n",
      "tf(xij) (cid:2) log(1 + xij)\n",
      "\n",
      "(14.5)\n",
      "\n",
      "This reduces the impact of words that occur many times within one document. Second, the inverse document frequency is deﬁned as\n",
      "\n",
      "idf(j) (cid:2) log\n",
      "\n",
      "1 +\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 I(xij > 0)\n",
      "\n",
      "N\n",
      "\n",
      "(14.6)\n",
      "\n",
      "14.2. Kernel functions\n",
      "\n",
      "481\n",
      "\n",
      "where N is the total number of documents, and the denominator counts how many documents contain term j. Finally, we deﬁne tf-idf(xi) (cid:2) [tf(xij) × idf(j)]V\n",
      "\n",
      "j=1\n",
      "\n",
      "(14.7)\n",
      "\n",
      "(There are several other ways to deﬁne the tf and idf terms, see (Manning et al. 2008) for details.) We then use this inside the cosine similarity measure. That is, our new kernel has the form\n",
      "\n",
      "κ(xi, xi(cid:2) ) =\n",
      "\n",
      "φ(xi)T φ(xi(cid:2) ) ||φ(xi)||2||φ(xi(cid:2) )||2\n",
      "\n",
      "(14.8)\n",
      "\n",
      "where φ(x) = tf-idf(x). This gives good results for information retrieval (Manning et al. 2008).\n",
      "\n",
      "A probabilistic interpretation of the tf-idf kernel is given in (Elkan 2005).\n",
      "\n",
      "14.2.3 Mercer (positive deﬁnite) kernels\n",
      "\n",
      "Some methods that we will study require that the kernel function satisfy the requirement that the Gram matrix, deﬁned by\n",
      "\n",
      "K =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎝\n",
      "\n",
      "κ(xN , x1)\n",
      "\n",
      "κ(x1, x1)\n",
      "\n",
      "· · ... · · ·\n",
      "\n",
      "κ(xN , xN )\n",
      "\n",
      "κ(x1, xN )\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎠\n",
      "\n",
      "(14.9)\n",
      "\n",
      "be positive deﬁnite for any set of inputs {xi}N i=1. We call such a kernel a Mercer kernel, or positive deﬁnite kernel. It can be shown (Schoelkopf and Smola 2002) that the Gaussian kernel is a Mercer kernel as is the cosine similarity kernel (Sahami and Heilman 2006).\n",
      "\n",
      "The importance of Mercer kernels is the following result, known as Mercer’s theorem. If the Gram matrix is positive deﬁnite, we can compute an eigenvector decomposition of it as follows\n",
      "\n",
      "K = UT ΛU\n",
      "\n",
      "(14.10)\n",
      "\n",
      "where Λ is a diagonal matrix of eigenvalues λi > 0. Now consider an element of K:\n",
      "\n",
      "2 U:,i)T (Λ 1 Let us deﬁne φ(xi) = Λ 1 kij = φ(xi)T φ(xj)\n",
      "\n",
      "kij = (Λ 1\n",
      "\n",
      "2 U:j)\n",
      "\n",
      "2 U:i. Then we can write\n",
      "\n",
      "(14.11)\n",
      "\n",
      "(14.12)\n",
      "\n",
      "Thus we see that the entries in the kernel matrix can be computed by performing an inner product of some feature vectors that are implicitly deﬁned by the eigenvectors U. In general, if the kernel is Mercer, then there exists a function φ mapping x ∈ X to RD such that\n",
      "\n",
      "(14.13) where φ depends on the eigen functions of κ (so D is a potentially inﬁnite dimensional space). For example, consider the (non-stationary) polynomial kernel κ(x, x(cid:4)) = (γxT x(cid:4) + r)M , where r > 0. One can show that the corresponding feature vector φ(x) will contain all terms up to degree M . For example, if M = 2, γ = r = 1 and x, x(cid:4) ∈ R2, we have\n",
      "\n",
      "κ(x, x(cid:4)) = φ(x)T φ(x(cid:4))\n",
      "\n",
      "(1 + xT x(cid:4))2 = (1 + x1x(cid:4) = 1 + 2x1x(cid:4)\n",
      "\n",
      "2)2 1 + x2x(cid:4) 2 + (x1x1)2 + (x2x(cid:4) 1 + 2x2x(cid:4)\n",
      "\n",
      "2)2 + 2x1x(cid:4)\n",
      "\n",
      "1x2x(cid:4) 2\n",
      "\n",
      "(14.14)\n",
      "\n",
      "(14.15)\n",
      "\n",
      "482\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "This can be written as φ(x)T φ(x(cid:4)), where √\n",
      "\n",
      "φ(x) = [1,\n",
      "\n",
      "√\n",
      "\n",
      "2x1,\n",
      "\n",
      "2x2, x2\n",
      "\n",
      "1, x2 2,\n",
      "\n",
      "√\n",
      "\n",
      "2x1x2]T\n",
      "\n",
      "(14.16)\n",
      "\n",
      "So using this kernel is equivalent to working in a 6 dimensional feature space. In the case of a Gaussian kernel, the feature map lives in an inﬁnite dimensional space. In such a case, it is clearly infeasible to explicitly represent the feature vectors.\n",
      "\n",
      "An example of a kernel that is not a Mercer kernel is the so-called sigmoid kernel, deﬁned\n",
      "\n",
      "by\n",
      "\n",
      "κ(x, x(cid:4)) = tanh(γxT x(cid:4) + r)\n",
      "\n",
      "(14.17)\n",
      "\n",
      "(Note that this uses the tanh function even though it is called a sigmoid kernel.) This kernel was inspired by the multi-layer perceptron (see Section 16.5), but there is no real reason to use it. (For a true “neural net kernel”, which is positive deﬁnite, see Section 15.4.5.)\n",
      "\n",
      "In general, establishing that a kernel is a Mercer kernel is difficult, and requires techniques from functional analysis. However, one can show that it is possible to build up new Mercer kernels from simpler ones using a set of standard rules. For example, if κ1 and κ2 are both Mercer, so is κ(x, x(cid:4)) = κ1(x, x(cid:4)) + κ2(x, x(cid:4)). See e.g., (Schoelkopf and Smola 2002) for details.\n",
      "\n",
      "14.2.4\n",
      "\n",
      "Linear kernels\n",
      "\n",
      "Deriving the feature vector implied by a kernel is in general quite difficult, and only possible if the kernel is Mercer. However, deriving a kernel from a feature vector is easy: we just use\n",
      "\n",
      "κ(x, x(cid:4)) = φ(x)T φ(x(cid:4)) = (cid:24)φ(x), φ(x(cid:4))(cid:25)\n",
      "\n",
      "(14.18)\n",
      "\n",
      "If φ(x) = x, we get thelinear kernel, deﬁned by\n",
      "\n",
      "κ(x, x(cid:4)) = xT x(cid:4)\n",
      "\n",
      "(14.19)\n",
      "\n",
      "This is useful if the original data is already high dimensional, and if the original features are individually informative, e.g., a bag of words representation where the vocabulary size is large, or the expression level of many genes. In such a case, the decision boundary is likely to be representable as a linear combination of the original features, so it is not necessary to work in some other feature space.\n",
      "\n",
      "Of course, not all high dimensional problems are linearly separable. For example, images are high dimensional, but individual pixels are not very informative, so image classiﬁcation typically requires non-linear kernels (see e.g., Section 14.2.7).\n",
      "\n",
      "14.2.5 Matern kernels\n",
      "\n",
      "The Matern kernel, which is commonly used in Gaussian process regression (see Section 15.2), has the following form (cid:11) √\n",
      "\n",
      "κ(r) =\n",
      "\n",
      "21−ν Γ(ν)\n",
      "\n",
      "2νr (cid:6)\n",
      "\n",
      "(cid:13)ν\n",
      "\n",
      "Kν\n",
      "\n",
      "(cid:11) √\n",
      "\n",
      "2νr (cid:6)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(14.20)\n",
      "\n",
      "14.2. Kernel functions\n",
      "\n",
      "483\n",
      "\n",
      "where r = ||x − x(cid:4)||, ν > 0, (cid:6) >0, and Kν is a modiﬁed Bessel function. As ν → ∞, this approaches the SE kernel. If ν = 1 2 , the kernel simpliﬁes to\n",
      "\n",
      "κ(r) = exp(−r/(cid:6))\n",
      "\n",
      "(14.21)\n",
      "\n",
      "If D = 1, and we use this kernel to deﬁne a Gaussian process (see Chapter 15), we get the Ornstein-Uhlenbeck process, which describes the velocity of a particle undergoing Brownian motion (the corresponding function is continuous but not differentiable, and hence is very “jagged”).\n",
      "\n",
      "14.2.6\n",
      "\n",
      "String kernels\n",
      "\n",
      "The real power of kernels arises when the inputs are structured objects. As an example, we now describe one way of comparing two variable length strings using a string kernel. We follow the presentation of (Rasmussen and Williams 2006, p100) and (Hastie et al. 2009, p668).\n",
      "\n",
      "Consider two strings x, and x(cid:4) of lengths D, D’, each deﬁned over the alphabet A. For example, consider two amino acid sequences, deﬁned over the 20 letter alphabet A = {A, R, N, D, C, E, Q, G, H, I, L, K, M, F, P, S, T, W, Y, V }. Let x be the following sequence of length 110\n",
      "\n",
      "IPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTLESQTVQGGTV ERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDYLQEFLGVMNTEWI\n",
      "\n",
      "and let x(cid:4) be the following sequence of length 153\n",
      "\n",
      "PHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAERLQENLQAYRTFHVLLA RLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEADGMLFEKK LWGLKVLQELSQWTVRSIHDLRFISSHQTGIP\n",
      "\n",
      "These strings have the substring LQE in common. We can deﬁne the similarity of two strings\n",
      "\n",
      "to be the number of substrings they have in common.\n",
      "\n",
      "More formally and more generally, let us say that s is a substring of x if we can write x = usv for some (possibly empty) strings u, s and v. Now let φs(x) denote the number of times that substring s appears in string x. We deﬁne the kernel between two strings x and x(cid:4) as\n",
      "\n",
      "κ(x, x(cid:4)) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "wsφs(x)φs(x(cid:4))\n",
      "\n",
      "(14.22)\n",
      "\n",
      "s∈A∗\n",
      "\n",
      "where ws ≥ 0 and A∗ is the set of all strings (of any length) from the alphabet A (this is known as the Kleene star operator). This is a Mercer kernel, and be computed in O(|x| + |x(cid:4)|) time (for certain settings of the weights {ws}) using suffix trees (Leslie et al. 2003; Vishwanathan and Smola 2003; Shawe-Taylor and Cristianini 2004).\n",
      "\n",
      "There are various cases of interest. If we set ws = 0 for |s| > 1 we get a bag-of-characters kernel. This deﬁnes φ(x) to be the number of times each character in A occurs in x. If we require s to be bordered by white-space, we get a bag-of-words kernel, where φ(x) counts how many times each possible word occurs. Note that this is a very sparse vector, since most words\n",
      "\n",
      "484\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "optimal partial matching matching\n",
      "\n",
      "Figure 14.1 of Kristen Grauman.\n",
      "\n",
      "Illustration of a pyramid match kernel computed from two images. Used with kind permission\n",
      "\n",
      "If we only consider strings of a ﬁxed length k, we get the k-spectrum will not be present. kernel. This has been used to classify proteins into SCOP superfamilies (Leslie et al. 2003). For example if k = 3, we have φLQE(x) = 1 and φLQE(x(cid:4)) = 2 for the two strings above.\n",
      "\n",
      "Various extensions are possible. For example, we can allow character mismatches (Leslie et al. 2003). And we can generalize string kernels to compare trees, as described in (Collins and Duffy 2002). This is useful for classifying (or ranking) parse trees, evolutionary trees, etc.\n",
      "\n",
      "14.2.7\n",
      "\n",
      "Pyramid match kernels\n",
      "\n",
      "In computer vision, it is common to create a bag-of-words representation of an image by computing a feature vector (often using SIFT (Lowe 1999)) from a variety of points in the image, commonly chosen by an interest point detector. The feature vectors at the chosen places are then vector-quantized to create a bag of discrete symbols.\n",
      "\n",
      "One way to compare two variable-sized bags of this kind is to use a pyramid match kernel (Grauman and Darrell 2007). The basic idea is illustrated in Figure 14.1. Each feature set is mapped to a multi-resolution histogram. These are then compared using weighted histogram intersection. It turns out that this provides a good approximation to the similarity measure one would obtain by performing an optimal bipartite match at the ﬁnest spatial resolution, and then summing up pairwise similarities between matched points. However, the histogram method is faster and is more robust to missing and unequal numbers of points. This is a Mercer kernel.\n",
      "\n",
      "14.2. Kernel functions\n",
      "\n",
      "485\n",
      "\n",
      "14.2.8\n",
      "\n",
      "Kernels derived from probabilistic generative models\n",
      "\n",
      "Suppose we have a probabilistic generative model of feature vectors, p(x|θ). Then there are several ways we can use this model to deﬁne kernel functions, and thereby make the model suitable for discriminative tasks. We sketch two approaches below.\n",
      "\n",
      "14.2.8.1\n",
      "\n",
      "Probability product kernels\n",
      "\n",
      "One approach is to deﬁne a kernel as follows:\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "κ(xi, xj) =\n",
      "\n",
      "p(x|xi)ρp(x|xj)ρdx\n",
      "\n",
      "(14.23)\n",
      "\n",
      "where ρ >0, and p(x|xi) is often approximated by p(x|ˆθ(xi)), where ˆθ(xi) is a parameter estimate computed using a single data vector. This is called a probability product kernel (Jebara et al. 2004).\n",
      "\n",
      "Although it seems strange to ﬁt a model to a single data point, it is important to bear in mind that the ﬁtted model is only being used to see how similar two objects are. In particular, if we ﬁt the model to xi and then the model thinks xj is likely, this means that xi and xj are similar. For example, suppose p(x|θ) =N (μ, σ2I), where σ2 is ﬁxed. If ρ = 1, and we use ˆμ(xi) = xi and ˆμ(xj) = xj, we ﬁnd (Jebara et al. 2004, p825) that (cid:8)\n",
      "\n",
      "(4πσ2)D/2 exp which is (up to a constant factor) the RBF kernel.\n",
      "\n",
      "κ(xi, xj) =\n",
      "\n",
      "1\n",
      "\n",
      "− 1 4σ2\n",
      "\n",
      "||xi − xj||2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(14.24)\n",
      "\n",
      "It turns out that one can compute Equation 14.23 for a variety of generative models, including ones with latent variables, such as HMMs. This provides one way to deﬁne kernels on variable length sequences. Furthermore, this technique works even if the sequences are of real-valued vectors, unlike the string kernel in Section 14.2.6. See (Jebara et al. 2004) for further details.\n",
      "\n",
      "14.2.8.2\n",
      "\n",
      "Fisher kernels\n",
      "\n",
      "A more efficient way to use generative models to deﬁne kernels is to use a Fisher kernel (Jaakkola and Haussler 1998) which is deﬁned as follows:\n",
      "\n",
      "κ(x, x(cid:4)) = g(x)T F−1g(x(cid:4))\n",
      "\n",
      "(14.25)\n",
      "\n",
      "where g is the gradient of the log likelihood, or score vector, evaluated at the MLE ˆθ\n",
      "\n",
      "g(x) (cid:2) ∇θ log p(x|θ)\n",
      "\n",
      "( (\n",
      "\n",
      "ˆθ\n",
      "\n",
      "(14.26)\n",
      "\n",
      "and F is the Fisher information matrix, which is essentially the Hessian:\n",
      "\n",
      "F = ∇∇ log p(x|θ)\n",
      "\n",
      "( (\n",
      "\n",
      "ˆθ\n",
      "\n",
      "(14.27)\n",
      "\n",
      "Note that ˆθ is a function of all the data, so the similarity of x and x(cid:4) is computed in the context of all the data as well. Also, note that we only have to ﬁt one model.\n",
      "\n",
      "The intuition behind the Fisher kernel is the following: let g(x) be the direction (in parameter space) in which x would like the parameters to move (from ˆθ) so as to maximize its own\n",
      "\n",
      "486\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "poly10\n",
      "\n",
      "rbf prototypes\n",
      "\n",
      "7\n",
      "\n",
      "7\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "5\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 14.2 (a) xor truth table. (b) Fitting a linear logistic regression classiﬁer using degree 10 polynomial expansion. (c) Same model, but using an RBF kernel with centroids speciﬁed by the 4 black crosses. Figure generated by logregXorDemo.\n",
      "\n",
      "likelihood; call this the directional gradient. Then we say that two vectors x and x(cid:4) are similar if their directional gradients are similar wrt the the geometry encoded by the curvature of the likelihood function (see Section 7.5.3).\n",
      "\n",
      "Interestingly, it was shown in (Saunders et al. 2003) that the string kernel of Section 14.2.6 is equivalent to the Fisher kernel derived from an L’th order Markov chain (see Section 17.2). Also, it was shown in (Elkan 2005) that a kernel deﬁned by the inner product of TF-IDF vectors (Section 14.2.2) is approximately equal to the Fisher kernel for a certain generative model of text based on the compound Dirichlet multinomial model (Section 3.5.5).\n",
      "\n",
      "14.3 Using kernels inside GLMs\n",
      "\n",
      "In this section, we discuss one simple way to use kernels for classiﬁcation and regression. We will see other approaches later.\n",
      "\n",
      "14.3.1\n",
      "\n",
      "Kernel machines\n",
      "\n",
      "We deﬁne a kernel machine to be a GLM where the input feature vector has the form\n",
      "\n",
      "φ(x) = [κ(x, μ1), . . . , κ(x, μK)]\n",
      "\n",
      "(14.28)\n",
      "\n",
      "where μk ∈ X are a set of K centroids. If κ is an RBF kernel, this is called an RBF network. We discuss ways to choose the μk parameters below. We will call Equation 14.28 a kernelised feature vector. Note that in this approach, the kernel need not be a Mercer kernel.\n",
      "\n",
      "We can use the kernelized feature vector for logistic regression by deﬁning p(y|x, θ) = Ber(wT φ(x)). This provides a simple way to deﬁne a non-linear decision boundary. As an example, consider the data coming from the exclusive or or xor function. This is a binary- valued function of two binary inputs. Its truth table is shown in Figure 14.2(a). In Figure 14.2(b), we have show some data labeled by the xor function, but we have jittered the points to make the picture clearer.1 We see we cannot separate the data even using a degree 10 polynomial.\n",
      "\n",
      "1. Jittering is a common visualization trick in statistics, wherein points in a plot/display that would otherwise land on top of each other are dispersed with uniform additive noise.\n",
      "\n",
      "14.3. Using kernels inside GLMs\n",
      "\n",
      "487\n",
      "\n",
      "20\n",
      "\n",
      "0.8\n",
      "\n",
      "10\n",
      "\n",
      "0.6\n",
      "\n",
      "5\n",
      "\n",
      "0.4\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "15\n",
      "\n",
      "−10 0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0 0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "0.04\n",
      "\n",
      "10\n",
      "\n",
      "0.03\n",
      "\n",
      "0.02\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0.01\n",
      "\n",
      "15\n",
      "\n",
      "−10 0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0 0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "8\n",
      "\n",
      "x 10\n",
      "\n",
      "−3\n",
      "\n",
      "10\n",
      "\n",
      "7.8\n",
      "\n",
      "7.6\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "7.4\n",
      "\n",
      "15\n",
      "\n",
      "−10 0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "7.2 0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "Figure 14.3 RBF basis in 1d. Left column: ﬁtted function. Middle column: basis functions evaluated on a grid. Right column: design matrix. Top to bottom we show different bandwidths: τ = 0.1, τ = 0.5, τ = 50. Figure generated by linregRbfDemo.\n",
      "\n",
      "However, using an RBF kernel and just 4 prototypes easily solves the problem as shown in Figure 14.2(c).\n",
      "\n",
      "We can also use the kernelized feature vector inside a linear regression model by deﬁning p(y|x, θ) = N (wT φ(x), σ2). For example, Figure 14.3 shows a 1d data set ﬁt with K = 10 uniformly spaced RBF prototypes, but with the bandwidth ranging from small to large. Small values lead to very wiggly functions, since the predicted function value will only be non-zero for points x that are close to one of the prototypes μk. If the bandwidth is very large, the design matrix reduces to a constant matrix of 1’s, since each point is equally close to every prototype; hence the corresponding function is just a straight line.\n",
      "\n",
      "14.3.2\n",
      "\n",
      "L1VMs, RVMs, and other sparse vector machines The main issue with kernel machines is: how do we choose the centroids μk? If the input is low-dimensional Euclidean space, we can uniformly tile the space occupied by the data with prototypes, as we did in Figure 14.2(c). However, this approach breaks down in higher numbers If μk ∈ RD, we can try to perform of dimensions because of the curse of dimensionality. (Haykin 1998)), or we can use MCMC numerical optimization of these parameters (see e.g., inference, (see e.g., (Andrieu et al. 2001; Kohn et al. 2001)), but the resulting objective function / posterior is highly multimodal. Furthermore, these techniques is hard to extend to structured input spaces, where kernels are most useful.\n",
      "\n",
      "Another approach is to ﬁnd clusters in the data and then to assign one prototype per cluster\n",
      "\n",
      "488\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "center (many clustering algorithms just need a similarity metric as input). However, the regions of space that have high density are not necessarily the ones where the prototypes are most useful for representing the output, that is, clustering is an unsupervised task that may not yield a representation that is useful for prediction. Furthermore, there is the need to pick the number of clusters.\n",
      "\n",
      "(14.29) Now we see D = N , so we have as many parameters as data points. However, we can use any of the sparsity-promoting priors for w discussed in Chapter 13 to efficiently select a subset of the training exemplars. We call this a sparse vector machine.\n",
      "\n",
      "(Note that in the multi-class case, it is necessary to use group lasso, since each exemplar is associated with C weights, one per class.) We call this L1VM, which stands for “(cid:6)1-regularized vector machine”. By analogy, we deﬁne the use of an (cid:6)2 regularizer to be a L2VM or “(cid:6)2-regularized vector machine”; this of course will not be sparse.\n",
      "\n",
      "A simpler approach is to make each example xi be a prototype, so we get φ(x) = [κ(x, x1), . . . , κ(x, xN )]\n",
      "\n",
      "The most natural choice is to use (cid:6)1 regularization (Krishnapuram et al. 2005).\n",
      "\n",
      "We can get even greater sparsity by using ARD/SBL, resulting in a method called the rele- vance vector machine or RVM (Tipping 2001). One can ﬁt this model using generic ARD/SBL algorithms, although in practice the most common method is the greedy algorithm in (Tipping and Faul 2003) (this is the algorithm implemented in Mike Tipping’s code, which is bundled with PMTK).\n",
      "\n",
      "Another very popular approach to creating a sparse kernel machine is to use a support vector machine or SVM. This will be discussed in detail in Section 14.5. Rather than using a sparsity-promoting prior, it essentially modiﬁes the likelihood term, which is rather unnatural from a Bayesian point of view. Nevertheless, the effect is similar, as we will see.\n",
      "\n",
      "In Figure 14.4, we compare L2VM, L1VM, RVM and an SVM using the same RBF kernel on a binary classiﬁcation problem in 2d. For simplicity, λ was chosen by hand for L2VM and L1VM; for RVMs, the parameters are estimated using empirical Bayes; and for the SVM, we use CV to pick C = 1/λ, since SVM performance is very sensitive to this parameter (see Section 14.5.3). We see that all the methods give similar performance. However, RVM is the sparsest (and hence fastest at test time), then L1VM, and then SVM. RVM is also the fastest to train, since CV for an (This is despite the fact that the RVM code is in Matlab and the SVM code is in SVM is slow. C.) This result is fairly typical.\n",
      "\n",
      "In Figure 14.5, we compare L2VM, L1VM, RVM and an SVM using an RBF kernel on a 1d regression problem. Again, we see that predictions are quite similar, but RVM is the sparsest, then L2VM, then SVM. This is further illustrated in Figure 14.6.\n",
      "\n",
      "14.4\n",
      "\n",
      "The kernel trick\n",
      "\n",
      "Rather than deﬁning our feature vector in terms of kernels, φ(x) = [κ(x, x1), . . . , κ(x, xN )], we can instead work with the original feature vectors x, but modify the algorithm so that it replaces all inner products of the form (cid:24)x, x(cid:4)(cid:25) with a call to the kernel function, κ(x, x(cid:4)). This is called the kernel trick. It turns out that many algorithms can be kernelized in this way. We give some examples below. Note that we require that the kernel be a Mercer kernel for this trick to work.\n",
      "\n",
      "14.4. The kernel trick\n",
      "\n",
      "489\n",
      "\n",
      "logregL2, nerr=174\n",
      "\n",
      "logregL1, nerr=169\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "RVM, nerr=173\n",
      "\n",
      "SVM, nerr=173\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 14.4 Example of non-linear binary classiﬁcation using an RBF kernel with bandwidth σ = 0.3. (a) L2VM with λ = 5. (c) RVM. (d) SVM with C = 1/λ chosen by cross validation. Black circles denote the support vectors. Figure generated by kernelBinaryClassifDemo.\n",
      "\n",
      "(b) L1VM with λ = 1.\n",
      "\n",
      "14.4.1\n",
      "\n",
      "Kernelized nearest neighbor classiﬁcation\n",
      "\n",
      "Recall that in a 1NN classiﬁer (Section 1.4.2), we just need to compute the Euclidean distance of a test vector to all the training points, ﬁnd the closest one, and look up its label. This can be kernelized by observing that\n",
      "\n",
      "||xi − xi(cid:2) ||2\n",
      "\n",
      "2 = (cid:24)xi, xi(cid:25) + (cid:24)xi(cid:2) , xi(cid:2) (cid:25) −2(cid:24) xi, xi(cid:2) (cid:25)\n",
      "\n",
      "(14.30)\n",
      "\n",
      "This allows us to apply the nearest neighbor classiﬁer to structured data objects.\n",
      "\n",
      "14.4.2\n",
      "\n",
      "Kernelized K-medoids clustering\n",
      "\n",
      "K-means clustering (Section 11.4.2.5) uses Euclidean distance to measure dissimilarity, which is not always appropriate for structured objects. We now describe how to develop a kernelized\n",
      "\n",
      "490\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "linregL2\n",
      "\n",
      "linregL1\n",
      "\n",
      "1.2\n",
      "\n",
      "1.2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "1.2\n",
      "\n",
      "RVM\n",
      "\n",
      "1.2\n",
      "\n",
      "SVM\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 14.5 Example of kernel based regression on the noisy sinc function using an RBF kernel with bandwidth σ = 0.3. (c) RVM. (d) SVM regression with C = 1/λ chosen by cross validation, and (cid:20) = 0.1 (the default for SVMlight). Red circles denote the retained training exemplars. Figure generated by kernelRegrDemo.\n",
      "\n",
      "(a) L2VM with λ = 0.5.\n",
      "\n",
      "(b) L1VM with λ = 0.5.\n",
      "\n",
      "version of the algorithm.\n",
      "\n",
      "The ﬁrst step is to replace the K-means algorithm with the K-medoids algorothm. This is similar to K-means, but instead of representing each cluster’s centroid by the mean of all data vectors assigned to this cluster, we make each centroid be one of the data vectors themselves. Thus we always deal with integer indexes, rather than data objects. We assign objects to their closest centroids as before. When we update the centroids, we look at each object that belongs to the cluster, and measure the sum of its distances to all the others in the same cluster; we then pick the one which has the smallest such sum:\n",
      "\n",
      "mk = argmin i:zi=k\n",
      "\n",
      "i(cid:2):zi(cid:2) =k\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "d(i, i(cid:4))\n",
      "\n",
      "(14.31)\n",
      "\n",
      "14.4. The kernel trick\n",
      "\n",
      "491\n",
      "\n",
      "weights for linregL2\n",
      "\n",
      "weights for linregL1\n",
      "\n",
      "0.12\n",
      "\n",
      "0.06\n",
      "\n",
      "0.1\n",
      "\n",
      "0.04\n",
      "\n",
      "0.08\n",
      "\n",
      "0.02\n",
      "\n",
      "0.06\n",
      "\n",
      "0\n",
      "\n",
      "0.04\n",
      "\n",
      "0.02\n",
      "\n",
      "−0.02\n",
      "\n",
      "0\n",
      "\n",
      "−0.04\n",
      "\n",
      "−0.02\n",
      "\n",
      "−0.06\n",
      "\n",
      "−0.04\n",
      "\n",
      "−0.06\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "weights for RVM\n",
      "\n",
      "weights for SVM\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−0.2\n",
      "\n",
      "−1.5\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 14.6 Coefficient vectors of length N = 100 for the models in Figure 14.6. Figure generated by kernelRegrDemo.\n",
      "\n",
      "where\n",
      "\n",
      "d(i, i(cid:4)) (cid:2) ||xi − xi(cid:2) ||2 2\n",
      "\n",
      "(14.32)\n",
      "\n",
      "This takes O(n2 k) work per cluster, whereas K-means takes O(nkD) to update each cluster. The pseudo-code is given in Algorithm 5. This method can be modiﬁed to derive a classiﬁer, by computing the nearest medoid for each class. This is known as nearest medoid classiﬁcation (Hastie et al. 2009, p671).\n",
      "\n",
      "d(i, i(cid:4)).\n",
      "\n",
      "This algorithm can be kernelized by using Equation 14.30 to replace the distance computation,\n",
      "\n",
      "492\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "Algorithm 14.1: K-medoids algorithm 1 initialize m1:K as a random subset of size K from {1, . . . , N }; 2 repeat 3\n",
      "\n",
      "4 5 until converged;\n",
      "\n",
      "zi = argmink d(i, mk) for i = 1 :N ; mk ← argmini:zi=k\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i(cid:2):zi(cid:2) =k d(i, i(cid:4)) for k = 1 : K;\n",
      "\n",
      "14.4.3\n",
      "\n",
      "Kernelized ridge regression\n",
      "\n",
      "Applying the kernel trick to distance-based methods was straightforward. It is not so obvious how to apply it to parametric models such as ridge regression. However, it can be done, as we now explain. This will serve as a good “warm up” for studying SVMs.\n",
      "\n",
      "14.4.3.1\n",
      "\n",
      "The primal problem Let x ∈ RD be some feature vector, and X be the corresponding N × D design matrix. We want to minimize\n",
      "\n",
      "J(w) = (y − Xw)T (y − Xw) +λ||w ||2\n",
      "\n",
      "(14.33)\n",
      "\n",
      "The optimal solution is given by\n",
      "\n",
      "w = (XT X + λID)−1XT y = (\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "xixT\n",
      "\n",
      "i + λID)−1XT y\n",
      "\n",
      "(14.34)\n",
      "\n",
      "i\n",
      "\n",
      "14.4.3.2\n",
      "\n",
      "The dual problem\n",
      "\n",
      "Equation 14.34 is not yet in the form of inner products. However, using the matrix inversion lemma (Equation 4.107) we rewrite the ridge estimate as follows\n",
      "\n",
      "(14.35) which takes O(N 3 + N 2D) time to compute. This can be advantageous if D is large. Further- more, we see that we can partially kernelize this, by replacing XXT with the Gram matrix K. But what about the leading XT term?\n",
      "\n",
      "w = XT (XXT + λIN )−1y\n",
      "\n",
      "Let us deﬁne the following dual variables: α (cid:2) (K + λIN )−1y\n",
      "\n",
      "(14.36)\n",
      "\n",
      "Then we can rewrite the primal variables as follows\n",
      "\n",
      "w = XT α =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αixi\n",
      "\n",
      "(14.37)\n",
      "\n",
      "i=1\n",
      "\n",
      "This tells us that the solution vector is just a linear sum of the N training vectors. When we plug this in at test time to compute the predictive mean, we get\n",
      "\n",
      "ˆf (x) = wT x =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αixT\n",
      "\n",
      "i x =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αiκ(x, xi)\n",
      "\n",
      "(14.38)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "14.4. The kernel trick\n",
      "\n",
      "493\n",
      "\n",
      "1.5\n",
      "\n",
      "Eigenvalue=22.558\n",
      "\n",
      "1.5\n",
      "\n",
      "Eigenvalue=20.936\n",
      "\n",
      "1.5\n",
      "\n",
      "Eigenvalue=4.648\n",
      "\n",
      "1.5\n",
      "\n",
      "Eigenvalue=3.988\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "Eigenvalue=3.372\n",
      "\n",
      "Eigenvalue=2.956\n",
      "\n",
      "Eigenvalue=2.760\n",
      "\n",
      "Eigenvalue=2.211\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "Figure 14.7 Visualization of the ﬁrst 8 kernel principal component basis functions derived from some 2d data. We use an RBF kernel with σ2 = 0.1. Figure generated by kpcaScholkopf, written by Bernhard Scholkopf.\n",
      "\n",
      "So we have succesfully kernelized ridge regression by changing from primal to dual variables. This technique can be applied to many other linear models, such as logistic regression.\n",
      "\n",
      "14.4.3.3\n",
      "\n",
      "Computational cost The cost of computing the dual variables α is O(N 3), whereas the cost of computing the primal variables w is O(D3). Hence the kernel method can be useful in high dimensional settings, even if we only use a linear kernel (c.f., the SVD trick in Equation 7.44). However, prediction using the dual variables takes O(N D) time, while prediction using the primal variables only takes O(D) time. We can speedup prediction by making α sparse, as we discuss in Section 14.5.\n",
      "\n",
      "14.4.4\n",
      "\n",
      "Kernel PCA\n",
      "\n",
      "In Section 12.2, we saw how we could compute a low-dimensional linear embedding of some data using PCA. This required ﬁnding the eigenvectors of the sample covariance matrix S =\n",
      "\n",
      "494\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "1 i = (1/N )XT X. However, we can also compute PCA by ﬁnding the eigenvectors N of the inner product matrix XXT , as we show below. This will allow us to produce a nonlinear embedding, using the kernel trick, a method known as kernel PCA (Schoelkopf et al. 1998).\n",
      "\n",
      "eigenvalues in Λ. By deﬁnition we have (XXT )U = UΛ. Pre-multiplying by XT gives\n",
      "\n",
      "First, let U be an orthogonal matrix containing the eigenvectors of XXT with corresponding\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 xixT\n",
      "\n",
      "(XT X)(XT U) = (XT U)Λ\n",
      "\n",
      "(14.39)\n",
      "\n",
      "from which we see that the eigenvectors of XT X (and hence of S) are V = XT U, with eigen- values given by Λ as before. However, these eigenvectors are not normalized, since ||vj||2 = j uj = λj. So the normalized eigenvectors are given by Vpca = XT UΛ− 1 j XXT uj = λjuT uT 2 . This is a useful trick for regular PCA if D > N , since XT X has size D × D, whereas XXT has size N × N . It will also allow us to use the kernel trick, as we now show.\n",
      "\n",
      "Now let K = XXT be the Gram matrix. Recall from Mercer’s theorem that the use of a kernel implies some underlying feature space, so we are implicitly replacing xi with φ(xi) = φi. Let Φ be the corresponding (notional) design matrix, and Sφ = 1 i be the corresponding N (notional) covariance matrix in feature space. The eigenvectors are given by Vkpca = ΦT UΛ− 1 2 , where U and Λ contain the eigenvectors and eigenvalues of K. Of course, we can’t actually compute Vkpca, since φi is potentially inﬁnite dimensional. However, we can compute the projection of a test vector x∗ onto the feature space as follows:\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i φiφT\n",
      "\n",
      "φT\n",
      "\n",
      "∗ Vkpca = φT\n",
      "\n",
      "∗ ΦUΛ− 1\n",
      "\n",
      "2 = kT\n",
      "\n",
      "∗ UΛ− 1\n",
      "\n",
      "2\n",
      "\n",
      "(14.40)\n",
      "\n",
      "where k∗ = [κ(x∗, x1), . . . , κ(x∗, xN )].\n",
      "\n",
      "There is one ﬁnal detail to worry about. So far, we have assumed the projected data has zero mean, which is not the case in general. We cannot simply subtract off the mean in feature space. However, there is a trick we can use. Deﬁne the centered feature vector as ˜φi = φ(xi) − 1 N\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "j=1 φ(xj). The Gram matrix of the centered feature vectors is given by\n",
      "\n",
      "˜Kij = ˜φ\n",
      "\n",
      "= φT\n",
      "\n",
      "T i ˜φj i φj − 1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "i φk − 1 φT N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "φT\n",
      "\n",
      "j φk +\n",
      "\n",
      "1 N 2\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "M(cid:2)\n",
      "\n",
      "l=1\n",
      "\n",
      "φT\n",
      "\n",
      "k φl\n",
      "\n",
      "(14.41)\n",
      "\n",
      "(14.42)\n",
      "\n",
      "= κ(xi, xj) − 1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "κ(xi, xk) − 1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "κ(xj, xk) +\n",
      "\n",
      "1 N 2\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "M(cid:2)\n",
      "\n",
      "l=1\n",
      "\n",
      "κ(xk, xl) (14.43)\n",
      "\n",
      "This can be expressed in matrix notation as follows:\n",
      "\n",
      "˜K = HKH\n",
      "\n",
      "(14.44)\n",
      "\n",
      "where H (cid:2) I − 1 pseudocode shown in Algorithm 9.\n",
      "\n",
      "Whereas linear PCA is limited to using L ≤ D components, in kPCA, we can use up to N components, since the rank of Φ is N ×D∗, where D∗ is the (potentially inﬁnite) dimensionality of embedded feature vectors. Figure 14.7 gives an example of the method applied to some D = 2 dimensional data using an RBF kernel. We project points in the unit grid onto the ﬁrst\n",
      "\n",
      "N 1N 1T N .\n",
      "\n",
      "is the centering matrix. We can convert all this algebra into the\n",
      "\n",
      "14.4. The kernel trick\n",
      "\n",
      "495\n",
      "\n",
      "Algorithm 14.2: Kernel PCA 1 Input: K of size N × N , K∗ of size N∗ × N , num. latent dimensions L; 2 O = 1N 1T 3 ˜K = K − OK − KO + OKO ; 4 [U, Λ] =eig( ˜K) ; 5 for i = 1 :N do vi = ui/ λi 6 1T 7 O∗ = 1N∗ N /N ; 8 ˜K∗ = K∗ − O∗K∗ − K∗O∗ + O∗K∗O∗ ; 9 Z = ˜K∗V(:, 1 :L)\n",
      "\n",
      "N /N ;\n",
      "\n",
      "√\n",
      "\n",
      "0.6\n",
      "\n",
      "pca\n",
      "\n",
      "0.6\n",
      "\n",
      "kpca\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 14.8 2d visualization of some 2d data. generated by kpcaDemo2, based on code by L.J.P. van der Maaten.\n",
      "\n",
      "(a) PCA projection.\n",
      "\n",
      "(b) Kernel PCA projection. Figure\n",
      "\n",
      "8 components and visualize the corresponding surfaces using a contour plot. We see that the ﬁrst two component separate the three clusters, and following components split the clusters.\n",
      "\n",
      "Although the features learned by kPCA can be useful for classiﬁcation (Schoelkopf et al. 1998), they are not necessarily so useful for data visualization. For example, Figure 14.8 shows the projection of the data from Figure 14.7 onto the ﬁrst 2 principal bases computed using PCA and kPCA. Obviously PCA perfectly represents the data. kPCA represents each cluster by a different line.\n",
      "\n",
      "Of course, there is no need to project 2d data back into 2d. So let us consider a different data set. We will use a 12 dimensional data set representing the three known phases of ﬂow in an oil pipeline. (This data, which is widely used to compare data visualization methods, is synthetic, and comes from (Bishop and James 1993).) We project this into 2d using PCA and kPCA (with an RBF kernel). The results are shown in Figure 14.9. If we perform nearest neighbor classiﬁcation in the low-dimensional space, kPCA makes 13 errors and PCA makes 20 (Lawrence\n",
      "\n",
      "496\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "0.3\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      ".25\n",
      "\n",
      "0.5\n",
      "\n",
      "0.2\n",
      "\n",
      "−1\n",
      "\n",
      ".15\n",
      "\n",
      "1.5\n",
      "\n",
      "0.1\n",
      "\n",
      "−2\n",
      "\n",
      ".05\n",
      "\n",
      "2.5\n",
      "\n",
      "−3\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0.05\n",
      "\n",
      "0.1\n",
      "\n",
      "0.15\n",
      "\n",
      "0.2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 14.9 2d representation of 12 dimensional oil ﬂow data. The different colors/symbols represent the 3 phases of oil ﬂow. (a) PCA. (b) Kernel PCA with Gaussian kernel. Compare to Figure 15.10(b). From Figure 1 of (Lawrence 2005). Used with kind permission of Neil Lawrence.\n",
      "\n",
      "2005). Nevertheless, the kPCA projection is rather unnatural. how to make kernelized versions of probabilistic PCA.\n",
      "\n",
      "In Section 15.5, we will discuss\n",
      "\n",
      "Note that there is a close connection between kernel PCA and a technique known as mul- tidimensional scaling or MDS. This methods ﬁnds a low-dimensional embedding such that Euclidean distance in the embedding space approximates the original dissimilarity matrix. See e.g., (Williams 2002) for details.\n",
      "\n",
      "14.5\n",
      "\n",
      "Support vector machines (SVMs)\n",
      "\n",
      "In Section 14.3.2, we saw one way to derive a sparse kernel machine, namely by using a GLM with kernel basis functions, plus a sparsity-promoting prior such as (cid:6)1 or ARD. An alternative approach is to change the objective function from negative log likelihood to some other loss In particular, consider the (cid:6)2 regularized empirical function, as we discussed in Section 6.5.5. risk function\n",
      "\n",
      "J(w, λ) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "L(yi, ˆyi) +λ||w ||2\n",
      "\n",
      "(14.45)\n",
      "\n",
      "i=1\n",
      "\n",
      "where ˆyi = wT xi + w0. moment.) deﬁned in Equation 6.73, this is equivalent to logistic regression.\n",
      "\n",
      "In the ridge regression case, we know that the solution to this has the form ˆw = (XT X + λI)−1XT y, and plug-in predictions take the form ˆw0 + ˆwT x. As we saw in Section 14.4.3, we can rewrite these equations in a way that only involves inner products of the form xT x(cid:4), which we can replace by calls to a kernel function, κ(x, x(cid:4)). This is kernelized, but not sparse. However, if we replace the quadratic/ log-loss with some other loss function, to be explained below, we can ensure that the solution is sparse, so that predictions only depend on a subset of the training data, known as support vectors. This combination of the kernel trick plus a modiﬁed loss function is known as a support vector machine or SVM. This technique was\n",
      "\n",
      "(So far this is in the original feature space; we introduce kernels in a If L is quadratic loss, this is equivalent to ridge regression, and if L is the log-loss\n",
      "\n",
      "14.5. Support vector machines (SVMs)\n",
      "\n",
      "497\n",
      "\n",
      "5\n",
      "\n",
      "y(x)\n",
      "\n",
      "4.5\n",
      "\n",
      "4\n",
      "\n",
      "L2 ε−insensitive huber\n",
      "\n",
      "ξ > 0\n",
      "\n",
      "3.5\n",
      "\n",
      "3\n",
      "\n",
      "y + (cid:2)\n",
      "\n",
      "2.5\n",
      "\n",
      "y\n",
      "\n",
      "2\n",
      "\n",
      "1.5\n",
      "\n",
      "ξ∗ > 0\n",
      "\n",
      "y − (cid:2)\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "x\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 14.10 (a) Illustration of (cid:7)2, Huber and (cid:20)-insensitive loss functions, where (cid:20) = 1.5. Figure generated (b) Illustration of the (cid:20)-tube used in SVM regression. Points above the tube have by huberLossDemo. ξi > 0 and ξ∗ i > 0. Points inside the tube have ξi = ξ∗\n",
      "\n",
      "i = 0. Based on Figure 7.7 of (Bishop 2006a).\n",
      "\n",
      "i = 0. Points below the tube have ξi = 0 and ξ∗\n",
      "\n",
      "originally designed for binary classiﬁcation, but can be extended to regression and multi-class classiﬁcation as we explain below.\n",
      "\n",
      "Note that SVMs are very unnatural from a probabilistic point of view. First, they encode sparsity in the loss function rather than the prior. Second, they encode kernels by using an algorithmic trick, rather than being an explicit part of the model. Finally, SVMs do not result in probabilistic outputs, which causes various difficulties, especially in the multi-class classiﬁcation setting (see Section 14.5.2.4 for details).\n",
      "\n",
      "It is possible to obtain sparse, probabilistic, multi-class kernel-based classiﬁers, which work as well or better than SVMs, using techniques such as the L1VM or RVM, discussed in Section 14.3.2. However, we include a discussion of SVMs, despite their non-probabilistic nature, for two main reasons. First, they are very popular and widely used, so all students of machine learning should know about them. Second, they have some computational advantages over probabilistic methods in the structured output case; see Section 19.7.\n",
      "\n",
      "14.5.1\n",
      "\n",
      "SVMs for regression\n",
      "\n",
      "The problem with kernelized ridge regression is that the solution vector w depends on all the training inputs. We now seek a method to produce a sparse estimate.\n",
      "\n",
      "Vapnik (Vapnik et al. 1997) proposed a variant of the Huber loss function (Section 7.4) called\n",
      "\n",
      "the epsilon insensitive loss function, deﬁned by\n",
      "\n",
      "L(cid:4)(y, ˆy) (cid:2)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "0 |y − ˆy| −(cid:8)\n",
      "\n",
      "if |y − ˆy| < (cid:8) otherwise\n",
      "\n",
      "(14.46)\n",
      "\n",
      "This means that any point lying inside an (cid:8)-tube around the prediction is not penalized, as in Figure 14.10.\n",
      "\n",
      "The corresponding objective function is usually written in the following form\n",
      "\n",
      "J = C\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(cid:4)(yi, ˆyi) +\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2\n",
      "\n",
      "(14.47)\n",
      "\n",
      "498\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "where ˆyi = f (xi) =w T xi + w0 and C = 1/λ is a regularization constant. This objective is convex and unconstrained, but not differentiable, because of the absolute value function in the loss term. As in Section 13.4, where we discussed the lasso problem, there are several possible algorithms we could use. One popular approach is to formulate the problem as a constrained In particular, we introduce slack variables to represent the degree to optimization problem. which each point lies outside the tube:\n",
      "\n",
      "yi ≤ f (xi) +(cid:8) + ξ+ i yi ≥ f (xi) − (cid:8) − ξ− i\n",
      "\n",
      "(14.48)\n",
      "\n",
      "(14.49)\n",
      "\n",
      "Given this, we can rewrite the objective as follows:\n",
      "\n",
      "J = C\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(ξ+\n",
      "\n",
      "i + ξ−\n",
      "\n",
      "i ) +\n",
      "\n",
      "i=1\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2\n",
      "\n",
      "(14.50)\n",
      "\n",
      "This is a quadratic function of w, and must be minimized subject to the linear constraints in Equations 14.48-14.49, as well as the positivity constraints ξ+ i ≥ 0. This is a standard quadratic program in 2N + D + 1 variables.\n",
      "\n",
      "i ≥ 0 and ξ−\n",
      "\n",
      "One can show (see e.g., (Schoelkopf and Smola 2002)) that the optimal solution has the form\n",
      "\n",
      "ˆw =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "αixi\n",
      "\n",
      "(14.51)\n",
      "\n",
      "i\n",
      "\n",
      "where αi ≥ 0. Furthermore, it turns out that the α vector is sparse, because we don’t care about errors which are smaller than (cid:8). The xi for which αi > 0 are called the support vectors; thse are points for which the errors lie on or outside the (cid:8) tube.\n",
      "\n",
      "Once the model is trained, we can then make predictions using ˆy(x) = ˆw0 + ˆwT x\n",
      "\n",
      "(14.52)\n",
      "\n",
      "Plugging in the deﬁnition of ˆw we get (cid:2)\n",
      "\n",
      "ˆy(x) = ˆw0 +\n",
      "\n",
      "αixT\n",
      "\n",
      "i x\n",
      "\n",
      "(14.53)\n",
      "\n",
      "i\n",
      "\n",
      "Finally, we can replace xT\n",
      "\n",
      "ˆy(x) = ˆw0 +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "αiκ(xi, x)\n",
      "\n",
      "i x with κ(xi, x) to get a kernelized solution:\n",
      "\n",
      "(14.54)\n",
      "\n",
      "i\n",
      "\n",
      "14.5.2\n",
      "\n",
      "SVMs for classiﬁcation\n",
      "\n",
      "We now discuss how to apply SVMs to classiﬁcation. We ﬁrst focus on the binary case, and then discuss the multi-class case in Section 14.5.2.4.\n",
      "\n",
      "14.5.2.1\n",
      "\n",
      "Hinge loss\n",
      "\n",
      "In Section 6.5.5, we showed that the negative log likelihood of a logistic regression model,\n",
      "\n",
      "L\n",
      "\n",
      "nll(y, η) = − log p(y|x, w) = log(1 + e−yη)\n",
      "\n",
      "(14.55)\n",
      "\n",
      "14.5. Support vector machines (SVMs)\n",
      "\n",
      "499\n",
      "\n",
      "was a convex upper bound on the 0-1 risk of a binary classiﬁer, where η = f (x) =w T x + w0 is the log odds ratio, and we have assumed the labels are y ∈ {1, −1} rather than {0, 1}. In this section, we replace the NLL loss with the hinge loss, deﬁned as\n",
      "\n",
      "L\n",
      "\n",
      "hinge(y, η) = max(0, 1 − yη) = (1 − yη)+\n",
      "\n",
      "(14.56)\n",
      "\n",
      "Here η = f (x) is our “conﬁdence” in choosing label y = 1; however, it need not have any probabilistic semantics. See Figure 6.7 for a plot. We see that the function looks like a door hinge, hence its name. The overall objective has the form\n",
      "\n",
      "min w,w0\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(1 − yif (xi))+\n",
      "\n",
      "i=1\n",
      "\n",
      "(14.57)\n",
      "\n",
      "Once again, this is non-differentiable, because of the max term. However, by introducing slack variables ξi, one can show that this is equivalent to solving\n",
      "\n",
      "min w,w0,ξ\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "ξi\n",
      "\n",
      "s.t.\n",
      "\n",
      "ξi ≥ 0, yi(xT\n",
      "\n",
      "i w + w0) ≥ 1 − ξi, i = 1 :N\n",
      "\n",
      "(14.58)\n",
      "\n",
      "This is a quadratic program in N + D + 1 variables, subjet to O(N ) constraints. We can eliminate the primal variables w, w0 and ξi, and just solve the N dual variables, which correspond to the Lagrange multipliers for the constraints. Standard solvers take O(N 3) time. However, specialized algorithms, which avoid the use of generic QP solvers, have been developed for this problem, such as the sequential minimal optimization or SMO algorithm (Platt 1998). In practice this can take O(N 2). However, even this can be too slow if N is large. In such settings, it is common to use linear SVMs, which take O(N ) time to train (Joachims 2006; Bottou et al. 2007).\n",
      "\n",
      "One can show that the solution has the form\n",
      "\n",
      "ˆw =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "αixi\n",
      "\n",
      "(14.59)\n",
      "\n",
      "i\n",
      "\n",
      "where αi = λiyi and where α is sparse (because of the hinge loss). The xi for which αi > 0 are called support vectors; these are points which are either incorrectly classiﬁed, or are classiﬁed correctly but are on or inside the margin (we disuss margins below). See Figure 14.12(b) for an illustration.\n",
      "\n",
      "At test time, prediction is done using\n",
      "\n",
      "ˆy(x) = sgn(f (x)) = sgn\n",
      "\n",
      "(cid:3) ˆw0 + ˆwT x\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(14.60)\n",
      "\n",
      "Using Equation 14.59 and the kernel trick we have\n",
      "\n",
      "ˆy(x) = sgn\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ˆw0 +\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αiκ(xi, x)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(14.61)\n",
      "\n",
      "i=1\n",
      "\n",
      "This takes O(sD) time to compute, where s ≤ N is the number of support vectors. This depends on the sparsity level, and hence on the regularizer C.\n",
      "\n",
      "500\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "Figure 14.11 Right: a separating hyper-plane with small margin.\n",
      "\n",
      "Illustration of the large margin principle. Left: a separating hyper-plane with large margin.\n",
      "\n",
      "y > 0\n",
      "\n",
      "y = −1\n",
      "\n",
      "y = 0\n",
      "\n",
      "y < 0\n",
      "\n",
      "R1\n",
      "\n",
      "ξ > 1\n",
      "\n",
      "y = 0\n",
      "\n",
      "R0\n",
      "\n",
      "x\n",
      "\n",
      "ξ < 1\n",
      "\n",
      "y = 1\n",
      "\n",
      "w\n",
      "\n",
      "r = f (x) (cid:6)w(cid:6)\n",
      "\n",
      "x⊥\n",
      "\n",
      "ξ = 0\n",
      "\n",
      "−w0 (cid:6)w(cid:6)\n",
      "\n",
      "ξ = 0\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 14.12 (a) Illustration of the geometry of a linear decision boundary in 2d. A point x is classiﬁed as belonging in decision region R1 if f (x) > 0, otherwise it belongs in decision region R2; here f (x) is known as a discriminant function. The decision boundary is the set of points such that f (x) = 0. w is a vector which is perpendicular to the decision boundary. The term w0 controls the distance of the decision boundary from the origin. The signed distance of x from its orthogonal projection onto the decision boundary, x⊥, is given by f (x)/||w||. Based on Figure 4.1 of (Bishop 2006a). (b) Illustration of the soft margin principle. Points with circles around them are support vectors. We also indicate the value of the corresponding slack variables. Based on Figure 7.3 of (Bishop 2006a).\n",
      "\n",
      "14.5. Support vector machines (SVMs)\n",
      "\n",
      "501\n",
      "\n",
      "14.5.2.2\n",
      "\n",
      "The large margin principle\n",
      "\n",
      "In this section, we derive Equation 14.58 form a completely different perspective. Recall that our goal is to derive a discriminant function f (x) which will be linear in the feature space implied by the choice of kernel. Consider a point x in this induced space. Referring to Figure 14.12(a), we see that\n",
      "\n",
      "x = x⊥ + r\n",
      "\n",
      "w ||w||\n",
      "\n",
      "(14.62)\n",
      "\n",
      "where r is the distance of x from the decision boundary whose normal vector is w, and x⊥ is the orthogonal projection of x onto this boundary. Hence\n",
      "\n",
      "f (x) =w T x + w0 = (wT x⊥ + w0) +r\n",
      "\n",
      "wT w ||w||\n",
      "\n",
      "(14.63)\n",
      "\n",
      "Now f (x⊥) = 0 so 0 =w T x⊥ + w0. Hence f (x) = r wT w√\n",
      "\n",
      "We would like to make this distance r = f (x)/||w|| as large as possible, for reasons illustrated in Figure 14.11. In particular, there might be many lines that perfectly separate the training data (especially if we work in a high dimensional feature space), but intuitively, the best one to pick is the one that maximizes the margin, i.e., the perpendicular distance to the closest point. In addition, we want to ensure each point is on the correct side of the boundary, hence we want f (xi)yi > 0. So our objective becomes\n",
      "\n",
      "wT w , and r =\n",
      "\n",
      "f (x) ||w|| .\n",
      "\n",
      "max w,w0\n",
      "\n",
      "N min i=1\n",
      "\n",
      "yi(wT xi + w0) ||w||\n",
      "\n",
      "(14.64)\n",
      "\n",
      "Note that by rescaling the parameters using w → kw and w0 → kw0, we do not change the distance of any point to the boundary, since the k factor cancels out when we divide by ||w||. Therefore let us deﬁne the scale factor such that yifi = 1 for the point that is closest to the decision boundary. We therefore want to optimize\n",
      "\n",
      "min w,w0\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2\n",
      "\n",
      "s.t.\n",
      "\n",
      "yi(wT xi + w0) ≥ 1, i = 1 :N\n",
      "\n",
      "(14.65)\n",
      "\n",
      "(The fact of 1 2 is added for convenience and doesn’t affect the optimal parameters.) The constraint says that we want all points to be on the correct side of the decision boundary with a margin of at least 1. For this reason, we say that an SVM is an example of a large margin classiﬁer.\n",
      "\n",
      "If the data is not linearly separable (even after using the kernel trick), there will be no feasible solution in which yifi ≥ 1 for all i. We therefore introduce slack variables ξi ≥ 0 such that ξi = 0 if the point is on or inside the correct margin boundary, and ξi = |yi − fi| otherwise. If 0 < ξi ≤ 1 the point lies inside the margin, but on the correct side of the decision boundary. If ξi > 1, the point lies on the wrong side of the decision boundary. See Figure 14.12(b).\n",
      "\n",
      "1 − ξi. The new objective becomes\n",
      "\n",
      "We replace the hard constraints that yifi ≥ 0 with the soft margin constraints that yifi ≥\n",
      "\n",
      "min w,w0,ξ\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "ξi\n",
      "\n",
      "s.t.\n",
      "\n",
      "ξi ≥ 0, yi(xT\n",
      "\n",
      "i w + w0) ≥ 1 − ξi\n",
      "\n",
      "(14.66)\n",
      "\n",
      "502\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "6\n",
      "\n",
      "Correct log−odds RVM y(x) SVM y(x)\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "1.2\n",
      "\n",
      "Figure 14.13 Log-odds vs x for 3 different methods. Based on Figure 10 of (Tipping 2001). Used with kind permission of Mike Tipping.\n",
      "\n",
      "which is the same as Equation 14.58. Since ξi > 1 means point i is misclassiﬁed, we can interpret\n",
      "\n",
      "The parameter C is a regularization parameter that controls the number of errors we are It is common to deﬁne this using C = 1/(νN ), where willing to tolerate on the training set. 0 < ν ≤ 1 controls the fraction of misclassiﬁed points that we allow during the training phase. This is called a ν-SVM classiﬁer. This is usually set using cross-validation (see Section 14.5.3).\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i ξi as an upper bound on the number of misclassiﬁed points.\n",
      "\n",
      "14.5.2.3\n",
      "\n",
      "Probabilistic output\n",
      "\n",
      "An SVM classiﬁer produces a hard-labeling, ˆy(x) = sign(f (x)). However, we often want a measure of conﬁdence in our prediction. One heuristic approach is to interpret f (x) as the p(y=1|x) log-odds ratio, log p(y=0|x) . We can then convert the output of an SVM to a probability using\n",
      "\n",
      "p(y = 1|x, θ) = σ(af (x) +b)\n",
      "\n",
      "(14.67)\n",
      "\n",
      "where a, b can be estimated by maximum likelihood on a separate validation set. (Using the training set to estimate a and b leads to severe overﬁtting.) This technique was ﬁrst proposed in (Platt 2000).\n",
      "\n",
      "However, the resulting probabilities are not particularly well calibrated, since there is nothing in the SVM training procedure that justiﬁes interpreting f (x) as a log-odds ratio. To illustrate this, consider an example from (Tipping 2001). Suppose we have 1d data where p(x|y = 0) = Unif(0, 1) and p(x|y = 1) = Unif(0.5, 1.5). Since the class-conditional distributions overlap in the middle, the log-odds of class 1 over class 0 should be zero in [0.5, 1.0], and inﬁnite outside this region. We sampled 1000 points from the model, and then ﬁt an RVM and an SVM with a Gaussian kenel of width 0.1. Both models can perfectly capture the decision boundary, and achieve a generalizaton error of 25%, which is Bayes optimal in this problem. The probabilistic output from the RVM is a good approximation to the true log-odds, but this is not the case for the SVM, as shown in Figure 14.13.\n",
      "\n",
      "14.5. Support vector machines (SVMs)\n",
      "\n",
      "503\n",
      "\n",
      "?\n",
      "\n",
      "C1\n",
      "\n",
      "C3\n",
      "\n",
      "R1\n",
      "\n",
      "R1\n",
      "\n",
      "R2\n",
      "\n",
      "?\n",
      "\n",
      "R3\n",
      "\n",
      "C1\n",
      "\n",
      "R3\n",
      "\n",
      "C2\n",
      "\n",
      "C1\n",
      "\n",
      "R2\n",
      "\n",
      "C2\n",
      "\n",
      "Not C1\n",
      "\n",
      "Not C2\n",
      "\n",
      "C2\n",
      "\n",
      "C3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 14.14 2. (Bishop 2006a).\n",
      "\n",
      "(a) The one-versus-rest approach. The green region is predicted to be both class 1 and class (b) The one-versus-one approach. The label of the green region is ambiguous. Based on Figure 4.2 of\n",
      "\n",
      "14.5.2.4\n",
      "\n",
      "SVMs for multi-class classiﬁcation\n",
      "\n",
      "In Section 8.3.7, we saw how we could “upgrade” a binary logistic regression model to the multi- class case, by replacing the sigmoid function with the softmax, and the Bernoulli distribution with the multinomial. Upgrading an SVM to the multi-class case is not so easy, since the outputs are not on a calibrated scale and hence are hard to compare to each other.\n",
      "\n",
      "The obvious approach is to use a one-versus-the-rest approach (also called one-vs-all), in which we train C binary classiﬁers, fc(x), where the data from class c is treated as positive, and the data from all the other classes is treated as negative. However, this can result in regions of input space which are ambiguously labeled, as shown in Figure 14.14(a).\n",
      "\n",
      "A common alternative is to pick ˆy(x) = arg maxc fc(x). However, this technique may not work either, since there is no guarantee that the different fc functions have comparable magnitudes. In addition, each binary subproblem is likely to suffer from the class imbalance problem. To see this, suppose we have 10 equally represented classes. When training f1, we will have 10% positive examples and 90% negative examples, which can hurt performance. It is possible to devise ways to train all C classiﬁers simultaneously (Weston and Watkins 1999), but the resulting method takes O(C 2N 2) time, instead of the usual O(CN 2) time.\n",
      "\n",
      "Another approach is to use the one-versus-one or OVO approach, also called all pairs, in which we train C(C −1)/2 classiﬁers to discriminate all pairs fc,c(cid:2) . We then classify a point into the class which has the highest number of votes. However, this can also result in ambiguities, as shown in Figure 14.14(b). Also, it takes O(C 2N 2) time to train and O(C 2Nsv) to test each data point, where Nsv is the number of support vectors.2 See also (Allwein et al. 2000) for an approach based on error-correcting output codes.\n",
      "\n",
      "It is worth remembering that all of these difficulties, and the plethora of heuristics that have been proposed to ﬁx them, fundamentally arise because SVMs do not model uncertainty using probabilities, so their output scores are not comparable across classes.\n",
      "\n",
      "2. We can reduce the test time by structuring the classes into a DAG (directed acyclic graph), and performing O(C) pairwise comparisons (Platt et al. 2000). However, the O(C2) factor in the training time is unavoidable.\n",
      "\n",
      "504\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "γ = 5.0\n",
      "\n",
      "0.5\n",
      "\n",
      "0.35\n",
      "\n",
      "0.4\n",
      "\n",
      "r o r r e\n",
      "\n",
      "v c\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "r o r r e\n",
      "\n",
      "v c\n",
      "\n",
      "0.3\n",
      "\n",
      "0.25\n",
      "\n",
      "0.1 1 10\n",
      "\n",
      "0 10\n",
      "\n",
      "γ\n",
      "\n",
      "10\n",
      "\n",
      "−1\n",
      "\n",
      "10\n",
      "\n",
      "−2\n",
      "\n",
      "0 10\n",
      "\n",
      "C\n",
      "\n",
      "2 10\n",
      "\n",
      "4 10\n",
      "\n",
      "0.2\n",
      "\n",
      "10\n",
      "\n",
      "−2\n",
      "\n",
      "10\n",
      "\n",
      "−1\n",
      "\n",
      "0 10\n",
      "\n",
      "1 10 C\n",
      "\n",
      "2 10\n",
      "\n",
      "3 10\n",
      "\n",
      "4 10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 14.15 (a) A cross validation estimate of the 0-1 error for an SVM classiﬁer with RBF kernel with different precisions γ = 1/(2σ2) and different regularizer λ = 1/C, applied to a synthetic data set drawn from a mixture of 2 Gaussians. (b) A slice through this surface for γ = 5 The red dotted line is the Bayes optimal error, computed using Bayes rule applied to the model used to generate the data. Based on Figure 12.6 of (Hastie et al. 2009). Figure generated by svmCgammaDemo.\n",
      "\n",
      "14.5.3\n",
      "\n",
      "Choosing C\n",
      "\n",
      "SVMs for both classiﬁcation and regression require that you specify the kernel function and the parameter C. Typically C is chosen by cross-validation. Note, however, that C interacts quite strongly with the kernel parameters. For example, suppose we are using an RBF kernel with precision γ = 1 If γ = 5, corresponding to narrow kernels, we need heavy regularization, and hence small C (so λ = 1/C is big). If γ = 1, a larger value of C should be used. So we see that γ and C are tightly coupled. This is illustrated in Figure 14.15, which shows the CV estimate of the 0-1 risk as a function of C and γ.\n",
      "\n",
      "The authors of libsvm recommend (Hsu et al. 2009) using CV over a 2d grid with values C ∈ {2−5, 2−3, . . . , 215} and γ ∈ {2−15, 2−13, . . . , 23}. In addition, it is important to standardize the data ﬁrst, for a spherical Gaussian kernel to make sense.\n",
      "\n",
      "To choose C efficiently, one can develop a path following algorithm in the spirit of lars (Section 13.3.4). The basic idea is to start with λ large, so that the margin 1/||w(λ)|| is wide, and hence all points are inside of it and have αi = 1. By slowly decreasing λ, a small set of points will move from inside the margin to outside, and their αi values will change from 1 to 0, as they cease to be support vectors. When λ is maximal, the function is completely smoothed, and no support vectors remain. See (Hastie et al. 2004) for the details.\n",
      "\n",
      "2σ2 .\n",
      "\n",
      "14.5.4\n",
      "\n",
      "Summary of key points\n",
      "\n",
      "Summarizing the above discussion, we recognize that SVM classiﬁers involve three key ingre- dients: the kernel trick, sparsity, and the large margin principle. The kernel trick is necessary to prevent underﬁtting, i.e., to ensure that the feature vector is sufficiently rich that a linear classiﬁer can separate the data. (Recall from Section 14.2.3 that any Mercer kernel can be viewed as implicitly deﬁning a potentially high dimensional feature vector.) If the original features are already high dimensional (as in many gene expression and text classiﬁcation problems), it suf- ﬁces to use a linear kernel, κ(x, x(cid:4)) =x T x(cid:4), which is equivalent to working with the original features.\n",
      "\n",
      "14.6. Comparison of discriminative kernel methods\n",
      "\n",
      "505\n",
      "\n",
      "Method Opt. w Convex L2VM Convex L1VM Not convex RVM Convex SVM N/A GP\n",
      "\n",
      "Opt. kernel EB CV EB CV EB\n",
      "\n",
      "Sparse No Yes Yes Yes No\n",
      "\n",
      "Prob. Multiclass Non-Mercer Yes Yes Yes No Yes\n",
      "\n",
      "Yes Yes Yes Indirectly Yes\n",
      "\n",
      "Yes Yes Yes No No\n",
      "\n",
      "Section 14.3.2 14.3.2 14.3.2 14.5 15\n",
      "\n",
      "Table 14.1 Comparison of various kernel based classiﬁers. EB = empirical Bayes, CV = cross validation. See text for details.\n",
      "\n",
      "The sparsity and large margin principles are necessary to prevent overﬁtting, i.e., to ensure that we do not use all the basis functions. These two ideas are closely related to each other, and both arise (in this case) from the use of the hinge loss function. However, there are other methods of achieving sparsity (such as (cid:6)1), and also other methods of maximizing the margin (such as boosting). A deeper discussion of this point takes us outside of the scope of this book. See e.g., (Hastie et al. 2009) for more information.\n",
      "\n",
      "14.5.5\n",
      "\n",
      "A probabilistic interpretation of SVMs\n",
      "\n",
      "In Section 14.3, we saw how to use kernels inside GLMs to derive probabilistic classiﬁers, such as the L1VM and RVM. And in Section 15.3, we will discuss Gaussian process classiﬁers, which also use kernels. However, all of these approaches use a logistic or probit likelihood, as opposed to the hinge loss used by SVMs. It is natural to wonder if one can interpret the SVM more directly as a probabilistic model. To do so, we must interpret Cg(m) as a negative log likelihood, where g(m) = (1 − m)+, where m = yf (x) is the margin. Hence p(y = 1|f ) = exp(−Cg(f )) and p(y = −1|f ) = exp(−Cg(−f )). By summing over both values of y, we require that exp(−Cg(f )) + exp(−Cg(−f )) be a constant independent of f . But it turns out this is not possible for any C > 0 (Sollich 2002).\n",
      "\n",
      "if we are willing to relax the sum-to-one condition, and work with a pseudo- likelihood, we can derive a probabilistic interpretation of the hinge loss (Polson and Scott 2011). In particular, one can show that\n",
      "\n",
      "However,\n",
      "\n",
      "exp(−2(1 − yixT\n",
      "\n",
      "i w)+) =\n",
      "\n",
      "(cid:28) ∞\n",
      "\n",
      "0\n",
      "\n",
      "1√\n",
      "\n",
      "2πλi\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(1 + λi − yixT\n",
      "\n",
      "λi\n",
      "\n",
      "i w)2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "dλi\n",
      "\n",
      "(14.68)\n",
      "\n",
      "Thus the exponential of the negative hinge loss can be represented as a Gaussian scale mixture. This allows one to ﬁt an SVM using EM or Gibbs sampling, where λi are the latent variables. This in turn opens the door to Bayesian methods for setting the hyper-parameters for the prior on w. See (Polson and Scott 2011) for details. (See also (Franc et al. 2011) for a different probabilistic interpretation of SVMs.)\n",
      "\n",
      "14.6\n",
      "\n",
      "Comparison of discriminative kernel methods\n",
      "\n",
      "We have mentioned several different methods for classiﬁcation and regression based on kernels, (GP stands for “Gaussian process”, which we discuss in which we summarize in Table 14.1. Chapter 15.) The columns have the following meaning:\n",
      "\n",
      "506\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "Optimize w: a key question is whether the objective J(w) = − log p(D|w) − log p(w) is convex or not. L2VM, L1VM and SVMs have convex objectives. RVMs do not. GPs are Bayesian methods that do not perform parameter estimation.\n",
      "\n",
      "Optimize kernel: all the methods require that one “tune” the kernel parameters, such as the bandwidth of the RBF kernel, as well as the level of regularization. For methods based on Gaussians, including L2VM, RVMs and GPs, we can use efficient gradient based optimizers to maximize the marginal likelihood. For SVMs, and L1VM, we must use cross validation, which is slower (see Section 14.5.3).\n",
      "\n",
      "Sparse: L1VM, RVMs and SVMs are sparse kernel methods, in that they only use a subset of the training examples. GPs and L2VM are not sparse: they use all the training examples. The principle advantage of sparsity is that prediction at test time is usually faster. In addition, one can sometimes get improved accuracy.\n",
      "\n",
      "Probabilistic: All the methods except for SVMs produce probabilistic output of the form p(y|x). SVMs produce a “conﬁdence” value that can be converted to a probability, but such probabilities are usually very poorly calibrated (see Section 14.5.2.3).\n",
      "\n",
      "Multiclass: All the methods except for SVMs naturally work in the multiclass setting, by using a multinoulli output instead of Bernoulli. The SVM can be made into a multiclass classiﬁer, but there are various difficulties with this approach, as discussed in Section 14.5.2.4.\n",
      "\n",
      "Mercer kernel: SVMs and GPs require that the kernel is positive deﬁnite; the other techniques\n",
      "\n",
      "do not.\n",
      "\n",
      "Apart from these differences, there is the natural question: which method works best? In a small experiment3, we found that all of these methods had similar accuracy when averaged over a range of problems, provided they have the same kernel, and provided the regularization constants are chosen appropriately.\n",
      "\n",
      "Given that the statistical performance is roughly the same, what about the computational performance? GPs and L2VM are generally the slowest, taking O(N 3) time, since they don’t exploit sparsity (although various speedups are possible, see Section 15.6). SVMs also take O(N 3) time to train (unless we use a linear kernel, in which case we only need O(N ) time (Joachims 2006)). However, the need to use cross validation can make SVMs slower than RVMs. L1VM should be faster than an RVM, since an RVM requires multiple rounds of (cid:6)1 minimization (see Section 13.7.4.3). However, in practice it is common to use a greedy method to train RVMs, which is faster than (cid:6)1 minimization. This is reﬂected in our empirical results.\n",
      "\n",
      "if speed matters, use an RVM, but if well-calibrated probabilistic output matters (e.g., for active learning or control problems), use a GP. The only circumstances under which using an SVM seems sensible is the structured output case, where likelihood-based methods can be slow. (We attribute the enormous popularity of SVMs not to their superiority, but to ignorance of the alternatives, and also to the lack of high quality software implementing the alternatives.)\n",
      "\n",
      "The conclusion of all this is as follows:\n",
      "\n",
      "Section 16.7.1 gives a more extensive experimental comparison of supervised learning methods,\n",
      "\n",
      "including SVMs and various non kernel methods.\n",
      "\n",
      "3. See http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutKernelClassif.html.\n",
      "\n",
      "14.7. Kernels for building generative models\n",
      "\n",
      "507\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "Boxcar Epanechnikov Tricube Gaussian\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "Figure 14.16 A comparison of some popular smoothing kernels. The boxcar kernel has compact support but is not smooth. The Epanechnikov kernel has compact support but is not differentiable at its boundary. The tri-cube has compact support and two continuous derivatives at the boundary of its support. The Gaussian is differentiable, but does not have compact support. Based on Figure 6.2 of (Hastie et al. 2009). Figure generated by smoothingKernelPlot.\n",
      "\n",
      "14.7\n",
      "\n",
      "Kernels for building generative models\n",
      "\n",
      "There is a different kind of kernel known as a smoothing kernel which can be used to create non-parametric density estimates. This can be used for unsupervised density estimation, p(x), as well as for creating generative models for classiﬁcation and regression by making models of the form p(y, x).\n",
      "\n",
      "14.7.1\n",
      "\n",
      "Smoothing kernels\n",
      "\n",
      "A smoothing kernel is a function of one argument which satisﬁes the following properties:\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "κ(x)dx = 1,\n",
      "\n",
      "xκ(x)dx = 0,\n",
      "\n",
      "x2κ(x)dx > 0\n",
      "\n",
      "(14.69)\n",
      "\n",
      "A simple example is the Gaussian kernel,\n",
      "\n",
      "κ(x) (cid:2) 1 (2π)\n",
      "\n",
      "1 2\n",
      "\n",
      "e−x2/2\n",
      "\n",
      "(14.70)\n",
      "\n",
      "We can control the width of the kernel by introducing a bandwidth parameter h:\n",
      "\n",
      "κh(x) (cid:2) 1 h\n",
      "\n",
      "κ(\n",
      "\n",
      "x h\n",
      "\n",
      ")\n",
      "\n",
      "(14.71)\n",
      "\n",
      "We can generalize to vector valued inputs by deﬁning an RBF kernel:\n",
      "\n",
      "κh(x) = κh(||x||)\n",
      "\n",
      "(14.72)\n",
      "\n",
      "In the case of the Gaussian kernel, this becomes\n",
      "\n",
      "κh(x) =\n",
      "\n",
      "1 hD(2π)D/2\n",
      "\n",
      "D(cid:27)\n",
      "\n",
      "j=1\n",
      "\n",
      "exp(− 1 2h2\n",
      "\n",
      "x2 j )\n",
      "\n",
      "(14.73)\n",
      "\n",
      "508\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "Although Gaussian kernels are popular, they have unbounded support. An alternative kernel,\n",
      "\n",
      "with compact support, is the Epanechnikov kernel, deﬁned by\n",
      "\n",
      "κ(x) (cid:2) 3 4\n",
      "\n",
      "(1 − x2)I(|x| ≤1)\n",
      "\n",
      "(14.74)\n",
      "\n",
      "This is plotted in Figure 14.16. Compact support can be useful for efficiency reasons, since one can use fast nearest neighbor methods to evaluate the density.\n",
      "\n",
      "Unfortunately, the Epanechnikov kernel is not differentiable at the boundary of its support.\n",
      "\n",
      "An alterative is the tri-cube kernel, deﬁned as follows:\n",
      "\n",
      "κ(x) (cid:2) 70 81\n",
      "\n",
      "(1 − |x|3)3I(|x| ≤ 1)\n",
      "\n",
      "(14.75)\n",
      "\n",
      "This has compact support and has two continuous derivatives at the boundary of its support. See Figure 14.16.\n",
      "\n",
      "The boxcar kernel is simply the uniform distribution:\n",
      "\n",
      "κ(x) (cid:2) I(|x| ≤1)\n",
      "\n",
      "(14.76)\n",
      "\n",
      "We will use this kernel below.\n",
      "\n",
      "14.7.2\n",
      "\n",
      "Kernel density estimation (KDE)\n",
      "\n",
      "Recall the Gaussian mixture model from Section 11.2.1. This is a parametric density estimator for data in RD. However, it requires specifying the number K and locations μk of the clusters. An alternative to estimating the μk is to allocate one cluster center per data point, so μi = xi. In this case, the model becomes\n",
      "\n",
      "p(x|D) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "N (x|xi, σ2I)\n",
      "\n",
      "(14.77)\n",
      "\n",
      "We can generalize the approach by writing\n",
      "\n",
      "ˆp(x) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "κh (x − xi)\n",
      "\n",
      "(14.78)\n",
      "\n",
      "This is called a Parzen window density estimator, or kernel density estimator (KDE), and is a simple non-parametric density model. The advantage over a parametric model is that no model ﬁtting is required (except for tuning the bandwidth, usually done by cross-validation). and there is no need to pick K. The disadvantage is that the model takes a lot of memory to store, and a lot of time to evaluate. It is also of no use for clustering tasks.\n",
      "\n",
      "Figure 14.17 illustrates KDE in 1d for two kinds of kernel. On the top, we use a boxcar kernel, κ(x) =I (−1 ≤ z ≤ 1). The result is equivalent to a histogram estimate of the density, since we just count how many data points land within an interval of size h around xi. On the bottom, we use a Gaussian kernel, which results in a smoother ﬁt.\n",
      "\n",
      "The usual way to pick h is to minimize an estimate (such as cross validation) of the frequentist risk (see e.g., (Bowman and Azzalini 1997)). In Section 25.2, we discuss a Bayesian approach to non-parametric density estimation, based on Dirichlet process mixture models, which allows us\n",
      "\n",
      "14.7. Kernels for building generative models\n",
      "\n",
      "509\n",
      "\n",
      "unif, h=1.000\n",
      "\n",
      "unif, h=2.000\n",
      "\n",
      "0.35\n",
      "\n",
      "0.14\n",
      "\n",
      "0.3\n",
      "\n",
      "0.12\n",
      "\n",
      "0.25\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.08\n",
      "\n",
      "0.15\n",
      "\n",
      "0.06\n",
      "\n",
      "0.1\n",
      "\n",
      "0.04\n",
      "\n",
      "0.05\n",
      "\n",
      "0.02\n",
      "\n",
      "0 −5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "0 −5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "gauss, h=1.000\n",
      "\n",
      "gauss, h=2.000\n",
      "\n",
      "0.16\n",
      "\n",
      "0.06\n",
      "\n",
      "0.14\n",
      "\n",
      "0.05\n",
      "\n",
      "0.12\n",
      "\n",
      "0.04\n",
      "\n",
      "0.1\n",
      "\n",
      "0.08\n",
      "\n",
      "0.03\n",
      "\n",
      "0.06\n",
      "\n",
      "0.02\n",
      "\n",
      "0.04\n",
      "\n",
      "0.01\n",
      "\n",
      "0.02\n",
      "\n",
      "0 −5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "0 −5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 14.17 A nonparametric (Parzen) density estimator in 1D estimated from 6 data points, denoted by x. Top row: uniform kernel. Bottom row: Gaussian kernel. Rows represent increasingly large band- width parameters. Based on http://en.wikipedia.org/wiki/Kernel_density_estimation. Figure generated by parzenWindowDemo2.\n",
      "\n",
      "to infer h. DP mixtures can also be more efficient than KDE, since they do not need to store all the data. See also Section 15.2.4 where we discuss an empirical Bayes approach to estimating kernel parameters in a Gaussian process model for classiﬁcation/ regression.\n",
      "\n",
      "14.7.3\n",
      "\n",
      "From KDE to KNN\n",
      "\n",
      "We can use KDE to deﬁne the class conditional densities in a generative classiﬁer. This turns out to provide an alternative derivation of the nearest neighbors classiﬁer, which we introduced in Section 1.4.2. To show this, we follow the presentation of (Bishop 2006a, p125). In kde with a boxcar kernel, we ﬁxed the bandwidth and count how many data points fall within the hyper-cube centered on a datapoint. Suppose that, instead of ﬁxing the bandwidth h, we instead\n",
      "\n",
      "510\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "1.2\n",
      "\n",
      "Gaussian kernel regression\n",
      "\n",
      "1\n",
      "\n",
      "true data estimate\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "Figure 14.18 An example of kernel regression in 1d using a Gaussian kernel. kernelRegressionDemo, based on code by Yi Cao.\n",
      "\n",
      "Figure generated by\n",
      "\n",
      "allow the bandwidth or volume to be different for each data point. Speciﬁcally, we will “grow” a volume around x until we encounter K data points, regardless of their class label. Let the resulting volume have size V (x) (this was previously hD), and let there be Nc(x) examples from class c in this volume. Then we can estimate the class conditional density as follows:\n",
      "\n",
      "p(x|y = c, D) =\n",
      "\n",
      "Nc(x) NcV (x)\n",
      "\n",
      "(14.79)\n",
      "\n",
      "where Nc is the total number of examples in class c in the whole data set. The class prior can be estimated by\n",
      "\n",
      "p(y = c|D) =\n",
      "\n",
      "Nc N\n",
      "\n",
      "(14.80)\n",
      "\n",
      "Hence the class posterior is given by\n",
      "\n",
      "where we used the fact that class) around every point. This is equivalent to Equation 1.2, since Nc(x) = c).\n",
      "\n",
      "p(y = c|x, D) =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "Nc N Nc(cid:2) (x) c(cid:2) Nc(cid:2) V (x) (cid:10)\n",
      "\n",
      "Nc(x) NcV (x)\n",
      "\n",
      "c Nc(x) = K, since we choose a total of K points (regardless of i∈NK (x,D) I(yi =\n",
      "\n",
      "Nc(cid:2) N\n",
      "\n",
      "=\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "Nc(x) c(cid:2) Nc(cid:2) (x)\n",
      "\n",
      "=\n",
      "\n",
      "Nc(x) K\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(14.81)\n",
      "\n",
      "14.7.4\n",
      "\n",
      "Kernel regression\n",
      "\n",
      "In Section 14.7.2, we discussed the use of kernel density estimation or KDE for unsupervised learning. We can also use KDE for regression. The goal is to compute the conditional expectation\n",
      "\n",
      "f (x) = E [y|x] =\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "y p(y|x)dy =\n",
      "\n",
      ") y p(x, y)dy ) p(x, y)dy\n",
      "\n",
      "(14.82)\n",
      "\n",
      "14.7. Kernels for building generative models\n",
      "\n",
      "511\n",
      "\n",
      "We can use KDE to approximate the joint density p(x, y) as follows:\n",
      "\n",
      "p(x, y) ≈ 1 N\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "κh(x − xi)κh(y − yi)\n",
      "\n",
      "(14.83)\n",
      "\n",
      "Hence\n",
      "\n",
      "f (x) =\n",
      "\n",
      "=\n",
      "\n",
      "1 i=1 κh(x − xi) N 1 i=1 κh(x − xi) N (cid:10)N i=1 κh(x − xi)yi (cid:10)N i=1 κh(x − xi)\n",
      "\n",
      "(cid:10)N (cid:10)N\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      "yκh(y − yi)dy κh(y − yi)dy\n",
      "\n",
      "(14.84)\n",
      "\n",
      "(14.85)\n",
      "\n",
      "To derive this result, we used two properties of smoothing kernels. First, that they integrate to yκh(y − yi)dy = yi. This follows by one, i.e., deﬁning x = y − yi and using the zero mean property of smoothing kernels:\n",
      "\n",
      "(cid:28)\n",
      "\n",
      ")\n",
      "\n",
      "κh(y − yi)dy = 1. And second, the fact that\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(cid:28)\n",
      "\n",
      ")\n",
      "\n",
      "(x + yi)κh(x)dx =\n",
      "\n",
      "xκh(x)dx + yi\n",
      "\n",
      "κh(x)dx = 0 + yi = yi\n",
      "\n",
      "(14.86)\n",
      "\n",
      "We can rewrite the above result as follows:\n",
      "\n",
      "f (x) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "wi(x)yi\n",
      "\n",
      "(14.87)\n",
      "\n",
      "i=1\n",
      "\n",
      "wi(x) (cid:2)\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "κh(x − xi) i(cid:2)=1 κh(x − xi(cid:2) )\n",
      "\n",
      "(14.88)\n",
      "\n",
      "We see that the prediction is just a weighted sum of the outputs at the training points, where the weights depend on how similar x is to the stored training points. This method is called kernel regression, kernel smoothing, or the Nadaraya-Watson model. See Figure 14.18 for an example, where we use a Gaussian kernel.\n",
      "\n",
      "Note that this method only has one free parameter, namely h. One can show (Bowman and Azzalini 1997) that for 1d data, if the true density is Gaussian and we are using Gaussian kernels, the optimal bandwidth h is given by\n",
      "\n",
      "h =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "4 3N\n",
      "\n",
      "(cid:9)1/5\n",
      "\n",
      "ˆσ\n",
      "\n",
      "(14.89)\n",
      "\n",
      "We can compute a robust approximation to the standard deviation by ﬁrst computing the mean absolute deviation\n",
      "\n",
      "MAD = median(|x − median(x)|)\n",
      "\n",
      "(14.90)\n",
      "\n",
      "and then using\n",
      "\n",
      "ˆσ = 1.4826 MAD =\n",
      "\n",
      "1 0.6745\n",
      "\n",
      "MAD\n",
      "\n",
      "The code used to produce Figure 14.18 estimated hx and hy separately, and then set h =\n",
      "\n",
      "(14.91)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "hxhy.\n",
      "\n",
      "512\n",
      "\n",
      "Chapter 14. Kernels\n",
      "\n",
      "Although these heuristics seem to work well, their derivation rests on some rather dubious assumptions (such as Gaussianity of the true density). Furthermore, these heuristics are limited to tuning just a single parameter. In Section 15.2.4 we discuss an empirical Bayes approach to estimating multiple kernel parameters in a Gaussian process model for classiﬁcation/ regression, which can handle many tuning parameters, and which is based on much more transparent principles (maximizing the marginal likelihood).\n",
      "\n",
      "14.7.5\n",
      "\n",
      "Locally weighted regression\n",
      "\n",
      "If we deﬁne κh(x − xi) = κ(x, xi), we can rewrite the prediction made by kernel regression as follows\n",
      "\n",
      "κ(x∗, xi) i(cid:2)=1 κ(x∗, xi(cid:2) ) Note that κ(x, xi) need not be a smoothing kernel. normalization term, so we can just write\n",
      "\n",
      "ˆf (x∗) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "yi\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "(14.92)\n",
      "\n",
      "If it is not, we no longer need the\n",
      "\n",
      "ˆf (x∗) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "yiκ(x∗, xi)\n",
      "\n",
      "(14.93)\n",
      "\n",
      "i=1\n",
      "\n",
      "This model is essentially ﬁtting a constant function locally. We can improve on this by ﬁtting a linear regression model for each point x∗ by solving\n",
      "\n",
      "min β(x∗)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "κ(x∗, xi)[yi − β(x∗)T φ(xi)]2\n",
      "\n",
      "(14.94)\n",
      "\n",
      "where φ(x) = [1, x]. This is called locally weighted regression. An example of such a method is LOESS, akaLOWESS, which stands for “locally-weighted scatterplot smoothing” (Cleveland and Devlin 1988). See also (Edakunni et al. 2010) for a Bayesian version of this model.\n",
      "\n",
      "We can compute the paramters β(x∗) for each test case by solving the following weighted\n",
      "\n",
      "least squares problem:\n",
      "\n",
      "(14.95) where Φ is an N × (D + 1) design matrix and D = diag(κ(x∗, xi)). The corresponding prediction has the form\n",
      "\n",
      "β(x∗) = (ΦT D(x∗)Φ)−1ΦT D(x∗)y\n",
      "\n",
      "ˆf (x∗) = φ(x∗)T β(x∗) = (ΦT D(x∗)Φ)−1ΦT D(x∗)y =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "wi(x∗)yi\n",
      "\n",
      "(14.96)\n",
      "\n",
      "i=1\n",
      "\n",
      "The term wi(x∗), which combines the local smoothing kernel with the effect of linear regression, is called the equivalent kernel. See also Section 15.4.2.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 14.1 Fitting an SVM classiﬁer by hand (Source: Jaakkola.) Consider a dataset with 2 points in 1d: (x1 = 0, y1 = −1) and (x2 = Consider mapping each point to 3d using the feature vector φ(x) = [1,\n",
      "\n",
      "√\n",
      "\n",
      "2, y2 = 1). 2x, x2]T . (This is equivalent to\n",
      "\n",
      "√\n",
      "\n",
      "14.7. Kernels for building generative models\n",
      "\n",
      "513\n",
      "\n",
      "using a second order polynomial kernel.) The max margin classiﬁer has the form\n",
      "\n",
      "min ||w||2\n",
      "\n",
      "s.t.\n",
      "\n",
      "(14.97)\n",
      "\n",
      "y1(wT φ(x1) +w 0) ≥ 1 y2(wT φ(x2) +w 0) ≥ 1\n",
      "\n",
      "(14.98)\n",
      "\n",
      "(14.99)\n",
      "\n",
      "a. Write down a vector that is parallel to the optimal vector w. Hint: recall from Figure 7.8 (12Apr10 version) that w is perpendicular to the decision boundary between the two points in the 3d feature space.\n",
      "\n",
      "b. What is the value of the margin that is achieved by this w? Hint: recall that the margin is the distance from each support vector to the decision boundary. Hint 2: think about the geometry of 2 points in space, with a line separating one from the other.\n",
      "\n",
      "c. Solve for w, using the fact the margin is equal to 1/||w||. d. Solve for w0 using your value for w and Equations 14.97 to 14.99. Hint:\n",
      "\n",
      "the points will be on the\n",
      "\n",
      "decision boundary, so the inequalities will be tight.\n",
      "\n",
      "e. Write down the form of the discriminant function f (x) = w0 + wT φ(x) as an explicit function of x.\n",
      "\n",
      "Exercise 14.2 Linear separability (Source: Koller..) Consider ﬁtting an SVM with C > 0 to a dataset that is linearly separable. Is the resulting decision boundary guaranteed to separate the classes?\n",
      "\n",
      "15 Gaussian processes\n",
      "\n",
      "15.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In supervised learning, we observe some inputs xi and some outputs yi. We assume that yi = f (xi), for some unknown function f , possibly corrupted by noise. The optimal approach is to infer a distribution over functions given the data, p(f |X, y), and then to use this to make predictions given new inputs, i.e., to compute (cid:28)\n",
      "\n",
      "p(y∗|x∗, X, y) =\n",
      "\n",
      "p(y∗|f, x∗)p(f |X, y)df\n",
      "\n",
      "(15.1)\n",
      "\n",
      "instead of inferring p(f |D), we infer p(θ|D). Bayesian inference over functions themselves.\n",
      "\n",
      "Up until now, we have focussed on parametric representations for the function f , so that In this chapter, we discuss a way to perform\n",
      "\n",
      "Our approach will be based on Gaussian processes or GPs. A GP deﬁnes a prior over functions, which can be converted into a posterior over functions once we have seen some data. Although it might seem difficult to represent a distribution over a function, it turns out that we only need to be able to deﬁne a distribution over the function’s values at a ﬁnite, but arbitrary, set of points, say x1, . . . , xN . A GP assumes that p(f (x1), . . . , f (xN )) is jointly Gaussian, with some mean μ(x) and covariance Σ(x) given by Σij = κ(xi, xj), where κ is a positive deﬁnite kernel function (see Section 14.2 information on kernels). The key idea is that if xi and xj are deemed by the kernel to be similar, then we expect the output of the function at those points to be similar, too. See Figure 15.1 for an illustration.\n",
      "\n",
      "It turns out that, in the regression setting, all these computations can be done in closed form, in O(N 3) time. (We discuss faster approximations in Section 15.6.) In the classiﬁcation setting, we must use approximations, such as the Gaussian approximation, since the posterior is no longer exactly Gaussian.\n",
      "\n",
      "GPs can be thought of as a Bayesian alternative to the kernel methods we discussed in Chap- ter 14, including L1VM, RVM and SVM. Although those methods are sparser and therefore faster, they do not give well-calibrated probabilistic outputs (see Section 15.4.4 for further discussion). Having properly tuned probabilistic output is important in certain applications, such as online tracking for vision and robotics (Ko and Fox 2009), reinforcement learning and optimal control (Engel et al. 2005; Deisenroth et al. 2009), global optimization of non-convex functions (Mockus et al. 1996; Lizotte 2008; Brochu et al. 2009), experiment design (Santner et al. 2003), etc.\n",
      "\n",
      "516\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "y1\n",
      "\n",
      "y2\n",
      "\n",
      "y(cid:2)\n",
      "\n",
      "f 1\n",
      "\n",
      "f 2\n",
      "\n",
      "f (cid:2)\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x(cid:2)\n",
      "\n",
      "Figure 15.1 A Gaussian process for 2 training points and 1 testing point, represented as a mixed directed and undirected graphical model representing p(y, f |x) =N (f |0, K(x)) i p(yi|fi). The hidden nodes fi = f (xi) represent the value of the function at each of the data points. These hidden nodes are fully interconnected by undirected edges, forming a Gaussian graphical model; the edge strengths represent the covariance terms Σij = κ(xi, xj). If the test point x∗ is similar to the training points x1 and x2, then the predicted output y∗ will be similar to y1 and y2.\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "Our presentation is closely based on (Rasmussen and Williams 2006), which should be con- sulted for futher details. See also (Diggle and Ribeiro 2007), which discusses the related approach known as kriging, which is widely used in the spatial statistics literature.\n",
      "\n",
      "15.2\n",
      "\n",
      "GPs for regression\n",
      "\n",
      "In this section, we discuss GPs for regression. Let the prior on the regression function be a GP, denoted by\n",
      "\n",
      "f (x) ∼ GP (m(x), κ(x, x(cid:4)))\n",
      "\n",
      "(15.2)\n",
      "\n",
      "where m(x) is the mean function and κ(x, x(cid:4)) is the kernel or covariance function, i.e.,\n",
      "\n",
      "κ(x, x(cid:4)) =E\n",
      "\n",
      "m(x) =E [f (x)]\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "(f (x) − m(x))(f (x(cid:4)) − m(x(cid:4)))T\n",
      "\n",
      "(15.3)\n",
      "\n",
      "(15.4)\n",
      "\n",
      "We obviously require that κ() be a positive deﬁnite kernel. For any ﬁnite set of points, this process deﬁnes a joint Gaussian:\n",
      "\n",
      "p(f |X) = N (f |μ, K)\n",
      "\n",
      "(15.5)\n",
      "\n",
      "where Kij = κ(xi, xj) and μ = (m(x1), . . . , m(xN )).\n",
      "\n",
      "Note that it is common to use a mean function of m(x) = 0, since the GP is ﬂexible enough to model the mean arbitrarily well, as we will see below. However, in Section 15.2.6 we will consider parametric models for the mean function, so the GP just has to model the residual errors. This semi-parametric approach combines the interpretability of parametric models with the accuracy of non-parametric models.\n",
      "\n",
      "15.2. GPs for regression\n",
      "\n",
      "517\n",
      "\n",
      "2\n",
      "\n",
      "2.5\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "−2\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 15.2 Left: some functions sampled from a GP prior with SE kernel. Right: some samples from a GP posterior, after conditioning on 5 noise-free observations. The shaded area represents E [f (x)]±2std(f (x). Based on Figure 2.2 of (Rasmussen and Williams 2006). Figure generated by gprDemoNoiseFree.\n",
      "\n",
      "15.2.1\n",
      "\n",
      "Predictions using noise-free observations\n",
      "\n",
      "Suppose we observe a training set D = {(xi, fi), i = 1 : N }, where fi = f (xi) is the noise-free observation of the function evaluated at xi. Given a test set X∗ of size N∗ × D, we want to predict the function outputs f∗.\n",
      "\n",
      "If we ask the GP to predict f (x) for a value of x that it has already seen, we want the GP to return the answer f (x) with no uncertainty. In other words, it should act as an interpolator of the training data. This will only happen if we assume the observations are noiseless. We will consider the case of noisy observations below.\n",
      "\n",
      "Now we return to the prediction problem. By deﬁnition of the GP, the joint distribution has\n",
      "\n",
      "the following form (cid:8)(cid:8) (cid:9)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "f f∗\n",
      "\n",
      "∼ N\n",
      "\n",
      "μ μ∗\n",
      "\n",
      "(cid:9)\n",
      "\n",
      ",\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "K K∗ KT ∗ K∗∗\n",
      "\n",
      "(cid:9)(cid:9)\n",
      "\n",
      "(15.6)\n",
      "\n",
      "where K = κ(X, X) is N ×N , K∗ = κ(X, X∗) is N ×N∗, and K∗∗ = κ(X∗, X∗) is N∗ ×N∗. By the standard rules for conditioning Gaussians (Section 4.3), the posterior has the following form\n",
      "\n",
      "p(f∗|X∗, X, f ) =N (f∗|μ∗, Σ∗) μ∗ = μ(X∗) +K T Σ∗ = K∗∗ − KT\n",
      "\n",
      "∗ K−1K∗\n",
      "\n",
      "∗ K−1(f − μ(X))\n",
      "\n",
      "(15.7)\n",
      "\n",
      "(15.8)\n",
      "\n",
      "(15.9)\n",
      "\n",
      "p(f |X), where we use a squared exponential kernel, aka Gaussian kernel or RBF kernel. 1d, this is given by\n",
      "\n",
      "This process is illustrated in Figure 15.2. On the left we show sample samples from the prior, In\n",
      "\n",
      "κ(x, x(cid:4)) = σ2\n",
      "\n",
      "f exp(− 1\n",
      "\n",
      "2(cid:6)2 (x − x(cid:4))2)\n",
      "\n",
      "(15.10)\n",
      "\n",
      "Here (cid:6) controls the horizontal length scale over which the function varies, and σ2 f controls the vertical variation. (We discuss how to estimate such kernel parameters below.) On the right we\n",
      "\n",
      "518\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "show samples from the posterior, p(f∗|X∗, X, f ). We see that the model perfectly interpolates the training data, and that the predictive uncertainty increases as we move further away from the observed data.\n",
      "\n",
      "One application of noise-free GP regression is as a computationally cheap proxy for the behavior of a complex simulator, such as a weather forecasting program. (If the simulator is stochastic, we can deﬁne f to be its mean output; note that there is still no observation noise.) One can then estimate the effect of changing simulator parameters by examining their effect on the GP’s predictions, rather than having to run the simulator many times, which may be prohibitively slow. This strategy is known as DACE, which stands for design and analysis of computer experiments (Santner et al. 2003).\n",
      "\n",
      "15.2.2\n",
      "\n",
      "Predictions using noisy observations\n",
      "\n",
      "Now let us consider the case where what we observe is a noisy version of the underlying function, y = f (x) +(cid:8) , where (cid:8) ∼ N (0, σ2 y). In this case, the model is not required to interpolate the data, but it must come “close” to the observed data. The covariance of the observed noisy responses is\n",
      "\n",
      "cov [yp, yq] = κ(xp, xq) +σ 2 where δpq = I(p = q). In other words\n",
      "\n",
      "yδpq\n",
      "\n",
      "(15.11)\n",
      "\n",
      "cov [y|X] = K + σ2\n",
      "\n",
      "yIN (cid:2) Ky\n",
      "\n",
      "(15.12)\n",
      "\n",
      "The second matrix is diagonal because we assumed the noise terms were independently added to each observation.\n",
      "\n",
      "The joint density of the observed data and the latent, noise-free function on the test points\n",
      "\n",
      "is given by (cid:8)\n",
      "\n",
      "y f∗\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "∼ N\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "0,\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "Ky K∗ KT ∗ K∗∗\n",
      "\n",
      "(cid:9)(cid:9)\n",
      "\n",
      "(15.13)\n",
      "\n",
      "where we are assuming the mean is zero, for notational simplicity. Hence the posterior predictive density is\n",
      "\n",
      "p(f∗|X∗, X, y) =N (f∗|μ∗, Σ∗) μ∗ = KT ∗ K−1 y y Σ∗ = K∗∗ − KT\n",
      "\n",
      "∗ K−1\n",
      "\n",
      "y K∗\n",
      "\n",
      "(15.14)\n",
      "\n",
      "(15.15)\n",
      "\n",
      "(15.16)\n",
      "\n",
      "In the case of a single test input, this simpliﬁes as follows ∗ K−1\n",
      "\n",
      "(15.17) where k∗ = [κ(x∗, x1), . . . , κ(x∗, xN )] and k∗∗ = κ(x∗, x∗). Another way to write the posterior mean is as follows:\n",
      "\n",
      "p(f∗|x∗, X, y) =N (f∗|kT\n",
      "\n",
      "∗ K−1\n",
      "\n",
      "y y, k∗∗ − kT\n",
      "\n",
      "y k∗)\n",
      "\n",
      "f ∗ = kT\n",
      "\n",
      "∗ K−1\n",
      "\n",
      "y y =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αiκ(xi, x∗)\n",
      "\n",
      "(15.18)\n",
      "\n",
      "i=1\n",
      "\n",
      "where α = K−1\n",
      "\n",
      "y y. We will revisit this expression later.\n",
      "\n",
      "15.2. GPs for regression\n",
      "\n",
      "519\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "−3\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 15.3 Some 1d GPs with SE kernels but different hyper-parameters ﬁt to 20 noisy observations. The kernel has the form in Equation 15.19. The hyper-parameters ((cid:7), σf , σy) are as follows: (a) (1,1,0.1) (b) (0.3, 0.1.08, 0.00005), (c) (3.0, 1.16, 0.89). Based on Figure 2.5 of (Rasmussen and Williams 2006). Figure generated by gprDemoChangeHparams, written by Carl Rasmussen.\n",
      "\n",
      "15.2.3\n",
      "\n",
      "Effect of the kernel parameters\n",
      "\n",
      "The predictive performance of GPs depends exclusively on the suitability of the chosen kernel. Suppose we choose the following squared-exponential (SE) kernel for the noisy observations\n",
      "\n",
      "κy(xp, xq) = σ2\n",
      "\n",
      "f exp(− 1\n",
      "\n",
      "2(cid:6)2 (xp − xq)2) +σ 2\n",
      "\n",
      "yδpq\n",
      "\n",
      "(15.19)\n",
      "\n",
      "Here (cid:6) is the horizontal scale over which the function changes, σ2 f controls the vertical scale of the function, and σ2 y is the noise variance. Figure 15.3 illustrates the effects of changing these parameters. We sampled 20 noisy data points from the SE kernel using ((cid:6), σf , σy) = (1, 1, 0.1), and then made predictions various parameters, conditional on the data. In Figure 15.3(a), we use ((cid:6), σf , σy) = (1, 1, 0.1), and the result is a good ﬁt. In Figure 15.3(b), we reduce the length scale to (cid:6) = 0.3 (the other parameters were optimized by maximum (marginal) likelihood, a technique we discuss below); now the function looks more “wiggly”. Also, the uncertainty goes up faster, since the effective distance from the training points increases more rapidly. In Figure 15.3(c), we increase the length scale to (cid:6) = 3; now the function looks smoother.\n",
      "\n",
      "520\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "y\n",
      "\n",
      "t u p t u o\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "y\n",
      "\n",
      "t u p t u o\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "input x2\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "input x1\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "input x2\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "input x1\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "y\n",
      "\n",
      "t\n",
      "\n",
      "u p u o\n",
      "\n",
      "t\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "input x2\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "input x1\n",
      "\n",
      "2\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 15.4 Some 2d functions sampled from a GP with an SE kernel but different hyper-parameters. The kernel has the form in Equation 15.20 where (a) M = I, (b) M = diag(1, 3)−2, (c) M = (1, −1; −1, 1) + diag(6, 6)−2. Based on Figure 5.1 of (Rasmussen and Williams 2006). Figure generated by gprDemoArd, written by Carl Rasmussen.\n",
      "\n",
      "We can extend the SE kernel to multiple dimensions as follows:\n",
      "\n",
      "κy(xp, xq) = σ2\n",
      "\n",
      "f exp(− 1 2\n",
      "\n",
      "(xp − xq)T M(xp − xq)) + σ2\n",
      "\n",
      "yδpq\n",
      "\n",
      "(15.20)\n",
      "\n",
      "We can deﬁne the matrix M in several ways. The simplest is to use an isotropic matrix, M1 = (cid:6)−2I. See Figure 15.4(a) for an example. We can also endow each dimension with its own characteristic length scale, M2 = diag((cid:9))−2. If any of these length scales become large, the corresponding feature dimension is deemed “irrelevant”, just as in ARD (Section 13.7). In Figure 15.4(b), we use M = M2 with (cid:9) = (1, 3), so the function changes faster along the x1 direction than the x2 direction.\n",
      "\n",
      "We can also create a matrix of the form M3 = ΛΛT +diag((cid:9))−2, where Λ is a D×K matrix, where K < D. (Rasmussen and Williams 2006, p107) calls this the factor analysis distance function, by analogy to the fact that factor analysis (Section 12.1) approximates a covariance matrix as a low rank matrix plus a diagonal matrix. The columns of Λ correspond to relevant directions in input space. In Figure 15.4(c), we use (cid:9) = (6; 6) and Λ = (1; −1), so the function changes mostly rapidly in the direction which is perpendicular to (1,1).\n",
      "\n",
      "15.2. GPs for regression\n",
      "\n",
      "521\n",
      "\n",
      "15.2.4\n",
      "\n",
      "Estimating the kernel parameters\n",
      "\n",
      "To estimate the kernel parameters, we could use exhaustive search over a discrete grid of values, with validation loss as an objective, but this can be quite slow. (This is the approach used to tune kernels used by SVMs.) Here we consider an empirical Bayes approach, which will allow us to use continuous optimization methods, which are much faster. In particular, we will maximize the marginal likelihood1\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(y|X) =\n",
      "\n",
      "p(y|f , X)p(f |X)df\n",
      "\n",
      "(15.21)\n",
      "\n",
      "Since p(f |X) = N (f |0, K), and p(y|f ) = log p(y|X) = log N (y|0, Ky) = − 1 2\n",
      "\n",
      "yK−1\n",
      "\n",
      "’\n",
      "\n",
      "i N (yi|fi, σ2 y y − 1 2\n",
      "\n",
      "log |Ky| −\n",
      "\n",
      "y), the marginal likelihood is given by\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log(2π)\n",
      "\n",
      "(15.22)\n",
      "\n",
      "The ﬁrst term is a data ﬁt term, the second term is a model complexity term, and the third term is just a constant. To understand the tradeoff between the ﬁrst two terms, consider a SE kernel in 1D, as we vary the length scale (cid:6) and hold σ2 y ﬁxed. Let J((cid:6)) =− log p(y|X, (cid:6)). For short length scales, the ﬁt will be good, so yT K−1 y y will be small. However, the model complexity will be high: K will be almost diagonal (as in Figure 14.3, top right), since most points will not be considered “near” any others, so the log |Ky| will be large. For long length scales, the ﬁt will be poor but the model complexity will be low: K will be almost all 1’s (as in Figure 14.3, bottom right), so log |Ky| will be small.\n",
      "\n",
      "called hyper-parameters) be denoted by θ. One can show that\n",
      "\n",
      "We now discuss how to maximize the marginal likelhiood. Let the kernel parameters (also\n",
      "\n",
      "∂ ∂θj\n",
      "\n",
      "log p(y|X) =\n",
      "\n",
      "=\n",
      "\n",
      "1 2 1 2\n",
      "\n",
      "yT K−1 (cid:8)\n",
      "\n",
      "tr\n",
      "\n",
      "(ααT − K−1 y )\n",
      "\n",
      "y\n",
      "\n",
      "∂Ky ∂θj\n",
      "\n",
      "K−1\n",
      "\n",
      "y y − 1 2 ∂Ky ∂θj\n",
      "\n",
      "tr(K−1 y (cid:9)\n",
      "\n",
      "∂Ky ∂θj\n",
      "\n",
      ")\n",
      "\n",
      "(15.23)\n",
      "\n",
      "(15.24)\n",
      "\n",
      "where α = K−1 y y. parameter to compute the gradient. ∂Ky ∂θj depends on the form of the kernel, and which parameter we are taking y ≥ 0.\n",
      "\n",
      "derivatives with respect to. Often we have constraints on the hyper-parameters, such as σ2 In this case, we can deﬁne θ = log(σ2\n",
      "\n",
      "The form of\n",
      "\n",
      "It takes O(N 3) time to compute K−1\n",
      "\n",
      "y), and then use the chain rule.\n",
      "\n",
      "y , and then O(N 2) time per hyper-\n",
      "\n",
      "Given an expression for the log marginal likelihood and its derivative, we can estimate the kernel parameters using any standard gradient-based optimizer. However, since the objective is not convex, local minima can be a problem, as we illustrate below.\n",
      "\n",
      "15.2.4.1\n",
      "\n",
      "Example Consider Figure 15.5. We use the SE kernel in Equation 15.19 with σ2 (where X and y are the 7 data points shown in panels b and c) as we vary (cid:6) and σ2\n",
      "\n",
      "f = 1, and plot log p(y|X, (cid:6), σ2 y) y. The two\n",
      "\n",
      "1. The reason it is called the marginal likelihood, rather than just likelihood, is because we have marginalized out the latent Gaussian vector f . This moves us up one level of the Bayesian hierarchy, and reduces the chances of overﬁtting (the number of kernel parameters is usually fairly small compared to a standard parametric model).\n",
      "\n",
      "522\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "n o i t a v e d d r a d n a t s e s o n\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "0 10\n",
      "\n",
      "10\n",
      "\n",
      "−1\n",
      "\n",
      "y\n",
      "\n",
      ", t u p t u o\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "characteristic lengthscale\n",
      "\n",
      "0 10\n",
      "\n",
      "1 10\n",
      "\n",
      "−2\n",
      "\n",
      "−5\n",
      "\n",
      "0 input, x\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "2\n",
      "\n",
      "y\n",
      "\n",
      "1\n",
      "\n",
      ", t u p t u o\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−5\n",
      "\n",
      "0 input, x\n",
      "\n",
      "5\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 15.5 likelihood vs σ2 corresponding to the lower left local minimum, ((cid:7), σ2 noise. smooth and has high noise. The data was generated using ((cid:7), σ2 (Rasmussen and Williams 2006). Figure generated by gprDemoMarglik, written by Carl Rasmussen.\n",
      "\n",
      "(c) The function corresponding to the top right local minimum, ((cid:7), σ2\n",
      "\n",
      "Illustration of local minima in the marginal likelihood surface. y and (cid:7), for ﬁxed σ2\n",
      "\n",
      "(a) We plot the log marginal f = 1, using the 7 data points shown in panels b and c. (b) The function n) ≈ (1, 0.2). This is quite “wiggly” and has low n) ≈ (10, 0.8). This is quite Source: Figure 5.5 of\n",
      "\n",
      "n) = (1, 0.1).\n",
      "\n",
      "local optima are indicated by +. The bottom left optimum corresponds to a low-noise, short- length scale solution (shown in panel b). The top right optimum corresponds to a high-noise, long-length scale solution (shown in panel c). With only 7 data points, there is not enough evidence to conﬁdently decide which is more reasonable, although the more complex model (panel b) has a marginal likelihood that is about 60% higher than the simpler model (panel c). With more data, the MAP estimate should come to dominate.\n",
      "\n",
      "Figure 15.5 illustrates some other interesting (and typical) features. The region where σ2 y ≈ 1 (top of panel a) corresponds to the case where the noise is very high; in this regime, the marginal likelihood is insensitive to the length scale (indicated by the horizontal contours), since all the data is explained as noise. The region where (cid:6) ≈ 0.5 (left hand side of panel a) corresponds to the case where the length scale is very short; in this regime, the marginal likelihood is insensitive to the noise level, since the data is perfectly interpolated. Neither of these regions would be chosen by a good optimizer.\n",
      "\n",
      "15.2. GPs for regression\n",
      "\n",
      "523\n",
      "\n",
      ") e d u t i n g a m ( g o\n",
      "\n",
      "l\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "−2.5\n",
      "\n",
      "z 2\n",
      "\n",
      "z 1\n",
      "\n",
      ") e d u t i n g a m ( g o\n",
      "\n",
      "l\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "−2.5\n",
      "\n",
      "−3\n",
      "\n",
      "2.8\n",
      "\n",
      "3\n",
      "\n",
      "3.2\n",
      "\n",
      "3.4\n",
      "\n",
      "−3\n",
      "\n",
      "2.8\n",
      "\n",
      "3\n",
      "\n",
      "3.2\n",
      "\n",
      "3.4\n",
      "\n",
      "log(length−scale)\n",
      "\n",
      "log(length−scale)\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "−0.5\n",
      "\n",
      ") e d u t i n g a m ( g o\n",
      "\n",
      "l\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "−2.5\n",
      "\n",
      "−3\n",
      "\n",
      "2.8\n",
      "\n",
      "3\n",
      "\n",
      "3.2\n",
      "\n",
      "3.4\n",
      "\n",
      "log(length−scale)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 15.6 Three different approximations to the posterior over hyper-parameters: grid-based, Monte Source: Figure 3.2 of (Vanhatalo 2010). Used with kind permission Carlo, and central composite design. of Jarno Vanhatalo.\n",
      "\n",
      "15.2.4.2\n",
      "\n",
      "Bayesian inference for the hyper-parameters\n",
      "\n",
      "An alternative to computing a point estimate of the hyper-parameters is to compute their poste- rior. Let θ represent all the kernel parameters, as well as σ2 y. If the dimensionality of θ is small, we can compute a discrete grid of possible values, centered on the MAP estimate ˆθ (computed as above). We can then approximate the posterior over the latent variables using\n",
      "\n",
      "p(f |D) ∝\n",
      "\n",
      "S(cid:2)\n",
      "\n",
      "p(f |D, θs)p(θs|D)δs\n",
      "\n",
      "(15.25)\n",
      "\n",
      "s=1\n",
      "\n",
      "where δs denotes the weight for grid point s.\n",
      "\n",
      "In higher dimensions, a regular grid suffers from the curse of dimensionality. An obvious alternative is Monte Carlo, but this can be slow. Another approach is to use a form of quasi- Monte Carlo, whereby we place grid points at the mode, and at a distance ±1sd from the mode along each dimension, for a total of 2|θ| + 1 points. This is called a central composite design (Rue et al. 2009). (This is also used in the unscented Kalman ﬁlter, see Section 18.5.2.) To make this Gaussian-like approximation more reasonable, we often log-transform the hyper-parameters. See Figure 15.6 for an illustration.\n",
      "\n",
      "524\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "15.2.4.3 Multiple kernel learning\n",
      "\n",
      "A quite different approach to optimizing kernel parameters known as multiple kernel learning. The idea is to deﬁne the kernel as a weighted sum of base kernels, κ(x, x(cid:4)) = j wjκj(x, x(cid:4)), and then to optimize the weights wj instead of the kernel parameters themselves. This is particularly useful if we have different kinds of data which we wish to fuse together. See (Rakotomamonjy et al. 2008) for an approach based on risk-minimization and convex e.g., optimization, and (Girolami and Rogers 2005) for an approach based on variational Bayes.\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "15.2.5\n",
      "\n",
      "Computational and numerical issues * ∗ K−1 The predictive mean is given by f∗ = kT y y. For reasons of numerical stability, it is unwise to directly invert Ky. A more robust alternative is to compute a Cholesky decomposition, Ky = LLT . We can then compute the predictive mean and variance, and the log marginal likelihood, as shown in the pseudo-code in Algorithm 6 (based on (Rasmussen and Williams 2006, p19)). It takes O(N 3) time to compute the Cholesky decomposition, and O(N 2) time to y y = L−T L−1y. We can then compute the mean using kT solve for α = K−1 ∗ α in O(N ) time ∗ L−T L−1k∗ in O(N 2) time for each test case. and the variance using k∗∗ − kT\n",
      "\n",
      "An alternative to Cholesky decomposition is to solve the linear system Kyα = y using conjugate gradients (CG). If we terminate this algorithm after k iterations, it takes O(kN 2) time. it gives the exact solution in O(N 3) time. Another approach is to If we run for k = N , approximate the matrix-vector multiplies needed by CG using the fast Gauss transform. (Yang et al. 2005); however, this doesn’t scale to high-dimensional inputs. See also Section 15.6 for a discussion of other speedup techniques.\n",
      "\n",
      "Algorithm 15.1: GP regression 1 L = cholesky(K + σ2 2 α = LT \\ (L \\ y); 3 E [f∗] = kT 4 v = L \\ k∗; 5 var [f∗] = κ(x∗, x∗) − vT v; (cid:10) 6 log p(y|X) = − 1\n",
      "\n",
      "∗ α ;\n",
      "\n",
      "2 yT α −\n",
      "\n",
      "yI);\n",
      "\n",
      "i log Lii − N\n",
      "\n",
      "2 log(2π)\n",
      "\n",
      "15.2.6\n",
      "\n",
      "Semi-parametric GPs *\n",
      "\n",
      "Sometimes it is useful to use a linear model for the mean of the process, as follows:\n",
      "\n",
      "(15.26) where r(x) ∼ GP(0, κ(x, x(cid:4))) models the residuals. This combines a parametric and a non- parametric model, and is known as a semi-parametric model.\n",
      "\n",
      "If we assume β ∼ N (b, B), we can integrate these parameters out to get a new GP (O’Hagan\n",
      "\n",
      "f (x) = βT φ(x) +r (x)\n",
      "\n",
      "1978):\n",
      "\n",
      "f (x) ∼ GP\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "φ(x)T b, κ(x, x(cid:4)) +φ( x)T Bφ(x(cid:4))\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(15.27)\n",
      "\n",
      "15.3. GPs meet GLMs\n",
      "\n",
      "525\n",
      "\n",
      "log p(yi|fi) log sigm(yifi) log Φ(yifi)\n",
      "\n",
      "∂ ∂fi ti − πi yiφ(fi) Φ(yifi)\n",
      "\n",
      "log p(yi|fi)\n",
      "\n",
      "∂2 ∂f 2 i −πi(1 − πi) − φ2\n",
      "\n",
      "Φ(yifi)2 − yifiφ(fi)\n",
      "\n",
      "log p(yi|fi)\n",
      "\n",
      "i\n",
      "\n",
      "Φ(yifi)\n",
      "\n",
      "Likelihood, gradient and Hessian for binary logistic/ probit GP regression. We assume yi ∈ Table 15.1 {−1, +1} and deﬁne ti = (yi +1)/2 ∈ {0, 1} and πi = sigm(fi) for logistic regression, and πi = Φ(fi) for probit regression. Also, φ and Φ are the pdf and cdf of N (0, 1). From (Rasmussen and Williams 2006, p43).\n",
      "\n",
      "Integrating out β, the corresponding predictive distribution for test inputs X∗ has the following form (Rasmussen and Williams 2006, p28):\n",
      "\n",
      "p(f∗|X∗, X, y) =N (f∗, cov [f∗]) f∗ = ΦT β = (ΦT K−1 cov [f∗] =K ∗∗ − KT\n",
      "\n",
      "R = Φ∗ − ΦK−1\n",
      "\n",
      "∗ β + KT\n",
      "\n",
      "y Φ + B−1)−1(ΦK−1\n",
      "\n",
      "∗ K−1 y Φ∗\n",
      "\n",
      "∗ K−1\n",
      "\n",
      "y y + B−1b) y K∗ + RT (B−1 + ΦK−1\n",
      "\n",
      "y (y − Φβ)\n",
      "\n",
      "y ΦT )−1R\n",
      "\n",
      "(15.28)\n",
      "\n",
      "(15.29)\n",
      "\n",
      "(15.30)\n",
      "\n",
      "(15.31)\n",
      "\n",
      "(15.32)\n",
      "\n",
      "The predictive mean is the output of the linear model plus a correction term due to the GP, and the predictive covariance is the usual GP covariance plus an extra term due to the uncertainty in β.\n",
      "\n",
      "15.3\n",
      "\n",
      "GPs meet GLMs\n",
      "\n",
      "In this section, we extend GPs to the GLM setting, focussing on the classiﬁcation case. As with Bayesian logistic regression, the main difficulty is that the Gaussian prior is not conjugate to the bernoulli/ multinoulli likelihood. There are several approximations one can adopt: Gaussian approximation (Section 8.4.3), expectation propagation (Kuss and Rasmussen 2005; Nickisch and Rasmussen 2008), variational (Girolami and Rogers 2006; Opper and Archambeau 2009), MCMC (Neal 1997; Christensen et al. 2006), etc. Here we focus on the Gaussian approximation, since it is the simplest and fastest.\n",
      "\n",
      "15.3.1\n",
      "\n",
      "Binary classiﬁcation\n",
      "\n",
      "In the binary case, we deﬁne the model as p(yi|xi) = σ(yif (xi)), where, following (Rasmussen and Williams 2006), we assume yi ∈ {−1, +1}, and we let σ(z) = sigm(z) (logistic regression) or σ(z) = Φ(z) (probit regression). As for GP regression, we assume f ∼ GP(0, κ).\n",
      "\n",
      "15.3.1.1\n",
      "\n",
      "Computing the posterior\n",
      "\n",
      "Deﬁne the log of the unnormalized posterior as follows: (cid:6)(f ) = log p(y|f ) + log p(f |X) = log p(y|f ) − 1 2\n",
      "\n",
      "f T K−1f − 1 2\n",
      "\n",
      "log |K| −\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log 2π (15.33)\n",
      "\n",
      "526\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "Let J(f ) (cid:2) −(cid:6)(f ) be the function we want to minimize. The gradient and Hessian of this are given by\n",
      "\n",
      "g = −∇ log p(y|f ) +K −1f H = −∇∇ log p(y|f ) +K −1 = W + K−1\n",
      "\n",
      "(15.34)\n",
      "\n",
      "(15.35)\n",
      "\n",
      "Note that W (cid:2) −∇∇ log p(y|f ) is a diagonal matrix because the data are iid (conditional on f ). Expressions for the gradient and Hessian of the log likelihood for the logit and probit case are given in Sections 8.3.1 and 9.4.1, and summarized in Table 15.1.\n",
      "\n",
      "We can use IRLS to ﬁnd the MAP estimate. The update has the form f new = f − H−1g = f + (K−1 + W)−1(∇ log p(y|f ) − K−1f )\n",
      "\n",
      "(15.36)\n",
      "\n",
      "= (K−1 + W)−1(Wf + ∇ log p(y|f ))\n",
      "\n",
      "(15.37)\n",
      "\n",
      "At convergence, the Gaussian approximation of the posterior takes the following form:\n",
      "\n",
      "p(f |X, y) ≈ N (ˆf , (K−1 + W)−1)\n",
      "\n",
      "(15.38)\n",
      "\n",
      "15.3.1.2\n",
      "\n",
      "Computing the posterior predictive\n",
      "\n",
      "We now compute the posterior predictive. First we predict the latent function at the test case x∗. For the mean we have (cid:28)\n",
      "\n",
      "E [f∗|x∗, X, y] =\n",
      "\n",
      "E [f∗|f , x∗, X, y] p(f |X, y)df\n",
      "\n",
      "(15.39)\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "=\n",
      "\n",
      "∗ K−1f p(f |X, y)df kT\n",
      "\n",
      "(15.40)\n",
      "\n",
      "= kT\n",
      "\n",
      "∗ K−1E [f |X, y] ≈ kT\n",
      "\n",
      "∗ K−1ˆf\n",
      "\n",
      "(15.41)\n",
      "\n",
      "where we used Equation 15.8 to get the mean of f∗ given noise-free f .\n",
      "\n",
      "To compute the predictive variance, we use the rule of iterated variance:\n",
      "\n",
      "var [f∗] =E [var [f∗|f ]] + var [E [f∗|f ]]\n",
      "\n",
      "(15.42)\n",
      "\n",
      "where all probabilities are conditioned on x∗, X, y. From Equation 15.9 we have\n",
      "\n",
      "E [var [f∗|f ]] = E\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "k∗∗ − kT\n",
      "\n",
      "∗ K−1k∗\n",
      "\n",
      "= k∗∗ − kT\n",
      "\n",
      "∗ K−1k∗\n",
      "\n",
      "(15.43)\n",
      "\n",
      "From Equation 15.9 we have\n",
      "\n",
      "var [E [f∗|f ]] = var\n",
      "\n",
      "(cid:31)\n",
      "\n",
      "k∗K−1f\n",
      "\n",
      "= kT\n",
      "\n",
      "∗ K−1cov [f ] K−1k∗\n",
      "\n",
      "(15.44)\n",
      "\n",
      "Combining these we get\n",
      "\n",
      "(15.45) From Equation 15.38 we have cov [f ] ≈ (K−1 + W)−1. Using the matrix inversion lemma we get\n",
      "\n",
      "var [f∗] =k ∗∗ − kT\n",
      "\n",
      "∗ (K−1 − K−1cov [f ] K−1)k∗\n",
      "\n",
      "var [f∗] ≈ k∗∗ − kT = k∗∗ − kT\n",
      "\n",
      "∗ K−1k∗ + kT ∗ (K + W−1)−1k∗\n",
      "\n",
      "∗ K−1(K−1 + W)−1K−1k∗\n",
      "\n",
      "(15.46)\n",
      "\n",
      "(15.47)\n",
      "\n",
      "15.3. GPs meet GLMs\n",
      "\n",
      "527\n",
      "\n",
      "So in summary we have\n",
      "\n",
      "p(f∗|x∗, X, y) = N (E [f∗] , var [f∗])\n",
      "\n",
      "(15.48)\n",
      "\n",
      "To convert this in to a predictive distribution for binary responses, we use\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "π∗ = p(y∗ = 1|x∗, X, y) ≈\n",
      "\n",
      "σ(f∗)p(f∗|x∗, X, y)df∗\n",
      "\n",
      "(15.49)\n",
      "\n",
      "This can be approximated using any of the methods discussed in Section 8.4.4, where we discussed Bayesian logistic regression. For example, using the probit approximation of Sec- tion 8.4.4.2, we have π∗ ≈ sigm(κ(v)E [f∗]), wherev = var [f∗] and κ2(v) = (1 + πv/8)−1.\n",
      "\n",
      "15.3.1.3\n",
      "\n",
      "Computing the marginal likelihood\n",
      "\n",
      "We need the marginal likelihood in order to optimize the kernel parameters. Using the Laplace approximation in Equation 8.54 we have\n",
      "\n",
      "log p(y|X) ≈ (cid:6)(ˆf ) − 1 2\n",
      "\n",
      "log |H| + const\n",
      "\n",
      "(15.50)\n",
      "\n",
      "Hence\n",
      "\n",
      "log p(y|X) ≈ log p(y|ˆf ) − 1 2\n",
      "\n",
      "ˆf T K−1ˆf − 1 2\n",
      "\n",
      "log |K| − 1 2\n",
      "\n",
      "log |K−1 + W|\n",
      "\n",
      "(15.51)\n",
      "\n",
      "is more complex than in the regression case, since ˆf and W, as well as K, depend on θ. Details can be found in (Rasmussen and Williams 2006, p125).\n",
      "\n",
      "Computing the derivatives\n",
      "\n",
      "∂ log p(y|X,θ) ∂θj\n",
      "\n",
      "15.3.1.4\n",
      "\n",
      "Numerically stable computation *\n",
      "\n",
      "To implement the above equations in a numerically stable way, it is best to avoid inverting K or W. (Rasmussen and Williams 2006, p45) suggest deﬁning\n",
      "\n",
      "B = IN + W 1\n",
      "\n",
      "2 KW 1\n",
      "\n",
      "2\n",
      "\n",
      "(15.52)\n",
      "\n",
      "which has eigenvalues bounded below by 1 (because of the I) and above by 1 + N (because wii = πi(1 − π) ≤ 0.25), and hence can be safely inverted.\n",
      "\n",
      "4 maxij Kij\n",
      "\n",
      "One can use the matrix inversion lemma to show (K−1 + W)−1 = K − KW 1\n",
      "\n",
      "2 B−1W 1\n",
      "\n",
      "2 K\n",
      "\n",
      "(15.53)\n",
      "\n",
      "Hence the IRLS update becomes\n",
      "\n",
      "f new = (K−1 + W)−1 (Wf + ∇ log p(y|f )) 6\n",
      "\n",
      "3\n",
      "\n",
      "45 b\n",
      "\n",
      "(15.54)\n",
      "\n",
      "= K(I − W 1 = K (b − W 1 3\n",
      "\n",
      "2 B−1W 1 2 LT \\ (L \\ (W 1 45 a\n",
      "\n",
      "2 K)b\n",
      "\n",
      "2 Kb))) 6\n",
      "\n",
      "(15.55)\n",
      "\n",
      "(15.56)\n",
      "\n",
      "528\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "where B = LLT is a Cholesky decomposition of B. The ﬁtting algorithm takes in O(T N 3) time and O(N 2) space, where T is the number of Newton iterations.\n",
      "\n",
      "At convergence we have a = K−1ˆf , so we can evaluate the log marginal likelihood (Equa-\n",
      "\n",
      "tion 15.51) using\n",
      "\n",
      "log p(y|X) = log p(y|ˆf ) − 1 2\n",
      "\n",
      "aT ˆf −\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "log Lii\n",
      "\n",
      "(15.57)\n",
      "\n",
      "where we exploited the fact that\n",
      "\n",
      "|B| = |K||K−1 + W| = |IN + W 1\n",
      "\n",
      "2 KW 1 2 |\n",
      "\n",
      "(15.58)\n",
      "\n",
      "∗ K−1ˆf , we exploit the fact that at the mode, ∇(cid:6) = 0, so ˆf = K(∇ log p(y|ˆf )). Hence we can rewrite the predictive mean as follows:2 E [f∗] = kT\n",
      "\n",
      "We now compute the predictive distribution. Rather than using E [f∗] = kT\n",
      "\n",
      "∗ ∇ log p(y|ˆf )\n",
      "\n",
      "(15.59)\n",
      "\n",
      "To compute the predictive variance, we exploit the fact that\n",
      "\n",
      "(K + W−1)−1 = W 1\n",
      "\n",
      "2 W− 1\n",
      "\n",
      "2 (K + W−1)−1W− 1\n",
      "\n",
      "2 W 1\n",
      "\n",
      "2 = W 1\n",
      "\n",
      "2 B−1W 1\n",
      "\n",
      "2\n",
      "\n",
      "(15.60)\n",
      "\n",
      "to get\n",
      "\n",
      "var [f∗] = k∗∗ − kT\n",
      "\n",
      "∗ W 1\n",
      "\n",
      "2 (LLT )−1W 1\n",
      "\n",
      "2 k∗ = k∗∗ − vT v\n",
      "\n",
      "(15.61)\n",
      "\n",
      "where v = L \\ (W 1\n",
      "\n",
      "2 k∗). We can then compute π∗.\n",
      "\n",
      "The whole algorithm is summarized in Algorithm 16, based on (Rasmussen and Williams 2006, p46). Fitting takes O(N 3) time, and prediction takes O(N 2N∗) time, where N∗ is the number of test cases.\n",
      "\n",
      "15.3.1.5\n",
      "\n",
      "Example\n",
      "\n",
      "In Figure 15.7, we show a synthetic binary classiﬁcation problem in 2d. We use an SE kernel. On the left, we show predictions using hyper-parameters set by hand; we use a short length scale, hence the very sharp turns in the decision boundary. On the right, we show the predictions using the learned hyper-parameters; the model favors a more parsimonious explanation of the data.\n",
      "\n",
      "15.3.2 Multi-class classiﬁcation\n",
      "\n",
      "In this section, we consider a model of the form p(yi|xi) = Cat(yi|S(fi)), where fi = (fi1, . . . , fiC), and we assume f.c ∼ GP(0, κc). Thus we have one latent function per class, which are a priori independent, and which may use different kernels. As before, we will use a Gaussian approximation to the posterior. (A similar model, but using the multinomial probit function instead of the multinomial logit, is described in (Girolami and Rogers 2006).)\n",
      "\n",
      "2. We see that training points that are well-predicted by the model, for which ∇i log p(yi|fi) ≈ 0, do not contribute strongly to the prediction at test points; this is similar to the behavior of support vectors in an SVM (see Section 14.5).\n",
      "\n",
      "15.3. GPs meet GLMs\n",
      "\n",
      "529\n",
      "\n",
      "Algorithm 15.2: GP binary classiﬁcation using Gaussian approximation\n",
      "\n",
      "9 10 until converged; 11 log p(y|X) = log p(y|f ) − 1 12 // Now perform prediction ; 13 E [f∗] = kT 14 v = L \\ (W 1 2 k∗); 15 var [f∗] = k∗∗ − vT v ; ) 16 p(y∗ = 1) =\n",
      "\n",
      "1 // First compute MAP estimate using IRLS; 2 f = 0; 3 repeat 4 W = −∇∇ log p(y|f ) ; B = IN + W 1 2 ; L = cholesky(B) ; b = Wf + ∇ log p(y|f ) ; a = b − W 1 f = Ka;\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "∗ ∇ log p(y|f );\n",
      "\n",
      "sigm(z)N (z|E [f∗] , var [f∗])dz;\n",
      "\n",
      "2 LT \\ (L \\ (W 1\n",
      "\n",
      "2 KW 1\n",
      "\n",
      "2 aT f −\n",
      "\n",
      "2 Kb));\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i log Lii;\n",
      "\n",
      "4\n",
      "\n",
      "SE kernel, l=0.500, σ2=10.000\n",
      "\n",
      "0.9\n",
      "\n",
      "4\n",
      "\n",
      "SE kernel, l=1.280, σ2=14.455\n",
      "\n",
      "0.9\n",
      "\n",
      "3\n",
      "\n",
      "0.8\n",
      "\n",
      "3\n",
      "\n",
      "0.8\n",
      "\n",
      "2\n",
      "\n",
      "0.7\n",
      "\n",
      "2\n",
      "\n",
      "0.7\n",
      "\n",
      "1\n",
      "\n",
      "0.6\n",
      "\n",
      "1\n",
      "\n",
      "0.6\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "−1\n",
      "\n",
      "0.4\n",
      "\n",
      "−1\n",
      "\n",
      "0.4\n",
      "\n",
      "−2\n",
      "\n",
      "0.3\n",
      "\n",
      "−2\n",
      "\n",
      "0.3\n",
      "\n",
      "−3\n",
      "\n",
      "0.2\n",
      "\n",
      "−3\n",
      "\n",
      "0.2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "0.1\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "0.1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 15.7 Contours of the posterior predictive probability for the red circle class generated by a GP with an SE kernel. Thick black line is the decision boundary if we threshold at a probability of 0.5. (a) Manual parameters, short length scale. (b) Learned parameters, long length scale. Figure generated by gpcDemo2d, based on code by Carl Rasmussen.\n",
      "\n",
      "530\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "15.3.2.1\n",
      "\n",
      "Computing the posterior\n",
      "\n",
      "The unnormalized log posterior is given by (cid:11)\n",
      "\n",
      "(cid:6)(f ) = − 1 2\n",
      "\n",
      "f T K−1f + yT f −\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "log\n",
      "\n",
      "C(cid:2)\n",
      "\n",
      "c=1\n",
      "\n",
      "exp fic\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "log |K| −\n",
      "\n",
      "CN\n",
      "\n",
      "2\n",
      "\n",
      "log 2π\n",
      "\n",
      "(15.62)\n",
      "\n",
      "where\n",
      "\n",
      "f = (f11, . . . , fN 1, f12, . . . , fN 2, · · · , f1C, . . . , fN C)T\n",
      "\n",
      "(15.63)\n",
      "\n",
      "and y is a dummy encoding of the yi’s which has the same layout as f . Also, K is a block diagonal matrix containing Kc, where Kc = [κc(xi, xj)] models the correlation of the c’th latent function.\n",
      "\n",
      "The gradient and Hessian are given by\n",
      "\n",
      "∇(cid:6) = −K−1f + y − π\n",
      "\n",
      "(15.64)\n",
      "\n",
      "(15.65) where W (cid:2) diag(π) − ΠΠT , where Π is a CN × N matrix obtained by stacking diag(π:c) vertically. (Compare these expressions to standard logistic regression in Section 8.3.7.)\n",
      "\n",
      "∇∇(cid:6) = −K−1 − W\n",
      "\n",
      "We can use IRLS to compute the mode. The Newton step has the form f new = (K−1 + W)−1(Wf + y − π)\n",
      "\n",
      "(15.66)\n",
      "\n",
      "Naively implementing this would take O(C 3N 3) time. However, we can reduce this to O(CN 3), as shown in (Rasmussen and Williams 2006, p52).\n",
      "\n",
      "15.3.2.2\n",
      "\n",
      "Computing the posterior predictive\n",
      "\n",
      "We can compute the posterior predictive in a manner analogous to Section 15.3.1.2. For the mean of the latent response we have\n",
      "\n",
      "E [f∗c] = kc(x∗)T K−1\n",
      "\n",
      "c ˆfc = kc(x∗)T (yc − ˆπc)\n",
      "\n",
      "(15.67)\n",
      "\n",
      "We can put this in vector form by writing\n",
      "\n",
      "E [f∗] = Q∗T (y − ˆπ)\n",
      "\n",
      "(15.68)\n",
      "\n",
      "where\n",
      "\n",
      "Q∗ =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎝\n",
      "\n",
      "k1(x∗)\n",
      "\n",
      "0\n",
      "\n",
      ". . . . . . . . . kC(x∗)\n",
      "\n",
      "0\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎠\n",
      "\n",
      "(15.69)\n",
      "\n",
      "Using a similar argument to Equation 15.47, we can show that the covariance of the latent response is given by\n",
      "\n",
      "cov [f∗] =Σ + QT\n",
      "\n",
      "∗ K−1(K−1 + W)−1K−1Q∗\n",
      "\n",
      "(15.70)\n",
      "\n",
      "= diag(k(x∗, x∗)) − QT\n",
      "\n",
      "∗ (K + W−1)−1Q∗\n",
      "\n",
      "(15.71)\n",
      "\n",
      "15.3. GPs meet GLMs\n",
      "\n",
      "531\n",
      "\n",
      "where Σ is a C × C diagonal matrix with Σcc = κc(x∗, x∗) − kT k(x∗, x∗) = [κc(x∗, x∗)].\n",
      "\n",
      "c (x∗)K−1\n",
      "\n",
      "c kc(x∗), and\n",
      "\n",
      "To compute the posterior predictive for the visible response, we need to use\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "p(y|x∗, X, y) ≈\n",
      "\n",
      "Cat(y|S(f∗))N (f∗|E [f∗] , cov [f∗])df∗\n",
      "\n",
      "(15.72)\n",
      "\n",
      "We can use any of deterministic approximations to the softmax function discussed in Sec- tion 21.8.1.1 to compute this. Alternatively, we can just use Monte Carlo.\n",
      "\n",
      "15.3.2.3\n",
      "\n",
      "Computing the marginal likelihood\n",
      "\n",
      "Using arguments similar to the binary case, we can show that (cid:13)\n",
      "\n",
      "log p(y|X) ≈ − 1 2\n",
      "\n",
      "ˆf T K−1ˆf + yT ˆf −\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "log\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "C(cid:2)\n",
      "\n",
      "c=1\n",
      "\n",
      "exp ˆfic\n",
      "\n",
      "− 1 2\n",
      "\n",
      "log |ICN + W 1\n",
      "\n",
      "2 KW 1\n",
      "\n",
      "2 |(15.73)\n",
      "\n",
      "This can be optimized numerically in the usual way.\n",
      "\n",
      "15.3.2.4\n",
      "\n",
      "Numerical and computational issues One can implement model ﬁtting in O(T CN 3) time and O(CN 2) space, where T is the number of Newton iterations, using the techniques described in (Rasmussen and Williams 2006, p50). Prediction takes O(CN 3 + CN 2N∗) time, where N∗ is the number of test cases.\n",
      "\n",
      "15.3.3\n",
      "\n",
      "GPs for Poisson regression\n",
      "\n",
      "In this section, we illustrate GPs for Poisson regression. An interesting application of this is to spatial disease mapping. For example, (Vanhatalo et al. 2010) discuss the problem of modeling the relative risk of heart attack in different regions in Finland. The data consists of the heart attacks in Finland from 1996-2000 aggregated into 20km x 20km lattice cells. The model has the following form:\n",
      "\n",
      "yi ∼ Poi(eiri)\n",
      "\n",
      "(15.74)\n",
      "\n",
      "where ei is the known expected number of deaths (related to the population of cell i and the overall death rate), and ri is the relative risk of cell i which we want to infer. Since the data counts are small, we regularize the problem by sharing information with spatial neighbors. Hence we assume f (cid:2) log(r) ∼ GP(0, κ), where we use a Matern kernel with ν = 3/2, and a length scale and magnitude that are estimated from data.\n",
      "\n",
      "Figure 15.8 gives an example of the kind of output one can obtain from this method, based on data from 911 locations. On the left we plot the posterior mean relative risk (RR), and on the right, the posterior variance. We see that the RR is higher in Eastern Finland, which is consistent with other studies. We also see that the variance in the North is higher, since there are fewer people living there.\n",
      "\n",
      "532\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "Posterior mean of the relative risk, FIC 60\n",
      "\n",
      "Posterior variance of the relative risk, FIC 60\n",
      "\n",
      "0.035\n",
      "\n",
      "1.4\n",
      "\n",
      "50\n",
      "\n",
      "1.3\n",
      "\n",
      "50\n",
      "\n",
      "0.03\n",
      "\n",
      "40\n",
      "\n",
      "1.2\n",
      "\n",
      "40\n",
      "\n",
      "0.025\n",
      "\n",
      "1.1\n",
      "\n",
      "0.02\n",
      "\n",
      "30\n",
      "\n",
      "30\n",
      "\n",
      "1\n",
      "\n",
      "0.015\n",
      "\n",
      "20\n",
      "\n",
      "0.9\n",
      "\n",
      "20\n",
      "\n",
      "0.01\n",
      "\n",
      "0.8\n",
      "\n",
      "10\n",
      "\n",
      "0.7\n",
      "\n",
      "10\n",
      "\n",
      "0.005\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 15.8 We show the relative risk of heart disease in Finland using a Poisson GP. Left: posterior mean. Right: posterior variance. Figure generated by gpSpatialDemoLaplace, written by Jarno Vanhatalo.\n",
      "\n",
      "15.4\n",
      "\n",
      "Connection with other methods\n",
      "\n",
      "There are variety of other methods in statistics and machine learning that are closely related to GP regression/ classiﬁcation. We give a brief review of some of these below.\n",
      "\n",
      "15.4.1\n",
      "\n",
      "Linear models compared to GPs\n",
      "\n",
      "Consider Bayesian linear regression for D-dimensional features, where the prior on the weights is p(w) = N (0, Σ). The posterior predictive distribution is given by the following;\n",
      "\n",
      "p(f∗|x∗, X, y) = N (μ, σ2)\n",
      "\n",
      "(15.75)\n",
      "\n",
      "where A = σ−2 follows\n",
      "\n",
      "1 σ2 y σ2 = xT ∗ A−1x∗ (15.77) y XT X + Σ−1. One can show that we can rewrite the above distribution as\n",
      "\n",
      "μ =\n",
      "\n",
      "∗ A−1XT y xT\n",
      "\n",
      "(15.76)\n",
      "\n",
      "μ = xT σ2 = xT\n",
      "\n",
      "∗ ΣXT (K + σ2 ∗ Σx∗ − xT\n",
      "\n",
      "∗ ΣXT (K + σ2I)−1XΣx∗\n",
      "\n",
      "yI)−1y\n",
      "\n",
      "(15.78)\n",
      "\n",
      "(15.79)\n",
      "\n",
      "where we have deﬁned K = XΣXT , which is of size N × N . Since the features only ever appear in the form XΣXT , xT ∗ Σx∗, we can kernelize the above expression by deﬁning κ(x, x(cid:4)) = xT Σx(cid:4).\n",
      "\n",
      "∗ ΣXT or xT\n",
      "\n",
      "Thus we see that Bayesian linear regression is equivalent to a GP with covariance function κ(x, x(cid:4)) = xT Σx(cid:4). Note, however, that this is a degenerate covariance function, since it has at most D non-zero eigenvalues. Intuitively this reﬂects the fact that the model can only represent a limited number of functions. This can result in underﬁtting, since the model is not ﬂexible enough to capture the data. What is perhaps worse, it can result in overconﬁdence, since the\n",
      "\n",
      "15.4. Connection with other methods\n",
      "\n",
      "533\n",
      "\n",
      "model’s prior is so impoverished that its posterior will become too concentrated. So not only is the model wrong, it think it’s right!\n",
      "\n",
      "15.4.2\n",
      "\n",
      "Linear smoothers compared to GPs\n",
      "\n",
      "A linear smoother is a regression function which is a linear function of the training outputs:\n",
      "\n",
      "ˆf (x∗) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "wi(x∗) yi\n",
      "\n",
      "(15.80)\n",
      "\n",
      "i\n",
      "\n",
      "where wi(x∗) is called the weight function (Silverman 1984). (Do not confuse this with a linear model, where the output is a linear function of the input vector.)\n",
      "\n",
      "locally weighted regression (Section 14.7.5), smoothing splines (Section 15.4.6), and GP regression. To see that GP regession is a linear smoother, note that the mean of the posterior predictive distribution of a GP is given by\n",
      "\n",
      "There are a variety of linear smoothers, such as kernel regression (Section 14.7.4),\n",
      "\n",
      "f (x∗) = kT\n",
      "\n",
      "∗ (K + σ2\n",
      "\n",
      "yIN )−1y =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "yiwi(x∗)\n",
      "\n",
      "(15.81)\n",
      "\n",
      "i=1\n",
      "\n",
      "where wi(x∗) = [(K + σ2\n",
      "\n",
      "yIN )−1k∗]i.\n",
      "\n",
      "In kernel regression, we derive the weight function from a smoothing kernel rather than a Mercer kernel, so it is clear that the weight function will then have local support. In the case of a GP, things are not as clear, since the weight function depends on the inverse of K. For certain GP kernel functions, we can analytically derive the form of wi(x); this is known as the i=1 wi(x∗) = 1, although we may equivalent kernel (Silverman 1984). One can show that have wi(x∗) < 0, so we are computing a linear combination but not a convex combination of the yi’s. More interestingly, wi(x∗) is a local function, even if the original kernel used by the GP is not local. Futhermore the effective bandwidth of the equivalent kernel of a GP automatically decreases as the sample size N increases, whereas in kernel smoothing, the bandwidth h needs to be set by hand to adapt to N . See e.g., (Rasmussen and Williams 2006, Sec 2.6,Sec 7.1) for details.\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "15.4.2.1\n",
      "\n",
      "Degrees of freedom of linear smoothers\n",
      "\n",
      "It is clear why this method is called “linear”, but why is it called a “smoother”? This is best explained in terms of GPs. Consider the prediction on the training set:\n",
      "\n",
      "f = K(K + σ2\n",
      "\n",
      "y)−1y\n",
      "\n",
      "(15.82)\n",
      "\n",
      "i . Since K is real and symmetric Now let K have the eigendecomposition K = positive deﬁnite, the eigenvalues λi are real and non-negative, and the eigenvectors ui are orthonormal. Now let y = i y. Then we can rewrite the above equation as follows:\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 γiui, where γi = uT\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 λiuiuT\n",
      "\n",
      "f =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "γiλi λi + σ2 y\n",
      "\n",
      "ui\n",
      "\n",
      "(15.83)\n",
      "\n",
      "534\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "This is the same as Equation 7.47, except we are working with the eigenvectors of the Gram matrix K instead of the data matrix X. In any case, the interpretation is similar: if (cid:10) 1, then the corresponding basis function ui will not have much inﬂuence. Consequently the high- frequency components in y are smoothed out. The effective degrees of freedom of the linear smoother is deﬁned as\n",
      "\n",
      "λi λi+σ2 y\n",
      "\n",
      "dof (cid:2) tr(K(K + σ2\n",
      "\n",
      "yI)−1) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "λi λi + σ2 y\n",
      "\n",
      "(15.84)\n",
      "\n",
      "This speciﬁes how “wiggly” the curve is.\n",
      "\n",
      "15.4.3\n",
      "\n",
      "SVMs compared to GPs\n",
      "\n",
      "We saw in Section 14.5.2 that the SVM objective for binary classiﬁcation is given by Equation 14.57\n",
      "\n",
      "We also know from Equation 14.59 that the optimal solution has the form w = i αixi, so ||w||2 = i xj. Kernelizing we get ||w||2 = αKα. From Equation 14.61, and absorbing the ˆw0 term into one of the kernels, we have f = Kα, so ||w||2 = f T K−1f . Hence the SVM objective can be rewritten as\n",
      "\n",
      "J(w) =\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "i,j αiαjxT\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "(1 − yifi)+\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(15.85)\n",
      "\n",
      "J(f ) =\n",
      "\n",
      "1 2\n",
      "\n",
      "f T f + C\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(1 − yifi)+\n",
      "\n",
      "i=1\n",
      "\n",
      "(15.86)\n",
      "\n",
      "Compare this to MAP estimation for GP classiﬁer:\n",
      "\n",
      "J(f ) =\n",
      "\n",
      "1 2\n",
      "\n",
      "f T f −\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "log p(yi|fi)\n",
      "\n",
      "(15.87)\n",
      "\n",
      "It is tempting to think that we can “convert” an SVM into a GP by ﬁguring out what likelihood would be equivalent to the hinge loss. However, it turns out there is no such likelihood (Sollich 2002), although there is a pseudo-likelihood that matches the SVM (see Section 14.5.5).\n",
      "\n",
      "From Figure 6.7 we saw that the hinge loss and the logistic loss (as well as the probit loss) are quite similar to each other. The main difference is that the hinge loss is strictly 0 for errors larger than 1. This gives rise to a sparse solution. In Section 14.3.2, we discussed other ways to derive sparse kernel machines. We discuss the connection between these methods and GPs below.\n",
      "\n",
      "15.4.4\n",
      "\n",
      "L1VM and RVMs compared to GPs\n",
      "\n",
      "Sparse kernel machines are just linear models with basis function expansion of the form φ(x) = [κ(x, x1), . . . , κ(x, xN )]. From Section 15.4.1, we know that this is equivalent to a GP with the following kernel:\n",
      "\n",
      "κ(x, x(cid:4)) =\n",
      "\n",
      "D(cid:2)\n",
      "\n",
      "j=1\n",
      "\n",
      "1 αj\n",
      "\n",
      "φj(x)φj(x(cid:4))\n",
      "\n",
      "(15.88)\n",
      "\n",
      "15.4. Connection with other methods\n",
      "\n",
      "535\n",
      "\n",
      "where p(w) = N (0, diag(α−1 j )). This kernel function has two interesting properties. First, it is degenerate, meaning it has at most N non-zero eigenvalues, so the joint distribution p(f , f∗) will be highly constrained. Second, the kernel depends on the training data. This can cause the model to be overconﬁdent when extrapolating beyond the training data. To see this, consider a point x∗ far outside the convex hull of the data. All the basis functions will have values close to 0, so the prediction will back off to the mean of the GP. More worryingly, the variance will back off to the noise variance. By contrast, when using a non-degenerate kernel function, the predictive variance increases as we move away from the training data, as desired. See (Rasmussen and Quiñonero-Candela 2005) for further discussion.\n",
      "\n",
      "15.4.5\n",
      "\n",
      "Neural networks compared to GPs\n",
      "\n",
      "In Section 16.5, we will discuss neural networks, which are a nonlinear generalization of GLMs. In the binary classiﬁcation case, a neural network is deﬁned by a logistic regression model applied to a logistic regression model:\n",
      "\n",
      "p(y|x, θ) = Ber\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "y|sigm\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "wT sigm(Vx)\n",
      "\n",
      "(cid:4)(cid:4)\n",
      "\n",
      "(15.89)\n",
      "\n",
      "It turns out there is an interesting connection between neural networks and Gaussian processes, as ﬁrst pointed out by (Neal 1996).\n",
      "\n",
      "To explain the connection, we follow the presentation of (Rasmussen and Williams 2006, p91).\n",
      "\n",
      "Consider a neural network for regression with one hidden layer. This has the form\n",
      "\n",
      "p(y|x, θ) = N (y|f (x; θ), σ2)\n",
      "\n",
      "(15.90)\n",
      "\n",
      "where\n",
      "\n",
      "f (x) = b +\n",
      "\n",
      "H(cid:2)\n",
      "\n",
      "vjg(x; uj)\n",
      "\n",
      "(15.91)\n",
      "\n",
      "j=1\n",
      "\n",
      "where b is the offset of bias term, vj is the output weight from hidden unit j to the response y, uj are the inputs weights to unit j from the input x, and g() is the hidden unit activation function. This is typically the sigmoid or tanh function, but can be any smooth function.\n",
      "\n",
      "u ∼\n",
      "\n",
      "Let us use the following priors on the weights: where b ∼ N (0, σ2\n",
      "\n",
      "’\n",
      "\n",
      "j p(uj) for some unspeciﬁed p(uj). Denoting all the weights by θ we have\n",
      "\n",
      "b ) v ∼\n",
      "\n",
      "’\n",
      "\n",
      "j N (vj|0, σ2\n",
      "\n",
      "w),\n",
      "\n",
      "Eθ [f (x)] = 0 Eθ [f (x)f (x(cid:4))] = σ2\n",
      "\n",
      "b +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "vEv [g(x; uj)g(x(cid:4); uj)] σ2\n",
      "\n",
      "(15.92)\n",
      "\n",
      "(15.93)\n",
      "\n",
      "= σ2\n",
      "\n",
      "j b + Hσ2\n",
      "\n",
      "vEu [g(x; u)g(x(cid:4); u)]\n",
      "\n",
      "(15.94)\n",
      "\n",
      "v scale as ω2/H where the last equality follows since the H hidden units are iid. (since more hidden units will increase the input to the ﬁnal node, so we should scale down the magnitude of the weights), then the last term becomes ω2Eu [g(x; u)g(x(cid:4); u)]. This is a sum over H iid random variables. Assuming that g is bounded, we can apply the central limit theorem. The result is that as H → ∞, we get a Gaussian process.\n",
      "\n",
      "If we let σ2\n",
      "\n",
      "536\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "4\n",
      "\n",
      "−0.5\n",
      "\n",
      "1\n",
      "\n",
      "σ = 10 σ = 3 σ = 1\n",
      "\n",
      "ʼ\n",
      "\n",
      "x\n",
      "\n",
      ", t u p n\n",
      "\n",
      "i\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "0.95\n",
      "\n",
      "0.95\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      ") x ( f\n",
      "\n",
      ", t u p t u o\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "0 input, x\n",
      "\n",
      "4\n",
      "\n",
      "−4\n",
      "\n",
      "0 input, x\n",
      "\n",
      "4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 15.9 (a) Covariance function κN N (x, x(cid:2)) for σ0 = 10, σ = 10. (b) Samples from from a GP with this kernel, using various values of σ. Figure generated by gpnnDemo, written by Chris Williams.\n",
      "\n",
      "2/ kernel has the form\n",
      "\n",
      "If we use as activation / transfer function g(x; u) =erf (u0 + √\n",
      "\n",
      "κN N (x, x(cid:4)) =\n",
      "\n",
      "π\n",
      "\n",
      ") z 0\n",
      "\n",
      "j=1 ujxj), where erf(z) = e−t2 dt, and we choose u ∼ N (0, Σ), then (Williams 1998) showed that the covariance\n",
      "\n",
      "2 π\n",
      "\n",
      "sin−1\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "2˜xT Σ˜x(cid:4) (1 + 2˜xT Σ˜x)(1 + 2(˜x(cid:4))T Σ˜x(cid:4))\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(cid:10)D\n",
      "\n",
      "(15.95)\n",
      "\n",
      "where ˜x = (1, x1, . . . , xD). This is a true “neural network” kernel, unlike the “sigmoid” kernel κ(x, x(cid:4)) = tanh(a + bxT x(cid:4)), which is not positive deﬁnite.\n",
      "\n",
      "0, σ2). Figure 15.9(b) shows some functions sampled from the corresponding GP. These are equivalent to functions which are superpositions of erf(u0 + ux) where u0 and u are random. As σ2 increases, the variance of u increases, so the function varies more quickly. Unlike the RBF kernel, functions sampled from this kernel do not tend to 0 away from the data, but rather they tend to remain at the same value they had at the “edge” of the data.\n",
      "\n",
      "Figure 15.9(a) illustrates this kernel when D = 2 and Σ = diag(σ2\n",
      "\n",
      "of the form g(x; u) = exp(−|x − u|2/(2σ2 coresponding kernel is equivalent to the RBF or SE kernel.\n",
      "\n",
      "Now suppose we use an RBF network, which is equivalent to a hidden unit activation function uI), one can show that the\n",
      "\n",
      "g)).\n",
      "\n",
      "If u ∼ N (0, σ2\n",
      "\n",
      "15.4.6\n",
      "\n",
      "Smoothing splines compared to GPs *\n",
      "\n",
      "Smoothing splines are a widely used non-parametric method for smoothly interpolating data (Green and Silverman 1994). They are are a special case of GPs, as we will see. They are usually used when the input is 1 or 2 dimensional.\n",
      "\n",
      "15.4.6.1\n",
      "\n",
      "Univariate splines\n",
      "\n",
      "The basic idea is to ﬁt a function f by minimizing the discrepancy to the data plus a smoothing If we penalize the m’th derivative of the term that penalizes functions that are “too wiggly”.\n",
      "\n",
      "15.4. Connection with other methods\n",
      "\n",
      "537\n",
      "\n",
      "function, the objective becomes\n",
      "\n",
      "J(f ) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(f (xi) − yi)2 + λ\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(\n",
      "\n",
      "dm dxm\n",
      "\n",
      "f (x))2dx\n",
      "\n",
      "(15.96)\n",
      "\n",
      "One can show (Green and Silverman 1994) that the solution is a piecewise polynomial where the polynomials have order 2m − 1 in the interior bins [xi−1, xi] (denoted I), and order m − 1 in the two outermost intervals (−∞, x1] and [xN , ∞): (cid:13)\n",
      "\n",
      "f (x) =\n",
      "\n",
      "m−1(cid:2)\n",
      "\n",
      "βjxj + I(x ∈ I)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αi(x − xi)2m−1\n",
      "\n",
      "+\n",
      "\n",
      "+ I(x (cid:13)∈ I)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αi(x − xi)m−1\n",
      "\n",
      "+\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(15.97)\n",
      "\n",
      "j=0\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "For example, if m = 2, we get the (natural) cubic spline\n",
      "\n",
      "f (x) =β 0 + β1x + I(x ∈ I)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αi(x − xi)3 +\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "+ I(x (cid:13)∈ I)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αi(x − xi)+\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(15.98)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "which is a series of truncated cubic polynomials, whose left hand sides are located at each of the N training points. (The fact that the model is linear on the edges prevents it from extrapolating too wildly beyond the range of the data; if we drop this requirement, we get an “unrestricted” spline.)\n",
      "\n",
      "columns of Φ are 1, xi and (x − xi)3 However, we can also derive an O(N ) time method (Green and Silverman 1994, Sec 2.3.3).\n",
      "\n",
      "We can clearly ﬁt this model using ridge regression: ˆw = (ΦT Φ + λIN )−1ΦT y, where the + for i = 2 :N − 1 and (x − xi)+ for i = 1 or i = N .\n",
      "\n",
      "15.4.6.2\n",
      "\n",
      "Regression splines\n",
      "\n",
      "In general, we can place the polynomials at a ﬁxed set of K locations known as knots, denoted ξk. The result is called a regression spline. This is a parametric model, which uses basis function expansion of the following form (where we drop the interior/ exterior distinction for simplicity):\n",
      "\n",
      "f (x) =β 0 + β1x +\n",
      "\n",
      "K(cid:2)\n",
      "\n",
      "αj(x − ξk)3 +\n",
      "\n",
      "(15.99)\n",
      "\n",
      "k=1\n",
      "\n",
      "Choosing the number and locations of the knots is just like choosing the number and values of the support vectors in Section 14.3.2. If we impose an (cid:6)2 regularizer on the regression coefficients αj, the method is known as penalized splines. See Section 9.6.1 for a practical example of penalized splines.\n",
      "\n",
      "15.4.6.3\n",
      "\n",
      "The connection with GPs\n",
      "\n",
      "One can show (Rasmussen and Williams 2006, p139) that the cubic spline is the MAP estimate of the following function\n",
      "\n",
      "f (x) =β 0 + β1x + r(x)\n",
      "\n",
      "(15.100)\n",
      "\n",
      "538\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "where p(βj) ∝ 1 (so that we don’t penalize the zero’th and ﬁrst derivatives of f ), and r(x) ∼ GP(0, σ2\n",
      "\n",
      "κsp(x, x(cid:4)) (cid:2)\n",
      "\n",
      "f κsp(x, x(cid:4))), where (cid:28) 1\n",
      "\n",
      "(x − u)+(x(cid:4) − u)+du\n",
      "\n",
      "(15.101)\n",
      "\n",
      "0\n",
      "\n",
      "Note that the kernel in Equation 15.101 is rather unnatural, and indeed posterior samples from the resulting GP are rather unsmooth. However, the posterior mode/mean is smooth. This shows that regularizers don’t always make good priors.\n",
      "\n",
      "15.4.6.4\n",
      "\n",
      "2d input (thin-plate splines)\n",
      "\n",
      "One can generalize cubic splines to 2d input by deﬁning a regularizer of the following form: (cid:8)\n",
      "\n",
      "(cid:28) (cid:28) (cid:18)(cid:8)\n",
      "\n",
      "∂2f (x) ∂x2 1\n",
      "\n",
      "(cid:9)2\n",
      "\n",
      "+ 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "∂2f (x) ∂x1∂x2\n",
      "\n",
      "(cid:9)2\n",
      "\n",
      "+\n",
      "\n",
      "∂2f (x) ∂x2 2\n",
      "\n",
      "(cid:9)2\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "dx1dx2\n",
      "\n",
      "(15.102)\n",
      "\n",
      "One can show that the solution has the form\n",
      "\n",
      "f (x) =β 0 + βT\n",
      "\n",
      "1 x +\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αiφi(x)\n",
      "\n",
      "(15.103)\n",
      "\n",
      "i=1\n",
      "\n",
      "where φi(x) = η(||x − xi||), and η(z) = z2 log z2. This is known as a thin plate spline. This is equivalent to MAP estimation with a GP whose kernel is deﬁned in (Williams and Fitzgibbon 2006).\n",
      "\n",
      "15.4.6.5\n",
      "\n",
      "Higher-dimensional inputs\n",
      "\n",
      "It is hard to analytically solve for the form of the optimal solution when using higher-order inputs. However, in the parametric regression spline setting, where we forego the regularizer on f , we have more freedom in deﬁning our basis functions. One way to handle multiple inputs is to use a tensor product basis, deﬁned as the cross product of 1d basis functions. For example, for 2d input, we can deﬁne\n",
      "\n",
      "f (x1, x2) =β 0 + (cid:2)\n",
      "\n",
      "+\n",
      "\n",
      "β12m(x1 − ξ1m)+(x2 − ξ2m)+\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "m\n",
      "\n",
      "β1m(x1 − ξ1m)+ +\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "m\n",
      "\n",
      "β2m(x2 − ξ2m)+\n",
      "\n",
      "(15.104)\n",
      "\n",
      "(15.105)\n",
      "\n",
      "m\n",
      "\n",
      "It is clear that for high-dimensional data, we cannot allow higher-order interactions, because there will be too many parameters to ﬁt. One approach to this problem is to use a search procedure to look for useful interaction terms. This is known as MARS, which stands for “multivariate adaptive regression splines”. See Section 16.3.3 for details.\n",
      "\n",
      "15.4.7\n",
      "\n",
      "RKHS methods compared to GPs *\n",
      "\n",
      "We can generalize the idea of penalizing derivatives of functions, as used in smoothing splines, to ﬁt functions with a more general notion of smoothness. Recall from Section 14.2.3 that\n",
      "\n",
      "15.4. Connection with other methods\n",
      "\n",
      "539\n",
      "\n",
      "Mercer’s theorem says that any positive deﬁnite kernel function can be represented in terms of eigenfunctions:\n",
      "\n",
      "κ(x, x(cid:4)) =\n",
      "\n",
      "∞(cid:2)\n",
      "\n",
      "λiφi(x)φi(x(cid:4))\n",
      "\n",
      "(15.106)\n",
      "\n",
      "i=1\n",
      "\n",
      "The φi form an orthormal basis for a function space:\n",
      "\n",
      "Hk = {f : f (x) =\n",
      "\n",
      "∞(cid:2)\n",
      "\n",
      "fiφi(x),\n",
      "\n",
      "∞(cid:2)\n",
      "\n",
      "f 2 i /λi < ∞}\n",
      "\n",
      "(15.107)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "Now deﬁne the inner product between two functions f (x) = (cid:10)∞\n",
      "\n",
      "i=1 giφi(x) in this space as follows:\n",
      "\n",
      "(cid:10)∞\n",
      "\n",
      "i=1 fiφi(x) and g(x) =\n",
      "\n",
      "(cid:14)f, g(cid:15)H (cid:2)\n",
      "\n",
      "∞(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "figi λi\n",
      "\n",
      "(15.108)\n",
      "\n",
      "In Exercise 15.1, we show that this deﬁnition implies that\n",
      "\n",
      "(cid:14)κ(x1, ·), κ(x2, ·)(cid:15)H = κ(x1, x2)\n",
      "\n",
      "(15.109)\n",
      "\n",
      "This is called the reproducing property, and the space of functions Hk is called a reproducing kernel Hilbert space or RKHS.\n",
      "\n",
      "Now consider an optimization problem of the form\n",
      "\n",
      "J(f ) =\n",
      "\n",
      "1 2σ2 y\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "(yi − f (xi))2 +\n",
      "\n",
      "i=1\n",
      "\n",
      "1 2\n",
      "\n",
      "||f ||2 H\n",
      "\n",
      "(15.110)\n",
      "\n",
      "where ||f ||J is the norm of a function:\n",
      "\n",
      "||f ||H = (cid:14)f, f (cid:15)H =\n",
      "\n",
      "∞(cid:2)\n",
      "\n",
      "i=1\n",
      "\n",
      "f 2 i λi\n",
      "\n",
      "(15.111)\n",
      "\n",
      "The intuition is that functions that are complex wrt the kernel will have large norms, because they will need many eigenfunctions to represent them. We want to pick a simple function that provides a good ﬁt to the data.\n",
      "\n",
      "One can show (see e.g., (Schoelkopf and Smola 2002)) that the solution must have the form\n",
      "\n",
      "f (x) =\n",
      "\n",
      "N(cid:2)\n",
      "\n",
      "αiκ(x, xi)\n",
      "\n",
      "(15.112)\n",
      "\n",
      "i=1\n",
      "\n",
      "This is known as the representer theorem, and holds for other convex loss functions besides squared error.\n",
      "\n",
      "We can solve for the α by substituting in f (x) =\n",
      "\n",
      "(cid:10)N\n",
      "\n",
      "i=1 αiκ(x, xi) and using the reproducing\n",
      "\n",
      "property to get\n",
      "\n",
      "J(α) =\n",
      "\n",
      "1 2σ2 y\n",
      "\n",
      "|y − Kα|2 +\n",
      "\n",
      "1 2\n",
      "\n",
      "αT Kα\n",
      "\n",
      "(15.113)\n",
      "\n",
      "540\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "Minimizing wrt α we ﬁnd\n",
      "\n",
      "ˆα = (K + σ2\n",
      "\n",
      "yI)−1\n",
      "\n",
      "(15.114)\n",
      "\n",
      "and hence\n",
      "\n",
      "ˆf (x∗) =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "ˆαiκ(x∗, xi) = kT\n",
      "\n",
      "∗ (K + σ2\n",
      "\n",
      "yI)−1y\n",
      "\n",
      "(15.115)\n",
      "\n",
      "i\n",
      "\n",
      "This is identical to Equation 15.18, the posterior mean of a GP predictive distribution. Indeed, since the mean and mode of a Gaussian are the same, we can see that linear regresson with an RKHS regularizer is equivalent to MAP estimation with a GP. An analogous statement holds for the GP logistic regression case, which also uses a convex likelihood / loss function.\n",
      "\n",
      "15.5\n",
      "\n",
      "GP latent variable model\n",
      "\n",
      "In Section 14.4.4, we discussed kernel PCA, which applies the kernel trick to regular PCA. In this section, we discuss a different way to combine kernels with probabilistic PCA. The resulting method is known as the GP-LVM, which stands for “Gaussian process latent variable model” (Lawrence 2005).\n",
      "\n",
      "To explain the method, we start with PPCA. Recall from Section 12.2.4 that the PPCA model is\n",
      "\n",
      "as follows:\n",
      "\n",
      "p(zi) =N (zi|0, I) p(yi|zi, θ) =N (yi|Wzi, σ2I)\n",
      "\n",
      "(15.116)\n",
      "\n",
      "(15.117)\n",
      "\n",
      "We can ﬁt this model by maximum likelihood, by integrating out the zi and maximizing W (and σ2). The objective is given by\n",
      "\n",
      "p(Y|W, σ2) = (2π)−DN/2|C|−N/2 exp\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "tr(C−1YT Y)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(15.118)\n",
      "\n",
      "where C = WWT + σ2I. As we showed in Theorem 12.2.2, the MLE for this can be computed in terms of the eigenvectors of YT Y.\n",
      "\n",
      "use a prior of the form p(W) =\n",
      "\n",
      "Now we consider the dual problem, whereby we maximize Z and integrate out W. We will\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "j N (wj|0, I). The corresponding likelihood becomes\n",
      "\n",
      "p(Y|Z, σ2) =\n",
      "\n",
      "= (2π)−DN/2|Kz|−D/2 exp\n",
      "\n",
      "d=1\n",
      "\n",
      "D(cid:6)\n",
      "\n",
      "N (y:,d|0, ZZT + σ2I) (cid:3)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "tr(K−1\n",
      "\n",
      "z YYT )\n",
      "\n",
      "(15.119)\n",
      "\n",
      "(15.120)\n",
      "\n",
      "where Kz = ZZT + σ2I. Based on our discussion of the connection between the eigenvalues of YYT and of YT Y in Section 14.4.4, it should come as no surprise that we can also solve the dual problem using eigenvalue methods (see (Lawrence 2005) for the details).\n",
      "\n",
      "If we use a linear kernel, we recover PCA. But we can also use a more general kernel: Kz = K + σ2I, where K is the Gram matrix for Z. The MLE for ˆZ will no longer be available\n",
      "\n",
      "15.5. GP latent variable model\n",
      "\n",
      "541\n",
      "\n",
      "0.3\n",
      "\n",
      ".25\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      ".15\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      ".05\n",
      "\n",
      "0.4\n",
      "\n",
      "0\n",
      "\n",
      "0.6\n",
      "\n",
      "0\n",
      "\n",
      "0.05\n",
      "\n",
      "0.1\n",
      "\n",
      "0.15\n",
      "\n",
      "0.2\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 15.10 2d representation of 12 dimensional oil ﬂow data. The different colors/symbols represent (b) GP-LVM with Gaussian kernel. The the 3 phases of oil ﬂow. shading represents the precision of the posterior, where lighter pixels have higher precision. From Figure 1 of (Lawrence 2005). Used with kind permission of Neil Lawrence.\n",
      "\n",
      "(a) Kernel PCA with Gaussian kernel.\n",
      "\n",
      "via eigenvalue methods; instead we must use gradient-based optimization. The objective is given by\n",
      "\n",
      "(cid:2) = −\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "log |Kz| − 1 2\n",
      "\n",
      "tr(K−1\n",
      "\n",
      "z YYT )\n",
      "\n",
      "(15.121)\n",
      "\n",
      "and the gradient is given by\n",
      "\n",
      "∂(cid:2) ∂Zij\n",
      "\n",
      "=\n",
      "\n",
      "∂(cid:2) ∂Kz\n",
      "\n",
      "∂Kz ∂Zij\n",
      "\n",
      "(15.122)\n",
      "\n",
      "where\n",
      "\n",
      "∂(cid:2) ∂Kz\n",
      "\n",
      "= K−1\n",
      "\n",
      "z YYT K−1\n",
      "\n",
      "z − DK−1 z\n",
      "\n",
      "(15.123)\n",
      "\n",
      "The form of ∂Kz where Kz = ZZT + σ2I, we have ∂Kz optimizer, such as conjugate gradient descent.\n",
      "\n",
      "∂Zij will of course depend on the kernel used. (For example, with a linear kernel, ∂Z = Z.) We can then pass this gradient to any standard\n",
      "\n",
      "Let us now compare GP-LVM to kernel PCA. In kPCA, we learn a kernelized mapping from the observed space to the latent space, whereas in GP-LVM, we learn a kernelized mapping from the latent space to the observed space. Figure 15.10 illustrates the results of applying kPCA and GP-LVM to visualize the 12 dimensional oil ﬂow data shown in In Figure 14.9(a). We see that the If we perform nearest neighbor classiﬁcation in embedding produced by GP-LVM is far better. the latent space, GP-LVM makes 4 errors, while kernel PCA (with the same kernel but separately optimized hyper-parameters) makes 13 errors, and regular PCA makes 20 errors.\n",
      "\n",
      "GP-LVM inherits the usual advantages of probabilistic generative models, such as the ability to handle missing data and data of different types, the ability to use gradient-based methods (instead of grid search) to tune the kernel parameters, the ability to handle prior information,\n",
      "\n",
      "542\n",
      "\n",
      "Chapter 15. Gaussian processes\n",
      "\n",
      "etc. For a discussion of some other probabilistic methods for (spectral) dimensionality reduction, see (Lawrence 2012).\n",
      "\n",
      "15.6\n",
      "\n",
      "Approximation methods for large datasets\n",
      "\n",
      "The principal drawback of GPs is that they take O(N 3) time to use. This is because of the need to invert (or compute the Cholesky decomposition of) the N × N kernel matrix K. A variety of approximation methods have been devised which take O(M 2N ) time, where M is a user-speciﬁable parameter. For details, see (Quinonero-Candela et al. 2007).\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 15.1 Reproducing property Prove Equation 15.109.\n",
      "\n",
      "16 Adaptive basis function models\n",
      "\n",
      "16.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In Chapters 14 and 15, we discussed kernel methods, which provide a powerful way to create non- linear models for regression and classiﬁcation. The prediction takes the form f (x) =w T φ(x), where we deﬁne\n",
      "\n",
      "φ(x) = [κ(x, μ1), . . . , κ(x, μN )]\n",
      "\n",
      "(16.1)\n",
      "\n",
      "and where μk are either all the training data or some subset. Models of this form essen- tially perform a form of template matching, whereby they compare the input x to the stored prototypes μk.\n",
      "\n",
      "it relies on having a good kernel function to measure the similarity between data vectors. Often coming up with a good kernel function is quite difficult. For example, how do we deﬁne the similarity between two images? Pixel-wise comparison of intensities (which is what a Gaussian kernel corresponds to) does not work well. Although it is possible (and indeed common) to hand-engineer kernels for speciﬁc tasks (see e.g., the pyramid match kernel in Section 14.2.7), it would be more interesting if we could learn the kernel.\n",
      "\n",
      "Although this can work well,\n",
      "\n",
      "In Section 15.2.4, we discussed a way to learn the parameters of a kernel function, by maxi-\n",
      "\n",
      "mizing the marginal likelihood. For example, if we use the ARD kernel,\n",
      "\n",
      "κ(x, x(cid:2)) = θ0 exp\n",
      "\n",
      "⎛ ⎝− 1 2\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "j=1\n",
      "\n",
      "θj(xj − x(cid:2)\n",
      "\n",
      "j)2\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "(16.2)\n",
      "\n",
      "we can can estimate the θj, and thus perform a form of nonlinear feature selection. However, such methods can be computationally expensive. Another approach, known as multiple kernel learning (see e.g., (Rakotomamonjy et al. 2008)) uses a convex combination of base kernels, κ(x, x(cid:2)) = j wjκj(x, x(cid:2)), and then estimates the mixing weights wj. But this relies on having good base kernels (and is also computationally expensive).\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "An alternative approach is to dispense with kernels altogether, and try to learn useful features φ(x) directly from the input data. That is, we will create what we call an adaptive basis- function model (ABM), which is a model of the form\n",
      "\n",
      "f (x) = w0 +\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "wmφm(x)\n",
      "\n",
      "(16.3)\n",
      "\n",
      "m=1\n",
      "\n",
      "544\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "where φm(x) is the m’th basis function, which is learned from data. This framework covers all of the models we will discuss in this chapter.\n",
      "\n",
      "Typically the basis functions are parametric, so we can write φm(x) =φ (x; vm), where vm are the parameters of the basis function itself. We will use θ = (w0, w1:M , {vm}M m=1) to denote the entire parameter set. The resulting model is not linear-in-the-parameters anymore, so we will only be able to compute a locally optimal MLE or MAP estimate of θ. Nevertheless, such models often signiﬁcantly outperform linear models, as we will see.\n",
      "\n",
      "16.2\n",
      "\n",
      "Classiﬁcation and regression trees (CART)\n",
      "\n",
      "Classiﬁcation and regression trees or CART models, also called decision trees (not to be confused with the decision trees used in decision theory) are deﬁned by recursively partitioning the input space, and deﬁning a local model in each resulting region of input space. This can be represented by a tree, with one leaf per region, as we explain below.\n",
      "\n",
      "16.2.1\n",
      "\n",
      "Basics\n",
      "\n",
      "To explain the CART approach, consider the tree in Figure 16.1(a). The ﬁrst node asks if x1 is less than some threshold t1. If yes, we then ask if x2 is less than some other threshold t2. If yes, we are in the bottom left quadrant of space, R1. If no, we ask if x1 is less than t3. And so on. The result of these axis parallel splits is to partition 2d space into 5 regions, as shown in Figure 16.1(b). We can now associate a mean response with each of these regions, resulting in the piecewise constant surface shown in Figure 16.1(c).\n",
      "\n",
      "We can write the model in the following form\n",
      "\n",
      "f (x) = E [y|x] =\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "wmI(x ∈ Rm) =\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "wmφ(x; vm)\n",
      "\n",
      "(16.4)\n",
      "\n",
      "m=1\n",
      "\n",
      "m=1\n",
      "\n",
      "where Rm is the m’th region, wm is the mean response in this region, and vm encodes the choice of variable to split on, and the threshold value, on the path from the root to the m’th leaf. This makes it clear that a CART model is just a an adaptive basis-function model, where the basis functions deﬁne the regions, and the weights specify the response value in each region. We discuss how to ﬁnd these basis functions below.\n",
      "\n",
      "We can generalize this to the classiﬁcation setting by storing the distribution over class labels in each leaf, instead of the mean response. This is illustrated in Figure 16.2. This model can be used to classify the data in Figure 1.1. For example, we ﬁrst check the color of the object. If it is blue, we follow the left branch and end up in a leaf labeled “4,0”, which means we have 4 positive examples and 0 negative examples which match this criterion. Hence we predict p(y = 1|x) = 4/4 if x is blue. if it is an ellipse, we end up in a leaf labeled “1,1”, so we predict p(y = 1|x) = 1/2. If it is red but not an ellipse, we predict p(y = 1|x) = 0/2; If it is some other colour, we check the size: if less than 10, we predict p(y = 1|x) = 4/4, otherwise p(y = 1|x) = 0/5. These probabilities are just the empirical fraction of positive examples that satisfy each conjunction of feature values, which deﬁnes a path from the root to a leaf.\n",
      "\n",
      "If it is red, we then check the shape:\n",
      "\n",
      "16.2. Classiﬁcation and regression trees (CART)\n",
      "\n",
      "545\n",
      "\n",
      "X1 ≤ t1\n",
      "\n",
      "10\n",
      "\n",
      "9\n",
      "\n",
      "8\n",
      "\n",
      "X2 ≤ t2\n",
      "\n",
      "X2 ≤ t4\n",
      "\n",
      "7\n",
      "\n",
      "6\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "R1\n",
      "\n",
      "X1 ≤ t3\n",
      "\n",
      "R2\n",
      "\n",
      "R3\n",
      "\n",
      "3\n",
      "\n",
      "2 10\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "R4\n",
      "\n",
      "R5\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 16.1 A simple regression tree on two inputs. Based on Figure 9.2 of (Hastie et al. 2009). Figure generated by regtreeSurfaceDemo.\n",
      "\n",
      "blue\n",
      "\n",
      "color\n",
      "\n",
      "red\n",
      "\n",
      "other\n",
      "\n",
      "4,0\n",
      "\n",
      "ellipse\n",
      "\n",
      "shape\n",
      "\n",
      "other\n",
      "\n",
      "size < 10 yes no\n",
      "\n",
      "1,1\n",
      "\n",
      "0,2\n",
      "\n",
      "4,0\n",
      "\n",
      "0,5\n",
      "\n",
      "Figure 16.2 A simple decision tree for the data in Figure 1.1. A leaf labeled as (n1, n0) means that there are n1 positive examples that match this path, and n0 negative examples. In this tree, most of the leaves are “pure”, meaning they only have examples of one class or the other; the only exception is leaf representing red ellipses, which has a label distribution of (1, 1). We could distinguish positive from negative red ellipses by adding a further test based on size. However, it is not always desirable to construct trees that perfectly model the training data, due to overﬁtting.\n",
      "\n",
      "16.2.2\n",
      "\n",
      "Growing a tree\n",
      "\n",
      "Finding the optimal partitioning of the data is NP-complete (Hyaﬁl and Rivest 1976), so it is common to use the greedy procedure shown in Algorithm 6 to compute a locally optimal MLE. This method is used by CART, (Breiman et al. 1984) C4.5(Quinlan 1993), and ID3 (Quinlan 1986), (See dtfit for a simple Matlab which are three popular implementations of the method. implementation.)\n",
      "\n",
      "The split function chooses the best feature, and the best value for that feature, as follows:\n",
      "\n",
      "(j∗, t∗) = arg min\n",
      "\n",
      "j∈{1,...,D}\n",
      "\n",
      "min t∈Tj\n",
      "\n",
      "cost({xi, yi : xij ≤ t}) +cost( {xi, yi : xij > t})\n",
      "\n",
      "(16.5)\n",
      "\n",
      "546\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "Algorithm 16.1: Recursive procedure to grow a classiﬁcation/ regression tree 1 function ﬁtTree(node, D, depth) ; 2 node.prediction = mean(yi : i ∈ D) // or class label distribution ; 3 (j∗, t∗, DL, DR) = split(D); 4 if not worthSplitting(depth, cost, DL, DR) then 5\n",
      "\n",
      "return node\n",
      "\n",
      "10\n",
      "\n",
      "6 else 7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "node.test = λx.xj∗ < t∗ // anonymous function; node.left = ﬁtTree(node, DL, depth+1); node.right = ﬁtTree(node, DR, depth+1); return node;\n",
      "\n",
      "where the cost function for a given dataset will be deﬁned below. For notational simplicity, we have assumed all inputs are real-valued or ordinal, so it makes sense to compare a feature xij to a numeric value t. The set of possible thresholds Tj for feature j can be obtained by sorting the unique values of xij. For example, if feature 1 has the values {4.5, −12, 72, −12}, then we set T1 = {−12, 4.5, 72}. In the case of categorical inputs, the most common approach is to consider splits of the form xij = ck and xij (cid:4)= ck, for each possible class label ck. Although we could allow for multi-way splits (resulting in non-binary trees), this would result in data fragmentation, meaning too little data might “fall” into each subtree, resulting in overﬁtting.\n",
      "\n",
      "The function that checks if a node is worth splitting can use several stopping heuristics, such\n",
      "\n",
      "as the following:\n",
      "\n",
      "\n",
      "\n",
      "is the reduction in cost too small? Typically we deﬁne the gain of using a feature to be a normalized measure of the reduction in cost: |DR| |D| cost(DR)\n",
      "\n",
      "Δ (cid:2) cost(D) −\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "|DL| |D| cost(DL) +\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(16.6)\n",
      "\n",
      "has the tree exceeded the maximum desired depth? •\n",
      "\n",
      "\n",
      "\n",
      "is the distribution of the response in either DL or DR sufficiently homogeneous (e.g., all labels are the same, so the distribution is pure)? is the number of examples in either DL or DR too small?\n",
      "\n",
      "All that remains is to specify the cost measure used to evaluate the quality of a proposed split. This depends on whether our goal is regression or classiﬁcation. We discuss both cases below.\n",
      "\n",
      "16.2.2.1\n",
      "\n",
      "Regression cost\n",
      "\n",
      "In the regression setting, we deﬁne the cost as follows:\n",
      "\n",
      "cost(D) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(yi − y)2\n",
      "\n",
      "(16.7)\n",
      "\n",
      "i∈D\n",
      "\n",
      "16.2. Classiﬁcation and regression trees (CART)\n",
      "\n",
      "547\n",
      "\n",
      "where y = 1 i∈D yi is the mean of the response variable in the speciﬁed set of data. |D| Alternatively, we can ﬁt a linear regression model for each leaf, using as inputs the features that were chosen on the path from the root, and then measure the residual error.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "16.2.2.2\n",
      "\n",
      "Classiﬁcation cost\n",
      "\n",
      "In the classiﬁcation setting, there are several ways to measure the quality of a split. First, we ﬁt a multinoulli model to the data in the leaf satisfying the test Xj < t by estimating the class-conditional probabilities as follows:\n",
      "\n",
      "ˆπc =\n",
      "\n",
      "1 |D|\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i∈D\n",
      "\n",
      "I(yi = c)\n",
      "\n",
      "(16.8)\n",
      "\n",
      "where D is the data in the leaf. Given this, there are several common error measures for evaluating a proposed partition:\n",
      "\n",
      "Misclassiﬁcation rate. We deﬁne the most probable class label as ˆyc = argmaxc ˆπc. The\n",
      "\n",
      "corresponding error rate is then\n",
      "\n",
      "1 |D|\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i∈D\n",
      "\n",
      "I(yi (cid:4)= ˆy) = 1 − ˆπˆy\n",
      "\n",
      "(16.9)\n",
      "\n",
      "Entropy, ordeviance:\n",
      "\n",
      "H ( ˆπ) = −\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "ˆπc log ˆπc\n",
      "\n",
      "(16.10)\n",
      "\n",
      "c=1\n",
      "\n",
      "Note that minimizing the entropy is equivalent to maximizing the information gain (Quinlan 1986) between test Xj < t and the class label Y , deﬁned by\n",
      "\n",
      "infoGain(Xj < t, Y ) (cid:2) H (Y ) − H (Y |Xj < t)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "−\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(y = c) log p(y = c)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(16.11)\n",
      "\n",
      "(16.12)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "c (cid:4)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "p(y = c|Xj < t) log p(c|Xj < t)\n",
      "\n",
      "(16.13)\n",
      "\n",
      "c\n",
      "\n",
      "since ˆπc is an MLE for the distribution p(c|Xj < t).1\n",
      "\n",
      "1. If Xj is categorical, and we use tests of the form Xj = k, then taking expectations over values of Xj gives k p(Xj = k)infoGain(Xj = k, Y ) = the mutual H (Y ) − H (Y |Xj ) = I (Y ; Xj ).\n",
      "\n",
      "information between Xj and Y : E [infoGain(Xj , Y )] =\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "548\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "0.5\n",
      "\n",
      "0.45\n",
      "\n",
      "Error rate Gini Entropy\n",
      "\n",
      "0.4\n",
      "\n",
      "0.35\n",
      "\n",
      "0.3\n",
      "\n",
      "0.25\n",
      "\n",
      "0.2\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "Figure 16.3 Node impurity measures for binary classiﬁcation. The horizontal axis corresponds to p, the probability of class 1. The entropy measure has been rescaled to pass through (0.5,0.5). Based on Figure 9.3 of (Hastie et al. 2009). Figure generated by giniDemo.\n",
      "\n",
      "Gini index\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "ˆπc(1 − ˆπc) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ˆπc −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ˆπ2 c = 1 −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ˆπ2 c\n",
      "\n",
      "(16.14)\n",
      "\n",
      "c=1\n",
      "\n",
      "c\n",
      "\n",
      "c\n",
      "\n",
      "c\n",
      "\n",
      "This is the expected error rate. To see this, note that ˆπc is the probability a random entry in the leaf belongs to class c, and (1 − ˆπc is the probability it would be misclassiﬁed.\n",
      "\n",
      "In the two-class case, where p = πm(1), the misclassiﬁcation rate is 1 − max(p, 1 − p), the entropy is H2(p), and the Gini index is 2p(1 − p). These are plotted in Figure 16.3. We see that the cross-entropy and Gini measures are very similar, and are more sensitive to changes in class probability than is the misclassiﬁcation rate. For example, consider a two-class problem with 400 cases in each class. Suppose one split created the nodes (300,100) and (100,300), while the other created the nodes (200,400) and (200,0). Both splits produce a misclassiﬁcation rate of 0.25. However, the latter seems preferable, since one of the nodes is pure, i.e., it only contains one class. The cross-entropy and Gini measures will favor this latter choice.\n",
      "\n",
      "16.2.2.3\n",
      "\n",
      "Example\n",
      "\n",
      "As an example, consider two of the four features from the 3-class iris dataset, shown in Fig- ure 16.4(a). The resulting tree is shown in Figure 16.5(a), and the decision boundaries are shown in Figure 16.4(b). We see that the tree is quite complex, as are the resulting decision boundaries. In Figure 16.5(b), we show that the CV estimate of the error is much higher than the training set error, indicating overﬁtting. Below we discuss how to perform a tree-pruning stage to simplify the tree.\n",
      "\n",
      "16.2. Classiﬁcation and regression trees (CART)\n",
      "\n",
      "549\n",
      "\n",
      "4.5\n",
      "\n",
      "4\n",
      "\n",
      "setosa versicolor virginica\n",
      "\n",
      "4.5\n",
      "\n",
      "4\n",
      "\n",
      "unpruned decision tree\n",
      "\n",
      "versicolor setosa virginica\n",
      "\n",
      "h t d w\n",
      "\n",
      "i\n",
      "\n",
      "3.5\n",
      "\n",
      "3.5\n",
      "\n",
      "l\n",
      "\n",
      "a p e S\n",
      "\n",
      "3\n",
      "\n",
      "y\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "2.5\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "4.5\n",
      "\n",
      "5\n",
      "\n",
      "5.5\n",
      "\n",
      "6 Sepal length\n",
      "\n",
      "6.5\n",
      "\n",
      "7\n",
      "\n",
      "7.5\n",
      "\n",
      "8\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "4.5\n",
      "\n",
      "5\n",
      "\n",
      "5.5\n",
      "\n",
      "6 x\n",
      "\n",
      "6.5\n",
      "\n",
      "7\n",
      "\n",
      "7.5\n",
      "\n",
      "8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 16.4 petal length and petal width. (b) Decision boundaries induced by the decision tree in Figure 16.5(a).\n",
      "\n",
      "(a) Iris data. We only show the ﬁrst two features, sepal length and sepal width, and ignore\n",
      "\n",
      "0.8\n",
      "\n",
      "W < 2.8\n",
      "\n",
      "SW >= 2.8\n",
      "\n",
      "SL < 5.45\n",
      "\n",
      "SL >= 5.45\n",
      "\n",
      "SL < 6.15\n",
      "\n",
      "SL >= 6.15\n",
      "\n",
      "0.7\n",
      "\n",
      "Cross−validation Training set Min + 1 std. err. Best choice\n",
      "\n",
      "versicolorsetosa\n",
      "\n",
      "SW < 3.45\n",
      "\n",
      "SW >= 3.45\n",
      "\n",
      "SL < 5.75\n",
      "\n",
      "SL >= 5.75\n",
      "\n",
      "setosa\n",
      "\n",
      "versicolor\n",
      "\n",
      "SW < 3.1\n",
      "\n",
      "SW >= 3.1\n",
      "\n",
      "versicolor\n",
      "\n",
      "SW < 2.95\n",
      "\n",
      "SW >= 2.95 versicolor\n",
      "\n",
      "versicolorvirginica\n",
      "\n",
      "SL < 7.05\n",
      "\n",
      "SL >= 7.05\n",
      "\n",
      "SW < 2.4\n",
      "\n",
      "SW >= 2.4\n",
      "\n",
      "virginica\n",
      "\n",
      "SL < 6.95\n",
      "\n",
      "SL >= 6.95\n",
      "\n",
      "SW < 3.15\n",
      "\n",
      "SW >= 3.15\n",
      "\n",
      "versicolor\n",
      "\n",
      "SL < 6.55\n",
      "\n",
      "SL >= 6.55\n",
      "\n",
      "virginica\n",
      "\n",
      ") r o r r e\n",
      "\n",
      "n o i t a c i f i s s a c s m\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "(\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "SW < 2.95\n",
      "\n",
      "SW >= 2.95\n",
      "\n",
      "SL < 6.65\n",
      "\n",
      "SL >= 6.65\n",
      "\n",
      "SL < 6.45\n",
      "\n",
      "SL >= 6.45\n",
      "\n",
      "virginicaversicolor\n",
      "\n",
      "SW < 2.65\n",
      "\n",
      "SW >= 2.65\n",
      "\n",
      "t s o C\n",
      "\n",
      "0.3\n",
      "\n",
      "SW < 2.85\n",
      "\n",
      "SW >= 2.85 versicolor\n",
      "\n",
      "virginica\n",
      "\n",
      "SW < 2.9\n",
      "\n",
      "SW >= 2.9\n",
      "\n",
      "0.2\n",
      "\n",
      "virginicaversicolor\n",
      "\n",
      "versicolorvirginica\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10 Number of terminal nodes\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 16.5 tree. Figure generated by dtreeDemoIris.\n",
      "\n",
      "(a) Unpruned decision tree for Iris data.\n",
      "\n",
      "(b) Plot of misclassiﬁcation error rate vs depth of\n",
      "\n",
      "16.2.3\n",
      "\n",
      "Pruning a tree\n",
      "\n",
      "To prevent overﬁtting, we can stop growing the tree if the decrease in the error is not sufficient to justify the extra complexity of adding an extra subtree. However, this tends to be too myopic. For example, on the xor data in Figure 14.2(c), it would might never make any splits, since each feature on its own has little predictive power.\n",
      "\n",
      "The standard approach is therefore to grow a “full” tree, and then to perform pruning. This can be done using a scheme that prunes the branches giving the least increase in the error. See (Breiman et al. 1984) for details.\n",
      "\n",
      "To determine how far to prune back, we can evaluate the cross-validated error on each such subtree, and then pick the tree whose CV error is within 1 standard error of the minimum. This is illustrated in Figure 16.4(b). The point with the minimum CV error corresponds to the simple tree in Figure 16.6(a).\n",
      "\n",
      "550\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "pruned decision tree\n",
      "\n",
      "4.5\n",
      "\n",
      "SL < 5.45\n",
      "\n",
      "SL >= 5.45\n",
      "\n",
      "versicolor setosa virginica\n",
      "\n",
      "4\n",
      "\n",
      "SW < 2.8\n",
      "\n",
      "SW >= 2.8\n",
      "\n",
      "SL < 6.15\n",
      "\n",
      "SL >= 6.15\n",
      "\n",
      "3.5\n",
      "\n",
      "y\n",
      "\n",
      "versicolor\n",
      "\n",
      "setosa\n",
      "\n",
      "SW < 3.45\n",
      "\n",
      "SW >= 3.45\n",
      "\n",
      "virginica\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "versicolor\n",
      "\n",
      "setosa\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "4.5\n",
      "\n",
      "5\n",
      "\n",
      "5.5\n",
      "\n",
      "6 x\n",
      "\n",
      "6.5\n",
      "\n",
      "7\n",
      "\n",
      "7.5\n",
      "\n",
      "8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 16.6 Pruned decision tree for Iris data. Figure generated by dtreeDemoIris.\n",
      "\n",
      "16.2.4\n",
      "\n",
      "Pros and cons of trees\n",
      "\n",
      "CART models are popular for several reasons: they are easy to interpret2, they can easily handle mixed discrete and continuous inputs, they are insensitive to monotone transformations of the inputs (because the split points are based on ranking the data points), they perform automatic variable selection, they are relatively robust to outliers, they scale well to large data sets, and they can be modiﬁed to handle missing inputs.3\n",
      "\n",
      "However, CART models also have some disadvantages. The primary one is that they do not predict very accurately compared to other kinds of model. This is in part due to the greedy nature of the tree construction algorithm. A related problem is that trees are unstable: small changes to the input data can have large effects on the structure of the tree, due to the hierarchical nature of the tree-growing process, causing errors at the top to affect the rest of the tree. In frequentist terminology, we say that trees are high variance estimators. We discuss a solution to this below.\n",
      "\n",
      "16.2.5\n",
      "\n",
      "Random forests\n",
      "\n",
      "One way to reduce the variance of an estimate is to average together many estimates. For example, we can train M different trees on different subsets of the data, chosen randomly with\n",
      "\n",
      "2. We can postprocess the tree to derive a series of logical rules such as “If x1 < 5.45 then ...” (Quinlan 1990). 3. The standard heuristic for handling missing inputs in decision trees is to look for a series of ”backup” variables, which can induce a similar partition to the chosen variable at any given split; these can be used in case the chosen variable is unobserved at test time. These are called surrogate splits. This method ﬁnds highly correlated features, and can be thought of as learning a local joint model of the input. This has the advantage over a generative model of not modeling the entire joint distribution of inputs, but it has the disadvantage of being entirely ad hoc. A simpler approach, applicable to categorical variables, is to code “missing” as a new value, and then to treat the data as fully observed.\n",
      "\n",
      "16.2. Classiﬁcation and regression trees (CART)\n",
      "\n",
      "551\n",
      "\n",
      "replacement, and then compute the ensemble\n",
      "\n",
      "f (x) =\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "m=1\n",
      "\n",
      "1 M\n",
      "\n",
      "fm(x)\n",
      "\n",
      "(16.15)\n",
      "\n",
      "where fm is the m’th tree. This technique is called bagging (Breiman 1996), which stands for “bootstrap aggregating”.\n",
      "\n",
      "Unfortunately, simply re-running the same learning algorithm on different subsets of the data can result in highly correlated predictors, which limits the amount of variance reduction that is possible. The technique known as random forests (Breiman 2001a) tries to decorrelate the base learners by learning trees based on a randomly chosen subset of input variables, as well as a randomly chosen subset of data cases. Such models often have very good predictive accuracy (Caruana and Niculescu-Mizil 2006), and have been widely used in many applications (e.g., for body pose recognition using Microsoft’s popular kinect sensor (Shotton et al. 2011)).\n",
      "\n",
      "Bagging is a frequentist concept. It is also possible to adopt a Bayesian approach to learning trees. In particular, (Chipman et al. 1998; Denison et al. 1998; Wu et al. 2007) perform approximate inference over the space of trees (structure and parameters) using MCMC. This reduces the variance of the predictions. We can also perform Bayesian inference over the space of ensembles of trees, which tends to work much better. This is known as Bayesian adaptive regression trees or BART (Chipman et al. 2010). Note that the cost of these sampling-based Bayesian methods is comparable to the sampling-based random forest method. That is, both approaches are farily slow to train, but produce high quality classiﬁers.\n",
      "\n",
      "Unfortunately, methods that use multiple trees (whether derived from a Bayesian or frequen- tist standpoint) lose their nice interpretability properties. Fortunately, various post-processing measures can be applied, as discussed in Section 16.8.\n",
      "\n",
      "16.2.6\n",
      "\n",
      "CART compared to hierarchical mixture of experts *\n",
      "\n",
      "An interesting alternative to a decision tree is known as the hierarchical mixture of experts. Figure 11.7(b) gives an illustration where we have two levels of experts. This can be thought of as a probabilistic decision tree of depth 2, since we recursively partition the space, and apply (Hastie et al. 2009, p331) write that “The a different expert to each partition. Hastie et al. HME approach is a promising competitor to CART trees”. Some of the advantages include the following:\n",
      "\n",
      "The model can partition the input space using any set of nested linear decision boundaries.\n",
      "\n",
      "By contrast, standard decision trees are constrained to use axis-parallel splits.\n",
      "\n",
      "The model makes predictions by averaging over all experts. By contrast,\n",
      "\n",
      "in a standard decision tree, predictions are made only based on the model in the corresponding leaf. Since leaves often contain few training examples, this can result in overﬁtting.\n",
      "\n",
      "Fitting an HME involves solving a smooth continuous optimization problem (usually using EM), which is likely to be less prone to local optima than the standard greedy discrete optimization methods used to ﬁt decision trees. For similar reasons, it is computationally easier to “be Bayesian” about the parameters of an HME (see e.g., (Peng et al. 1996; Bishop\n",
      "\n",
      "552\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "and Svensén 2003)) than about the structure and parameters of a decision tree (see e.g., (Wu et al. 2007)).\n",
      "\n",
      "16.3\n",
      "\n",
      "Generalized additive models\n",
      "\n",
      "A simple way to create a nonlinear model with multiple inputs is to use a generalized additive model (Hastie and Tibshirani 1990), which is a model of the form\n",
      "\n",
      "f (x) = α + f1(x1) +· · · + fD(xD)\n",
      "\n",
      "(16.16)\n",
      "\n",
      "Here each fj can be modeled by some scatterplot smoother, and f (x) can be mapped to p(y|x) using a link function, as in a GLM (hence the term generalized additive model).\n",
      "\n",
      "If we use regression splines (or some other ﬁxed basis function expansion approach) for the fj, then each fj(xj) can be written as βT j φj(xj), so the whole model can be written as f (x) = βT φ(x), where φ(x) = [1, φ1(x1), . . . , φD(xD)]. However, it is more common to use smoothing splines (Section 15.4.6) for the fj. In this case, the objective (in the regression setting) becomes\n",
      "\n",
      "J(α, f1, . . . , fD) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝yi − α −\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "fj(xij)\n",
      "\n",
      "⎞ 2\n",
      "\n",
      "⎠\n",
      "\n",
      "+\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "λj\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "j (tj)2dtj f (cid:2)(cid:2)\n",
      "\n",
      "(16.17)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "j=1\n",
      "\n",
      "where λj is the strength of the regularizer for fj.\n",
      "\n",
      "16.3.1\n",
      "\n",
      "Backﬁtting\n",
      "\n",
      "We now discuss how to ﬁt the model using MLE. The constant α is not uniquely identiﬁable, since we can always add or subtract constants to any of the fj functions. The convention is to assume\n",
      "\n",
      "To ﬁt the rest of the model, we can center the responses (by subtracting ˆα), and then iteratively update each fj in turn, using as a target vector the residuals obtained by omitting term fj:\n",
      "\n",
      "ˆfj := smoother({yi −\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 fj(xij) = 0 for all j. In this case, the MLE for α is just ˆα = 1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ˆfk(xik)}N\n",
      "\n",
      "i=1)\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 yi.\n",
      "\n",
      "(16.18)\n",
      "\n",
      "k(cid:5)=j\n",
      "\n",
      "We should then ensure the output is zero mean using\n",
      "\n",
      "ˆfj := ˆfj − 1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "ˆfj(xij)\n",
      "\n",
      "(16.19)\n",
      "\n",
      "This is called the backﬁtting algorithm (Hastie and Tibshirani 1990). If X has full column rank, then the above objective is convex (since each smoothing spline is a linear operator, as shown in Section 15.4.2), so this procedure is guaranteed to converge to the global optimum.\n",
      "\n",
      "In the GLM case, we need to modify the method somewhat. The basic idea is to replace the weighted least squares step of IRLS (see Section 8.3.4) with a weighted backﬁtting algorithm. In the logistic regression case, each response has weight si = μi(1 − μi) associated with it, where μi = sigm(ˆα +\n",
      "\n",
      "(cid:7)D\n",
      "\n",
      "j=1\n",
      "\n",
      "ˆfj(xij)).)\n",
      "\n",
      "16.3. Generalized additive models\n",
      "\n",
      "553\n",
      "\n",
      "16.3.2\n",
      "\n",
      "Computational efficiency\n",
      "\n",
      "Each call to the smoother takes O(N ) time, so the total cost is O(N DT ), where T is the number of iterations. If we have high-dimensional inputs, ﬁtting a GAM is expensive. One approach is to combine it with a sparsity penalty, see e.g., the SpAM (sparse additive model) approach of (Ravikumar et al. 2009). Alternatively, we can use a greedy approach, such as boosting (see Section 16.4.6)\n",
      "\n",
      "16.3.3 Multivariate adaptive regression splines (MARS)\n",
      "\n",
      "We can extend GAMs by allowing for interaction effects. decomposition:\n",
      "\n",
      "In general, we can create an ANOVA\n",
      "\n",
      "f (x) = β0 +\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "fj(xj) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "fjk(xj, xk) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "fjkl(xj, xk, xl) +· · ·\n",
      "\n",
      "(16.20)\n",
      "\n",
      "j=1\n",
      "\n",
      "j,k\n",
      "\n",
      "j,k,l\n",
      "\n",
      "Of course, we cannot allow for too many higher-order interactions, because there will be too many parameters to ﬁt.\n",
      "\n",
      "It is common to use greedy search to decide which variables to add. The multivariate adaptive regression splines or MARS algorithm is one example of this (Hastie et al. 2009, Sec9.4). It ﬁts models of the form in Equation 16.20, where it uses a tensor product basis of regression splines to represent the multidimensional regression functions. For example, for 2d input, we might use\n",
      "\n",
      "f (x1, x2) =β 0 + (cid:4)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "m β2m(t2m − x2)+ +\n",
      "\n",
      "β1m(x1 − t1m)+\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "β12m(x1 − t1m)+(t2m − x2)+\n",
      "\n",
      "(16.21)\n",
      "\n",
      "m\n",
      "\n",
      "m\n",
      "\n",
      "To create such a function, we start with a set of candidate basis functions of the form\n",
      "\n",
      "C = {(xj − t)+, (t − xj)+ : t ∈ {x1j, . . . , xN j}, j = 1, . . . , D}\n",
      "\n",
      "(16.22)\n",
      "\n",
      "These are 1d linear splines where the knots are at all the observed values for that variable. We consider splines sloping up in both directions; this is called a reﬂecting pair. See Figure 16.7(a). Let M represent the current set of basis functions. We initialize by using M = {1}. We consider creating a new basis function pair by multplying an hm ∈ M with one of the reﬂecting pairs in C. For example, we might initially get\n",
      "\n",
      "f (x) = 25 − 4(x1 − 5)+ + 20(5 − x1)+\n",
      "\n",
      "(16.23)\n",
      "\n",
      "obtained by multiplying h0(x) = 1 with a reﬂecting pair involving x1 with knot t = 5. This pair is added to M. See Figure 16.7(b). At the next step, we might create a model such as\n",
      "\n",
      "f (x) = = 2 − 2(x1 − 5)+ + 3(5− x1)+\n",
      "\n",
      "−(x2 − 10)+ × (5 − x1) +− 1.2(10 − x2)+ × (5 − x1)+\n",
      "\n",
      "(16.24)\n",
      "\n",
      "obtained by multiplying (5−x1)+ from M by the new reﬂecting pair (x2−10)+ and (10−x2)+. This new function is shown in Figure 16.7(c).\n",
      "\n",
      "554\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "15\n",
      "\n",
      "120\n",
      "\n",
      "100\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "80\n",
      "\n",
      "0\n",
      "\n",
      "60\n",
      "\n",
      "−20\n",
      "\n",
      "40\n",
      "\n",
      "−40\n",
      "\n",
      "5\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "−60 20\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "−20\n",
      "\n",
      "−40\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(a) Linear spline function with a knot at 5. Solid blue: (x − 5)+. Dotted red: (5 − x)+. (b) A Figure 16.7 MARS model in 1d given by Equation 16.23. (c) A simple MARS model in 2d given by Equation 16.24. Figure generated by marsDemo.\n",
      "\n",
      "(We may impose an upper bound on the order of interactions.) Then we prune backwards, at each step eliminating the basis function that causes the smallest increase in the residual error, until the CV error stops improving.\n",
      "\n",
      "We proceed in this way until the model becomes very large.\n",
      "\n",
      "The whole procedure is closely related to CART. To see this, suppose we replace the piecewise linear basis functions by step functions I(xj > t) and I(xj < t). Multiplying by a pair of reﬂected step functions is equivalent to splitting a node. Now suppose we impose the constraint that once a variable is involved in a multiplication by a candidate term, that variable gets replaced by the interaction, so the original variable is no longer available. This ensures that a variable can not be split more than once, thus guaranteeing that the resulting model can be represented as a tree. In this case, the MARS growing strategy is the same as the CART growing strategy.\n",
      "\n",
      "16.4\n",
      "\n",
      "Boosting\n",
      "\n",
      "Boosting (Schapire and Freund 2012) is a greedy algorithm for ﬁtting adaptive basis-function models of the form in Equation 16.3, where the φm are generated by an algorithm called a weak learner or a base learner. The algorithm works by applying the weak learner sequentially to weighted versions of the data, where more weight is given to examples that were misclassiﬁed by earlier rounds.\n",
      "\n",
      "This weak learner can be any classiﬁcation or regression algorithm, but it is common to use a CART model. In 1998, the late Leo Breiman called boosting, where the weak learner is a shallow decision tree, the “best off-the-shelf classiﬁer in the world” (Hastie et al. 2009, p340). This is supported by an extensive empirical comparison of 10 different classiﬁers in (Caruana and Niculescu-Mizil 2006), who showed that boosted decision trees were the best both in terms of misclassiﬁcation error and in terms of producing well-calibrated probabilities, as judged by ROC curves. (The second best method was random forests, invented by Breiman; see Section 16.2.5.) By contrast, single decision trees performed very poorly.\n",
      "\n",
      "Boosting was originally derived in the computational learning theory literature (Schapire 1990; Freund and Schapire 1996), where the focus is binary classiﬁcation. In these papers, it was proved that one could boost the performance (on the training set) of any weak learner arbitrarily\n",
      "\n",
      "16.4. Boosting\n",
      "\n",
      "555\n",
      "\n",
      "0.16\n",
      "\n",
      "0.14\n",
      "\n",
      "train test\n",
      "\n",
      "0.12\n",
      "\n",
      "0.1\n",
      "\n",
      "0.08\n",
      "\n",
      "0.06\n",
      "\n",
      "0.04\n",
      "\n",
      "0.02\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "120\n",
      "\n",
      "140\n",
      "\n",
      "Figure 16.8 Performance of adaboost using a decision stump as a weak learner on the data in Figure 16.10. Training (solid blue) and test (dotted red) error vs number of iterations. Figure generated by boostingDemo, written by Richard Stapenhurst.\n",
      "\n",
      "high, provided the weak learner could always perform slightly better than chance. For example, in Figure 16.8, we plot the training and test error for boosted decision stumps on a 2d dataset shown in Figure 16.10. We see that the training set error rapidly goes to near zero. What is more surprising is that the test set error continues to decline even after the training set error has reached zero (although the test set error will eventually go up). Thus boosting is very resistant to overﬁtting. (Boosted decision stumps form the basis of a very successful face detector (Viola and Jones 2001), which was used to generate the results in Figure 1.6, and which is used in many digital cameras.)\n",
      "\n",
      "In view of its stunning empirical success, statisticians started to become interested in this method. Breiman (Breiman 1998) showed that boosting can be interpreted as a form of gradient descent in function space. This view was then extended in (Friedman et al. 2000), who showed how boosting could be extended to handle a variety of loss functions, including for regression, robust regression, Poisson regression, etc. In this section, we shall present this statistical inter- pretation of boosting, drawing on the reviews in (Buhlmann and Hothorn 2007) and (Hastie et al. 2009, ch10), which should be consulted for further details.\n",
      "\n",
      "16.4.1\n",
      "\n",
      "Forward stagewise additive modeling\n",
      "\n",
      "The goal of boosting is to solve the following optimization problem:\n",
      "\n",
      "min f\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(yi, f (xi))\n",
      "\n",
      "(16.25)\n",
      "\n",
      "and L(y, ˆy) is some loss function, and f is assumed to be an ABM model as in Equation 16.3. Common choices for the loss function are listed in Table 16.1.\n",
      "\n",
      "If we use squared error loss, the optimal estimate is given by f ∗(x) = argmin\n",
      "\n",
      "f (x)\n",
      "\n",
      "= Ey|x\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(Y − f (x))2\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "= E [Y |x]\n",
      "\n",
      "(16.26)\n",
      "\n",
      "556\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "Name Squared error Absolute error Exponential loss Logloss\n",
      "\n",
      "Loss 1 2 (yi − f (xi))2 |yi − f (xi)| exp(−˜yif (xi)) −˜yi exp(−˜yif (xi)) log(1 + e−˜yifi )\n",
      "\n",
      "Derivative yi − f (xi) sgn(yi − f (xi))\n",
      "\n",
      "yi − πi\n",
      "\n",
      "f ∗ E [y|xi] median(y|xi) Gradient boosting 2 log πi 1 1−πi 2 log πi 1−πi\n",
      "\n",
      "1\n",
      "\n",
      "Algorithm L2Boosting\n",
      "\n",
      "AdaBoost LogitBoost\n",
      "\n",
      "Some commonly used loss functions, their gradients, their population minimizers f ∗, and Table 16.1 some algorithms to minimize the loss. For binary classiﬁcation problems, we assume ˜yi ∈ {−1, +1}, yi ∈ {0, 1} and πi = sigm(2f (xi)). For regression problems, we assume yi ∈ R. Adapted from (Hastie et al. 2009, p360) and (Buhlmann and Hothorn 2007, p483).\n",
      "\n",
      "7\n",
      "\n",
      "0−1 logloss exp\n",
      "\n",
      "6\n",
      "\n",
      "5\n",
      "\n",
      "s s o\n",
      "\n",
      "l\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0 y− f\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "2\n",
      "\n",
      "Figure 16.9 Illustration of various loss functions for binary classiﬁcation. The horizontal axis is the margin yη, the vertical axis is the loss. The log loss uses log base 2. Figure generated by hingeLossPlot.\n",
      "\n",
      "as we showed in Section 5.7.1.3. Of course, this cannot be computed in practice since it requires knowing the true conditional distribution p(y|x). Hence this is sometimes called the population minimizer, where the expectation is interpreted in a frequentist sense. Below we will see that boosting will try to approximate this conditional expectation.\n",
      "\n",
      "Instead it is common to use logloss, which is a convex upper bound on 0-1 loss, as we showed in Section 6.5.5. In this case, one can show that the optimal estimate is given by\n",
      "\n",
      "For binary classiﬁcation, the obvious loss is 0-1 loss, but this is not differentiable.\n",
      "\n",
      "f ∗(x) =\n",
      "\n",
      "1 2\n",
      "\n",
      "log\n",
      "\n",
      "p(˜y = 1|x) p(˜y = −1|x)\n",
      "\n",
      "(16.27)\n",
      "\n",
      "where ˜y ∈ {−1, +1}. One can generalize this framework to the multiclass case, but we will not discuss that here.\n",
      "\n",
      "An alternative convex upper bound is exponential loss, deﬁned by\n",
      "\n",
      "L(˜y, f ) = exp(−˜yf )\n",
      "\n",
      "(16.28)\n",
      "\n",
      "See Figure 16.9 for a plot. This will have some computational advantages over the logloss, It turns out that the optimal estimate for this loss is also f ∗(x) = to be discussed below.\n",
      "\n",
      "16.4. Boosting\n",
      "\n",
      "557\n",
      "\n",
      "1 2 log zero:\n",
      "\n",
      "∂ ∂f (x)\n",
      "\n",
      "p(˜y=1|x) p(˜y=−1|x) . To see this, we can just set the derivative of the expected loss (for each x) to\n",
      "\n",
      "E\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "e−˜yf (x)|x\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "=\n",
      "\n",
      "∂ ∂f (x)\n",
      "\n",
      "[p(˜y = 1|x)e−f (x) + p(˜y = −1|x)ef (x)]\n",
      "\n",
      "(16.29)\n",
      "\n",
      "= −p(˜y = 1|x)e−f (x) + p(˜y = −1|x)ef (x)\n",
      "\n",
      "(16.30)\n",
      "\n",
      "= 0 ⇒\n",
      "\n",
      "p(˜y = 1|x) p(˜y = 1 − |x)\n",
      "\n",
      "= e2f (x)\n",
      "\n",
      "(16.31)\n",
      "\n",
      "So in both cases, we can see that boosting should try to approximate (half) the log-odds ratio. Since ﬁnding the optimal f is hard, we shall tackle it sequentially. We initialise by deﬁning\n",
      "\n",
      "f0(x) = arg min\n",
      "\n",
      "γ\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(yi, f (xi; γ))\n",
      "\n",
      "(16.32)\n",
      "\n",
      "For example, if we use squared error, we can set f0(x) = y, and if we use log-loss or exponential loss , we can set f0(x) = 1 i=1 I(yi = 1). We could also use a more powerful model for our baseline, such as a GLM.\n",
      "\n",
      "Then at iteration m, we compute\n",
      "\n",
      "2 log ˆπ\n",
      "\n",
      "1−ˆπ , where ˆπ = 1\n",
      "\n",
      "N\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "(βm, γm) = argmin\n",
      "\n",
      "β,γ\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(yi, fm−1(xi) +βφ (xi; γ))\n",
      "\n",
      "(16.33)\n",
      "\n",
      "and then we set\n",
      "\n",
      "fm(x) = fm−1(x) +β mφ(x; γm)\n",
      "\n",
      "(16.34)\n",
      "\n",
      "The key point is that we do not go back and adjust earlier parameters. This is why the method is called forward stagewise additive modeling.\n",
      "\n",
      "We continue this for a ﬁxed number of iterations M . In fact M is the main tuning parameter of the method. Often we pick it by monitoring the performance on a separate validation set, and then stopping once performance starts to decrease; this is called early stopping. Alternatively, we can use model selection criteria such as AIC or BIC (see e.g., (Buhlmann and Hothorn 2007) for details).\n",
      "\n",
      "In practice, better (test set) performance can be obtained by performing “partial updates” of\n",
      "\n",
      "the form\n",
      "\n",
      "fm(x) = fm−1(x) +νβ mφ(x; γm)\n",
      "\n",
      "(16.35)\n",
      "\n",
      "Here 0 < ν ≤ 1 is a step-size parameter. In practice it is common to use a small value such as ν = 0.1. This is called shrinkage.\n",
      "\n",
      "Below we discuss how to solve the suproblem in Equation 16.33. This will depend on the\n",
      "\n",
      "form of loss function. However, it is independent of the form of weak learner.\n",
      "\n",
      "16.4.2\n",
      "\n",
      "L2boosting\n",
      "\n",
      "Suppose we used squared error loss. Then at step m the loss has the form\n",
      "\n",
      "L(yi, fm−1(xi) +βφ (xi; γ)) = (rim − φ(xi; γ))2\n",
      "\n",
      "(16.36)\n",
      "\n",
      "558\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 16.10 Example of adaboost using a decision stump as a weak learner. The degree of blackness represents the conﬁdence in the red class. The degree of whiteness represents the conﬁdence in the blue class. The size of the datapoints represents their weight. Decision boundary is in yellow. (a) After 1 (c) After 120 rounds. Figure generated by boostingDemo, written by Richard round. Stapenhurst.\n",
      "\n",
      "(b) After 3 rounds.\n",
      "\n",
      "where rim (cid:2) yi − fm−1(xi) is the current residual, and we have set β = 1 without loss of generality. Hence we can ﬁnd the new basis function by using the weak learner to predict rm. This is called L2boosting, or least squares boosting (Buhlmann and Yu 2003). In Section 16.4.6, we will see that this method, with a suitable choice of weak learner, can be made to give the same results as LARS, which can be used to perform variable selection (see Section 13.4.2).\n",
      "\n",
      "16.4.3\n",
      "\n",
      "AdaBoost\n",
      "\n",
      "Consider a binary classiﬁcation problem with exponential loss. At step m we have to minimize\n",
      "\n",
      "Lm(φ) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "exp[−˜yi(fm−1(xi) + βφ(xi))] =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "wi,m exp(−β ˜yiφ(xi))\n",
      "\n",
      "(16.37)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "where wi,m (cid:2) exp(−˜yifm−1(xi)) is a weight applied to datacase i, and ˜yi ∈ {−1, +1}. We can rewrite this objective as follows: wi,m + eβ\n",
      "\n",
      "Lm = e−β\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "wi,m\n",
      "\n",
      "(16.38)\n",
      "\n",
      "˜yi=φ(xi)\n",
      "\n",
      "˜yi(cid:5)=φ(xi)\n",
      "\n",
      "= (eβ − e−β)\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "wi,mI(˜yi (cid:4)= φ(xi)) + e−β\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "wi,m\n",
      "\n",
      "(16.39)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "Consequently the optimal function to add is\n",
      "\n",
      "φm = argmin\n",
      "\n",
      "φ\n",
      "\n",
      "wi,mI(˜yi (cid:4)= φ(xi))\n",
      "\n",
      "(16.40)\n",
      "\n",
      "This can be found by applying the weak learner to a weighted version of the dataset, with weights wi,m. Subsituting φm into Lm and solving for β we ﬁnd\n",
      "\n",
      "βm =\n",
      "\n",
      "1 2\n",
      "\n",
      "log\n",
      "\n",
      "1 − errm errm\n",
      "\n",
      "(16.41)\n",
      "\n",
      "16.4. Boosting\n",
      "\n",
      "559\n",
      "\n",
      "where\n",
      "\n",
      "errm =\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 wiI(˜yi (cid:4)= φm(xi)) i=1 wi,m\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "(16.42)\n",
      "\n",
      "The overall update becomes\n",
      "\n",
      "fm(x) = fm−1(x) +β mφm(x)\n",
      "\n",
      "(16.43)\n",
      "\n",
      "With this, the weights at the next iteration become\n",
      "\n",
      "wi,m+1 = wi,me−βm ˜yiφm(xi)\n",
      "\n",
      "(16.44)\n",
      "\n",
      "= wi,meβm(2I(˜yi(cid:5)=φm(xi))−1) = wi,me2βmI(˜yi(cid:5)=φm(xi))e−βm\n",
      "\n",
      "(16.45)\n",
      "\n",
      "(16.46)\n",
      "\n",
      "where we exploited the fact that −˜yiφm(xi) =−1 if ˜yi = φm(xi) and −˜yiφm(xi) = +1 otherwise. Since e−βm will cancel out in the normalization step, we can drop it. The result is the algorithm shown in Algorithm 7, known Adaboost.M1.4\n",
      "\n",
      "An example of this algorithm in action, using decision stumps as the weak learner, is given in Figure 16.10. We see that after many iterations, we can “carve out” a complex decision boundary. What is rather surprising is that AdaBoost is very slow to overﬁt, as is apparent in Figure 16.8. See Section 16.4.8 for a discussion of this point.\n",
      "\n",
      "Algorithm 16.2: Adaboost.M1, for binary classiﬁcation with exponential loss 1 wi = 1/N ; 2 for m = 1 :M do 3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7 Return f (x) = sgn\n",
      "\n",
      "Fit a classiﬁer φm(x) to the training set using weights w; i=1 wi,mI(˜yi(cid:5)=φm(xi)) i=1 wi,m)\n",
      "\n",
      "Compute errm = Compute αm = log[(1 − errm)/errm]; Set wi ← wi exp[αmI(˜yi (cid:4)= φm(xi))]; (cid:15)(cid:7)M\n",
      "\n",
      "(cid:2)N\n",
      "\n",
      "m=1 αmφm(x)\n",
      "\n",
      "(cid:2)N\n",
      "\n",
      "(cid:16)\n",
      "\n",
      ";\n",
      "\n",
      ";\n",
      "\n",
      "16.4.4\n",
      "\n",
      "LogitBoost\n",
      "\n",
      "The trouble with exponential loss is that it puts a lot of weight on misclassiﬁed examples, as is apparent from the exponential blowup on the left hand side of Figure 16.9. This makes the method very sensitive to outliers (mislabeled examples). In addition, e−˜yf is not the logarithm of any pmf for binary variables ˜y ∈ {−1, +1}; consequently we cannot recover probability estimates from f (x).\n",
      "\n",
      "4. In (Friedman et al. 2000), this is called discrete AdaBoost, since it assumes that the base classiﬁer φm returns a binary class label. If φm returns a probability instead, a modiﬁed algorithm, known as real AdaBoost, can be used. See (Friedman et al. 2000) for details.\n",
      "\n",
      "560\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "A natural alternative is to use logloss instead. This only punishes mistakes linearly, as is clear from Figure 16.9. Furthermore, it means that we will be able to extract probabilities from the ﬁnal learned function, using\n",
      "\n",
      "p(y = 1|x) =\n",
      "\n",
      "ef (x) e−f (x) + ef (x) =\n",
      "\n",
      "1 1 + e−2f (x)\n",
      "\n",
      "(16.47)\n",
      "\n",
      "The goal is to minimze the expected log-loss, given by\n",
      "\n",
      "Lm(φ) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "log [1 + exp (−2˜yi(fm−1(x) +φ (xi)))]\n",
      "\n",
      "(16.48)\n",
      "\n",
      "i=1\n",
      "\n",
      "By performing a Newton upate on this objective (similar to IRLS), one can derive the algorithm shown in Algorithm 8. This is known as logitBoost (Friedman et al. 2000). It can be generalized to the multi-class setting, as explained in (Friedman et al. 2000).\n",
      "\n",
      "Algorithm 16.3: LogitBoost, for binary classiﬁcation with log-loss 1 wi = 1/N , πi = 1/2; 2 for m = 1 :M do\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8 Return f (x) = sgn\n",
      "\n",
      "Compute the working response zi = Compute the weights wi = πi(1 − πi); (cid:7)N i=1 wi(zi − φ(xi))2; φm = argminφ Update f (x) ← f (x) + 1 2 φm(x); Compute πi = 1/(1 + exp(−2f (xi))); (cid:16) (cid:15)(cid:7)M\n",
      "\n",
      "m=1 φm(x)\n",
      "\n",
      ";\n",
      "\n",
      "y∗ i −πi πi(1−πi) ;\n",
      "\n",
      "16.4.5\n",
      "\n",
      "Boosting as functional gradient descent\n",
      "\n",
      "Rather than deriving new versions of boosting for every different loss function, it is possible to derive a generic version, known as gradient boosting (Friedman 2001; Mason et al. 2000). To explain this, imagine minimizing\n",
      "\n",
      "ˆf = argmin\n",
      "\n",
      "f\n",
      "\n",
      "L(f )\n",
      "\n",
      "(16.49)\n",
      "\n",
      "where f = (f (x1), . . . , f (xN )) are the “parameters”. We will solve this stagewise, using gradient descent. At step m, letg m be the gradient of L(f ) evaluated at f = fm−1:\n",
      "\n",
      "gim =\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "∂L(yi, f (xi)) ∂f (xi)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "f =fm−1\n",
      "\n",
      "(16.50)\n",
      "\n",
      "Gradients of some common loss functions are given in Table 16.1. We then make the update\n",
      "\n",
      "fm = fm−1 − ρmgm\n",
      "\n",
      "(16.51)\n",
      "\n",
      "16.4. Boosting\n",
      "\n",
      "561\n",
      "\n",
      "where ρm is the step length, chosen by\n",
      "\n",
      "ρm = argmin\n",
      "\n",
      "ρ\n",
      "\n",
      "L(fm−1 − ρgm)\n",
      "\n",
      "(16.52)\n",
      "\n",
      "This is called functional gradient descent.\n",
      "\n",
      "In its current form, this is not much use, since it only optimizes f at a ﬁxed set of N points, so we do not learn a function that can generalize. However, we can modify the algorithm by ﬁtting a weak learner to approximate the negative gradient signal. That is, we use this update\n",
      "\n",
      "γm = argmin\n",
      "\n",
      "γ\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "(−gim − φ(xi; γ))2\n",
      "\n",
      "i=1\n",
      "\n",
      "(16.53)\n",
      "\n",
      "The overall algorithm is summarized in Algorithm 6. which is not strictly necessary, as argued in (Buhlmann and Hothorn 2007).)\n",
      "\n",
      "(We have omitted the line search step,\n",
      "\n",
      "Algorithm 16.4: Gradient boosting (cid:7)N 1 Initialize f0(x) = argminγ 2 for m = 1 :M do\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5 6 Return f (x) = fM (x)\n",
      "\n",
      "∂L(yi,f (xi)) ∂f (xi) (cid:7)N Use the weak learner to compute γm which minimizes Update fm(x) = fm−1(x) +νφ (x; γm);\n",
      "\n",
      "Compute the gradient residual using rim = −\n",
      "\n",
      "i=1 L(yi, φ(xi; γ));\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "i=1(rim − φ(xi; γm))2;\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "f (xi)=fm−1(xi)\n",
      "\n",
      ";\n",
      "\n",
      "If we apply this algorithm to log-loss, we get an algorithm known as BinomialBoost (Buhlmann and Hothorn 2007). The advantage of this over LogitBoost is that it does not need to be able to do weighted ﬁtting: it just applies any black-box regression model to the gradient vector. Also, it is relatively easy to extend to the multi-class case (see (Hastie et al. 2009, p387)). We can also apply this algorithm to other loss functions, such as the Huber loss (Section 7.4), which is more robust to outliers than squared error loss.\n",
      "\n",
      "If we apply this algorithm using squared loss, we recover L2Boosting.\n",
      "\n",
      "16.4.6\n",
      "\n",
      "Sparse boosting\n",
      "\n",
      "Suppose we use as our weak learner the following algorithm: search over all possible variables j = 1 :D , and pick the one j(m) that best predicts the residual vector:\n",
      "\n",
      "j(m) = argmin\n",
      "\n",
      "ˆβjm =\n",
      "\n",
      "j (cid:7)N\n",
      "\n",
      "i=1 xijrim (cid:7)N i=1 x2 ij\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(rim − ˆβjmxij)2\n",
      "\n",
      "(16.54)\n",
      "\n",
      "(16.55)\n",
      "\n",
      "φm(x) = ˆβj(m),m xj(m)\n",
      "\n",
      "(16.56)\n",
      "\n",
      "562\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "This method, which is known as sparse boosting (Buhlmann and Yu 2006), is identical to the matching pursuit algorithm discussed in Section 13.2.3.1.\n",
      "\n",
      "It is clear that this will result in a sparse estimate, at least if M is small. To see this, let us\n",
      "\n",
      "rewrite the update as follows:\n",
      "\n",
      "βm := βm−1 + ν(0, . . . , 0, ˆβj(m),m, 0, . . . , 0)\n",
      "\n",
      "(16.57)\n",
      "\n",
      "where the non-zero entry occurs in location j(m). This is known as forward stagewise linear regression (Hastie et al. 2009, p608), which becomes equivalent to the LAR algorithm discussed in Section 13.4.2 as ν → 0. Increasing the number of steps m in boosting is analogous to decreasing the regularization penalty λ. If we modify boosting to allow some variable deletion steps (Zhao and Yu 2007), we can make it equivalent to the LARS algorithm, which computes the full regularization path for the lasso problem. The same algorithm can be used for sparse logistic regression, by simply modifying the residual to be the appropriate negative gradient.\n",
      "\n",
      "Now consider a weak learner that is similar to the above, except it uses a smoothing spline instead of linear regression when mapping from xj to the residual. The result is a sparse generalized additive model (see Section 16.3). It can obviously be extended to pick pairs of variables at a time. The resulting method often works much better than MARS (Buhlmann and Yu 2006).\n",
      "\n",
      "16.4.7 Multivariate adaptive regression trees (MART)\n",
      "\n",
      "It is quite common to use CART models as weak learners. It is usually advisable to use a shallow tree, so that the variance is low. Even though the bias will be high (since a shallow tree is likely to be far from the “truth”), this will compensated for in subsequent rounds of boosting.\n",
      "\n",
      "The height of the tree is an additional tuning parameter (in addition to M , the number of rounds of boosting, and ν, the shrinkage factor). Suppose we restrict to trees with J leaves. If J = 2, we get a stump, which can only split on a single variable. If J = 3, we allow for In general, it is recommended (e.g., in (Hastie et al. 2009, p363) two-variable interactions, etc. and (Caruana and Niculescu-Mizil 2006)) to use J ≈ 6.\n",
      "\n",
      "If we combine the gradient boosting algorithm with (shallow) regression trees, we get a model known as MART, which stands for “multivariate adaptive regression trees”. This actually includes a slight reﬁnement to the basic gradient boosting algorithm: after ﬁtting a regression tree to the residual (negative gradient), we re-estimate the parameters at the leaves of the tree to minimize the loss:\n",
      "\n",
      "γjm = argmin\n",
      "\n",
      "γ\n",
      "\n",
      "xi∈Rjm\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "L(yi, fm−1(xi) +γ )\n",
      "\n",
      "(16.58)\n",
      "\n",
      "where Rjm is the region for leaf j in the m’th tree, and γjm is the corresponding parameter (the mean response of y for regression problems, or the most probable class label for classiﬁcation problems).\n",
      "\n",
      "16.4.8 Why does boosting work so well?\n",
      "\n",
      "We have seen that boosting works very well, especially for classiﬁers. There are two main reasons for this. First, it can be seen as a form of (cid:2)1 regularization, which is known to help\n",
      "\n",
      "16.5. Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "563\n",
      "\n",
      "prevent overﬁtting by eliminating “irrelevant” features. To see this, imagine pre-computing all possible weak-learners, and deﬁning a feature vector of the form φ(x) = [φ1(x), . . . , φK(x)]. We could use (cid:2)1 regularization to select a subset of these. Alternatively we can use boosting, where at each step, the weak learner creates a new φk on the ﬂy. It is possible to combine boosting and (cid:2)1 regularization, to get an algorithm known as L1-Adaboost (Duchi and Singer 2009). Essentially this method greedily adds the best features (weak learners) using boosting, and then prunes off irrelevant ones using (cid:2)1 regularization.\n",
      "\n",
      "Another explanation has to do with the concept of margin, which we introduced in Sec- tion 14.5.2.2. (Schapire et al. 1998; Ratsch et al. 2001) proved that AdaBoost maximizes the margin on the training data. (Rosset et al. 2004) generalized this to other loss functions, such as log-loss.\n",
      "\n",
      "16.4.9\n",
      "\n",
      "A Bayesian view\n",
      "\n",
      "So far, our presentation of boosting has been very frequentist, since it has focussed on greedily minimizing loss functions. A likelihood interpretation of the algorithm was given in (Neal and MacKay 1998; Meek et al. 2002). The idea is to consider a mixture of experts model of the form\n",
      "\n",
      "p(y|x, θ) =\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "πmp(y|x, γm)\n",
      "\n",
      "(16.59)\n",
      "\n",
      "m=1\n",
      "\n",
      "where each expert p(y|x, γm) is like a weak learner. We usually ﬁt all M experts at once using EM, but we can imagine a sequential scheme, whereby we only update the parameters In the E step, the posterior responsibilities will reﬂect how well the for one expert at a time. if this is a poor ﬁt, these data points will have existing experts explain a given data point; more inﬂuence on the next expert that is ﬁtted. (This view naturally suggest a way to use a boosting-like algorithm for unsupervised learning: we simply sequentially ﬁt mixture models, instead of mixtures of experts.)\n",
      "\n",
      "Notice that this is a rather “broken” MLE procedure, since it never goes back to update the parameters of an old expert. Similarly, if boosting ever wants to change the weight assigned to a weak learner, the only way to do this is to add the weak learner again with a new weight. This can result in unnecessarily large models. By contrast, the BART model (Chipman et al. 2006, 2010) uses a Bayesian version of backﬁtting to ﬁt a small sum of weak learners (typically trees).\n",
      "\n",
      "16.5\n",
      "\n",
      "Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "A feedforward neural network, aka multi-layer perceptron (MLP), is a series of logistic regression models stacked on top of each other, with the ﬁnal layer being either another logistic regression or a linear regression model, depending on whether we are solving a classiﬁcation or regression problem. For example, if we have two layers, and we are solving a regression problem, the model has the form\n",
      "\n",
      "p(y|x, θ) =N (y|wT z(x), σ2)\n",
      "\n",
      "(16.60)\n",
      "\n",
      "(16.61) where g is a non-linear activation or transfer function (commonly the logistic function), z(x) =φ( x, V) is called the hidden layer (a deterministic function of the input), H is the\n",
      "\n",
      "z(x) =g\n",
      "\n",
      "(Vx) = [g(vT\n",
      "\n",
      "1 x), . . . , g(vT\n",
      "\n",
      "H x)]\n",
      "\n",
      "564\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "znH\n",
      "\n",
      "xnD\n",
      "\n",
      "xni\n",
      "\n",
      "vij\n",
      "\n",
      "... znj\n",
      "\n",
      "wjk\n",
      "\n",
      "ynC\n",
      "\n",
      "ynk\n",
      "\n",
      "xn1\n",
      "\n",
      "...\n",
      "\n",
      "yn1\n",
      "\n",
      "zn1\n",
      "\n",
      "Figure 16.11 A neural network with one hidden layer.\n",
      "\n",
      "number of hidden units, V is the weight matrix from the inputs to the hidden nodes, and w is the weight vector from the hidden nodes to the output. It is important that g be non- linear, otherwise the whole model collapses into a large linear regression model of the form y = wT (Vx). One can show that an MLP is a universal approximator, meaning it can model any suitably smooth function, given enough hidden units, to any desired level of accuracy (Hornik 1991).\n",
      "\n",
      "To handle binary classiﬁcation, we pass the output through a sigmoid, as in a GLM:\n",
      "\n",
      "p(y|x, θ) = Ber(y|sigm(wT z(x)))\n",
      "\n",
      "(16.62)\n",
      "\n",
      "We can easily extend the MLP to predict multiple outputs. For example, in the regression case, we have\n",
      "\n",
      "p(y|x, θ) = N (y|W φ(x, V), σ2I)\n",
      "\n",
      "(16.63)\n",
      "\n",
      "See Figure 16.11 for an illustration. If we add mutual inhibition arcs between the output units, ensuring that only one of them turns on, we can enforce a sum-to-one constraint, which can be used for multi-class classiﬁcation. The resulting model has the form\n",
      "\n",
      "p(y|x, θ) = Cat(y|S(Wz(x))\n",
      "\n",
      "(16.64)\n",
      "\n",
      "16.5.1\n",
      "\n",
      "Convolutional neural networks\n",
      "\n",
      "The purpose of the hidden units is to learn non-linear combinations of the original inputs; this is called feature extraction or feature construction. These hidden features are then passed as input to the ﬁnal GLM. This approach is particularly useful for problems where the original input features are not very individually informative. For example, each pixel in an image is not very informative; it is the combination of pixels that tells us what objects are present. Conversely, for a task such as document classiﬁcation using a bag of words representation, each feature (word count) is informative on its own, so extracting “higher order” features is less important. Not suprisingly, then, much of the work in neural networks has been motivated by visual pattern\n",
      "\n",
      "16.5. Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "565\n",
      "\n",
      "Source: http://www.codep Figure 16.12 The convolutional neural network from (Simard et al. 2003). roject.com/KB/library/NeuralNetRecognition.aspx . Used with kind permission of Mike O’Neill.\n",
      "\n",
      "recognition (e.g., (LeCun et al. 1989)), although they have also been applied to other types of data, including text (e.g., (Collobert and Weston 2008)).\n",
      "\n",
      "A form of MLP which is particularly well suited to 1d signals like speech or text, or 2d signals like images, is the convolutional neural network. This is an MLP in which the hidden units have local receptive ﬁelds (as in the primary visual cortex), and in which the weights are tied or shared across the image, in order to reduce the number of parameters. Intuitively, the effect of such spatial parameter tying is that any useful features that are “discovered” in some portion of the image can be re-used everywhere else without having to be independently learned. The resulting network then exhibits translation invariance, meaning it can classify patterns no matter where they occur inside the input image.\n",
      "\n",
      "Figure 16.12 gives an example of a convolutional network, designed by Simard and colleagues (Simard et al. 2003), with 5 layers (4 layers of adjustable parameters) designed to classify 29 × 29 gray-scale images of handwritten digits from the MNIST dataset (see Section 1.2.1.3). In layer 1, we have 6 feature maps each of which has size 13 × 13. Each hidden node in one of these feature maps is computed by convolving the image with a 5 × 5 weight matrix (sometimes called a kernel), adding a bias, and then passing the result through some form of nonlinearity. There are therefore 13 × 13 × 6 = 1014 neurons in Layer 1, and (5 × 5 + 1) × 6 = 156 weights. (The \"+1\" is for the bias.) If we did not share these parameters, there would be 1014 × 26 = 26, 364 weights at the ﬁrst layer. In layer 2, we have 50 feature maps, each of which is obtained by convolving each feature map in layer 1 with a 5 × 5 weight matrix, adding them up, adding a bias, and passing through a nonlinearity. There are therefore 5 × 5 × 50 = 1250 neurons in Layer 2, (5 × 5 + 1) × 6 × 50 = 7800 adjustable weights (one kernel for each pair of feature\n",
      "\n",
      "566\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "maps in layers 1 and 2), and 1250 × 26 = 32, 500 connections. Layer 3 is fully connected to layer 2, and has 100 neurons and 100 × (1250 + 1) = 125, 100 weights. Finally, layer 4 is also fully connected, and has 10 neurons, and 10 × (100 + 1) = 1010 weights. Adding the above numbers, there are a total of 3,215 neurons, 134,066 adjustable weights, and 184,974 connections. This model is usually trained using stochastic gradient descent (see Section 16.5.4 for details). A single pass over the data set is called an epoch. When Mike O’Neill did these experiments in 2006, he found that a single epoch took about 40 minutes (recall that there are 60,000 training examples in MNIST). Since it took about 30 epochs for the error rate to converge, the total training time was about 20 hours.5 Using this technique, he obtained a misclassiﬁcation rate on the 10,000 test cases of about 1.40%.\n",
      "\n",
      "To further reduce the error rate, a standard trick is to expand the training set by including distorted versions of the original data, to encourage the network to be invariant to small changes that don’t affect the identity of the digit. These can be created by applying a random ﬂow ﬁeld to shift pixels around. See Figure 16.13 for some examples. (If we use online training, such as stochastic gradient descent, we can create these distortions on the ﬂy, rather than having to store them.) Using this technique, Mike O’Neill obtained a misclassiﬁcation rate on the 10,000 test cases of about 0.74%, which is close to the current state of the art.6\n",
      "\n",
      "Yann Le Cun and colleagues (LeCun et al. 1998) obtained similar performance using a slightly more complicated architecture shown in Figure 16.14. This model is known as LeNet5, and historically it came before the model in Figure 16.12. There are two main differences. First, LeNet5 has a subsampling layer between each convolutional layer, which either averages or computes the max over each small window in the previous layer, in order to reduce the size, and to obtain a small amount of shift invariance. The convolution and sub-sampling combination was inspired by Hubel and Wiesel’s model of simple and complex cells in the visual cortex (Hubel and Wiesel 1962), and it continues to be popular in neurally-inspired models of visual object recognition (Riesenhuber and Poggio 1999). A similar idea ﬁrst appeared in Fukushima’s neocognitron (Fukushima 1975), though no globally supervised training algorithm was available at that time.\n",
      "\n",
      "The second difference between LeNet5 and the Simard architecture is that the ﬁnal layer is actually an RBF network rather than a more standard sigmoidal or softmax layer. This model gets a test error rate of about 0.95% when trained with no distortions, and 0.8% when trained with distortions. Figure 16.15 shows all 82 errors made by the system. Some are genuinely ambiguous, but several are errors that a person would never make. A web-based demo of the LeNet5 can be found at http://yann.lecun.com/exdb/lenet/index.html.\n",
      "\n",
      "Of course, classifying isolated digits is of limited applicability: in the real world, people usually write strings of digits or other letters. This requires both segmentation and classiﬁcation. Le Cun and colleagues devised a way to combine convolutional neural networks with a model similar to a conditional random ﬁeld (described in Section 19.6) to solve this problem. The system was eventually deployed by the US postal service. (See (LeCun et al. 1998) for a more detailed account of the system, which remains one of the best performing systems for this task.)\n",
      "\n",
      "5. Implementation details: Mike used C++ code and a variety of speedup tricks. He was using standard 2006 era hardware (an Intel Pentium 4 hyperthreaded processor running at 2.8GHz). See http://www.codeproject.com/KB/ library/NeuralNetRecognition.aspx for details. 6. A list of various methods, along with their misclassiﬁcation rates on the MNIST test set, is available from http: //yann.lecun.com/exdb/mnist/. Error rates within 0.1–0.2% of each other are not statistically signiﬁcantly different.\n",
      "\n",
      "16.5. Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "567\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(e)\n",
      "\n",
      "(f)\n",
      "\n",
      "Figure 16.13 Several synthetic warpings of a handwritten digit. Based on Figure 5.14 of (Bishop 2006a). Figure generated by elasticDistortionsDemo, written by Kevin Swersky.\n",
      "\n",
      "INPUT 32x32\n",
      "\n",
      "C1: feature maps 6@28x28\n",
      "\n",
      "C3: f. maps 16@10x10\n",
      "\n",
      "S4: f. maps 16@5x5\n",
      "\n",
      "S2: f. maps 6@14x14\n",
      "\n",
      "C5: layer 120\n",
      "\n",
      "F6: layer 84\n",
      "\n",
      "OUTPUT 10\n",
      "\n",
      "Full connection\n",
      "\n",
      "Gaussian connections\n",
      "\n",
      "Convolutions\n",
      "\n",
      "Subsampling\n",
      "\n",
      "Convolutions\n",
      "\n",
      "Subsampling\n",
      "\n",
      "Full connection\n",
      "\n",
      "Figure 16.14 (LeCun et al. 1998) . Used with kind permission of Yann LeCun.\n",
      "\n",
      "LeNet5, a convolutional neural net for classifying handwritten digits. Source: Figure 2 from\n",
      "\n",
      "568\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "4−>6 3−>5 8−>2 2−>1 5−>3 4−>8 2−>8 3−>5 6−>5 7−>3\n",
      "\n",
      "9−>4 8−>0 7−>8 5−>3 8−>7 0−>6 3−>7 2−>7 8−>3 9−>4\n",
      "\n",
      "8−>2 5−>3 4−>8 3−>9 6−>0 9−>8 4−>9 6−>1 9−>4 9−>1\n",
      "\n",
      "9−>4 2−>0 6−>1 3−>5 3−>2 9−>5 6−>0 6−>0 6−>0 6−>8\n",
      "\n",
      "4−>6 7−>3 9−>4 4−>6 2−>7 9−>7 4−>3 9−>4 9−>4 9−>4\n",
      "\n",
      "8−>7 4−>2 8−>4 3−>5 8−>4 6−>5 8−>5 3−>8 3−>8 9−>8\n",
      "\n",
      "1−>5 9−>8 6−>3 0−>2 6−>5 9−>5 0−>7 1−>6 4−>9 2−>1\n",
      "\n",
      "2−>8 8−>5 4−>9 7−>2 7−>2 6−>5 9−>7 6−>1 5−>6 5−>0\n",
      "\n",
      "4−>9 2−>8\n",
      "\n",
      "Figure 16.15 These are the 82 errors made by LeNet5 on the 10,000 test cases of MNIST. Below each image is a label of the form correct-label → estimated-label. Source: Figure 8 of (LeCun et al. 1998). Used with kind permission of Yann LeCun. (Compare to Figure 28.4(b) which shows the results of a deep generative model.)\n",
      "\n",
      "16.5.2\n",
      "\n",
      "Other kinds of neural networks\n",
      "\n",
      "Other network topologies are possible besides the ones discussed above. For example, we can have skip arcs that go directly from the input to the output, skipping the hidden layer; we can have sparse connections between the layers; etc. However, the MLP always requires that the weights form a directed acyclic graph. If we allow feedback connections, the model is known as a recurrent neural network; this deﬁnes a nonlinear dynamical system, but does not have a simple probabilistic interpretation. Such RNN models are currently the best approach for language modeling (i.e., performing word prediction in natural language) (Tomas et al. 2011), signiﬁcantly outperforming the standard n-gram-based methods discussed in Section 17.2.2.\n",
      "\n",
      "If we allow symmetric connections between the hidden units, the model is known as a Hop- ﬁeld network or associative memory; its probabilistic counterpart is known as a Boltzmann machine (see Section 27.7) and can be used for unsupervised learning.\n",
      "\n",
      "16.5.3\n",
      "\n",
      "A brief history of the ﬁeld\n",
      "\n",
      "Neural networks have been the subject of great interest for many decades, due to the desire to understand the brain, and to build learning machines. It is not possible to review the entire history here. Instead, we just give a few “edited highlights”.\n",
      "\n",
      "The ﬁeld is generally viewed as starting with McCulloch and Pitts (McCullich and Pitts 1943), who devised a simple mathematical model of the neuron in 1943, in which they approximated the\n",
      "\n",
      "16.5. Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "569\n",
      "\n",
      "output as a weighted sum of inputs passed through a threshold function, y = I( i wixi > θ), for some threshold θ. This is similar to a sigmoidal activation function. Frank Rosenblatt invented the perceptron learning algorithm in 1957, which is a way to estimate the parameters of a McCulloch-Pitts neuron (see Section 8.5.4 for details). A very similar model called the adaline (for adaptive linear element) was invented in 1960 by Widrow and Hoff.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "In 1969, Minsky and Papert (Minsky and Papert 1969) published a famous book called “Percep- trons” in which they showed that such linear models, with no hidden layers, were very limited in their power, since they cannot classify data that is not linearly separable. This considerably reduced interest in the ﬁeld.\n",
      "\n",
      "In 1986, Rumelhart, Hinton and Williams (Rumelhart et al. 1986) discovered the backpropa- (The gation algorithm (see Section 16.5.4), which allows one to ﬁt models with hidden layers. backpropagation algorithm was originally discovered in (Bryson and Ho 1969), and independently in (Werbos 1974); however, it was (Rumelhart et al. 1986) that brought the algorithm to people’s attention.) This spawned a decade of intense interest in these models.\n",
      "\n",
      "In 1987, Sejnowski and Rosenberg (Sejnowski and Rosenberg 1987) created the famous NETtalk system, that learned a mapping from English words to phonetic symbols which could be fed into a speech synthesizer. An audio demo of the system as it learns over time can be found at http://www.cnl.salk.edu/ParallelNetsPronounce/nettalk.mp3. The systems starts by “babbling” and then gradually learns to pronounce English words. NETtalk learned a distributed representation (via its hidden layer) of various sounds, and its success spawned a big debate in psychology between connectionism, based on neural networks, and computationalism, based on syntactic rules. This debate lives on to some extent in the machine learning community, where there are still arguments about whether learning is best performed using low-level, “neural- like” representations, or using more structured models.\n",
      "\n",
      "In 1989, Yann Le Cun and others (LeCun et al. 1989) created the famous LeNet system described\n",
      "\n",
      "in Section 16.5.1.\n",
      "\n",
      "In 1992, the support vector machine (see Section 14.5) was invented (Boser et al. 1992). SVMs provide similar prediction accuracy to neural networks while being considerably easier to train (since they use a convex objective function). This spawned a decade of interest in kernel methods in general.7 Note, however, that SVMs do not use adaptive basis functions, so they require a fair amount of human expertise to design the right kernel function.\n",
      "\n",
      "In 2002, Geoff Hinton invented the contrastive divergence training procedure (Hinton 2002), which provided a way, for the ﬁrst time, to learn deep networks, by training one layer at a time in an unsupervised fashion (see Section 27.7.2.4 for details). This in turn has spawned renewed interest in neural networks over the last few years (see Chapter 28).\n",
      "\n",
      "16.5.4\n",
      "\n",
      "The backpropagation algorithm\n",
      "\n",
      "Unlike a GLM, the NLL of an MLP is a non-convex function of its parameters. Nevertheless, we can ﬁnd a locally optimal ML or MAP estimate using standard gradient-based optimization methods. Since MLPs have lots of parameters, they are often trained on very large data sets.\n",
      "\n",
      "7. It became part of the folklore during the 1990s that to get published in the top machine learning conference known as NIPS, which stands for “neural information processing systems”, it was important to ensure your paper did not contain the word “neural network”!\n",
      "\n",
      "570\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "tanh sigmoid\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.8\n",
      "\n",
      "−1 −10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "Figure 16.16 Two possible activation functions. tanh maps R to [−1, +1] and is the preferred nonlin- earity for the hidden nodes. sigm maps R to [0, 1] and is the preferred nonlinearity for binary nodes at the output layer. Figure generated by tanhPlot.\n",
      "\n",
      "Consequently it is common to use ﬁrst-order online methods, such as stochastic gradient descent (Section 8.5.2), whereas GLMs are usually ﬁt with IRLS, which is a second-order offline method. We now discuss how to compute the gradient vector of the NLL by applying the chain rule of calculus. The resulting algorithm is known as backpropagation, for reasons that will become apparent.\n",
      "\n",
      "It is helpful to distinguish the pre- and post-synaptic values of a neuron, that is, before and after we apply the nonlinearity. Let xn be the n’th input, an = Vxn be the pre-synaptic hidden layer, and zn = g(an) be the post-synaptic hidden layer, where g is some transfer function. We typically use g(a) = sigm(a), but we may also use g(a) = tanh(a): see Figure 16.16 for a comparison. (When the input to sigm or tanh is a vector, we assume it is applied component-wise.)\n",
      "\n",
      "We now convert this hidden layer to the output layer as follows. Let bn = Wzn be the pre-synaptic output layer, and ˆyn = h(bn) be the post-synaptic output layer, where h is another nonlinearity, corresponding to the canonical link for the GLM. (We reserve the notation yn, without the hat, for the output corresponding to the n’th training case.) For a regression model, we use h(b) = b; for binary classifcation, we use h(b) = [sigm(b1), . . . , sigm(bc)]; for multi-class classiﬁcation, we use h(b) = S(b). We can write the overall model as follows: W→ bn\n",
      "\n",
      "For notational simplicity, we shall assume a model with just one hidden layer.\n",
      "\n",
      "xn\n",
      "\n",
      "V→ an\n",
      "\n",
      "g→ zn\n",
      "\n",
      "h→ ˆyn\n",
      "\n",
      "(16.65)\n",
      "\n",
      "The parameters of the model are θ = (V, W), the ﬁrst and second layer weight matrices. Offset or bias terms can be accomodated by clamping an element of xn and zn to 1.8\n",
      "\n",
      "8. In the regression setting, we can easily estimate the variance of the output noise using the empirical variance of the residual errors, ˆσ2 = 1 ||ˆy(ˆθ) − y||2, after training is complete. There will be one value of σ2 for each output node, N if we are performing multi-target regression, as we usually assume.\n",
      "\n",
      "16.5. Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "571\n",
      "\n",
      "In the regression case, with K outputs, the NLL is given by the squared error:\n",
      "\n",
      "J(θ) = −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(ˆynk(θ) − ynk)2\n",
      "\n",
      "(16.66)\n",
      "\n",
      "n\n",
      "\n",
      "k\n",
      "\n",
      "In the classiﬁcation case, with K classes, the NLL is given by the cross entropy\n",
      "\n",
      "J(θ) = −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ynk log ˆynk(θ)\n",
      "\n",
      "(16.67)\n",
      "\n",
      "n\n",
      "\n",
      "k\n",
      "\n",
      "Our task is to compute ∇θJ. We will derive this for each n separately; the overall gradient is obtained by summing over n, although often we just use a mini-batch (see Section 8.5.2).\n",
      "\n",
      "since bnk = wT Equation 9.91 tells us that\n",
      "\n",
      "Let us start by considering the output layer weights. We have ∂Jn ∂bnk k zn. Assuming h is the canonical\n",
      "\n",
      "∇wk\n",
      "\n",
      "Jn =\n",
      "\n",
      "∂Jn ∂bnk\n",
      "\n",
      "∇wk\n",
      "\n",
      "bnk =\n",
      "\n",
      "zn\n",
      "\n",
      "link function for the output GLM, then\n",
      "\n",
      "(16.68)\n",
      "\n",
      "∂Jn ∂bnk\n",
      "\n",
      "(cid:2) δw\n",
      "\n",
      "nk = (ˆynk − ynk)\n",
      "\n",
      "(16.69)\n",
      "\n",
      "which is the error signal. So the overall gradient is\n",
      "\n",
      "(16.70) which is the pre-synaptic input to the output layer, namely zn, times the error signal, namely δw nk.\n",
      "\n",
      "∇wk\n",
      "\n",
      "Jn = δw\n",
      "\n",
      "nkzn\n",
      "\n",
      "For the input layer weights, we have\n",
      "\n",
      "∇vj Jn =\n",
      "\n",
      "∂Jn ∂anj\n",
      "\n",
      "∇vj anj (cid:2) δv\n",
      "\n",
      "njxn\n",
      "\n",
      "(16.71)\n",
      "\n",
      "where we exploited the fact that anj = vT error signal δv\n",
      "\n",
      "nj. We have\n",
      "\n",
      "j xn. All that remains is to compute the ﬁrst level\n",
      "\n",
      "δv nj =\n",
      "\n",
      "∂Jn ∂anj\n",
      "\n",
      "=\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "∂Jn ∂bnk\n",
      "\n",
      "∂bnk ∂anj\n",
      "\n",
      "=\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "δw nk\n",
      "\n",
      "∂bnk ∂anj\n",
      "\n",
      "(16.72)\n",
      "\n",
      "Now\n",
      "\n",
      "bnk =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "wkjg(anj)\n",
      "\n",
      "(16.73)\n",
      "\n",
      "j\n",
      "\n",
      "so\n",
      "\n",
      "∂bnk ∂anj\n",
      "\n",
      "= wkjg(cid:2)(anj)\n",
      "\n",
      "(16.74)\n",
      "\n",
      "where g(cid:2)(a) = d for sigmoid units, g(cid:2)(a) = d\n",
      "\n",
      "da g(a). For tanh units, g(cid:2)(a) = d\n",
      "\n",
      "da σ(a) = σ(a)(1 − σ(a)). Hence\n",
      "\n",
      "da tanh(a) = 1 − tanh2(a) = sech\n",
      "\n",
      "2(a), and\n",
      "\n",
      "δv nj =\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "nkwkjg(cid:2)(anj) δw\n",
      "\n",
      "(16.75)\n",
      "\n",
      "k=1\n",
      "\n",
      "572\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "Thus the layer 1 errors can be computed by passing the layer 2 errors back through the W matrix; hence the term “backpropagation”. The key property is that we can compute the gradients locally: each node only needs to know about its immediate neighbors. This is supposed to make the algorithm “neurally plausible”, although this interpretation is somewhat controversial.\n",
      "\n",
      "Putting it all together, we can compute all the gradients as follows: we ﬁrst perform a forwards pass to compute an, zn, bn and ˆyn. We then compute the error for the output layer, δ(2) n = ˆyn − yn, which we pass backwards through W using Equation 16.75 to compute the error for the hidden layer, δ(1) (cid:4)\n",
      "\n",
      "∇θJ(θ) =\n",
      "\n",
      "[δv\n",
      "\n",
      "nxn, δw\n",
      "\n",
      "n . We then compute the overall gradient as follows:\n",
      "\n",
      "n zn]\n",
      "\n",
      "(16.76)\n",
      "\n",
      "n\n",
      "\n",
      "16.5.5\n",
      "\n",
      "Identiﬁability\n",
      "\n",
      "It is easy to see that the parameters of a neural network are not identiﬁable. For example, we can change the sign of the weights going into one of the hidden units, so long as we change the sign of all the weights going out of it; these effects cancel, since tanh is an odd function, so tanh(−a) = − tanh(a). There will be H such sign ﬂip symmetries, leading to 2H equivalent settings of the parameters. Similarly, we can change the identity of the hidden units without affecting the likelihood. There are H! such permutations. The total number of equivalent parameter settings (with the same likelihood) is therefore H!2H .\n",
      "\n",
      "In addition, there may be local minima due to the non-convexity of the NLL. This can be a more serious problem, although with enough data, these local optima are often quite “shallow”, and simple stochastic optimization methods can avoid them. In addition, it is common to perform multiple restarts, and to pick the best solution, or to average over the resulting predictions. (It does not make sense to average the parameters themselves, since they are not identiﬁable.)\n",
      "\n",
      "16.5.6\n",
      "\n",
      "Regularization\n",
      "\n",
      "As usual, the MLE can overﬁt, especially if the number of nodes is large. A simple way to prevent this is called early stopping, which means stopping the training procedure when the error on the validation set ﬁrst starts to increase. This method works because we usually initialize from small random weights, so the model is initially simple (since the tanh and sigm functions are nearly linear near the origin). As training progresses, the weights become larger, and the model becomes nonlinear. Eventually it will overﬁt.\n",
      "\n",
      "Another way to prevent overﬁtting, that is more in keeping with the approaches used elsewhere in this book, is to impose a prior on the parameters, and then use MAP estimation. It is standard to use a N (0, α−1I) prior (equivalent to (cid:2)2 regularization), where α is the precision (strength) of the prior. In the neural networks literature, this is called weight decay, since it encourages small weights, and hence simpler models. The penalized NLL objective becomes\n",
      "\n",
      "J(θ) = −\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "n=1\n",
      "\n",
      "log p(yn|xn, θ) +\n",
      "\n",
      "α\n",
      "\n",
      "2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "[\n",
      "\n",
      "ij\n",
      "\n",
      "v2 ij +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "jk\n",
      "\n",
      "w2\n",
      "\n",
      "jk]\n",
      "\n",
      "(16.77)\n",
      "\n",
      "16.5. Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "573\n",
      "\n",
      "(Note that we don’t penalize the bias terms.) The gradient of the modiﬁed objective becomes\n",
      "\n",
      "∇θJ(θ) = [\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "δv nxn + αv,\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "δw n zn + αw]\n",
      "\n",
      "(16.78)\n",
      "\n",
      "n\n",
      "\n",
      "n\n",
      "\n",
      "as in Section 8.3.6.\n",
      "\n",
      "If the regularization is sufficiently strong, it does not matter if we have too many hidden units (apart from wasted computation). Hence it is advisable to set H to be as large as you can afford (say 10–100), and then to choose an appropriate regularizer. We can set the α parameter by cross validation or empirical Bayes (see Section 16.5.7.5).\n",
      "\n",
      "As with ridge regression, it is good practice to standardize the inputs to zero mean and unit\n",
      "\n",
      "variance, so that the spherical Gaussian prior makes sense.\n",
      "\n",
      "16.5.6.1\n",
      "\n",
      "Consistent Gaussian priors *\n",
      "\n",
      "One can show (MacKay 1992) that using the same regularization parameter for both the ﬁrst and second layer weights results in the lack of a certain desirable invariance property. In particular, suppose we linearly scale and shift the inputs and/or outputs to a neural network regression model. Then we would like the model to learn to predict the same function, by suitably scaling its internal weights and bias terms. However, the amount of scaling needed by the ﬁrst and second layer weights to compensate for a change in the inputs and/or outputs is not the same. Therefore we need to use a different regularization strength for the ﬁrst and second layer. Fortunately, this is easy to do — we just use the following prior: I)N (c|0, 1 I)N (V|0, 1 αc αv\n",
      "\n",
      "p(θ) = N (W|0, 1 αw\n",
      "\n",
      "I)N (b|0, 1 αb\n",
      "\n",
      "I)\n",
      "\n",
      "(16.79)\n",
      "\n",
      "where b and c are the bias terms.9\n",
      "\n",
      "To get a feeling for the effect of these hyper-parameters, we can sample MLP parameters from this prior and plot the resulting random functions. Figure 16.17 shows some examples. Decreasing αv allows the ﬁrst layer weights to get bigger, making the sigmoid-like shape of the functions steeper. Decreasing αb allows the ﬁrst layer biases to get bigger, which allows the center of the sigmoid to shift left and right more. Decreasing αw allows the second layer weights to get bigger, making the functions more “wiggly” (greater sensitivity to change in the input, and hence larger dynamic range). And decreasing αc allows the second layer biases to get bigger, allowing the mean level of the function to move up and down more. (In Chapter 15, we will see an easier way to deﬁne priors over functions.)\n",
      "\n",
      "16.5.6.2 Weight pruning\n",
      "\n",
      "Since there are many weights in a neural network, it is often helpful to encourage sparsity. Various ad-hoc methods for doing this, with names such as “optimal brain damage”, were devised in the 1990s; see e.g., (Bishop 1995) for details.\n",
      "\n",
      "9. Since we are regularizing the output bias terms, it is helpful, in the case of regression, to normalize the target responses in the training set to zero mean, to be consistent with the fact that the prior on the output bias has zero mean.\n",
      "\n",
      "574\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "10\n",
      "\n",
      "aw1=0.010, ab1=0.100, aw2=1.000, ab2=1.000\n",
      "\n",
      "10\n",
      "\n",
      "aw1=0.001, ab1=0.100, aw2=1.000, ab2=1.000\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−8\n",
      "\n",
      "−8\n",
      "\n",
      "−10\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "−10\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "10\n",
      "\n",
      "aw1=0.010, ab1=0.010, aw2=1.000, ab2=1.000\n",
      "\n",
      "10\n",
      "\n",
      "aw1=0.010, ab1=0.100, aw2=0.100, ab2=1.000\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−8\n",
      "\n",
      "−8\n",
      "\n",
      "−10\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "−10\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "10\n",
      "\n",
      "aw1=0.010, ab1=0.100, aw2=1.000, ab2=0.100\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−8\n",
      "\n",
      "−10\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "(e)\n",
      "\n",
      "Figure 16.17 The effects of changing the hyper-parameters on an MLP. αv = 0.01, αb = 0.1, αw = 1, αc = 1. factor of 10. mlpPriorsDemo.\n",
      "\n",
      "(d) Decreasing αw by factor of 10.\n",
      "\n",
      "(b) Decreasing αv by factor of 10.\n",
      "\n",
      "(a) Default parameter values (c) Decreasing αb by (e) Decreasing αc by factor of 10. Figure generated by\n",
      "\n",
      "16.5. Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "575\n",
      "\n",
      "Neural Network\n",
      "\n",
      "4\n",
      "\n",
      "h\n",
      "\n",
      "40\n",
      "\n",
      "h\n",
      "\n",
      "41\n",
      "\n",
      "h\n",
      "\n",
      "42\n",
      "\n",
      "y\n",
      "\n",
      "h\n",
      "\n",
      "43\n",
      "\n",
      "h\n",
      "\n",
      "44\n",
      "\n",
      "3\n",
      "\n",
      "Data Deep Neural Net\n",
      "\n",
      "2\n",
      "\n",
      "h\n",
      "\n",
      "30\n",
      "\n",
      "h\n",
      "\n",
      "31\n",
      "\n",
      "h\n",
      "\n",
      "32\n",
      "\n",
      "h\n",
      "\n",
      "33\n",
      "\n",
      "h\n",
      "\n",
      "34\n",
      "\n",
      "1\n",
      "\n",
      "h\n",
      "\n",
      "20\n",
      "\n",
      "h\n",
      "\n",
      "21\n",
      "\n",
      "h\n",
      "\n",
      "22\n",
      "\n",
      "h\n",
      "\n",
      "23\n",
      "\n",
      "h\n",
      "\n",
      "24\n",
      "\n",
      "0\n",
      "\n",
      "h\n",
      "\n",
      "10\n",
      "\n",
      "h\n",
      "\n",
      "11\n",
      "\n",
      "h\n",
      "\n",
      "12\n",
      "\n",
      "h\n",
      "\n",
      "13\n",
      "\n",
      "h\n",
      "\n",
      "14\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "x 0\n",
      "\n",
      "x 1\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 16.18 (a) A deep but sparse neural network. The connections are pruned using (cid:7)1 regularization. At each level, nodes numbered 0 are clamped to 1, so their outgoing weights correspond to the offset/bias (b) Predictions made by the model on the training set. Figure generated by sparseNnetDemo, terms. written by Mark Schmidt.\n",
      "\n",
      "However, we can also use the more principled sparsity-promoting techniques we discussed in Chapter 13. One approach is to use an (cid:2)1 regularizer. See Figure 16.18 for an example. Another approach is to use ARD; this is discussed in more detail in Section 16.5.7.5.\n",
      "\n",
      "16.5.6.3\n",
      "\n",
      "Soft weight sharing*\n",
      "\n",
      "Another way to regularize the parameters is to encourage similar weights to share statistical strength. But how do we know which parameters to group together? We can learn this, by using a mixture model. That is, we model p(θ) as a mixture of (diagonal) Gaussians. Parameters that are assigned to the same cluster will share the same mean and variance and thus will have similar values (assuming the variance for that cluster is low). This is called soft weight sharing (Nowlan and Hinton 1992). In practice, this technique is not widely used. See e.g., (Bishop 2006a, p271) if you want to know the details.\n",
      "\n",
      "16.5.6.4\n",
      "\n",
      "Semi-supervised embedding *\n",
      "\n",
      "An interesting way to regularize “deep” feedforward neural networks is to encourage the hidden layers to assign similar objects to similar representations. This is useful because it is often easy to obtain “side” information consisting of sets of pairs of similar and dissimilar objects. For example, in a video classiﬁcation task, neighboring frames can be deemed similar, but frames that are distant in time can be deemed dis-similar (Mobahi et al. 2009). Note that this can be done without collecting any labels.\n",
      "\n",
      "Let Sij = 1 if examples i and j are similar, and Sij = 0 otherwise. Let f (xi) be some embedding of item xi, e.g., f (xi) = z(xi, θ), where z is the hidden layer of a neural network. Now deﬁne a loss function L(f (xi), f (xj), Sij) that depends on the embedding of two objects,\n",
      "\n",
      "576\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "and the observed similarity measure. For example, we might want to force similar objects to have similar embeddings, and to force the embeddings of dissimilar objects to be a minimal distance apart:\n",
      "\n",
      "L(fi, fj, Sij) =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "||fi − fj||2 max(0, m − ||fi − fj||2)\n",
      "\n",
      "if Sij = 1 if Sij = 0\n",
      "\n",
      "(16.80)\n",
      "\n",
      "where m is some minimal margin. We can now deﬁne an augmented loss function for training the neural network:\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "NLL(f (xi), yi) +λ\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "L(f (xi), f (xj), Sij)\n",
      "\n",
      "(16.81)\n",
      "\n",
      "i∈L\n",
      "\n",
      "i,j∈U\n",
      "\n",
      "where L is the labeled training set, U is the unlabeled training set, and λ ≥ 0 is some tradeoff parameter. This is called semi-supervised embedding (Weston et al. 2008).\n",
      "\n",
      "Such an objective can be easily optimized by stochastic gradient descent. At each itera- tion, pick a random labeled training example, (xn, yn), and take a gradient step to optimize NLL(f (xi), yi). Then pick a random pair of similar unlabeled examples xi, xj (these can sometimes be generated on the ﬂy rather than stored in advance), and make a gradient step to optimize λL(f (xi), f (xj), 1), Finally, pick a random unlabeled example xk, which with high probability is dissimilar to xi, and make a gradient step to optimize λL(f (xi), f (xk), 0).\n",
      "\n",
      "Note that this technique is effective because it can leverage massive amounts of data. In a related approach, (Collobert and Weston 2008) trained a neural network to distinguish valid English sentences from invalid ones. This was done by taking all 631 million words from English Wikipedia (en.wikipedia.org), and then creating windows of length 11 containing neighboring words. This constitutes the positive examples. To create negative examples, the middle word of each window was replaced by a random English word (this is likely to be an “invalid” sentence — either grammatically and/or semantically — with high probability). This neural network was then trained over the course of 1 week, and its latent representation was then used as the input to a supervised semantic role labeling task, for which very little labeled training data is available. (See also (Ando and Zhang 2005) for related work.)\n",
      "\n",
      "16.5.7\n",
      "\n",
      "Bayesian inference *\n",
      "\n",
      "Although MAP estimation is a succesful way to reduce overﬁtting, there are still some good reasons to want to adopt a fully Bayesian approach to “ﬁtting” neural networks:\n",
      "\n",
      "\n",
      "\n",
      "Integrating out the parameters instead of optimizing them is a much stronger form of regu- larization than MAP estimation.\n",
      "\n",
      "We can use Bayesian model selection to determine things like the hyper-parameter settings and the number of hidden units. This is likely to be much faster than cross validation, especially if we have many hyper-parameters (e.g., as in ARD).\n",
      "\n",
      "Modelling uncertainty in the parameters will induce uncertainty in our predictive distribu- tions, which is important for certain problems such as active learning and risk-averse decision making.\n",
      "\n",
      "16.5. Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "577\n",
      "\n",
      "We can use online inference methods, such as the extended Kalman ﬁlter, to do online\n",
      "\n",
      "learning (Haykin 2001).\n",
      "\n",
      "One can adopt a variety of approximate Bayesian inference techniques in this context. In this section, we discuss the Laplace approximation, ﬁrst suggested in (MacKay 1992, 1995b). One can also use hybrid Monte Carlo (Neal 1996), or variational Bayes (Hinton and Camp 1993; Barber and Bishop 1998).\n",
      "\n",
      "16.5.7.1\n",
      "\n",
      "Parameter posterior for regression\n",
      "\n",
      "We start by considering regression, following the presentation of (Bishop 2006a, sec 5.7), which summarizes the work of (MacKay 1992, 1995b). We will use a prior of the form p(w) = N (w|0, (1/α)I), where w represents all the weights combined. We will denote the precision of the noise by β = 1/σ2.\n",
      "\n",
      "The posterior can be approximated as follows:\n",
      "\n",
      "p(w|D, α, β) ∝ exp(−E(w))\n",
      "\n",
      "E(w) (cid:2) βED(w) +αE W (w) N(cid:4) ED(w) (cid:2) 1 2 EW (w) (cid:2) 1 2\n",
      "\n",
      "wT w\n",
      "\n",
      "n=1\n",
      "\n",
      "(yn − f (xn, w))2\n",
      "\n",
      "(16.82) (16.83)\n",
      "\n",
      "(16.84)\n",
      "\n",
      "(16.85)\n",
      "\n",
      "where ED is the data error, EW is the prior error, and E is the overall error (negative log prior plus log likelihood). Now let us make a second-order Taylor series approximation of E(w) around its minimum (the MAP estimate)\n",
      "\n",
      "E(w) ≈ E(wM P ) +\n",
      "\n",
      "1 2\n",
      "\n",
      "(w − wM P )T A(w − wM P )\n",
      "\n",
      "(16.86)\n",
      "\n",
      "where A is the Hessian of E:\n",
      "\n",
      "A = ∇∇E(wM P ) = βH + αI\n",
      "\n",
      "(16.87)\n",
      "\n",
      "where H = ∇∇ED(wM P ) is the Hessian of the data error. This can be computed exactly in O(d2) time, where d is the number of parameters, using a variant of backpropagation (see if we use a quasi-Newton method to ﬁnd (Bishop 2006a, sec 5.4) for details). Alternatively, the mode, we can use its internally computed (low-rank) approximation to H. (Note that diagonal approximations of H are usually very inaccurate.) In either case, using this quadratic approximation, the posterior becomes Gaussian:\n",
      "\n",
      "p(w|α, β, D) ≈ N (w|wM P , A−1)\n",
      "\n",
      "(16.88)\n",
      "\n",
      "578\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "16.5.7.2\n",
      "\n",
      "Parameter posterior for classiﬁcation\n",
      "\n",
      "The classiﬁcation case is the same as the regression case, except β = 1 and ED is a cross- entropy error of the form\n",
      "\n",
      "ED(w) (cid:2)\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "[yn ln f (xn, w) + (1− yn) ln f (xn, w)]\n",
      "\n",
      "(16.89)\n",
      "\n",
      "n=1\n",
      "\n",
      "(16.90)\n",
      "\n",
      "16.5.7.3\n",
      "\n",
      "Predictive posterior for regression\n",
      "\n",
      "The posterior predictive density is given by (cid:12)\n",
      "\n",
      "p(y|x, D, α, β) =\n",
      "\n",
      "N (y|f (x, w), 1/β)N (w|wM P , A−1)dw\n",
      "\n",
      "(16.91)\n",
      "\n",
      "This is not analytically tractable because of the nonlinearity of f (x, w). Let us therefore construct a ﬁrst-order Taylor series approximation around the mode:\n",
      "\n",
      "f (x, w) ≈ f (x, wM P ) +g T (w − wM P )\n",
      "\n",
      "(16.92)\n",
      "\n",
      "where\n",
      "\n",
      "g = ∇wf (x, w)|w=wM P\n",
      "\n",
      "(16.93)\n",
      "\n",
      "We now have a linear-Gaussian model with a Gaussian prior on the weights. From Equation 4.126 we have\n",
      "\n",
      "p(y|x, D, α, β) ≈ N (y|f (x, wM P ), σ2(x))\n",
      "\n",
      "(16.94)\n",
      "\n",
      "where the predictive variance depends on the input x as follows:\n",
      "\n",
      "σ2(x) = β−1 + gT A−1g\n",
      "\n",
      "(16.95)\n",
      "\n",
      "The error bars will be larger in regions of input space where we have little training data. See Figure 16.19 for an example.\n",
      "\n",
      "16.5.7.4\n",
      "\n",
      "Predictive posterior for classiﬁcation\n",
      "\n",
      "In this section, we discuss how to approximate p(y|x, D) in the case of binary classiﬁcation. The situation is similar to the case of logistic regression, discussed in Section 8.4.4, except in addition the posterior predictive mean is a non-linear function of w. Speciﬁcally, we have μ = E [y|x, w] = sigm(a(x, w)), where a(x, w) is the pre-synaptic output of the ﬁnal layer. Let us make a linear approximation to this: a(x, w) ≈ aM P (x) +g T (w − wM P )\n",
      "\n",
      "(16.96)\n",
      "\n",
      "where aM P (x) =a( x, wM P ) and g = ∇xa(x, wM P ) can be found by a modiﬁed version of backpropagation. Clearly\n",
      "\n",
      "p(a|x, D) ≈ N (a(x, wM P ), g(x)T A−1g(x))\n",
      "\n",
      "(16.97)\n",
      "\n",
      "16.5. Feedforward neural networks (multilayer perceptrons)\n",
      "\n",
      "579\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "data function network error bars\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "t e g r a T\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "Input\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "0\n",
      "\n",
      "Data Function Prediction Samples\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 16.19 The posterior predictive density for an MLP with 3 hidden nodes, trained on 16 data points. The dashed green line is the true function. (a) Result of using a Laplace approximation, after performing empirical Bayes to optimize the hyperparameters. The solid red line is the posterior mean prediction, and the dotted blue lines are 1 standard deviation above and below the mean. Figure generated by mlpRegEvidenceDemo. (b) Result of using hybrid Monte Carlo, using the same trained hyperparameters as in (a). The solid red line is the posterior mean prediction, and the dotted blue lines are samples from the posterior predictive. Figure generated by mlpRegHmcDemo, written by Ian Nabney.\n",
      "\n",
      "Hence the posterior predictive for the output is\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(y = 1|x, D) =\n",
      "\n",
      "sigm(a)p(a|x, D)da ≈ sigm(κ(σ2\n",
      "\n",
      "a)bT wM P )\n",
      "\n",
      "(16.98)\n",
      "\n",
      "where κ is deﬁned by Equation 8.70, which we repeat here for convenience:\n",
      "\n",
      "κ(σ2) (cid:2) (1 + πσ2/8)− 1\n",
      "\n",
      "2\n",
      "\n",
      "(16.99)\n",
      "\n",
      "Of course, a simpler (and potentially more accurate) alternative to this is to draw a few samples\n",
      "\n",
      "from the Gaussian posterior and to approximate the posterior predictive using Monte Carlo.\n",
      "\n",
      "In either case, the effect of taking uncertainty of the parameters into account, as in Sec- tion 8.4.4, is to “moderate” the conﬁdence of the output; the decision boundary itself is unaf- fected, however.\n",
      "\n",
      "16.5.7.5\n",
      "\n",
      "ARD for neural networks\n",
      "\n",
      "Once we have made the Laplace approximation to the posterior, we can optimize the marginal likelihood wrt the hyper-parameters α using the same ﬁxed-point equations as in Section 13.7.4.2. Typically we use one hyper-parameter for the weight vector leaving each node, to achieve an effect similar to group lasso (Section 13.5.1). That is, the prior has the form\n",
      "\n",
      "p(θ) =\n",
      "\n",
      "D(cid:20)\n",
      "\n",
      "i=1\n",
      "\n",
      "N (v:,i|0, 1 αv,i\n",
      "\n",
      "I)\n",
      "\n",
      "H(cid:20)\n",
      "\n",
      "j=1\n",
      "\n",
      "N (w:,j|0, 1 αw,j\n",
      "\n",
      "I)\n",
      "\n",
      "(16.100)\n",
      "\n",
      "If we ﬁnd αv,i = ∞, then input feature i is irrelevant, and its weight vector v:,i is pruned out. Similarly, if we ﬁnd αw,j = ∞, then hidden feature j is irrelevant. This is known as automatic\n",
      "\n",
      "580\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "relevancy determination or ARD, which was discussed in detail in Section 13.7. Applying this to neural networks gives us an efficient means of variable selection in non-linear models.\n",
      "\n",
      "The software package NETLAB contains a simple example of ARD applied to a neural network, called demard. This demo creates some data according to a nonlinear regression function f (x1, x2, x3) = sin(2πx1) + (cid:16), where x2 is a noisy copy of x1. We see that x2 and x3 are irrelevant for predicting the target. However, x2 is correlated with x1, which is relevant. Using ARD, the ﬁnal hyper-parameters are as follows:\n",
      "\n",
      "α = [0.2, 21.4, 249001.8]\n",
      "\n",
      "(16.101)\n",
      "\n",
      "This clearly indicates that feature 3 is irrelevant, feature 2 is only weakly relevant, and feature 1 is very relevant.\n",
      "\n",
      "16.6\n",
      "\n",
      "Ensemble learning\n",
      "\n",
      "Ensemble learning refers to learning a weighted combination of base models of the form\n",
      "\n",
      "f (y|x, π) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "wmfm(y|x)\n",
      "\n",
      "(16.102)\n",
      "\n",
      "m∈M\n",
      "\n",
      "where the wm are tunable parameters. Ensemble learning is sometimes called a committee method, since each base model fm gets a weighted “vote”.\n",
      "\n",
      "Clearly ensemble learning is closely related to learning adaptive-basis function models. In fact, one can argue that a neural net is an ensemble method, where fm represents the m’th hidden unit, and wm are the output layer weights. Also, we can think of boosting as kind of ensemble learning, where the weights on the base models are determined sequentially. Below we describe some other forms of ensemble learning.\n",
      "\n",
      "16.6.1\n",
      "\n",
      "Stacking\n",
      "\n",
      "An obvious way to estimate the weights in Equation 16.102 is to use\n",
      "\n",
      "ˆw = argmin\n",
      "\n",
      "w\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(yi,\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "m=1\n",
      "\n",
      "wmfm(x))\n",
      "\n",
      "(16.103)\n",
      "\n",
      "However, this will result in overﬁtting, with wm being large for the most complex model. A simple solution to this is to use cross-validation. In particular, we can use the LOOCV estimate\n",
      "\n",
      "ˆw = argmin\n",
      "\n",
      "w\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(yi,\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "m=1\n",
      "\n",
      "wm ˆf −i\n",
      "\n",
      "m (x))\n",
      "\n",
      "(16.104)\n",
      "\n",
      "where ˆf −i m (x) is the predictor obtained by training on data excluding (xi, yi). This is known as stacking, which stands for “stacked generalization” (Wolpert 1992). This technique is more robust to the case where the “true” model is not in the model class than standard BMA (Clarke 2003). This approach was used by the Netﬂix team known as “The Ensemble”, which tied the submission of the winning team (BellKor’s Pragmatic Chaos) in terms of accuracy (Sill et al. 2009). Stacking has also been used for problems such as image segmentation and labeling.\n",
      "\n",
      "16.6. Ensemble learning\n",
      "\n",
      "581\n",
      "\n",
      "Class C1 C2 C3 C4 C5 C6 0 0 1 1\n",
      "\n",
      "0 1\n",
      "\n",
      "9\n",
      "\n",
      "1 0\n",
      "\n",
      "0\n",
      "\n",
      "1 0\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0 1\n",
      "\n",
      "1\n",
      "\n",
      "0 1 ... 0\n",
      "\n",
      "0\n",
      "\n",
      "· · C15 · · · 1 · · · 0\n",
      "\n",
      "· ·\n",
      "\n",
      "0\n",
      "\n",
      "Table 16.2 Part of a 15-bit error-correcting output code for a 10-class problem. Each row deﬁnes a two-class problem. Based on Table 16.1 of (Hastie et al. 2009).\n",
      "\n",
      "16.6.2\n",
      "\n",
      "Error-correcting output codes\n",
      "\n",
      "An interesting form of ensemble learning is known as error-correcting output codes or ECOC (Dietterich and Bakiri 1995), which can be used in the context of multi-class classiﬁcation. The idea is that we are trying to decode a symbol (namely the class label) which has C possible states. We could use a bit vector of length B = (cid:13)log2 C(cid:14) to encode the class label, and train B separate binary classiﬁers to predict each bit. However, by using more bits, and by designing the codewords to have maximal Hamming distance from each other, we get a method that is more resistant to individual bit-ﬂipping errors (misclassiﬁcation). For example, in Table 16.2, we use B = 15 bits to encode a C = 10 class problem. The minimum Hamming distance between any pair of rows is 7. The decoding rule is\n",
      "\n",
      "ˆc(x) = min\n",
      "\n",
      "c\n",
      "\n",
      "B(cid:4)\n",
      "\n",
      "b=1\n",
      "\n",
      "|Ccb − ˆpb(x)|\n",
      "\n",
      "(16.105)\n",
      "\n",
      "where Ccb is the b’th bit of the codeword for class c. (James and Hastie 1998) showed that a random code worked just as well as the optimal code: both methods work by averaging the results of multiple classiﬁers, thereby reducing variance.\n",
      "\n",
      "16.6.3\n",
      "\n",
      "Ensemble learning is not equivalent to Bayes model averaging\n",
      "\n",
      "In Section 5.3, we discussed Bayesian model selection. An alternative to picking the best model, and then using this to make predictions, is to make a weighted average of the predictions made by each model, i.e., we compute\n",
      "\n",
      "p(y|x, D) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(y|x, m,D )p(m|D)\n",
      "\n",
      "(16.106)\n",
      "\n",
      "m∈M\n",
      "\n",
      "This is called Bayes model averaging (BMA), and can sometimes give better performance than using any single model (Hoeting et al. 1999). Of course, averaging over all models is typically computationally infeasible (analytical integration is obviously not possible in a discrete space, although one can sometimes use dynamic programming to perform the computation exactly, e.g., (Meila and Jaakkola 2006)). A simple approximation is to sample a few models from the posterior. An even simpler approximation (and the one most widely used in practice) is to just use the MAP model.\n",
      "\n",
      "It is important to note that BMA is not equivalent to ensemble learning (Minka 2000c). This latter technique corresponds to enlarging the model space, by deﬁning a single new model\n",
      "\n",
      "582\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "model bst-dt rf bag-dt svm ann knn bst-stmp dt logreg nb\n",
      "\n",
      "1st 0.580 0.390 0.030 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n",
      "\n",
      "2nd 0.228 0.525 0.232 0.008 0.007 0.000 0.000 0.000 0.000 0.000\n",
      "\n",
      "3rd 0.160 0.084 0.571 0.148 0.035 0.000 0.002 0.000 0.000 0.000\n",
      "\n",
      "4th 0.023 0.001 0.150 0.574 0.230 0.009 0.013 0.000 0.000 0.000\n",
      "\n",
      "5th 0.009 0.000 0.017 0.240 0.606 0.114 0.014 0.000 0.000 0.000\n",
      "\n",
      "6th 0.000 0.000 0.000 0.029 0.122 0.592 0.257 0.000 0.000 0.000\n",
      "\n",
      "7th 0.000 0.000 0.000 0.001 0.000 0.245 0.710 0.004 0.040 0.000\n",
      "\n",
      "8th 0.000 0.000 0.000 0.000 0.000 0.038 0.004 0.616 0.312 0.030\n",
      "\n",
      "9th 0.000 0.000 0.000 0.000 0.000 0.002 0.000 0.291 0.423 0.284\n",
      "\n",
      "10th 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.089 0.225 0.686\n",
      "\n",
      "Table 16.3 Fraction of time each method achieved a speciﬁed rank, when sorting by mean performance across 11 datasets and 8 metrics. Based on Table 4 of (Caruana and Niculescu-Mizil 2006). Used with kind permission of Alexandru Niculescu-Mizil.\n",
      "\n",
      "which is a convex combination of base models, as follows:\n",
      "\n",
      "p(y|x, π) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "πmp(y|x, m)\n",
      "\n",
      "(16.107)\n",
      "\n",
      "m∈M\n",
      "\n",
      "In principle, we can now perform Bayesian inference to compute p(π|D); we then make pre- dictions using p(y|x, D) = p(y|x, π)p(π|D)dπ. However, it is much more common to use point estimation methods for π, as we saw above.\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "16.7\n",
      "\n",
      "Experimental comparison\n",
      "\n",
      "We have described many different methods for classiﬁcation and regression. Which one should you use? That depends on which inductive bias you think is most appropriate for your domain. Usually this is hard to assess, so it is common to just try several different methods, and see how they perform empirically. Below we summarize two such comparisons that were carefully conducted (although the data sets that were used are relatively small). See the website mlcomp.org for a distributed way to perform large scale comparisons of this kind. Of course, we must always remember the no free lunch theorem (Section 1.4.9), which tells us that there is no universally best learning method.\n",
      "\n",
      "16.7.1\n",
      "\n",
      "Low-dimensional features\n",
      "\n",
      "In 2006, Rich Caruana and Alex Niculescu-Mizil (Caruana and Niculescu-Mizil 2006) conducted a very extensive experimental comparison of 10 different binary classiﬁcation methods, on 11 different data sets. The 11 data sets all had 5000 training cases, and had test sets containing ∼ 10, 000 examples on average. The number of features ranged from 9 to 200, so this is much lower dimensional than the NIPS 2003 feature selection challenge. 5-fold cross validation was used to assess average test error. (This is separate from any internal CV a method may need to use for model selection.)\n",
      "\n",
      "16.7. Experimental comparison\n",
      "\n",
      "583\n",
      "\n",
      "The methods they compared are as follows (listed in roughly decreasing order of performance,\n",
      "\n",
      "as assessed by Table 16.3):\n",
      "\n",
      "BST-DT: boosted decision trees • RF: random forest • BAG-DT: bagged decision trees • SVM: support vector machine • ANN: artiﬁcial neural network • KNN: K-nearest neighbors • BST-STMP: boosted stumps • DT: decision tree • LOGREG: logistic regression • NB: naive Bayes\n",
      "\n",
      "They used 8 different performance measures, which can be divided into three groups. Thresh- old metrics just require a point estimate as output. These include accuracy, F-score (Sec- tion 5.7.2.3), etc. Ordering/ ranking metrics measure how well positive cases are ordered before the negative cases. These include area under the ROC curve (Section 5.7.2.1), average precision, and the precision/recall break even point. Finally, the probability metrics included cross-entropy (log-loss) and squared error, (y − ˆp)2. Methods such as SVMs that do not produce calibrated probabilities were post-processed using Platt’s logistic regression trick (Section 14.5.2.3), or using isotonic regression. Performance measures were standardized to a 0:1 scale so they could be compared.\n",
      "\n",
      "Obviously the results vary by dataset and by metric. Therefore just averaging the performance does not necessarily give reliable conclusions. However, one can perform a bootstrap analysis, which shows how robust the conclusions are to such changes. The results are shown in Table 16.3. We see that most of the time, boosted decision trees are the best method, followed by random forests, bagged decision trees, SVMs and neural networks. However, the following methods all did relatively poorly: KNN, stumps, single decision trees, logistic regression and naive Bayes.\n",
      "\n",
      "These results are generally consistent with conventional wisdom of practioners in the ﬁeld. Of course, the conclusions may change if there the features are high dimensional and/ or there are lots of irrelevant features (as in Section 16.7.2), or if there is lots of noise, etc.\n",
      "\n",
      "16.7.2\n",
      "\n",
      "High-dimensional features\n",
      "\n",
      "In 2003, the NIPS conference ran a competition where the goal was to solve binary classiﬁcation problems with large numbers of (mostly irrelevant) features, given small training sets. (This was called a “feature selection” challenge, but performance was measured in terms of predictive accuracy, not in terms of the ability to select features.) The ﬁve datasets that were used are summarized in Table 16.4. The term probe refers to artiﬁcal variables that were added to the problem to make it harder. These have no predictive power, but are correlated with the original features.\n",
      "\n",
      "approach based on Bayesian neural networks (Neal and Zhang 2006).\n",
      "\n",
      "Results of the competition are discussed in (Guyon et al. 2006). The overall winner was an In a follow-up study\n",
      "\n",
      "584\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "Dataset Aracene Dexter Dorothea Drug discovery Gisette Madelon\n",
      "\n",
      "Domain Mass spectrometry Dense Sparse Text classiﬁcation Sparse Dense Dense\n",
      "\n",
      "Digit recognition Artiﬁcial\n",
      "\n",
      "Type\n",
      "\n",
      "D 10,000 20,000 100,000 5000 500\n",
      "\n",
      "% probes Ntrain Nval Ntest 30 50 50 30 96\n",
      "\n",
      "100 300 800 6000 2000\n",
      "\n",
      "100 300 350 1000 600\n",
      "\n",
      "700 2000 800 6500 1800\n",
      "\n",
      "Table 16.4 datasets, the features are binary. For the others, the features are real-valued.\n",
      "\n",
      "Summary of the data used in the NIPS 2003 “feature selection” challenge. For the Dorothea\n",
      "\n",
      "Screened features\n",
      "\n",
      "ARD\n",
      "\n",
      "Method HMC MLP Boosted MLP Bagged MLP Boosted trees Random forests\n",
      "\n",
      "Avg rank 1.5 3.8 3.6 3.4 2.7\n",
      "\n",
      "Avg time 384 (138) 9.4 (8.6) 3.5 (1.1) 3.03 (2.5) 1.9 (1.7)\n",
      "\n",
      "Avg rank 1.6 2.2 4.0 4.0 3.2\n",
      "\n",
      "Avg time 600 (186) 35.6 (33.5) 6.4 (4.4) 34.1 (32.4) 11.2 (9.3)\n",
      "\n",
      "Table 16.5 (HMC stands for hybrid Monte Carlo; see Section 24.5.4.) We report the average rank (lower is better) across the 5 datasets. We also report the average training time in minutes (standard error in brackets). The MCMC and bagged MLPs use two hidden layers of 20 and 8 units. The boosted MLPs use one hidden layer with 2 or 4 hidden units. The boosted trees used depths between 2 and 9, and shrinkage between 0.001 and 0.1. Each tree was trained on 80% of the data chosen at random at each step (so-called stochastic gradient boosting). From Table 11.3 of (Hastie et al. 2009).\n",
      "\n",
      "Performance of different methods on the NIPS 2003 “feature selection” challenge.\n",
      "\n",
      "(Johnson 2009), Bayesian neural nets (MLPs with 2 hidden layers) were compared to several other in methods based on bagging and boosting. Note that all of these methods are quite similar: each case, the prediction has the form\n",
      "\n",
      "ˆf (x∗) =\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "wmE [y|x∗, θm]\n",
      "\n",
      "(16.108)\n",
      "\n",
      "m=1\n",
      "\n",
      "The Bayesian MLP was ﬁt by MCMC (hybrid Monte Carlo), so we set wm = 1/M and set θm In bagging, we set wm = 1/M and θm is estimated by ﬁtting to a draw from the posterior. In boosting, we set wm = 1 and the θm are the model to a bootstrap sample from the data. estimated sequentially.\n",
      "\n",
      "To improve computational and statistical performance, some feature selection was performed. Two methods were considered: simple uni-variate screening using T-tests, and a method based on MLP+ARD. Results of this follow-up study are shown in Table 16.5. We see that Bayesian MLPs In second place are either random forests or boosted MLPs, depending are again the winner. on the preprocessing. However, it is not clear how statistically signiﬁcant these differences are, since the test sets are relatively small.\n",
      "\n",
      "In terms of training time, we see that MCMC is much slower than the other methods. It would be interesting to see how well deterministic Bayesian inference (e.g., Laplace approximation) would perform. (Obviously it will be much faster, but the question is: how much would one lose\n",
      "\n",
      "16.8.\n",
      "\n",
      "Interpreting black-box models\n",
      "\n",
      "585\n",
      "\n",
      "0 2\n",
      "\n",
      "0 2\n",
      "\n",
      "0 2\n",
      "\n",
      "0 2\n",
      "\n",
      "0 2\n",
      "\n",
      "e c n e d n e p e d\n",
      "\n",
      "l\n",
      "\n",
      "a i t r a p\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "0 1\n",
      "\n",
      "0 1\n",
      "\n",
      "0 1\n",
      "\n",
      "0 1\n",
      "\n",
      "0 1\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "0.2 0.4 0.6 0.8\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "0.2 0.4 0.6 0.8\n",
      "\n",
      "0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "0.2 0.4 0.6 0.8\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "x4\n",
      "\n",
      "x5\n",
      "\n",
      "0 2\n",
      "\n",
      "0 2\n",
      "\n",
      "0 2\n",
      "\n",
      "0 2\n",
      "\n",
      "0 2\n",
      "\n",
      "e c n e d n e p e d\n",
      "\n",
      "l\n",
      "\n",
      "a i t r a p\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "8 1\n",
      "\n",
      "6 1\n",
      "\n",
      "4 1\n",
      "\n",
      "2 1\n",
      "\n",
      "0 1\n",
      "\n",
      "0 1\n",
      "\n",
      "0 1\n",
      "\n",
      "0 1\n",
      "\n",
      "0 1\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "0.2 0.4 0.6 0.8\n",
      "\n",
      "0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "0.2 0.4 0.6 0.8\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "0.2 0.4 0.6 0.8\n",
      "\n",
      "x6\n",
      "\n",
      "x7\n",
      "\n",
      "x8\n",
      "\n",
      "x9\n",
      "\n",
      "x10\n",
      "\n",
      "Figure 16.20 Partial dependence plots for the 10 predictors in Friedman’s synthetic 5-dimensional re- gression problem. Source: Figure 4 of (Chipman et al. 2010) . Used with kind permission of Hugh Chipman.\n",
      "\n",
      "in statistical performance?)\n",
      "\n",
      "16.8\n",
      "\n",
      "Interpreting black-box models\n",
      "\n",
      "Linear models are popular in part because they are easy to interpet. However, they often are poor predictors, which makes them a poor proxy for “nature’s mechanism”. Thus any conclusions about the importance of particular variables should only be based on models that have good (Interestingly, many standard statistical tests of “goodness predictive accuracy (Breiman 2001b). of ﬁt” do not test the predictive accuracy of a model.)\n",
      "\n",
      "In this chapter, we studied black-box models, which do have good predictive accuracy. Unfortunately, they are hard to interpret directly. Fortunately, there are various heuristics we can use to “probe” such models, in order to assess which input variables are the most important.\n",
      "\n",
      "As a simple example, consider the following non-linear function, ﬁrst proposed (Friedman\n",
      "\n",
      "1991) to illustrate the power of MARS:\n",
      "\n",
      "f (x) = 10 sin(πx1x2) + 20(x3 − 0.5)2 + 10x4 + 5x5 + (cid:16)\n",
      "\n",
      "(16.109)\n",
      "\n",
      "where (cid:16) ∼ N (0, 1). We see that the output is a complex function of the inputs. By augmenting the x vector with additional irrelevant random variables, all drawn uniform on [0, 1], we can create a challenging feature selection problem. In the experiments below, we add 5 extra dummy variables.\n",
      "\n",
      "586\n",
      "\n",
      "Chapter 16. Adaptive basis function models\n",
      "\n",
      "5 2 . 0\n",
      "\n",
      "1 2\n",
      "\n",
      "e g a s u\n",
      "\n",
      "0 2 . 0\n",
      "\n",
      "5 1 . 0\n",
      "\n",
      "0 1\n",
      "\n",
      ". 0\n",
      "\n",
      "2 3 4 1 5\n",
      "\n",
      "1\n",
      "\n",
      "3 4 2 5\n",
      "\n",
      "2 1 3 4\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "1 2 3 4 5\n",
      "\n",
      "5 0 . 0\n",
      "\n",
      "0 0 . 0\n",
      "\n",
      "5 4 3 2 1\n",
      "\n",
      "5 4 3 2 1\n",
      "\n",
      "5 4 3 2 1\n",
      "\n",
      "5 4 3 2 1\n",
      "\n",
      "5 4 3 2 1\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "Figure 16.21 Average usage of each variable in a BART model ﬁt to data where only the ﬁrst 5 features are Source: relevant. The different coloured lines correspond to different numbers of trees in the ensemble. Figure 5 of (Chipman et al. 2010) . Used with kind permission of Hugh Chipman.\n",
      "\n",
      "One useful way to measure the effect of a set s of variables on the output is to compute a partial dependence plot (Friedman 2001). This is a plot of f (xs) vs xs, where f (xs) is deﬁned as the response to xs with the other predictors averaged out:\n",
      "\n",
      "f (xs) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "f (xs, xi,−s)\n",
      "\n",
      "(16.110)\n",
      "\n",
      "Figure 16.20 shows an example where we use sets corresponding to each single variable. The data was generated from Equation 16.109, with 5 irrelevant variables added. We then ﬁt a BART model (Section 16.2.5) and computed the partial dependence plots. We see that the predicted response is invariant for s ∈ {6, . . . , 10}, indicating that these variables are (marginally) irrelevant. The response is roughly linear in x4 and x5, and roughly quadratic in x3. (The error bars are obtained by computing empirical quantiles of f (x, θ) based on posterior samples of θ; alternatively, we can use bootstrap.)\n",
      "\n",
      "Another very useful summary computes the relative importance of predictor variables. This can be thought of as a nonlinear, or even “model free”, way of performing variable selection, although the technique is restricted to ensembles of trees. The basic idea, originally proposed in (Breiman et al. 1984), is to count how often variable j is used as a node in any of the trees. In particular, let vj = 1 m=1 I(j ∈ Tm) be the proportion of all splitting rules that use xj, M where Tm is the m’th tree. If we can sample the posterior of trees, p(T1:M |D), we can easily compute the posterior for vj. Alternatively, we can use bootstrap.\n",
      "\n",
      "(cid:7)M\n",
      "\n",
      "Figure 16.21 gives an example, using BART. We see that the ﬁve relevant variables are chosen much more than the ﬁve irrelevant variables. As we increase the number M of trees, all the variables are more likely to be chosen, reducing the sensitivity of this method, but for small M , the method is farily diagnostic.\n",
      "\n",
      "16.8.\n",
      "\n",
      "Interpreting black-box models\n",
      "\n",
      "587\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 16.1 Nonlinear regression for inverse dynamics In this question, we ﬁt a model which can predict what torques a robot needs to apply in order to make its arm reach a desired point in space. The data was collected from a SARCOS robot arm with 7 degrees of freedom. The input vector x ∈ R21 encodes the desired position, velocity and accelaration of the 7 joints. The output vector y ∈ R7 encodes the torques that should be applied to the joints to reach that point. The mapping from x to y is highly nonlinear. We have N = 48, 933 training points and Ntest = 4, 449 testing points. For simplicity, we following standard practice and focus on just predicting a scalar output, namely the torque for the ﬁrst joint. Download the data from http://www.gaussianprocess.org/gpml. Standardize the inputs so they have zero mean and unit variance on the training set, and center the outputs so they have zero mean on the training set. Apply the corresponding transformations to the test data. Below we will describe various models which you should ﬁt to this transformed data. Then make predictions and compute the standardized mean squared error on the test set as follows:\n",
      "\n",
      "where σ2 = 1\n",
      "\n",
      "SM SE =\n",
      "\n",
      "Ntrain\n",
      "\n",
      "1 Ntest\n",
      "\n",
      "σ2 (cid:2)N train i=1\n",
      "\n",
      "(cid:2)Ntest\n",
      "\n",
      "i=1 (yi − ˆyi)2\n",
      "\n",
      "(yi − y)2 is the variance of the output computed on the training set.\n",
      "\n",
      "(16.111)\n",
      "\n",
      "a. The ﬁrst method you should try is standard linear regression. Turn in your numbers and code. (According to (Rasmussen and Williams 2006, p24), you should be able to achieve a SMSE of 0.075 using this method.)\n",
      "\n",
      "b. Now try running K-means clustering (using cross validation to pick K). Then ﬁt an RBF network to the data, using the μk estimated by K-means. Use CV to estimate the RBF bandwidth. What SMSE do you get? Turn in your numbers and code. (According to (Rasmussen and Williams 2006, p24), Gaussian process regression can get an SMSE of 0.011, so the goal is to get close to that.)\n",
      "\n",
      "c. Now try ﬁtting a feedforward neural network. Use CV to pick the number of hidden units and the\n",
      "\n",
      "strength of the (cid:7)2 regularizer. What SMSE do you get? Turn in your numbers and code.\n",
      "\n",
      "17 Markov and hidden Markov models\n",
      "\n",
      "17.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In this chapter, we discuss probabilistic models for sequences of observations, X1, . . . , XT , of arbitrary length T . Such models have applications in computational biology, natural language processing, time series forecasting, etc. We focus on the case where we the observations occur at discrete “time steps”, although “time” may also refer to locations within a sequence.\n",
      "\n",
      "17.2 Markov models\n",
      "\n",
      "Recall from Section 10.2.2 that the basic idea behind a Markov chain is to assume that Xt captures all the relevant information for predicting the future (i.e., we assume it is a sufficient statistic). If we assume discrete time steps, we can write the joint distribution as follows:\n",
      "\n",
      "p(X1:T ) = p(X1)p(X2|X1)p(X3|X2) . . . = p(X1)\n",
      "\n",
      "T(cid:20)\n",
      "\n",
      "p(Xt|Xt−1)\n",
      "\n",
      "(17.1)\n",
      "\n",
      "t=2\n",
      "\n",
      "This is called a Markov chain or Markov model.\n",
      "\n",
      "If we assume the transition function p(Xt|Xt−1) is independent of time, then the chain is called homogeneous, stationary, or time-invariant. This is an example of parameter tying, since the same parameter is shared by multiple variables. This assumption allows us to model an arbitrary number of variables using a ﬁxed number of parameters; such models are called stochastic processes.\n",
      "\n",
      "If we assume that the observed variables are discrete, so Xt ∈ {1, . . . , K}, this is called a discrete-state or ﬁnite-state Markov chain. We will make this assumption throughout the rest of this section.\n",
      "\n",
      "17.2.1\n",
      "\n",
      "Transition matrix\n",
      "\n",
      "When Xt is discrete, so Xt ∈ {1, . . . , K}, the conditional distribution p(Xt|Xt−1) can be written as a K × K matrix, known as the transition matrix A, where Aij = p(Xt = j|Xt−1 = i) is the probability of going from state i to state j. Each row of the matrix sums to one,\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "j Aij = 1, so this is called a stochastic matrix.\n",
      "\n",
      "590\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "1 − α\n",
      "\n",
      "α\n",
      "\n",
      "1 − β\n",
      "\n",
      "A11\n",
      "\n",
      "A22\n",
      "\n",
      "A33\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "A12\n",
      "\n",
      "2\n",
      "\n",
      "A23\n",
      "\n",
      "3\n",
      "\n",
      "β\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 17.1 3-state left-to-right chain.\n",
      "\n",
      "State transition diagrams for some simple Markov chains. Left: a 2-state chain. Right: a\n",
      "\n",
      "A stationary, ﬁnite-state Markov chain is equivalent to a stochastic automaton. It is common to visualize such automata by drawing a directed graph, where nodes represent states and arrows represent legal transitions, i.e., non-zero elements of A. This is known as a state transition diagram. The weights associated with the arcs are the probabilities. For example, the following 2-state chain\n",
      "\n",
      "A =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 − α β\n",
      "\n",
      "α 1 − β\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(17.2)\n",
      "\n",
      "is illustrated in Figure 17.1(left). The following 3-state chain ⎞\n",
      "\n",
      "A =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "A11 A12 0 0\n",
      "\n",
      "0 A22 A23 1 0\n",
      "\n",
      "⎠\n",
      "\n",
      "(17.3)\n",
      "\n",
      "is illustrated in Figure 17.1(right). This is called a left-to-right transition matrix, and is com- monly used in speech recognition (Section 17.6.2).\n",
      "\n",
      "one step. The n-step transition matrix A(n) is deﬁned as\n",
      "\n",
      "The Aij element of the transition matrix speciﬁes the probability of getting from i to j in\n",
      "\n",
      "(17.4) which is the probability of getting from i to j in exactly n steps. Obviously A(1) = A. The Chapman-Kolmogorov equations state that\n",
      "\n",
      "Aij(n) (cid:2) p(Xt+n = j|Xt = i)\n",
      "\n",
      "Aij(m + n) =\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "Aik(m)Akj(n)\n",
      "\n",
      "(17.5)\n",
      "\n",
      "k=1\n",
      "\n",
      "In words, the probability of getting from i to j in m + n steps is just the probability of getting from i to k in m steps, and then from k to j in n steps, summed up over all k. We can write the above as a matrix multiplication\n",
      "\n",
      "A(m + n) = A(m)A(n)\n",
      "\n",
      "(17.6)\n",
      "\n",
      "Hence\n",
      "\n",
      "A(n) = A A(n − 1) = A A A(n − 2) = · · · = An\n",
      "\n",
      "(17.7)\n",
      "\n",
      "Thus we can simulate multiple steps of a Markov chain by “powering up” the transition matrix.\n",
      "\n",
      "17.2. Markov models\n",
      "\n",
      "591\n",
      "\n",
      "SAYS IT’S NOT IN THE CARDS LEGENDARY RECONNAISSANCE BY ROLLIE DEMOCRACIES UNSUSTAINABLE COULD STRIKE REDLINING VISITS TO PROFIT BOOKING WAIT HERE AT MADISON SQUARE GARDEN COUNTY COURTHOUSE WHERE HE HAD BEEN DONE IN THREE ALREADY IN ANY WAY IN WHICH A TEACHER\n",
      "\n",
      "Table 17.1 Example output from an 4-gram word model, trained using backoff smoothing on the Broadcast News corpus. The ﬁrst 4 words are speciﬁed by hand, the model generates the 5th word, and then the results are fed back into the model. Source: http://www.fit.vutbr.cz/~imikolov/rnnlm/gen-4gra m.txt .\n",
      "\n",
      "17.2.2\n",
      "\n",
      "Application: Language modeling\n",
      "\n",
      "One important application of Markov models is to make statistical language models, which are probability distributions over sequences of words. We deﬁne the state space to be all the words in English (or some other language). The marginal probabilities p(Xt = k) are called unigram statistics. If we use a ﬁrst-order Markov model, then p(Xt = k|Xt−1 = j) is called a bigram If we use a second-order Markov model, then p(Xt = k|Xt−1 = j, Xt−2 = i) is model. called a trigram model. And so on. In general these are called n-gram models. For example, Figure 17.2 shows 1-gram and 2-grams counts for the letters {a, . . . , z,−} (where - represents space) estimated from Darwin’s On The Origin Of Species.\n",
      "\n",
      "Language models can be used for several things, such as the following:\n",
      "\n",
      "Sentence completion A language model can predict the next word given the previous words in a sentence. This can be used to reduce the amount of typing required, which is particularly important for disabled users (see e.g., David Mackay’s Dasher system1), or uses of mobile devices.\n",
      "\n",
      "Data compression Any density model can be used to deﬁne an encoding scheme, by assigning short codewords to more probable strings. The more accurate the predictive model, the fewer the number of bits it requires to store the data.\n",
      "\n",
      "Text classiﬁcation Any density model can be used as a class-conditional density and hence turned into a (generative) classiﬁer. Note that using a 0-gram class-conditional density (i.e., only unigram statistics) would be equivalent to a naive Bayes classiﬁer (see Section 3.5). • Automatic essay writing One can sample from p(x1:t) to generate artiﬁcial text. This is In Table 17.1, we give an example of text one way of assessing the quality of the model. generated from a 4-gram model, trained on a corpus with 400 million words. ((Tomas et al. 2011) describes a much better language model, based on recurrent neural networks, which generates much more semantically plausible text.)\n",
      "\n",
      "1. http://www.inference.phy.cam.ac.uk/dasher/\n",
      "\n",
      "592\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "Unigrams\n",
      "\n",
      "Bigrams\n",
      "\n",
      "_ a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "\n",
      "1 0.16098 _ 2 0.06687 a 3 0.01414 b 4 0.02938 c 5 0.03107 d 6 0.11055 e 7 0.02325 f 8 0.01530 g 9 0.04174 h 10 0.06233 i 11 0.00060 j 12 0.00309 k 13 0.03515 l 14 0.02107 m 15 0.06007 n 16 0.06066 o 17 0.01594 p 18 0.00077 q 19 0.05265 r 20 0.05761 s 21 0.07566 t 22 0.02149 u 23 0.00993 v 24 0.01341 w 25 0.00208 x 26 0.01381 y 27 0.00039 z\n",
      "\n",
      "_ a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "\n",
      "Figure 17.2 Unigram and bigram counts from Darwin’s On The Origin Of Species. The 2D picture on the right is a Hinton diagram of the joint distribution. The size of the white squares is proportional to the value of the entry in the corresponding vector/ matrix. Based on (MacKay 2003, p22). Figure generated by ngramPlot.\n",
      "\n",
      "17.2.2.1 MLE for Markov language models\n",
      "\n",
      "We now discuss a simple way to estimate the transition matrix from training data. The proba- bility of any particular sequence of length T is given by\n",
      "\n",
      "p(x1:T |θ) =π (x1)A(x1, x2) . . . A(xT −1, xT )\n",
      "\n",
      "(17.8)\n",
      "\n",
      "=\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "(πj)I(x1=j)\n",
      "\n",
      "T(cid:20)\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "(Ajk)I(xt=k,xt−1=j)\n",
      "\n",
      "(17.9)\n",
      "\n",
      "j=1\n",
      "\n",
      "t=2\n",
      "\n",
      "j=1\n",
      "\n",
      "k=1\n",
      "\n",
      "Hence the log-likelihood of a set of sequences D = (x1, . . . , xN ), where xi = (xi1, . . . , xi,Ti ) is a sequence of length Ti, is given by\n",
      "\n",
      "log p(D|θ) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "log p(xi|θ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "N 1\n",
      "\n",
      "j log πj +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Njk log Ajk\n",
      "\n",
      "(17.10)\n",
      "\n",
      "i=1\n",
      "\n",
      "j\n",
      "\n",
      "j\n",
      "\n",
      "k\n",
      "\n",
      "where we deﬁne the following counts:\n",
      "\n",
      "N 1\n",
      "\n",
      "j (cid:2)\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "I(xi1 = j), Njk (cid:2)\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "Ti−1(cid:4)\n",
      "\n",
      "I(xi,t = j, xi,t+1 = k)\n",
      "\n",
      "(17.11)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "t=1\n",
      "\n",
      "17.2. Markov models\n",
      "\n",
      "593\n",
      "\n",
      "Hence we can write the MLE as the normalized counts:\n",
      "\n",
      "ˆπj =\n",
      "\n",
      "N 1 j(cid:7) j N 1 j\n",
      "\n",
      ", ˆAjk =\n",
      "\n",
      "Njk(cid:7)\n",
      "\n",
      "k Njk\n",
      "\n",
      "(17.12)\n",
      "\n",
      "These results can be extended in a straightforward way to higher order Markov models. However, the problem of zero-counts becomes very acute whenever the number of states K, and/or the order of the chain, n, is large. An n-gram models has O(K n) parameters. If we have K ∼ 50, 000 words in our vocabulary, then a bi-gram model will have about 2.5 billion free parameters, corresponding to all possible word pairs. It is very unlikely we will see all of these in our training data. However, we do not want to predict that a particular word string is totally impossible just because we happen not to have seen it in our training text — that would be a severe form of overﬁtting.2\n",
      "\n",
      "A simple solution to this is to use add-one smoothing, where we simply add one to all the empirical counts before normalizing. The Bayesian justiﬁcation for this is given in Section 3.3.4.1. However add-one smoothing assumes all n-grams are equally likely, which is not very realistic. A more sophisticated Bayesian approach is discussed in Section 17.2.2.2.\n",
      "\n",
      "An alternative to using smart priors is to gather lots and lots of data. For example, Google has ﬁt n-gram models (for n = 1 : 5) based on one trillion words extracted from the web. Their data, which is over 100GB when uncompressed, is publically available.3 An example of their data, for a set of 4-grams, is shown below.\n",
      "\n",
      "serve as the incoming 92 serve as the incubator 99 serve as the independent 794 serve as the index 223 serve as the indication 72 serve as the indicator 120 serve as the indicators 45 serve as the indispensable 111 serve as the indispensible 40 serve as the individual 234 ...\n",
      "\n",
      "Although such an approach, based on “brute force and ignorance”, can be successful, it is rather unsatisfying, since it is clear that this is not how humans learn (see e.g., (Tenenbaum and Xu 2000)). A more reﬁned Bayesian approach, that needs much less data, is described in Section 17.2.2.2.\n",
      "\n",
      "17.2.2.2\n",
      "\n",
      "Empirical Bayes version of deleted interpolation\n",
      "\n",
      "A common heuristic used to ﬁx the sparse data problem is called deleted interpolation (Chen and Goodman 1996). This deﬁnes the transition matrix as a convex combination of the bigram\n",
      "\n",
      "2. A famous example of an improbable, but syntactically valid, English word string, due to Noam Chomsky, is “colourless green ideas sleep furiously”. We would not want our model to predict that this string is impossible. Even ungrammatical constructs should be allowed by our model with a certain probability, since people frequently violate grammatical rules, especially in spoken language. 3. See http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html for de- tails.\n",
      "\n",
      "594\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "frequencies fjk = Njk/Nj and the unigram frequencies fk = Nk/N :\n",
      "\n",
      "Ajk = (1 − λ)fjk + λfk\n",
      "\n",
      "(17.13)\n",
      "\n",
      "The term λ is usually set by cross validation. There is also a closely related technique called backoff smoothing; the idea is that if fjk is too small, we “back off” to a more reliable estimate, namely fk.\n",
      "\n",
      "We will now show that the deleted interpolation heuristic is an approximation to the predic- tions made by a simple hierarchical Bayesian model. Our presentation follows (McKay and Peto 1995). First, let us use an independent Dirichlet prior on each row of the transition matrix:\n",
      "\n",
      "Aj ∼ Dir(α0m1, . . . , α0mK) = Dir(α0m) = Dir(α)\n",
      "\n",
      "(17.14)\n",
      "\n",
      "where Aj is row j of the transition matrix, m is the prior mean (satisfying α0 is the prior strength. We will use the same prior for each row: see Figure 17.3.\n",
      "\n",
      "The posterior is given by Aj ∼ Dir(α + Nj), where Nj = (Nj1, . . . , NjK) is the vector that records the number of times we have transitioned out of state j to each of the other states. From Equation 3.51, the posterior predictive density is\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "k mk = 1) and\n",
      "\n",
      "p(Xt+1 = k|Xt = j, D) = Ajk =\n",
      "\n",
      "Njk + αmk Nj + α0\n",
      "\n",
      "=\n",
      "\n",
      "fjkNj + αmk Nj + α0\n",
      "\n",
      "= (1 − λj)fjk + λjmk(17.15)\n",
      "\n",
      "where Ajk = E [Ajk|D, α] and\n",
      "\n",
      "λj =\n",
      "\n",
      "α Nj + α0\n",
      "\n",
      "(17.16)\n",
      "\n",
      "This is very similar to Equation 17.13 but not identical. The main difference is that the Bayesian model uses a context-dependent weight λj to combine mk with the empirical frequency fjk, rather than a ﬁxed weight λ. This is like adaptive deleted interpolation. Furthermore, rather than backing off to the empirical marginal frequencies fk, we back off to the model parameter mk.\n",
      "\n",
      "The only remaining question is: what values should we use for α and m? Let’s use empirical Bayes. Since we assume each row of the transition matrix is a priori independent given α, the marginal likelihood for our Markov model is found by applying Equation 5.24 to each row:\n",
      "\n",
      "p(D|α) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "j\n",
      "\n",
      "B(Nj + α) B(α)\n",
      "\n",
      "(17.17)\n",
      "\n",
      "where Nj = (Nj1, . . . , NjK) are the counts for leaving state j and B(α) is the generalized beta function.\n",
      "\n",
      "We can ﬁt this using the methods discussed in (Minka 2000e). However, we can also use the\n",
      "\n",
      "following approximation (McKay and Peto 1995, p12):\n",
      "\n",
      "mk ∝ |{j : Njk > 0}|\n",
      "\n",
      "(17.18)\n",
      "\n",
      "This says that the prior probability of word k is given by the number of different contexts in which it occurs, rather than the number of times it occurs. To justify the reasonableness of this result, Mackay and Peto (McKay and Peto 1995) give the following example.\n",
      "\n",
      "17.2. Markov models\n",
      "\n",
      "595\n",
      "\n",
      "Figure 17.3 A Markov chain in which we put a different Dirichlet prior on every row of the transition matrix A, but the hyperparameters of the Dirichlet are shared.\n",
      "\n",
      "Imagine, you see, that the language, you see, has, you see, a frequently occuring couplet ’you see’, you see, in which the second word of the couplet, see, follows the first word, you, with very high probability, you see. Then the marginal statistics, you see, are going to become hugely dominated, you see, by the words you and see, with equal frequency, you see.\n",
      "\n",
      "If we use the standard smoothing formula, Equation 17.13, then P(you|novel) and P(see|novel), for some novel context word not seen before, would turn out to be the same, since the marginal frequencies of ’you’ and ’see’ are the same (11 times each). However, this seems unreasonable. ’You’ appears in many contexts, so P(you|novel) should be high, but ’see’ only follows ’you’, so P(see|novel) should be low. If we use the Bayesian formula Equation 17.15, we will get this effect for free, since we back off to mk not fk, and mk will be large for ’you’ and small for ’see’ by Equation 17.18.\n",
      "\n",
      "Unfortunately, although elegant, this Bayesian model does not beat the state-of-the-art lan- guage model, known as interpolated Kneser-Ney (Kneser and Ney 1995; Chen and Goodman 1998). However, in (Teh 2006), it was shown how one can build a non-parametric Bayesian model which outperforms interpolated Kneser-Ney, by using variable-length contexts. In (Wood et al. 2009), this method was extended to create the “sequence memoizer”, which is currently (2010) the best-performing language model.4\n",
      "\n",
      "17.2.2.3\n",
      "\n",
      "Handling out-of-vocabulary words\n",
      "\n",
      "While the above smoothing methods handle the case where the counts are small or even zero, In none of them deal with the case where the test set may contain a completely novel word. particular, they all assume that the words in the vocabulary (i.e., the state space of Xt) is ﬁxed and known (typically it is the set of unique words in the training data, or in some dictionary).\n",
      "\n",
      "4. Interestingly, these non-parametric methods are based on posterior inference using MCMC (Section 24.1) and/or particle ﬁltering (Section 23.5), rather than optimization methods such as EB. Despite this, they are quite efficient.\n",
      "\n",
      "596\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "1\n",
      "\n",
      "1.0\n",
      "\n",
      "0.5\n",
      "\n",
      "1.0\n",
      "\n",
      "2\n",
      "\n",
      "0.5\n",
      "\n",
      "3\n",
      "\n",
      "0.1\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "0.9\n",
      "\n",
      "0.1\n",
      "\n",
      "2\n",
      "\n",
      "0.5\n",
      "\n",
      "3\n",
      "\n",
      "0.5\n",
      "\n",
      "1.0\n",
      "\n",
      "4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 17.4\n",
      "\n",
      "Some Markov chains. (a) A 3-state aperiodic chain. (b) A reducible 4-state chain.\n",
      "\n",
      "Even if all Ajk’s are non-zero, none of these models will predict a novel word outside of this set, and hence will assign zero probability to a test sentence with an unfamiliar word. (Unfamiliar words are bound to occur, because the set of words is an open class. For example, the set of proper nouns (names of people and places) is unbounded.)\n",
      "\n",
      "A standard heuristic to solve this problem is to replace all novel words with the special symbol unk, which stands for “unknown”. A certain amount of probability mass is held aside for this event.\n",
      "\n",
      "A more principled solution would be to use a Dirichlet process, which can generate a countably inﬁnite state space, as the amount of data increases (see Section 25.2.2). If all novel words are “accepted” as genuine words, then the system has no predictive power, since any misspelling will be considered a new word. So the novel word has to be seen frequently enough to warrant being added to the vocabulary. See e.g., (Friedman and Singer 1999; Griffiths and Tenenbaum 2001) for details.\n",
      "\n",
      "17.2.3\n",
      "\n",
      "Stationary distribution of a Markov chain *\n",
      "\n",
      "We have been focussing on Markov models as a way of deﬁning joint probability distributions over sequences. However, we can also interpret them as stochastic dynamical systems, where we “hop” from one state to another at each time step. In this case, we are often interested in the long term distribution over states, which is known as the stationary distribution of the chain. In this section, we discuss some of the relevant theory. Later we will consider two important applications: Google’s PageRank algorithm for ranking web pages (Section 17.2.4), and the MCMC algorithm for generating samples from hard-to-normalize probability distributions (Chapter 24).\n",
      "\n",
      "17.2.3.1 What is a stationary distribution?\n",
      "\n",
      "Let Aij = p(Xt = j|Xt−1 = i) be the one-step transition matrix, and let πt(j) =p( Xt = j) be the probability of being in state j at time t. It is conventional in this context to assume that π is a row vector. If we have an initial distribution over states of π0, then at time 1 we have\n",
      "\n",
      "π1(j) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "π0(i)Aij\n",
      "\n",
      "(17.19)\n",
      "\n",
      "i\n",
      "\n",
      "or, in matrix notation,\n",
      "\n",
      "π1 = π0A\n",
      "\n",
      "(17.20)\n",
      "\n",
      "17.2. Markov models\n",
      "\n",
      "597\n",
      "\n",
      "We can imagine iterating these equations. If we ever reach a stage where\n",
      "\n",
      "π = πA\n",
      "\n",
      "(17.21)\n",
      "\n",
      "then we say we have reached the stationary distribution (also called the invariant distribution or equilibrium distribution). Once we enter the stationary distribution, we will never leave.\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "For example, consider the chain in Figure 17.4(a). To ﬁnd its stationary distribution, we write\n",
      "\n",
      "π1 π2 π3\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "π1 π2 π3\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "1 − A12 − A13 A21 A31\n",
      "\n",
      "A12 1 − A21 − A23 A32\n",
      "\n",
      "A13 A23 1 − A31 − A32\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠(17.22)\n",
      "\n",
      "so\n",
      "\n",
      "π1 = π1(1 − A12 − A12) +π 2A21 + π3A31\n",
      "\n",
      "(17.23)\n",
      "\n",
      "or\n",
      "\n",
      "π1(A12 + A13) = π2A21 + π3A31\n",
      "\n",
      "(17.24)\n",
      "\n",
      "In general, we have (cid:4)\n",
      "\n",
      "πi\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Aij =\n",
      "\n",
      "πjAji\n",
      "\n",
      "(17.25)\n",
      "\n",
      "j(cid:5)=i\n",
      "\n",
      "j(cid:5)=i\n",
      "\n",
      "In other words, the probability of being in state i times the net ﬂow out of state i must equal the probability of being in each other state j times the net ﬂow from that state into i. These are called the global balance equations. We can then solve these equations, subject to the constraint that\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "j πj = 1.\n",
      "\n",
      "17.2.3.2\n",
      "\n",
      "Computing the stationary distribution To ﬁnd the stationary distribution, we can just solve the eigenvector equation AT v = v, and then to set π = vT , where v is an eigenvector with eigenvalue 1. (We can be sure such an eigenvector exists, since A is a row-stochastic matrix, so A1 = 1; also recall that the eigenvalues of A and AT are the same.) Of course, since eigenvectors are unique only up to constants of proportionality, we must normalize v at the end to ensure it sums to one.\n",
      "\n",
      "Note, however, that the eigenvectors are only guaranteed to be real-valued if the matrix is positive, Aij > 0 (and hence Aij < 1, due to the sum-to-one constraint). A more general approach, which can handle chains where some transition probabilities are 0 or 1 (such as Figure 17.4(a)), is as follows (Resnick 1992, p138). We have K constraints from π(I − A) = 0K×1 and 1 constraint from π1K×1 = 0. Since we only have K unknowns, this is overconstrained. So let us replace any column (e.g., the last) of I − A with 1, to get a new matrix, call it M. Next we deﬁne r = [0, 0, . . . , 1], where the 1 in the last position corresponds to the column of all 1s in M. We then solve πM = r. For example, for a 3 state chain we have to solve this linear system:\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "π1 π2 π3\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "1 − A11 −A12 1 − A22 −A21 −A32 −A31\n",
      "\n",
      "1 1 1\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠ =\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(17.26)\n",
      "\n",
      "598\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "For the chain in Figure 17.4(a) we ﬁnd π = [0.4, 0.4, 0.2]. We can easily verify this is correct, since π = πA. See mcStatDist for some Matlab code.\n",
      "\n",
      "Unfortunately, not all chains have a stationary distribution. as we explain below.\n",
      "\n",
      "17.2.3.3 When does a stationary distribution exist? *\n",
      "\n",
      "Consider the 4-state chain in Figure 17.4(b). If we start in state 4, we will stay there forever, since 4 is anabsorbing state. Thus π = (0, 0, 0, 1) is one possible stationary distribution. However, if we start in 1 or 2, we will oscillate between those two states for ever. So π = (0.5, 0.5, 0, 0) is another possible stationary distribution. If we start in state 3, we could end up in either of the above stationary distributions.\n",
      "\n",
      "We see from this example that a necessary condition to have a unique stationary distribution is that the state transition diagram be a singly connected component, i.e., we can get from any state to any other state. Such chains are called irreducible.\n",
      "\n",
      "Now consider the 2-state chain in Figure 17.1(a). This is irreducible provided α, β > 0. Suppose α = β = 0.9. It is clear by symmetry that this chain will spend 50% of its time in each state. Thus π = (0.5, 0.5). But now suppose α = β = 1. In this case, the chain will oscillate between the two states, but the long-term distribution on states depends on where you start from. If we start in state 1, then on every odd time step (1,3,5,...) we will be in state 1; but if we start in state 2, then on every odd time step we will be in state 2.\n",
      "\n",
      "distribution if πj = limn→∞ An ij exists and is independent of i, for all j. the long-run distribution over states will be independent of the starting state:\n",
      "\n",
      "This example motivates the following deﬁnition. Let us say that a chain has a limiting If this holds, then\n",
      "\n",
      "P (Xt = j) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "P (X0 = i)Aij(t) → πj as t → ∞\n",
      "\n",
      "(17.27)\n",
      "\n",
      "i\n",
      "\n",
      "Let us now characterize when a limiting distribution exists. Deﬁne the period of state i to be\n",
      "\n",
      "d(i) =gcd{t : Aii(t) > 0}\n",
      "\n",
      "(17.28)\n",
      "\n",
      "where gcd stands for greatest common divisor, i.e., the largest integer that divides all the members of the set. For example, in Figure 17.4(a), we have d(1) = d(2) = gcd(2, 3, 4, 6, ...) = 1 and d(3) = gcd(3, 5, 6, ...) = 1. We say a state i is aperiodic if d(i) = 1. (A sufficient condition to ensure this is if state i has a self-loop, but this is not a necessary condition.) We say a chain is aperiodic if all its states are aperiodic. One can show the following important result:\n",
      "\n",
      "Theorem 17.2.1. Every irreducible (singly connected), aperiodic ﬁnite state Markov chain has a limiting distribution, which is equal to π, its unique stationary distribution.\n",
      "\n",
      "A special case of this result says that every regular ﬁnite state chain has a unique stationary distribution, where a regular chain is one whose transition matrix satisﬁes An ij > 0 for some integer n and all i, j, i.e., it is possible to get from any state to any other state in n steps. Consequently, after n steps, the chain could be in any state, no matter where it started. One can show that sufficient conditions to ensure regularity are that the chain be irreducible (singly connected) and that every state have a self-transition.\n",
      "\n",
      "To handle the case of Markov chains whose state-space is not ﬁnite (e.g, the countable set of all integers, or all the uncountable set of all reals), we need to generalize some of the earlier\n",
      "\n",
      "17.2. Markov models\n",
      "\n",
      "599\n",
      "\n",
      "deﬁnitions. Since the details are rather technical, we just brieﬂy state the main results without proof. See e.g., (Grimmett and Stirzaker 1992) for details.\n",
      "\n",
      "For a stationary distribution to exist, we require irreducibility (singly connected) and aperiod- icity, as before. But we also require that each state is recurrent. (A chain in which all states are recurrent is called a recurrent chain.) Recurrent means that you will return to that state with probability 1. As a simple example of a non-recurrent state (i.e., a transient state), consider Figure 17.4(b): states 3 is transient because one immediately leaves it and either spins around state 4 forever, or oscillates between states 1 and 2 forever. There is no way to return to state 3. It is clear that any ﬁnite-state irreducible chain is recurrent, since you can always get back to where you started from. But now consider an example with an inﬁnite state space. Suppose we perform a random walk on the integers, X = {. . . ,− 2, −1, 0, 1, 2, . . .}. Let Ai,i+1 = p be the probability of moving right, and Ai,i−1 = 1 − p be the probability of moving left. Suppose we start at X1 = 0. If p >0. 5, we will shoot off to +∞; we are not guaranteed to return. Similarly, if p <0. 5, we will shoot off to −∞. So in both cases, the chain is not recurrent, even though it is irreducible.\n",
      "\n",
      "It should be intuitively obvious that we require all states to be recurrent for a stationary distribution to exist. However, this is not sufficient. To see this, consider the random walk on the integers again, and suppose p = 0.5. In this case, we can return to the origin an it takes inﬁnitely long to do inﬁnite number of times, so the chain is recurrent. However, so. This prohibits it from having a stationary distribution. The intuitive reason is that the distribution keeps spreading out over a larger and larger set of the integers, and never converges to a stationary distribution. More formally, we deﬁne a state to be non-null recurrent if the expected time to return to this state is ﬁnite. A chain in which all states are non-null is called a non-null chain.\n",
      "\n",
      "For brevity, we we say that a state is ergodic if it is aperiodic, recurrent and non-null, and\n",
      "\n",
      "we say a chain is ergodic if all its states are ergodic.\n",
      "\n",
      "We can now state our main theorem:\n",
      "\n",
      "Theorem 17.2.2. Every irreducible (singly connected), ergodic Markov chain has a limiting distri- bution, which is equal to π, its unique stationary distribution.\n",
      "\n",
      "This generalizes Theorem 17.2.1, since for irreducible ﬁnite-state chains, all states are recurrent\n",
      "\n",
      "and non-null.\n",
      "\n",
      "17.2.3.4\n",
      "\n",
      "Detailed balance\n",
      "\n",
      "Establishing ergodicity can be difficult. We now give an alternative condition that is easier to verify.\n",
      "\n",
      "(17.29) These are called the detailed balance equations. This says that the ﬂow from i to j must equal the ﬂow from j to i, weighted by the appropriate source probabilities.\n",
      "\n",
      "We say that a Markov chain A is time reversible if there exists a distribution π such that πiAij = πjAji\n",
      "\n",
      "We have the following important result.\n",
      "\n",
      "Theorem 17.2.3. balance wrt distribution π, then π is a stationary distribution of the chain.\n",
      "\n",
      "If a Markov chain with transition matrix A is regular and satisﬁes detailed\n",
      "\n",
      "600\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "X1\n",
      "\n",
      "X4\n",
      "\n",
      "X2\n",
      "\n",
      "X3\n",
      "\n",
      "X6\n",
      "\n",
      "X5\n",
      "\n",
      "Figure 17.5 A very small world wide web. Figure generated by pagerankDemo, written by Tim Davis.\n",
      "\n",
      "Proof. To see this, note that\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "πiAij =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "πjAji = πj\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Aji = πj\n",
      "\n",
      "(17.30)\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "and hence π = Aπ.\n",
      "\n",
      "Note that this condition is sufficient but not necessary (see Figure 17.4(a) for an example of a\n",
      "\n",
      "chain with a stationary distribution which does not satisfy detailed balance).\n",
      "\n",
      "In Section 24.1, we will discuss Markov chain Monte Carlo or MCMC methods. These take as input a desired distribution π and construct a transition matrix (or in general, a transition kernel) A which satisﬁes detailed balance wrt π. Thus by sampling states from such a chain, we will eventually enter the stationary distribution, and will visit states with probabilities given by π.\n",
      "\n",
      "17.2.4\n",
      "\n",
      "Application: Google’s PageRank algorithm for web page ranking *\n",
      "\n",
      "The results in Section 17.2.3 form the theoretical underpinnings to Google’s PageRank algorithm, which is used for information retrieval on the world-wide web. We sketch the basic idea below; see (Byran and Leise 2006) for a more detailed explanation.\n",
      "\n",
      "We will treat the web as a giant directed graph, where nodes represent web pages (documents) and edges represent hyper-links.5 We then perform a process called web crawling. We start at a few designated root nodes, such as dmoz.org, the home of the Open Directory Project, and then follows the links, storing all the pages that we encounter, until we run out of time.\n",
      "\n",
      "Next, all of the words in each web page are entered into a data structure called an inverted index. That is, for each word, we store a list of the documents where this word occurs. (In practice, we store a list of hash codes representing the URLs.) At test time, when a user enters\n",
      "\n",
      "5. In 2008, Google said it had indexed 1 trillion (1012) unique URLs. If we assume there are about 10 URLs per page (on average), this means there were about 100 billion unique web pages. Estimates for 2010 are about 121 billion unique web pages. Source: thenextweb.com/shareables/2011/01/11/infographic-how-big-is-the-internet.\n",
      "\n",
      "17.2. Markov models\n",
      "\n",
      "601\n",
      "\n",
      "a query, we can just look up all the documents containing each word, and intersect these lists (since queries are deﬁned by a conjunction of search terms). We can get a reﬁned search by storing the location of each word in each document. We can then test if the words in a document occur in the same order as in the query.\n",
      "\n",
      "Let us give an example, from http://en.wikipedia.org/wiki/Inverted_index. We have 3 documents, T0 = “it is what it is”, T1 = “what is it” and T2 = “it is a banana”. Then we can create the following inverted index, where each pair represents a document and word location:\n",
      "\n",
      "\"a\": {(2, 2)} \"banana\": {(2, 3)} \"is\": \"it\": \"what\":\n",
      "\n",
      "{(0, 1), (0, 4), (1, 1), (2, 1)} {(0, 0), (0, 3), (1, 2), (2, 0)} {(0, 2), (1, 0)}\n",
      "\n",
      "For example, we see that the word “what” occurs at location 2 (counting from 0) in document 0, and location 0 in document 1. Suppose we search for “what is it”. If we ignore word order, we retrieve the following documents:\n",
      "\n",
      "{T0, T1} ∩ {T0, T1, T2} ∩ {T0, T1, T2} = {T0, T1}\n",
      "\n",
      "(17.31)\n",
      "\n",
      "If we require that the word order matches, only document T1 would be returned. More generally, we can allow out-of-order matches, but can give “bonus points” to documents whose word order matches the query’s word order, or to other features, such as if the words occur in the title of a document. We can then return the matching documents in decreasing order of their score/ relevance. This is called document ranking.\n",
      "\n",
      "So far, we have described the standard process of information retrieval. But the link structure of the web provides an additional source of information. The basic idea is that some web pages are more authoritative than others, so these should be ranked higher (assuming they match the query). A web page is an authority if it is linked to by many other pages. But to protect against the effect of so-called link farms, which are dummy pages which just link to a given site to boost its apparent relevance, we will weight each incoming link by the source’s authority. Thus we get the following recursive deﬁnition for the authoritativeness of page j, also called its PageRank:\n",
      "\n",
      "πj =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Aijπi\n",
      "\n",
      "(17.32)\n",
      "\n",
      "i\n",
      "\n",
      "where Aij is the probability of following a link from i to j. We recognize Equation 17.32 as the stationary distribution of a Markov chain.\n",
      "\n",
      "In the simplest setting, we deﬁne Ai. as a uniform distribution over all states that i is connected to. However, to ensure the distribution is unique, we need to make the chain into a regular chain. This can be done by allowing each state i to jump to any other state (including itself) with some small probability. This effectively makes the transition matrix aperiodic and fully connected (although the adjacency matrix Gij of the web itself is highly sparse).\n",
      "\n",
      "We discuss efficient methods for computing the leading eigenvector of this giant matrix below. But ﬁrst, let us give an example of the PageRank algorithm. Consider the small web in Figure 17.5.\n",
      "\n",
      "602\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "0\n",
      "\n",
      "0.02\n",
      "\n",
      "50\n",
      "\n",
      "0.018\n",
      "\n",
      "100\n",
      "\n",
      "0.016\n",
      "\n",
      "150\n",
      "\n",
      "0.014\n",
      "\n",
      "200\n",
      "\n",
      "0.012\n",
      "\n",
      "250\n",
      "\n",
      "0.01\n",
      "\n",
      "300\n",
      "\n",
      "0.008\n",
      "\n",
      "350\n",
      "\n",
      "0.006\n",
      "\n",
      "400\n",
      "\n",
      "0.004\n",
      "\n",
      "450\n",
      "\n",
      "0.002\n",
      "\n",
      "500\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "nz = 2636\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 17.6 (a) Web graph of 500 sites rooted at www.harvard.edu. (b) Corresponding page rank vector. Figure generated by pagerankDemoPmtk, Based on code by Cleve Moler (Moler 2004).\n",
      "\n",
      "We ﬁnd that the stationary distribution is\n",
      "\n",
      "π = (0.3209, 0.1706, 0.1065, 0.1368, 0.0643, 0.2008)\n",
      "\n",
      "(17.33)\n",
      "\n",
      "So a random surfer will visit site 1 about 32% of the time. We see that node 1 has a higher PageRank than nodes 4 or 6, even though they all have the same number of in-links. This is because being linked to from an inﬂuential nodehelps increase your PageRank score more than being linked to by a less inﬂuential node.\n",
      "\n",
      "harvard.edu. Figure 17.6(b) shows the corresponding PageRank vector.\n",
      "\n",
      "As a slightly larger example, Figure 17.6(a) shows a web graph, derived from the root of\n",
      "\n",
      "17.2.4.1\n",
      "\n",
      "Efficiently computing the PageRank vector\n",
      "\n",
      "Let Gij = 1 iff there is a link from j to i. Now imagine performing a random walk on this graph, where at every time step, with probability p = 0.85 you follow one of the outlinks uniformly at random, and with probability 1 − p you jump to a random node, again chosen uniformly at random. If there are no outlinks, you just jump to a random page. (These random jumps, including self-transitions, ensure the chain is irreducible (singly connected) and regular. Hence we can solve for its unique stationary distribution using eigenvector methods.) This deﬁnes the following transition matrix:\n",
      "\n",
      "Mij =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "pGij/cj + δ 1/n\n",
      "\n",
      "if cj (cid:4)= 0 if cj = 0\n",
      "\n",
      "(17.34)\n",
      "\n",
      "where n is the number of nodes, δ = (1 − p)/n is the probability of jumping from one page (cid:7) to another without following a link and cj = (If n = 4 · 109 and p = 0.85, then δ = 3.75 · 10−11.) Here M is a stochastic matrix in which columns sum to one. Note that M = AT in our earlier notation.\n",
      "\n",
      "We can represent the transition matrix compactly as follows. Deﬁne the diagonal matrix D\n",
      "\n",
      "i Gij represents the out-degree of page j.\n",
      "\n",
      "with entries (cid:19)\n",
      "\n",
      "djj =\n",
      "\n",
      "1/cj 0\n",
      "\n",
      "if cj (cid:4)= 0 if cj = 0\n",
      "\n",
      "(17.35)\n",
      "\n",
      "17.3. Hidden Markov models\n",
      "\n",
      "603\n",
      "\n",
      "Deﬁne the vector z with components\n",
      "\n",
      "zj =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "δ 1/n\n",
      "\n",
      "if cj (cid:4)= 0 if cj = 0\n",
      "\n",
      "(17.36)\n",
      "\n",
      "Then we can rewrite Equation 17.34 as follows:\n",
      "\n",
      "M = pGD + 1zT\n",
      "\n",
      "(17.37)\n",
      "\n",
      "The matrix M is not sparse, but it is a rank one modiﬁcation of a sparse matrix. Most of the elements of M are equal to the small constant δ. Obviously these do not need to be stored explicitly.\n",
      "\n",
      "Our goal is to solve v = Mv, where v = πT . One efficient method to ﬁnd the leading eigenvector of a large matrix is known as the power method. This simply consists of repeated matrix-vector multiplication, followed by normalization:\n",
      "\n",
      "v ∝ Mv = pGDv + 1zT v\n",
      "\n",
      "(17.38)\n",
      "\n",
      "It is possible to implement the power method without using any matrix multiplications, by simply sampling from the transition matrix and counting how often you visit each state. This is essentially a Monte Carlo approximation to the sum implied by v = Mv. Applying this to the data in Figure 17.6(a) yields the stationary distribution in Figure 17.6(b). This took 13 iterations to converge, starting from a uniform distribution. (See also the function pagerankDemo, by Tim Davis, for an animation of the algorithm in action, applied to the small web example.) To handle changing web structure, we can re-run this algorithm every day or every week, starting v off at the old distribution (Langville and Meyer 2006).\n",
      "\n",
      "For details on how to perform this Monte Carlo power method in a parallel distributed\n",
      "\n",
      "computing environment, see e.g., (Rajaraman and Ullman 2010).\n",
      "\n",
      "17.2.4.2 Web spam\n",
      "\n",
      "PageRank is not foolproof. For example, consider the strategy adopted by JC Penney, a depart- ment store in the USA. During the Christmas season of 2010, it planted many links to its home page on 1000s of irrelevant web pages, thus increasing its ranking on Google’s search engine (Segal 2011). Even though each of these source pages has low PageRank, there were so many of them that their effect added up. Businesses call this search engine optimization; Google calls it web spam. When Google was notiﬁed of this scam (by the New York Times), it manually downweighted JC Penney, since such behavior violates Google’s code of conduct. The result was that JC Penney dropped from rank 1 to rank 65, essentially making it disappear from view. Automatically detecting such scams relies on various techniques which are beyond the scope of this chapter.\n",
      "\n",
      "17.3 Hidden Markov models\n",
      "\n",
      "As we mentioned in Section 10.2.2, a hidden Markov model or HMM consists of a discrete-time, discrete-state Markov chain, with hidden states zt ∈ {1, . . . , K}, plus an observation model\n",
      "\n",
      "604\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "20\n",
      "\n",
      "3\n",
      "\n",
      "2.8\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "13\n",
      "\n",
      "12\n",
      "\n",
      "9\n",
      "\n",
      "2.6\n",
      "\n",
      "2.4\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "3 15\n",
      "\n",
      "7\n",
      "\n",
      "17 456 16 2\n",
      "\n",
      "10\n",
      "\n",
      "11\n",
      "\n",
      "14\n",
      "\n",
      "8 20\n",
      "\n",
      "18\n",
      "\n",
      "2.2\n",
      "\n",
      "2\n",
      "\n",
      "1.8\n",
      "\n",
      "1.6\n",
      "\n",
      "1.4\n",
      "\n",
      "−5\n",
      "\n",
      "1\n",
      "\n",
      "19\n",
      "\n",
      "1.2\n",
      "\n",
      "1\n",
      "\n",
      "−10\n",
      "\n",
      "−20\n",
      "\n",
      "−15\n",
      "\n",
      "−10\n",
      "\n",
      "−5\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "18\n",
      "\n",
      "20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 17.7 hidden state sequence. Based on Figure 13.8 of (Bishop 2006b). Figure generated by hmmLillypadDemo.\n",
      "\n",
      "(a) Some 2d data sampled from a 3 state HMM. Each state emits from a 2d Gaussian. (b) The\n",
      "\n",
      "p(xt|zt). The corresponding joint distribution has the form\n",
      "\n",
      "p(z1:T , x1:T ) =p( z1:T )p(x1:T |z1:T ) =\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "p(z1)\n",
      "\n",
      "T(cid:20)\n",
      "\n",
      "p(zt|zt−1)\n",
      "\n",
      "(cid:25) (cid:24)\n",
      "\n",
      "T(cid:20)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "p(xt|zt)\n",
      "\n",
      "(17.39)\n",
      "\n",
      "t=2\n",
      "\n",
      "t=1\n",
      "\n",
      "The observations in an HMM can be discrete or continuous. If they are discrete, it is common for the observation model to be an observation matrix:\n",
      "\n",
      "p(xt = l|zt = k, θ) = B(k, l)\n",
      "\n",
      "(17.40)\n",
      "\n",
      "If the observations are continuous, it is common for the observation model to be a conditional Gaussian:\n",
      "\n",
      "p(xt|zt = k, θ) = N (xt|μk, Σk)\n",
      "\n",
      "(17.41)\n",
      "\n",
      "Figure 17.7 shows an example where we have 3 states, each of which emits a different Gaussian. The resulting model is similar to a Gaussian mixture model, except the cluster membership (Indeed, HMMs are sometimes called Markov switching models has Markovian dynamics. (Fruhwirth-Schnatter 2007).) We see that we tend to get multiple observations in the same location, and then a sudden jump to a new cluster.\n",
      "\n",
      "17.3.1\n",
      "\n",
      "Applications of HMMs\n",
      "\n",
      "HMMs can be used as black-box density models on sequences. They have the advantage over Markov models in that they can represent long-range dependencies between observations, In particular, note that they do not assume the Markov mediated via the latent variables. property holds for the observations themselves. Such black-box models are useful for time- series prediction (Fraser 2008). They can also be used to deﬁne class-conditional densities inside a generative classiﬁer.\n",
      "\n",
      "However, it is more common to imbue the hidden states with some desired meaning, and to then try to estimate the hidden states from the observations, i.e., to compute p(zt|x1:t) if we are\n",
      "\n",
      "17.3. Hidden Markov models\n",
      "\n",
      "605\n",
      "\n",
      "bat rat cat gnat goat\n",
      "\n",
      "x x . . . x - C - A G - A - A G - C A G - A A - - A A A C - - C A G - - 3 1 . . 2 .\n",
      "\n",
      "(a)\n",
      "\n",
      "D\n",
      "\n",
      "D\n",
      "\n",
      "D\n",
      "\n",
      "I\n",
      "\n",
      "I\n",
      "\n",
      "I\n",
      "\n",
      "I\n",
      "\n",
      "Begin\n",
      "\n",
      "M M\n",
      "\n",
      "M M\n",
      "\n",
      "M\n",
      "\n",
      "End\n",
      "\n",
      "0 0\n",
      "\n",
      "1\n",
      "\n",
      "2 (b)\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "Figure 17.8 (a) Some DNA sequences. (b) State transition diagram for a proﬁle HMM. Source: Figure 5.7 of (Durbin et al. 1998). Used with kind permission of Richard Durbin.\n",
      "\n",
      "in an online scenario, or p(zt|x1:T ) if we are in an offline scenario (see Section 17.4.1 for further discussion of the differences between these two approaches). Below we give some examples of applications which use HMMs in this way:\n",
      "\n",
      "Automatic speech recognition. Here xt represents features extracted from the speech signal, and zt represents the word that is being spoken. The transition model p(zt|zt−1) represents the language model, and the observation model p(xt|zt) represents the acoustic model. See e.g., (Jelinek 1997; Jurafsky and Martin 2008) for details.\n",
      "\n",
      "Activity recognition. Here xt represents features extracted from a video frame, and zt is the class of activity the person is engaged in (e.g., running, walking, sitting, etc.) See e.g., (Szeliski 2010) for details.\n",
      "\n",
      "Part of speech tagging. Here xt represents a word, and zt represents its part of speech (noun, verb, adjective, etc.) See Section 19.6.2.1 for more information on POS tagging and\n",
      "\n",
      "606\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "Gene ﬁnding. Here xt represents the DNA nucleotides (A,C,G,T), and zt represents whether we are inside a gene-coding region or not. See e.g., (Schweikerta et al. 2009) for details. • Protein sequence alignment. Here xt represents an amino acid, and zt represents whether this matches the latent consensus sequence at this location. This model is called a proﬁle HMM and is illustrated in Figure 17.8. The HMM has 3 states, called match, insert and delete. If zt is a match state, then xt is equal to the t’th value of the consensus. If zt is an insert state, then xt is generated from a uniform distribution that is unrelated to the consensus sequence. If zt is a delete state, then xt = −. In this way, we can generate noisy copies of the consensus sequence of different lengths. In Figure 17.8(a), the consensus is “AGC”, and we see various versions of this below. A path through the state transition diagram, shown in Figure 17.8(b), speciﬁes how to align a sequence to the consensus, e.g., for the gnat, the most probable path is D, D, I, I, I, M . This means we delete the A and G parts of the consensus sequence, we insert 3 A’s, and then we match the ﬁnal C. We can estimate the model parameters by counting the number of such transitions, and the number of emissions from each kind of state, as shown in Figure 17.8(c). See Section 17.5 for more information on training an HMM, and (Durbin et al. 1998) for details on proﬁle HMMs.\n",
      "\n",
      "related tasks.\n",
      "\n",
      "Note that for some of these tasks, conditional random ﬁelds, which are essentially discrimi-\n",
      "\n",
      "native versions of HMMs, may be more suitable; see Chapter 19 for details.\n",
      "\n",
      "17.4\n",
      "\n",
      "Inference in HMMs\n",
      "\n",
      "We now discuss how to infer the hidden state sequence of an HMM, assuming the parameters are known. Exactly the same algorithms apply to other chain-structured graphical models, such as chain CRFs (see Section 19.6.1). In Chapter 20, we generalize these methods to arbitrary graphs. And in Section 17.5.2, we show how we can use the output of inference in the context of parameter estimation.\n",
      "\n",
      "17.4.1\n",
      "\n",
      "Types of inference problems for temporal models\n",
      "\n",
      "There are several different kinds of inferential tasks for an HMM (and SSM in general). To illustrate the differences, we will consider an example called the occasionally dishonest casino, In this model, xt ∈ {1, 2, . . . , 6} represents which dice face shows from (Durbin et al. 1998). up, and zt represents the identity of the dice that is being used. Most of the time the casino uses a fair dice, z = 1, but occasionally it switches to a loaded dice, z = 2, for a short period. If z = 1 the observation distribution is a uniform multinoulli over the symbols {1, . . . , 6}. If z = 2, the observation distribution is skewed towards face 6 (see Figure 17.9). If we sample from this model, we may observe data such as the following:\n",
      "\n",
      "Listing 17.1 Example output of casinoDemo\n",
      "\n",
      "Rolls : Die :\n",
      "\n",
      "6 6 4 1 5 3 2 1 6 1 6 2 1 1 5 2 3 4 6 5 3 2 1 4 3 5 6 6 3 4 2 6 1 6 5 5 2 3 4 2 3 2 3 1 5 1 4 2 4 6 4 1 5 6 6 6 3 2 4 6 L L L L L L L L L L L L L L F F F F F F L L L L L L L L L L L L L L F F F F F F F F F F F F F F F F F F L L L L L L L L\n",
      "\n",
      "Here “rolls” refers to the observed symbol and “die” refers to the hidden state (L is loaded and F is fair). Thus we see that the model generates a sequence of symbols, but the statistics of the\n",
      "\n",
      "17.4.\n",
      "\n",
      "Inference in HMMs\n",
      "\n",
      "607\n",
      "\n",
      "(cid:19)(cid:17)(cid:28)(cid:24)\n",
      "\n",
      "(cid:19)(cid:17)(cid:28)(cid:19)\n",
      "\n",
      "(cid:20)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:25)\n",
      "\n",
      "(cid:21)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:25)\n",
      "\n",
      "(cid:19)(cid:17)(cid:20)\n",
      "\n",
      "(cid:20)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:20)(cid:19)\n",
      "\n",
      "(cid:21)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:20)(cid:19)\n",
      "\n",
      "(cid:22)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:25)\n",
      "\n",
      "(cid:22)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:20)(cid:19)\n",
      "\n",
      "(cid:23)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:25)\n",
      "\n",
      "(cid:19)(cid:17)(cid:19)(cid:24)\n",
      "\n",
      "(cid:23)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:20)(cid:19)\n",
      "\n",
      "(cid:24)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:25)\n",
      "\n",
      "(cid:24)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:20)(cid:19)\n",
      "\n",
      "(cid:25)(cid:29)(cid:3)(cid:3)(cid:20)(cid:18)(cid:25)\n",
      "\n",
      "(cid:25)(cid:29)(cid:3)(cid:3)(cid:24)(cid:18)(cid:20)(cid:19)\n",
      "\n",
      "Figure 17.9 An HMM for the occasionally dishonest casino. The blue arrows visualize the state transition diagram A. Based on (Durbin et al. 1998, p54).\n",
      "\n",
      "filtered\n",
      "\n",
      "smoothed\n",
      "\n",
      "Viterbi\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      ") d e d a o l ( p\n",
      "\n",
      "0.5\n",
      "\n",
      ") d e d a o l ( p\n",
      "\n",
      "0.5\n",
      "\n",
      ") d e d a o = 1 , r i a f = 0 (\n",
      "\n",
      "l\n",
      "\n",
      "e t a t s P A M\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "50\n",
      "\n",
      "100\n",
      "\n",
      "150 roll number\n",
      "\n",
      "200\n",
      "\n",
      "250\n",
      "\n",
      "300\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "50\n",
      "\n",
      "100\n",
      "\n",
      "150 roll number\n",
      "\n",
      "200\n",
      "\n",
      "250\n",
      "\n",
      "300\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "50\n",
      "\n",
      "100\n",
      "\n",
      "150 roll number\n",
      "\n",
      "200\n",
      "\n",
      "250\n",
      "\n",
      "300\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 17.10 Inference in the dishonest casino. Vertical gray bars denote the samples that we generated using a loaded die. (c) MAP trajectory. Figure generated by casinoDemo.\n",
      "\n",
      "(a) Filtered estimate of probability of using a loaded dice.\n",
      "\n",
      "(b) Smoothed estimates.\n",
      "\n",
      "distribution changes abruptly every now and then. In a typical application, we just see the rolls and want to infer which dice is being used. But there are different kinds of inference, which we summarize below.\n",
      "\n",
      "Filtering means to compute the belief state p(zt|x1:t) online, or recursively, as the data streams in. This is called “ﬁltering” because it reduces the noise more than simply estimating the hidden state using just the current estimate, p(zt|xt). We will see below that we can perform ﬁltering by simply applying Bayes rule in a sequential fashion. See Figure 17.10(a) for an example.\n",
      "\n",
      "Smoothing means to compute p(zt|x1:T ) offline, given all the evidence. See Figure 17.10(b) for an example. By conditioning on past and future data, our uncertainty will be signiﬁcantly reduced. To understand this intuitively, consider a detective trying to ﬁgure out who com- mitted a crime. As he moves through the crime scene, his uncertainty is high until he ﬁnds the key clue; then he has an “aha” moment, his uncertainty is reduced, and all the previously confusing observations are, in hindsight, easy to explain.\n",
      "\n",
      "608\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "(cid:87)\n",
      "\n",
      "(cid:73)(cid:76)(cid:79)(cid:87)(cid:72)(cid:85)(cid:76)(cid:81)(cid:74)\n",
      "\n",
      "(cid:87)\n",
      "\n",
      "(cid:83)(cid:85)(cid:72)(cid:71)(cid:76)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)\n",
      "\n",
      "(cid:75)\n",
      "\n",
      "(cid:73)(cid:76)(cid:91)(cid:72)(cid:71)(cid:16)(cid:79)(cid:68)(cid:74)(cid:3) (cid:86)(cid:80)(cid:82)(cid:82)(cid:87)(cid:75)(cid:76)(cid:81)(cid:74)\n",
      "\n",
      "(cid:79)\n",
      "\n",
      "(cid:87)\n",
      "\n",
      "(cid:73)(cid:76)(cid:91)(cid:72)(cid:71)(cid:16)(cid:79)(cid:68)(cid:74)(cid:3) (cid:86)(cid:80)(cid:82)(cid:82)(cid:87)(cid:75)(cid:76)(cid:81)(cid:74)(cid:3) (cid:11)(cid:82)(cid:73)(cid:73)(cid:79)(cid:76)(cid:81)(cid:72)(cid:12)\n",
      "\n",
      "(cid:87)\n",
      "\n",
      "(cid:55)\n",
      "\n",
      "Figure 17.11 The main kinds of inference for state-space models. The shaded region is the interval for which we have data. The arrow represents the time step at which we want to perform inference. t is the current time, T is the sequence length, (cid:7) is the lag and h is the prediction horizon. See text for details.\n",
      "\n",
      "Fixed lag smoothing is an interesting compromise between online and offline estimation; it involves computing p(zt−(cid:8)|x1:t), where (cid:2) >0 is called the lag. This gives better performance than ﬁltering, but incurs a slight delay. By changing the size of the lag, one can trade off accuracy vs delay.\n",
      "\n",
      "Prediction Instead of predicting the past given the future, as in ﬁxed lag smoothing, we might want to predict the future given the past, i.e., to compute p(zt+h|x1:t), where h > 0 is called the prediction horizon. For example, suppose h = 2; then we have\n",
      "\n",
      "p(zt+2|x1:t) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(zt+2|zt+1)p(zt+1|zt)p(zt|x1:t)\n",
      "\n",
      "(17.42)\n",
      "\n",
      "zt+1\n",
      "\n",
      "zt\n",
      "\n",
      "It is straightforward to perform this computation: we just power up the transition matrix and apply it to the current belief state. The quantity p(zt+h|x1:t) is a prediction about future hidden states; it can be converted into a prediction about future observations using\n",
      "\n",
      "p(xt+h|x1:t) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(xt+h|zt+h)p(zt+h|x1:t)\n",
      "\n",
      "(17.43)\n",
      "\n",
      "zt+h\n",
      "\n",
      "This is the posterior predictive density, and can be used for time-series forecasting (see (Fraser 2008) for details). See Figure 17.11 for a sketch of the relationship between ﬁltering, smoothing, and prediction.\n",
      "\n",
      "MAP estimation This means computing arg maxz1:T\n",
      "\n",
      "able state sequence.\n",
      "\n",
      "p(z1:T |x1:T ), which is a most prob- In the context of HMMs, this is known as Viterbi decoding (see\n",
      "\n",
      "17.4.\n",
      "\n",
      "Inference in HMMs\n",
      "\n",
      "609\n",
      "\n",
      "Section 17.4.4). Figure 17.10 illustrates the difference between ﬁltering, smoothing and MAP decoding for the occasionally dishonest casino HMM. We see that the smoothed (offline) estimate is indeed smoother than the ﬁltered (online) estimate. If we threshold the estimates at 0.5 and compare to the true sequence, we ﬁnd that the ﬁltered method makes 71 errors out of 300, and the smoothed method makes 49/300; the MAP path makes 60/300 errors. It is not surprising that smoothing makes fewer errors than Viterbi, since the optimal way to min- imize bit-error rate is to threshold the posterior marginals (see Section 5.7.1.1). Nevertheless, for some applications, we may prefer the Viterbi decoding, as we discuss in Section 17.4.4. • Posterior samples If there is more than one plausible interpretation of the data, it can be useful to sample from the posterior, z1:T ∼ p(z1:T |x1:T ). These sample paths contain much more information than the sequence of marginals computed by smoothing.\n",
      "\n",
      "Probability of the evidence We can compute the probability of the evidence, p(x1:T ), p(z1:T , x1:T ). This can be used to by summing up over all hidden paths, p(x1:T ) = classify sequences (e.g., if the HMM is used as a class conditional density), for model-based clustering, for anomaly detection, etc.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "z1:T\n",
      "\n",
      "17.4.2\n",
      "\n",
      "The forwards algorithm\n",
      "\n",
      "We now describe how to recursively compute the ﬁltered marginals, p(zt|x1:t) in an HMM.\n",
      "\n",
      "one-step-ahead predictive density; this acts as the new prior for time t:\n",
      "\n",
      "The algorithm has two steps. First comes the prediction step, in which we compute the\n",
      "\n",
      "p(zt = j|x1:t−1) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(zt = j|zt−1 = i)p(zt−1 = i|x1:t−1)\n",
      "\n",
      "(17.44)\n",
      "\n",
      "i\n",
      "\n",
      "Next comes the update step, in which we absorb the observed data from time t using Bayes rule:\n",
      "\n",
      "αt(j) (cid:2) p(zt = j|x1:t) = p(zt = j|xt, x1:t−1)\n",
      "\n",
      "(17.45)\n",
      "\n",
      "=\n",
      "\n",
      "1 Zt\n",
      "\n",
      "p(xt|zt = j,(cid:3)(cid:3)(cid:3)x1:t−1)p(zt = j|x1:t−1)\n",
      "\n",
      "(17.46)\n",
      "\n",
      "where the normalization constant is given by\n",
      "\n",
      "Zt (cid:2) p(xt|x1:t−1) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(zt = j|x1:t−1)p(xt|zt = j)\n",
      "\n",
      "(17.47)\n",
      "\n",
      "j\n",
      "\n",
      "This process is known as the predict-update cycle. The distribution p(zt|x1:t) is called the (ﬁltered) belief state at time t, and is a vector of K numbers, often denoted by αt. In matrix- vector notation, we can write the update in the following simple form:\n",
      "\n",
      "αt ∝ ψt (cid:17) (ΨT αt−1)\n",
      "\n",
      "(17.48)\n",
      "\n",
      "where ψt(j) =p( xt|zt = j) is the local evidence at time t, Ψ(i, j) =p( zt = j|zt−1 = i) is the transition matrix, and u (cid:17) v is the Hadamard product, representing elementwise vector multiplication. See Algorithm 6 for the pseudo-code, and hmmFilter for some Matlab code.\n",
      "\n",
      "610\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "In addition to computing the hidden states, we can use this algorithm to compute the log\n",
      "\n",
      "probability of the evidence:\n",
      "\n",
      "log p(x1:T |θ) =\n",
      "\n",
      "T(cid:4)\n",
      "\n",
      "log p(xt|x1:t−1) =\n",
      "\n",
      "T(cid:4)\n",
      "\n",
      "log Zt\n",
      "\n",
      "(17.49)\n",
      "\n",
      "t=1\n",
      "\n",
      "t=1\n",
      "\n",
      "(We need to work in the log domain to avoid numerical underﬂow.)\n",
      "\n",
      "2 [α1, Z1] = normalize(ψ1 (cid:17) π) ; 3 for t = 2 :T do 4 5 Return α1:T and log p(y1:T ) =\n",
      "\n",
      "6 Subroutine: [v, Z] = normalize(u) : Z =\n",
      "\n",
      "Algorithm 17.1: Forwards algorithm 1 Input: Transition matrices ψ(i, j) = p(zt = j|zt−1 = i), local evidence vectors\n",
      "\n",
      "ψt(j) = p(xt|zt = j), initial state distribution π(j) = p(z1 = j);\n",
      "\n",
      "[αt, Zt] = normalize(ψt (cid:17) (ΨT αt−1)) ; t log Zt; (cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "j uj; vj = uj/Z;\n",
      "\n",
      "17.4.3\n",
      "\n",
      "The forwards-backwards algorithm\n",
      "\n",
      "In Section 17.4.2, we explained how to compute the ﬁltered marginals p(zt = j|x1:t) using online inference. We now discuss how to compute the smoothed marginals, p(zt = j|x1:T ), using offline inference.\n",
      "\n",
      "17.4.3.1\n",
      "\n",
      "Basic idea\n",
      "\n",
      "The key decomposition relies on the fact that we can break the chain into two parts, the past and the future, by conditioning on zt:\n",
      "\n",
      "p(zt = j|x1:T ) ∝ p(zt = j, xt+1:T |x1:t) ∝ p(zt = j|x1:t)p(xt+1:T |zt = j,(cid:3)(cid:3)x1:t)\n",
      "\n",
      "(17.50)\n",
      "\n",
      "Let αt(j) (cid:2) p(zt = j|x1:t) be the ﬁltered belief state as before. Also, deﬁne\n",
      "\n",
      "βt(j) (cid:2) p(xt+1:T |zt = j)\n",
      "\n",
      "(17.51)\n",
      "\n",
      "likelihood of future evidence given that the hidden state at time t is j. as the conditional (Note that this is not a probability distribution over states, since it does not need to satisfy (cid:7)\n",
      "\n",
      "j βt(j) = 1.) Finally, deﬁne\n",
      "\n",
      "γt(j) (cid:2) p(zt = j|x1:T )\n",
      "\n",
      "(17.52)\n",
      "\n",
      "as the desired smoothed posterior marginal. From Equation 17.50, we have\n",
      "\n",
      "γt(j) ∝ αt(j)βt(j)\n",
      "\n",
      "(17.53)\n",
      "\n",
      "17.4.\n",
      "\n",
      "Inference in HMMs\n",
      "\n",
      "611\n",
      "\n",
      "We have already described how to recursively compute the α’s in a left-to-right fashion in Section 17.4.2. We now describe how to recursively compute the β’s in a right-to-left fashion. If we have already computed βt, we can compute βt−1 as follows:\n",
      "\n",
      "βt−1(i) =p( xt:T |zt−1 = i) (cid:4)\n",
      "\n",
      "=\n",
      "\n",
      "p(zt = j, xt, xt+1:T |zt−1 = i)\n",
      "\n",
      "(17.54)\n",
      "\n",
      "(17.55)\n",
      "\n",
      "=\n",
      "\n",
      "j (cid:4)\n",
      "\n",
      "p(xt+1:T |zt = j,(cid:3)(cid:3)(cid:3)(cid:3)\n",
      "\n",
      "zt−1 = i,(cid:4)(cid:4)xt)p(zt = j, xt|zt−1 = i)\n",
      "\n",
      "(17.56)\n",
      "\n",
      "=\n",
      "\n",
      "j (cid:4)\n",
      "\n",
      "p(xt+1:T |zt = j)p(xt|zt = j,(cid:3)(cid:3)(cid:3)(cid:3)\n",
      "\n",
      "zt−1 = i)p(zt = j|zt−1 = i)\n",
      "\n",
      "(17.57)\n",
      "\n",
      "=\n",
      "\n",
      "j (cid:4)\n",
      "\n",
      "βt(j)ψt(j)ψ(i, j)\n",
      "\n",
      "(17.58)\n",
      "\n",
      "j\n",
      "\n",
      "We can write the resulting equation in matrix-vector form as\n",
      "\n",
      "βt−1 = Ψ(ψt (cid:17) βt)\n",
      "\n",
      "(17.59)\n",
      "\n",
      "The base case is\n",
      "\n",
      "βT (i) = p(xT +1:T |zT = i) =p( ∅|zT = i) = 1\n",
      "\n",
      "(17.60)\n",
      "\n",
      "which is the probability of a non-event.\n",
      "\n",
      "Having computed the forwards and backwards messages, we can combine them to compute γt(j) ∝ αt(j)βt(j). The overall algorithm is known as the forwards-backwards algorithm. The pseudo code is very similar to the forwards case; see hmmFwdBack for an implementation. We can think of this algorithm as passing “messages” from left to right, and then from right to left, and then combining them at each node. We will generalize this intuition in Section 20.2, when we discuss belief propagation.\n",
      "\n",
      "17.4.3.2\n",
      "\n",
      "Two-slice smoothed marginals\n",
      "\n",
      "When we estimate the parameters of the transition matrix using EM (see Section 17.5), we will need to compute the expected number of transitions from state i to state j:\n",
      "\n",
      "Nij =\n",
      "\n",
      "T −1(cid:4)\n",
      "\n",
      "E [I(zt = i, zt+1 = j)|x1:T ] =\n",
      "\n",
      "T −1(cid:4)\n",
      "\n",
      "p(zt = i, zt+1 = j|x1:T )\n",
      "\n",
      "(17.61)\n",
      "\n",
      "t=1\n",
      "\n",
      "t=1\n",
      "\n",
      "The term p(zt = i, zt+1 = j|x1:T ) is called a (smoothed) two-slice marginal, and can be computed as follows\n",
      "\n",
      "ξt,t+1(i, j) (cid:2) p(zt = i, zt+1 = j|x1:T )\n",
      "\n",
      "(17.62)\n",
      "\n",
      "∝ p(zt|x1:t)p(zt+1|zt, xt+1:T ) ∝ p(zt|x1:t)p(xt+1:T |zt, zt+1)p(zt+1|zt) ∝ p(zt|x1:t)p(xt+1|zt+1)p(xt+2:T |zt+1)p(zt+1|zt) = αt(i)φt+1(j)βt+1(j)ψ(i, j)\n",
      "\n",
      "(17.63)\n",
      "\n",
      "(17.64)\n",
      "\n",
      "(17.65)\n",
      "\n",
      "(17.66)\n",
      "\n",
      "612\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "In matrix-vector form, we have\n",
      "\n",
      "ξt,t+1 ∝ Ψ (cid:17) (αt(φt+1 (cid:17) βt+1)T )\n",
      "\n",
      "(17.67)\n",
      "\n",
      "For another interpretation of these equations, see Section 20.2.4.3.\n",
      "\n",
      "17.4.3.3\n",
      "\n",
      "Time and space complexity It is clear that a straightforward implementation of FB takes O(K 2T ) time, since we must perform a K × K matrix multiplication at each step. For some applications, such as speech recognition, K is very large, so the O(K 2) term becomes prohibitive. Fortunately, if the in a left-to-right transition matrix is sparse, we can reduce this substantially. For example, transition matrix, the algorithm takes O(T K) time.\n",
      "\n",
      "In some cases, we can exploit special properties of the state space, even if the transition matrix is not sparse. In particular, suppose the states represent a discretization of an underlying continuous state-space, and the transition matrix has the form ψ(i, j) ∝ exp(−σ2|zi − zj|), where zi is the continuous vector represented by state i. Then one can implement the forwards- backwards algorithm in O(T K log K) time. This is very useful for models with large state spaces. See Section 22.2.6.1 for details.\n",
      "\n",
      "In some cases, the bottleneck is memory, not time. The expected sufficient statistics needed t ξt−1,t(i, j); this takes constant space (independent of T ); however, to compute by EM are them, we need O(KT ) working space, since we must store αt for t = 1, . . . , T until we do the backwards pass. It is possible to devise a simple divide-and-conquer algorithm that reduces the space complexity from O(KT ) to O(K log T ) at the cost of increasing the running time from O(K2T ) to O(K 2T log T ): see (Binder et al. 1997; Zweig and Padmanabhan 2000) for details.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "17.4.4\n",
      "\n",
      "The Viterbi algorithm\n",
      "\n",
      "The Viterbi algorithm (Viterbi 1967) can be used to compute the most probable sequence of states in a chain-structured graphical model, i.e., it can compute\n",
      "\n",
      "z∗ = arg max z1:T\n",
      "\n",
      "p(z1:T |x1:T )\n",
      "\n",
      "(17.68)\n",
      "\n",
      "This is equivalent to computing a shortest path through the trellis diagram in Figure 17.12, where the nodes are possible states at each time step, and the node and edge weights are log probabilities. That is, the weight of a path z1, z2, . . . , zT is given by\n",
      "\n",
      "log π1(z1) + log φ1(z1) +\n",
      "\n",
      "T(cid:4)\n",
      "\n",
      "[log ψ(zt−1, zt) + log φt(zt)]\n",
      "\n",
      "(17.69)\n",
      "\n",
      "t=2\n",
      "\n",
      "17.4.4.1 MAP vs MPE\n",
      "\n",
      "Before discussing how the algorithm works, let us make one important remark: the (jointly) most probable sequence of states is not necessarily the same as the sequence of (marginally) most probable states. The former is given by Equation 17.68, and is what Viterbi computes, whereas the latter is given by the maximizer of the posterior marginals or MPM:\n",
      "\n",
      "ˆz = (arg max\n",
      "\n",
      "z1\n",
      "\n",
      "p(z1|x1:T ), . . . , arg max\n",
      "\n",
      "zT\n",
      "\n",
      "p(zT |x1:T ))\n",
      "\n",
      "(17.70)\n",
      "\n",
      "17.4.\n",
      "\n",
      "Inference in HMMs\n",
      "\n",
      "613\n",
      "\n",
      "(cid:49)\n",
      "\n",
      "(cid:17)(cid:3)(cid:17)(cid:3)(cid:17)\n",
      "\n",
      "(cid:17) (cid:17) (cid:17)\n",
      "\n",
      "(cid:17) (cid:17) (cid:17) (cid:17) (cid:17) (cid:17)\n",
      "\n",
      "(cid:17) (cid:17) (cid:17)\n",
      "\n",
      "(cid:40) (cid:55) (cid:36) (cid:55) (cid:54)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:17)(cid:3)(cid:17)(cid:3)(cid:17)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:17)(cid:3)(cid:17)(cid:3)(cid:17)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:17)(cid:3)(cid:17)(cid:3)(cid:17)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:17)(cid:3)(cid:17)(cid:3)(cid:17)\n",
      "\n",
      "(cid:55)\n",
      "\n",
      "(cid:50)(cid:37)(cid:54)(cid:40)(cid:53)(cid:57)(cid:36)(cid:55)(cid:44)(cid:50)(cid:49)\n",
      "\n",
      "Figure 17.12 The trellis of states vs time for a Markov chain. Based on (Rabiner 1989).\n",
      "\n",
      "As a simple example of the difference, consider a chain with two time steps, deﬁning the\n",
      "\n",
      "following joint:\n",
      "\n",
      "X2 = 0 X2 = 1\n",
      "\n",
      "X1 = 0 X1 = 1\n",
      "\n",
      "0.04 0.36 0.4\n",
      "\n",
      "0.3 0.3 0.6\n",
      "\n",
      "0.34 0.66\n",
      "\n",
      "The joint MAP estimate is (0, 1), whereas the sequence of marginal MPMs is (1, 1). The advantage of the joint MAP estimate is that is is always globally consistent. For example, suppose we are performing speech recognition and someones says “recognize speech”. This could be mis-heard as “wreck a nice beach”. Locally it may appear that “beach” is the most probable interpretation of that particular window of sound, but when we add the requirement that the data be explained by a single linguistically plausible path, this interpretation becomes less likely.\n",
      "\n",
      "why, note that in Viterbi, when we estimate zt, we “max out” the other variables:\n",
      "\n",
      "On the other hand, the MPM estimates can be more robust (Marroquin et al. 1987). To see\n",
      "\n",
      "z∗ t = arg max\n",
      "\n",
      "zt\n",
      "\n",
      "max z1:t−1,zt+1:T\n",
      "\n",
      "p(z1:t−1, zt, zt+1:T |x1:T )\n",
      "\n",
      "(17.71)\n",
      "\n",
      "whereas we when we use forwards-backwards, we sum out the other variables:\n",
      "\n",
      "p(zt|x1:T ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(z1:t−1, zt, zt+1:T |x1:T )\n",
      "\n",
      "(17.72)\n",
      "\n",
      "z1:t−1,zt+1:T\n",
      "\n",
      "This makes the MPM in Equation 17.70 more robust, since we estimate each node averaging over its neighbors, rather than conditioning on a speciﬁc value of its neighbors.6\n",
      "\n",
      "6. In general, we may want to mix max and sum. For example, consider a joint distribution where we observe\n",
      "\n",
      "614\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "17.4.4.2\n",
      "\n",
      "Details of the algorithm\n",
      "\n",
      "It is tempting to think that we can implement Viterbi by just replacing the sum-operator in forwards-backwards with a max-operator. The former is called the sum-product, and the latter the max-product algorithm. If there is a unique mode, running max-product and then computing using Equation 17.70 will give the same result as using Equation 17.68 (Weiss and Freeman 2001b), but in general, it can lead to incorrect results if there are multiple equally probably joint assignments. The reasons is that each node breaks ties independently and hence may do so in a manner that is inconsistent with its neighbors. The Viterbi algorithm is therefore not quite as simple as replacing sum with max. In particular, the forwards pass does use max- product, but the backwards pass uses a traceback procedure to recover the most probable path through the trellis of states. Essentially, once zt picks its most probable state, the previous nodes condition on this event, and therefore they will break ties consistently.\n",
      "\n",
      "In more detail, deﬁne\n",
      "\n",
      "δt(j) (cid:2)\n",
      "\n",
      "max z1,...,zt−1\n",
      "\n",
      "p(z1:t−1, zt = j|x1:t)\n",
      "\n",
      "(17.73)\n",
      "\n",
      "This is the probability of ending up in state j at time t, given that we take the most probable path. The key insight is that the most probable path to state j at time t must consist of the most probable path to some other state i at time t − 1, followed by a transition from i to j. Hence\n",
      "\n",
      "δt(j) = max\n",
      "\n",
      "i\n",
      "\n",
      "δt−1(i)ψ(i, j)φt(j)\n",
      "\n",
      "(17.74)\n",
      "\n",
      "We also keep track of the most likely previous state, for each possible state that we end up in:\n",
      "\n",
      "at(j) = argmax\n",
      "\n",
      "i\n",
      "\n",
      "δt−1(i)ψ(i, j)φt(j)\n",
      "\n",
      "(17.75)\n",
      "\n",
      "That is, at(j) tells us the most likely previous state on the most probable path to zt = j. We initialize by setting\n",
      "\n",
      "δ1(j) = πjφ1(j)\n",
      "\n",
      "(17.76)\n",
      "\n",
      "and we terminate by computing the most probable ﬁnal state z∗ T :\n",
      "\n",
      "z∗ T = arg max\n",
      "\n",
      "i\n",
      "\n",
      "δT (i)\n",
      "\n",
      "(17.77)\n",
      "\n",
      "We can then compute the most probable sequence of states using traceback:\n",
      "\n",
      "t = at+1(z∗ z∗\n",
      "\n",
      "t+1)\n",
      "\n",
      "(17.78)\n",
      "\n",
      "As usual, we have to worry about numerical underﬂow. We are free to normalize the δt terms at each step; this will not affect the maximum. However, unlike the forwards-backwards case,\n",
      "\n",
      "v and we want to query q; q = (cid:2) p(xq, xn|xv), where we max over xq and sum over xn. By contrast, we deﬁne the MPE or arg maxxq xn most probable explanation as (x∗ n) = arg maxxq ,xn p(xq, xn|xv), where we max over both xq and xn. This terminology is due to (Pearl 1988), although it is not widely used outside the Bayes net literatire. Obviously MAP=MPE if n = ∅. However, if n (cid:3)= ∅, then summing out the nuisance variables can give different results than maxing them out. Summing out nuisance variables is more sensible, but computationally harder, because of the need to combine max and sum operations (Lerner and Parr 2001).\n",
      "\n",
      "let n be the remaining nuisance variables. We deﬁne the MAP estimate as x∗\n",
      "\n",
      "q , x∗\n",
      "\n",
      "17.4.\n",
      "\n",
      "Inference in HMMs\n",
      "\n",
      "615\n",
      "\n",
      "(cid:54)(cid:20) (cid:19)(cid:17)(cid:24)\n",
      "\n",
      "(cid:19)(cid:17)(cid:22)(cid:15)(cid:3)(cid:19)(cid:17)(cid:22)\n",
      "\n",
      "(cid:54)(cid:20) (cid:19)(cid:17)(cid:19)(cid:23)(cid:24)\n",
      "\n",
      "(cid:19)(cid:17)(cid:22)(cid:15)(cid:3)(cid:19)(cid:17)(cid:19)\n",
      "\n",
      "(cid:54)(cid:20) (cid:19)(cid:17)(cid:19)\n",
      "\n",
      "(cid:54)(cid:20) (cid:19)(cid:17)(cid:19)\n",
      "\n",
      "(cid:19)(cid:17)(cid:26)(cid:15)(cid:3)(cid:19)(cid:17)(cid:21)\n",
      "\n",
      "(cid:19)(cid:17)(cid:26)(cid:15)(cid:3)(cid:19)(cid:17)(cid:26)\n",
      "\n",
      "(cid:38)(cid:20)\n",
      "\n",
      "(cid:19)(cid:17)(cid:24)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:38)(cid:21)\n",
      "\n",
      "(cid:19)(cid:17)(cid:22)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:54)(cid:21) (cid:19)(cid:17)(cid:19)\n",
      "\n",
      "(cid:54)(cid:21) (cid:19)(cid:17)(cid:19)(cid:26)\n",
      "\n",
      "(cid:19)(cid:17)(cid:28)(cid:15)(cid:3)(cid:19)(cid:17)(cid:26)\n",
      "\n",
      "(cid:54)(cid:21) (cid:19)(cid:17)(cid:19)(cid:23)(cid:23)(cid:20)\n",
      "\n",
      "(cid:19)(cid:17)(cid:28)(cid:15)(cid:3)(cid:19)(cid:17)(cid:19)\n",
      "\n",
      "(cid:54)(cid:21) (cid:19)(cid:17)(cid:19)\n",
      "\n",
      "(cid:38)(cid:22)\n",
      "\n",
      "(cid:19)(cid:17)(cid:21)\n",
      "\n",
      "(cid:19)(cid:17)(cid:21)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:38)(cid:23)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)(cid:17)(cid:26)\n",
      "\n",
      "(cid:19)(cid:17)(cid:20)\n",
      "\n",
      "(cid:19)(cid:17)(cid:20)(cid:15)(cid:3)(cid:19)(cid:17)(cid:20)\n",
      "\n",
      "(cid:19)(cid:17)(cid:20)(cid:15)(cid:3)(cid:19)(cid:17)(cid:24)\n",
      "\n",
      "(cid:38)(cid:24)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)(cid:17)(cid:20)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:38)(cid:25)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)(cid:17)(cid:24)\n",
      "\n",
      "(cid:54)(cid:22) (cid:19)(cid:17)(cid:19)\n",
      "\n",
      "(cid:54)(cid:22) (cid:19)(cid:17)(cid:19)\n",
      "\n",
      "(cid:54)(cid:22) (cid:19)(cid:17)(cid:19)(cid:19)(cid:19)(cid:26)\n",
      "\n",
      "(cid:19)(cid:17)(cid:23)(cid:15)(cid:3)(cid:19)(cid:17)(cid:24)\n",
      "\n",
      "(cid:54)(cid:22) (cid:19)(cid:17)(cid:19)(cid:19)(cid:21)(cid:21)\n",
      "\n",
      "(cid:38)(cid:26)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:19)(cid:17)(cid:23)\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 17.13 Illustration of Viterbi decoding in a simple HMM for speech recognition. (a) A 3-state HMM for a single phone. We are visualizing the state transition diagram. We assume the observations have been vector quantized into 7 possible symbols, C1, . . . , C7. Each state z1, z2, z3 has a different distribution over these symbols. Based on Figure 15.20 of (Russell and Norvig 2002). (b) Illustration of the Viterbi algorithm applied to this model, with data sequence C1, C3, C4, C6. The columns represent time, and the rows represent states. An arrow from state i at t − 1 to state j at t is annotated with two numbers: the ﬁrst is the probability of the i → j transition, and the second is the probability of generating observation xt from state j. The bold lines/ circles represent the most probable sequence of states. Based on Figure 24.27 of (Russell and Norvig 1995).\n",
      "\n",
      "we can also easily work in the log domain. The key difference is that log max = max log, whereas log\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:4)=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "log. Hence we can use\n",
      "\n",
      "log δt(j) (cid:2) max z1:t−1\n",
      "\n",
      "log p(z1:t−1, zt = j|x1:t)\n",
      "\n",
      "(17.79)\n",
      "\n",
      "= max\n",
      "\n",
      "i\n",
      "\n",
      "log δt−1(i) + log ψ(i, j) + log φt(j)\n",
      "\n",
      "(17.80)\n",
      "\n",
      "In the case of Gaussian observation models, this can result in a signiﬁcant (constant factor) speedup, since computing log p(xt|zt) can be much faster than computing p(xt|zt) for a high- dimensional Gaussian. This is one reason why the Viterbi algorithm is widely used in the E step of EM (Section 17.5.2) when training large speech recognition systems based on HMMs.\n",
      "\n",
      "17.4.4.3\n",
      "\n",
      "Example\n",
      "\n",
      "Figure 17.13 gives a worked example of the Viterbi algorithm, based on (Russell et al. 1995). Suppose we observe the discrete sequence of observations x1:4 = (C1, C3, C4, C6), representing codebook entries in a vector-quantized version of a speech signal. The model starts in state z1. The probability of generating C1 in z1 is 0.5, so we have δ1(1) = 0.5, and δ1(i) = 0 for all other states. Next we can self-transition to z1 with probability 0.3, or transition to z2 with proabability 0.7. If we end up in z1, the probability of generating C3 is 0.3; if we end up in z2,\n",
      "\n",
      "616\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "the probability of generating C3 is 0.2. Hence we have\n",
      "\n",
      "δ2(1) = δ1(1)ψ(1, 1)φ2(1) = 0.5 · 0.3 · 0.3 = 0.045 δ2(2) = δ1(1)ψ(1, 2)φ2(2) = 0.5 · 0.7 · 0.2 = 0.07\n",
      "\n",
      "(17.81)\n",
      "\n",
      "(17.82)\n",
      "\n",
      "Thus state 2 is more probable at t = 2; see the second column of Figure 17.13(b). In time step 3, we see that there are two paths into z2, from z1 and from z2. The bold arrow indicates that the latter is more probable. Hence this is the only one we have to remember. The algorithm continues in this way until we have reached the end of the sequence. One we have reached the end, we can follow the black arrows back to recover the MAP path (which is 1,2,2,3).\n",
      "\n",
      "17.4.4.4\n",
      "\n",
      "Time and space complexity\n",
      "\n",
      "is clearly O(K 2T ) in general, and the space complexity The time complexity of Viterbi is O(KT ), both the same as forwards-backwards. If the transition matrix has the form ψ(i, j) ∝ exp(−σ2||zi − zj||2), where zi is the continuous vector represented by state i, we can implement Viterbi in O(T K) time, instead of O(T K log K) needed by forwards-backwards. See Section 22.2.6.1 for details.\n",
      "\n",
      "17.4.4.5\n",
      "\n",
      "N-best list\n",
      "\n",
      "The Viterbi algorithm returns one of the most probable paths. It can be extended to return the top N paths (Schwarz and Chow 1990; Nilsson and Goldberger 2001). This is called the N-best list. Once can then use a discriminative method to rerank the paths based on global features derived from the fully observed state sequence (as well as the visible features). This technique is widely used in speech recognition. For example, consider the sentence “recognize speech”. It is possible that the most probable interpretation by the system of this acoustic signal is “wreck a nice speech”, or maybe “wreck a nice beach”. Maybe the correct interpretation is much lower down on the list. However, by using a re-ranking system, we may be able to improve the score of the correct interpretation based on a more global context.\n",
      "\n",
      "One problem with the N -best list is that often the top N paths are very similar to each other, rather than representing qualitatively different interpretations of the data. Instead we might want to generate a more diverse set of paths to more accurately represent posterior uncertainty. One way to do this is to sample paths from the posterior, as we discuss below. For some other ways to generate diverse MAP estimates, see e.g., (Yadollahpour et al. 2011; Kulesza and Taskar 2011).\n",
      "\n",
      "17.4.5\n",
      "\n",
      "Forwards ﬁltering, backwards sampling\n",
      "\n",
      "It is often useful to sample paths from the posterior:\n",
      "\n",
      "zs 1:T ∼ p(z1:T |x1:T )\n",
      "\n",
      "(17.83)\n",
      "\n",
      "We can do this is as follow: run forwards backwards, to compute the two-slice smoothed posteri- ors, p(zt−1,t|x1:T ); next compute the conditionals p(zt|zt−1, x1:T ) by normalizing; sample from the initial pair of states, z∗ 1,2 ∼ p(z1,2|x1:T ); ﬁnally, recursively sample z∗ t−1, x1:T ). Note that the above solution requires a forwards-backwards pass, and then an additional forwards sampling pass. An alternative is to do the forwards pass, and then perform sampling\n",
      "\n",
      "t ∼ p(zt|z∗\n",
      "\n",
      "17.5. Learning for HMMs\n",
      "\n",
      "617\n",
      "\n",
      "in the backwards pass. The key insight into how to do this is that we can write the joint from right to left using\n",
      "\n",
      "p(z1:T |x1:T ) = p(zT |x1:T )\n",
      "\n",
      "1(cid:20)\n",
      "\n",
      "p(zt|zt+1, x1:T )\n",
      "\n",
      "(17.84)\n",
      "\n",
      "t=T −1\n",
      "\n",
      "We can then sample zt given future sampled states using\n",
      "\n",
      "t ∼ p(zt|zt+1:T , x1:T ) = p(zt|zt+1,(cid:3)(cid:3)(cid:3)zt+2:T , x1:t,(cid:3)(cid:3)(cid:3)xt+1:T ) = p(zt|zs zs\n",
      "\n",
      "t+1, x1:t)\n",
      "\n",
      "(17.85)\n",
      "\n",
      "The sampling distribution is given by\n",
      "\n",
      "p(zt = i|zt+1 = j, x1:t) =p(\n",
      "\n",
      "zt|zt+1, x1:t,(cid:3)(cid:3)(cid:3)xt+1)\n",
      "\n",
      "(17.86)\n",
      "\n",
      "=\n",
      "\n",
      "∝\n",
      "\n",
      "=\n",
      "\n",
      "p(zt+1, zt|x1:t+1) p(zt+1|x1:t+1) p(xt+1|zt+1,(cid:4)(cid:4)zt,(cid:3)(cid:3)x1:t)p(zt+1, zt|x1:t) p(zt+1|x1:t+1) p(xt+1|zt+1)p(zt+1|zt,(cid:3)(cid:3)x1:t)p(zt|x1:t) p(zt+1|x1:t+1)\n",
      "\n",
      "(17.87)\n",
      "\n",
      "(17.88)\n",
      "\n",
      "(17.89)\n",
      "\n",
      "=\n",
      "\n",
      "φt+1(j)ψ(i, j)αt(i) αt+1(j)\n",
      "\n",
      "(17.90)\n",
      "\n",
      "The base case is\n",
      "\n",
      "zs T ∼ p(zT = i|x1:T ) = αT (i)\n",
      "\n",
      "(17.91)\n",
      "\n",
      "This algorithm forms the basis of blocked-Gibbs sampling methods for parameter inference,\n",
      "\n",
      "as we will see below.\n",
      "\n",
      "17.5\n",
      "\n",
      "Learning for HMMs\n",
      "\n",
      "We now discuss how to estimate the parameters θ = (π, A, B), where π(i) = p(z1 = i) is the initial state distribution, A(i, j) =p( zt = j|zt−1 = i) is the transition matrix, and B are the parameters of the class-conditional densities p(xt|zt = j). We ﬁrst consider the case where z1:T is observed in the training set, and then the harder case where z1:T is hidden.\n",
      "\n",
      "17.5.1\n",
      "\n",
      "Training with fully observed data\n",
      "\n",
      "If we observe the hidden state sequences, we can compute the MLEs for A and π exactly as in Section 17.2.2.1. If we use a conjugate prior, we can also easily compute the posterior.\n",
      "\n",
      "The details on how to estimate B depend on the form of the observation model. The situation is identical to ﬁtting a generative classiﬁer. For example, if each state has a multinoulli distribution associated with it, with parameters Bjl = p(Xt = l|zt = j), where l ∈ {1, . . . , L} represents the observed symbol, the MLE is given by\n",
      "\n",
      "ˆBjl =\n",
      "\n",
      "N X jl Nj\n",
      "\n",
      ", N X\n",
      "\n",
      "jl (cid:2)\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "Ti(cid:4)\n",
      "\n",
      "t=1\n",
      "\n",
      "I(zi,t = j, xi,t = l)\n",
      "\n",
      "(17.92)\n",
      "\n",
      "618\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "This result is quite intuitive: we simply add up the number of times we are in state j and we see a symbol l, and divide by the number of times we are in state j.\n",
      "\n",
      "Similarly, if each state has a Gaussian distribution associated with it, we have (from Sec-\n",
      "\n",
      "tion 4.2.4) the following MLEs:\n",
      "\n",
      "ˆμk =\n",
      "\n",
      "xk Nk\n",
      "\n",
      ", ˆΣk =\n",
      "\n",
      "(xx)T\n",
      "\n",
      "k − Nk ˆμk ˆμT\n",
      "\n",
      "k\n",
      "\n",
      "Nk\n",
      "\n",
      "(17.93)\n",
      "\n",
      "where the sufficient statistics are given by\n",
      "\n",
      "xk (cid:2)\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "Ti(cid:4)\n",
      "\n",
      "I(zi,t = k)xi,t\n",
      "\n",
      "(17.94)\n",
      "\n",
      "(xx)T\n",
      "\n",
      "k (cid:2)\n",
      "\n",
      "i=1 N(cid:4)\n",
      "\n",
      "t=1 Ti(cid:4)\n",
      "\n",
      "I(zi,t = k)xi,txT i,t\n",
      "\n",
      "(17.95)\n",
      "\n",
      "i=1\n",
      "\n",
      "t=1\n",
      "\n",
      "Analogous results can be derived for other kinds of distributions. One can also easily extend all of these results to compute MAP estimates, or even full posteriors over the parameters.\n",
      "\n",
      "17.5.2\n",
      "\n",
      "EM for HMMs (the Baum-Welch algorithm)\n",
      "\n",
      "If the zt variables are not observed, we are in a situation analogous to ﬁtting a mixture model. The most common approach is to use the EM algorithm to ﬁnd the MLE or MAP parameters, although of course one could use other gradient-based methods (see e.g., (Baldi and Chauvin 1994)). In this Section, we derive the EM algorithm. When applied to HMMs, this is also known as the Baum-Welch algorithm (Baum et al. 1970).\n",
      "\n",
      "17.5.2.1\n",
      "\n",
      "E step\n",
      "\n",
      "It is straightforward to show that the expected complete data log likelihood is given by\n",
      "\n",
      "Q(θ, θold) =\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "N 1 k\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "log πk +\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "E [Njk] log Ajk\n",
      "\n",
      "(17.96)\n",
      "\n",
      "k=1\n",
      "\n",
      "j=1\n",
      "\n",
      "k=1\n",
      "\n",
      "+\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "Ti(cid:4)\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "p(zt = k|xi, θold) log p(xi,t|φk)\n",
      "\n",
      "(17.97)\n",
      "\n",
      "i=1\n",
      "\n",
      "t=1\n",
      "\n",
      "k=1\n",
      "\n",
      "where the expected counts are given by\n",
      "\n",
      "E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "N 1 k\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "=\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "p(zi1 = k|xi, θold)\n",
      "\n",
      "(17.98)\n",
      "\n",
      "E [Njk] =\n",
      "\n",
      "i=1 N(cid:4)\n",
      "\n",
      "Ti(cid:4)\n",
      "\n",
      "p(zi,t−1 = j, zi,t = k|xi, θold)\n",
      "\n",
      "(17.99)\n",
      "\n",
      "E [Nj] =\n",
      "\n",
      "i=1 N(cid:4)\n",
      "\n",
      "t=2 Ti(cid:4)\n",
      "\n",
      "p(zi,t = j|xi, θold)\n",
      "\n",
      "(17.100)\n",
      "\n",
      "i=1\n",
      "\n",
      "t=1\n",
      "\n",
      "17.5. Learning for HMMs\n",
      "\n",
      "619\n",
      "\n",
      "These expected sufficient statistics can be computed by running the forwards-backwards algo- rithm on each sequence. In particular, this algorithm computes the following smoothed node and edge marginals:\n",
      "\n",
      "γi,t(j) (cid:2) p(zt = j|xi,1:Ti\n",
      "\n",
      ", θ)\n",
      "\n",
      "(17.101)\n",
      "\n",
      "ξi,t(j, k) (cid:2) p(zt−1 = j, zt = k|xi,1:Ti , θ)\n",
      "\n",
      "(17.102)\n",
      "\n",
      "17.5.2.2 M step\n",
      "\n",
      "Based on Section 11.3, we have that the M step for A and π is to just normalize the expected counts:\n",
      "\n",
      "ˆAjk =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "E [Njk] k(cid:2) E [Njk(cid:2) ]\n",
      "\n",
      ", ˆπk =\n",
      "\n",
      "E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "N 1 k N\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(17.103)\n",
      "\n",
      "This result is quite intuitive: we simply add up the expected number of transitions from j to k, and divide by the expected number of times we transition from j to anything else.\n",
      "\n",
      "For a multinoulli observation model, the expected sufficient statistics are\n",
      "\n",
      "E [Mjl] =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "Ti(cid:4)\n",
      "\n",
      "γi,t(j)I(xi,t = l) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "γi,t(j)\n",
      "\n",
      "(17.104)\n",
      "\n",
      "i=1\n",
      "\n",
      "t=1\n",
      "\n",
      "i=1\n",
      "\n",
      "t:xi,t=l\n",
      "\n",
      "The M step has the form\n",
      "\n",
      "ˆBjl =\n",
      "\n",
      "E [Mjl] E [Nj]\n",
      "\n",
      "(17.105)\n",
      "\n",
      "This result is quite intuitive: we simply add up the expected number of times we are in state j and we see a symbol l, and divide by the expected number of times we are in state j. For a Gaussian observation model, the expected sufficient statistics are given by\n",
      "\n",
      "E [xk] =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "Ti(cid:4)\n",
      "\n",
      "γi,t(k)xi,t\n",
      "\n",
      "(17.106)\n",
      "\n",
      "E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(xx)T k\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "=\n",
      "\n",
      "i=1 N(cid:4)\n",
      "\n",
      "t=1 Ti(cid:4)\n",
      "\n",
      "γi,t(k)xi,txT i,t\n",
      "\n",
      "(17.107)\n",
      "\n",
      "i=1\n",
      "\n",
      "t=1\n",
      "\n",
      "The M step becomes\n",
      "\n",
      "ˆμk =\n",
      "\n",
      "E [xk] E [Nk]\n",
      "\n",
      ", ˆΣk =\n",
      "\n",
      "E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(xx)T k\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "− E [Nk] ˆμk ˆμT E [Nk]\n",
      "\n",
      "k\n",
      "\n",
      "(17.108)\n",
      "\n",
      "This can (and should) be regularized in the same way we regularize GMMs.\n",
      "\n",
      "17.5.2.3\n",
      "\n",
      "Initialization\n",
      "\n",
      "As usual with EM, we must take care to ensure that we initialize the parameters carefully, to minimize the chance of getting stuck in poor local optima. There are several ways to do this, such as\n",
      "\n",
      "620\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "Use some fully labeled data to initialize the parameters.\n",
      "\n",
      "\n",
      "\n",
      "Initially ignore the Markov dependencies, and estimate the observation parameters using the standard mixture model estimation methods, such as K-means or EM.\n",
      "\n",
      "Randomly initialize the parameters, use multiple restarts, and pick the best solution.\n",
      "\n",
      "Techniques such as deterministic annealing (Ueda and Nakano 1998; Rao and Rose 2001) can help mitigate the effect of local minima. Also, just as K-means is often used to initialize EM for GMMs, so it is common to initialize EM for HMMs using Viterbi training, which means approximating the posterior over paths with the single most probable path. (This is not necessarily a good idea, since initially the parameters are often poorly estimated, so the Viterbi path will be fairly arbitrary. A safer option is to start training using forwards-backwards, and to switch to Viterbi near convergence.)\n",
      "\n",
      "17.5.3\n",
      "\n",
      "Bayesian methods for “ﬁtting” HMMs *\n",
      "\n",
      "EM returns a MAP estimate of the parameters. In this section, we brieﬂy discuss some methods for Bayesian parameter estimation in HMMs. (These methods rely on material that we will cover later in the book.)\n",
      "\n",
      "One approach is to use variational Bayes EM (VBEM), which we discuss in general terms in Section 21.6. The details for the HMM case can be found in (MacKay 1997; Beal 2003), but the basic idea is this: The E step uses forwards-backwards, but where (roughly speaking) we plug in the posterior mean parameters instead of the MAP estimates. The M step updates the parameters of the conjugate posteriors, instead of updating the parameters themselves.\n",
      "\n",
      "An alternative to VBEM is to use MCMC. A particularly appealing algorithm is block Gibbs sampling, which we discuss in general terms in Section 24.2.8. The details for the HMM case can be found in (Fruhwirth-Schnatter 2007), but the basic idea is this: we sample z1:T given the data and parameters using forwards-ﬁltering, backwards-sampling, and we then sample the parameters from their posteriors, conditional on the sampled latent paths. This is simple to implement, but one does need to take care of unidentiﬁability (label switching), just as with mixture models (see Section 11.3.1).\n",
      "\n",
      "17.5.4\n",
      "\n",
      "Discriminative training\n",
      "\n",
      "Sometimes HMMs are used as the class conditional density inside a generative classiﬁer. In this case, p(x|y = c, θ) can be computed using the forwards algorithm. We can easily maximize the i=1 p(xi, yi|θ) by using EM (or some other method) to ﬁt the HMM for each joint likelihood class-conditional density separately.\n",
      "\n",
      "(cid:26)N\n",
      "\n",
      "However, we might like to ﬁnd the parameters that maximize the conditional likelihood\n",
      "\n",
      "N(cid:20)\n",
      "\n",
      "i=1\n",
      "\n",
      "p(yi|xi, θ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "i\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "p(yi|θ)p(xi|yi, θ) c p(yi = c|θ)p(xi|c, θ)\n",
      "\n",
      "(17.109)\n",
      "\n",
      "This is more expensive than maximizing the joint likelihood, since the denominator couples all C class-conditional HMMs together. Furthermore, EM can no longer be used, and one must resort\n",
      "\n",
      "17.6. Generalizations of HMMs\n",
      "\n",
      "621\n",
      "\n",
      "to generic gradient based methods. Nevertheless, discriminative training can result in improved accuracies. The standard practice in speech recognition is to initially train the generative models separately using EM, and then to ﬁne tune them discriminatively (Jelinek 1997).\n",
      "\n",
      "17.5.5 Model selection\n",
      "\n",
      "In HMMs, the two main model selection issues are: how many states, and what topology to use for the state transition diagram. We discuss both of these issues below.\n",
      "\n",
      "17.5.5.1\n",
      "\n",
      "Choosing the number of hidden states\n",
      "\n",
      "Choosing the number of hidden states K in an HMM is analogous to the problem of choosing the number of mixture components. Here are some possible solutions:\n",
      "\n",
      "Use grid-search over a range of K’s, using as an objective function cross-validated likelihood,\n",
      "\n",
      "the BIC score, or a variational lower bound to the log-marginal likelihood.\n",
      "\n",
      "Use reversible jump MCMC. See (Fruhwirth-Schnatter 2007) for details. Note that this is very\n",
      "\n",
      "slow and is not widely used.\n",
      "\n",
      "Use variational Bayes to “extinguish” unwanted components, by analogy to the GMM case\n",
      "\n",
      "discussed in Section 21.6.1.6. See (MacKay 1997; Beal 2003) for details.\n",
      "\n",
      "Use an “inﬁnite HMM”, which is based on the hierarchical Dirichlet process. See e.g., (Beal\n",
      "\n",
      "et al. 2002; Teh et al. 2006) for details.\n",
      "\n",
      "17.5.5.2\n",
      "\n",
      "Structure learning\n",
      "\n",
      "The term structure learning in the context of HMMs refers to learning a sparse transition matrix. That is, we want to learn the structure of the state transition diagram, not the structure of the graphical model (which is ﬁxed). A large number of heuristic methods have been proposed. Most alternate between parameter estimation and some kind of heuristic split merge method (see e.g., (Stolcke and Omohundro 1992)).\n",
      "\n",
      "Alternatively, one can pose the problem as MAP estimation using a minimum entropy prior,\n",
      "\n",
      "of the form\n",
      "\n",
      "p(Ai,:) ∝ exp(−H (Ai,:))\n",
      "\n",
      "(17.110)\n",
      "\n",
      "This prior prefers states whose outgoing distribution is nearly deterministic, and hence has low entropy (Brand 1999). The corresponding M step cannot be solved in closed form, but numerical methods can be used. The trouble with this is that we might prune out all incoming transitions to a state, creating isolated “islands” in state-space. The inﬁnite HMM presents an interesting alternative to these methods. See e.g., (Beal et al. 2002; Teh et al. 2006) for details.\n",
      "\n",
      "17.6\n",
      "\n",
      "Generalizations of HMMs\n",
      "\n",
      "Many variants of the basic HMM model have been proposed. We brieﬂy discuss some of them below.\n",
      "\n",
      "622\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "Dt−1\n",
      "\n",
      "Dt\n",
      "\n",
      "Dt+1\n",
      "\n",
      "Qt − 1\n",
      "\n",
      "Qt\n",
      "\n",
      "Qt − 1\n",
      "\n",
      "Xt−1\n",
      "\n",
      "Xt\n",
      "\n",
      "Xt+1\n",
      "\n",
      "Figure 17.14 Encoding a hidden semi-Markov model as a DGM. Dt are deterministic duration counters.\n",
      "\n",
      "17.6.1\n",
      "\n",
      "Variable duration (semi-Markov) HMMs\n",
      "\n",
      "In a standard HMM, the probability we remain in state i for exactly d steps is\n",
      "\n",
      "p(ti = d) = (1 − Aii)Ad\n",
      "\n",
      "ii ∝ exp(d log Aii)\n",
      "\n",
      "(17.111)\n",
      "\n",
      "where Aii is the self-loop probability. This is called the geometric distribution. However, this kind of exponentially decaying function of d is sometimes unrealistic.\n",
      "\n",
      "To allow for more general durations, one can use a semi-Markov model. It is called semi- Markov because to predict the next state, it is not sufficient to condition on the past state: we also need to know how long we’ve been in that state. When the state space is not observed directly, the result is called a hidden semi-Markov model (HSMM), a variable duration HMM, or an explicit duration HMM.\n",
      "\n",
      "HSMMs are widely used in many gene ﬁnding programs, since the length distribution of exons and introns is not geometric (see e.g., (Schweikerta et al. 2009)), and in some chip-Seq data analysis programs (see e.g., (Kuan et al. 2009)).\n",
      "\n",
      "HSMMs are useful not only because they can model the waiting time of each state more accurately, but also because they can model the distribution of a whole batch of observations at once, instead of assuming all observations are conditionally iid. That is, they can use likelihood models of the form p(xt:t+l|zt = k, dt = l), which generate l correlated observations if the duration in state k is for l time steps. This is useful for modeling data that is piecewise linear, or shows other local trends (Ostendorf et al. 1996).\n",
      "\n",
      "17.6.1.1\n",
      "\n",
      "HSMM as augmented HMMs\n",
      "\n",
      "(In this One way to represent a HSMM is to use the graphical model shown in Figure 17.14. ﬁgure, we have assumed the observations are iid within each state, but this is not required, as mentioned above.) The Dt ∈ {0, 1, . . . , D} node is a state duration counter, where D is the maximum duration of any state. When we ﬁrst enter state j, we sample Dt from the duration distribution for that state, Dt ∼ pj(·). Thereafer, Dt deterministically counts down\n",
      "\n",
      "17.6. Generalizations of HMMs\n",
      "\n",
      "623\n",
      "\n",
      "0.012\n",
      "\n",
      "0.01\n",
      "\n",
      "n=1 n=2 n=5\n",
      "\n",
      "0.008\n",
      "\n",
      "0.006\n",
      "\n",
      "p\n",
      "\n",
      "p\n",
      "\n",
      "p\n",
      "\n",
      "p\n",
      "\n",
      "0.004\n",
      "\n",
      "1 − p\n",
      "\n",
      "1 − p\n",
      "\n",
      "1 − p\n",
      "\n",
      "0.002\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "300\n",
      "\n",
      "400\n",
      "\n",
      "500\n",
      "\n",
      "600\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 17.15 over sequence lengths, for p = 0.99 and various n. Figure generated by hmmSelfLoopDist.\n",
      "\n",
      "(a) A Markov chain with n = 4 repeated states and self loops. (b) The resulting distribution\n",
      "\n",
      "until Dt = 0. While Dt > 0, the state zt is not allowed to change. When Dt = 0, we make a stochastic transition to a new state.\n",
      "\n",
      "More precisely, we deﬁne the CPDs as follows:\n",
      "\n",
      "p(zt = k|zt−1 = j, Dt−1 = d) =\n",
      "\n",
      "p(Dt = d(cid:2)|Dt−1 = d, zt = j) =\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩ ⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "pj(d(cid:2)) 1 0\n",
      "\n",
      "1 Ajk 0\n",
      "\n",
      "if d = 0 if d(cid:2) = d − 1 and d ≥ 1 otherwise\n",
      "\n",
      "if d >0 and j = k if d = 0 otherwise\n",
      "\n",
      "(17.112)\n",
      "\n",
      "(17.113)\n",
      "\n",
      "Note that pj(d) could be represented as a table (a non-parametric approach) or as some kind of parametric distribution, such as a Gamma distribution. If pj(d) is a geometric distribution, this emulates a standard HMM.\n",
      "\n",
      "One can perform inference in this model by deﬁning a mega-variable Yt = (Dt, zt). However, this is rather inefficient, since Dt is deterministic. It is possible to marginalize Dt out, and derive special purpose inference procedures. See (Guedon 2003; Yu and Kobayashi 2006) for details. Unfortunately, all these methods take O(T K 2D) time, where T is the sequence length, K is the number of states, and D is the maximum duration of any state.\n",
      "\n",
      "17.6.1.2\n",
      "\n",
      "Approximations to semi-Markov models\n",
      "\n",
      "A more efficient, but less ﬂexible, way to model non-geometric waiting times is to replace each state with n new states, each with the same emission probabilities as the original state. For example, consider the model in Figure 17.15(a). Obviously the smallest sequence this can generate is of length n = 4. Any path of length d through the model has probability pd−n(1 − p)n; multiplying by the number of possible paths we ﬁnd that the total probability of a path of length d is\n",
      "\n",
      "p(d) =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "d − 1 n − 1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "pd−n(1 − p)n\n",
      "\n",
      "(17.114)\n",
      "\n",
      "624\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "words\n",
      "\n",
      "on\n",
      "\n",
      "need\n",
      "\n",
      "the\n",
      "\n",
      "phones\n",
      "\n",
      "aa\n",
      "\n",
      "n\n",
      "\n",
      "end\n",
      "\n",
      "n\n",
      "\n",
      "iy\n",
      "\n",
      "d\n",
      "\n",
      "end\n",
      "\n",
      "dh\n",
      "\n",
      "ax\n",
      "\n",
      "end\n",
      "\n",
      "n\n",
      "\n",
      "iy\n",
      "\n",
      "sub- phones\n",
      "\n",
      "end\n",
      "\n",
      "end\n",
      "\n",
      "Figure 17.16 An example of an HHMM for an ASR system which can recognize 3 words. The top level represents bigram word probabilities. The middle level represents the phonetic spelling of each word. The bottom level represents the subphones of each phone. (It is traditional to represent a phone as a 3 state HMM, representing the beginning, middle and end.) Based on Figure 7.5 of (Jurafsky and Martin 2000).\n",
      "\n",
      "This is equivalent to the negative binomial distribution. By adjusting n and the self-loop probabilities p of each state, we can model a wide range of waiting times: see Figure 17.15(b).\n",
      "\n",
      "Let E be the number of expansions of each state needed to approximate pj(d). Forwards- backwards on this model takes O(T (KE)Fin) time, where Fin is the average number of predecessor states, compared to O(T K(Fin +D)) for the HSMM. For typical speech recognition applications, Fin ∼ 3, D ∼ 50, K ∼ 106, T ∼ 105. (Similar ﬁgures apply to problems such as gene ﬁnding, which also often uses HSMMs.) Since Fin + D (cid:19) EFin, the expanded state method is much faster than an HSMM. See (Johnson 2005) for details.\n",
      "\n",
      "17.6.2\n",
      "\n",
      "Hierarchical HMMs\n",
      "\n",
      "A hierarchical HMM (HHMM) (Fine et al. 1998) is an extension of the HMM that is designed to model domains with hierarchical structure. Figure 17.16 gives an example of an HHMM used in automatic speech recognition. The phone and subphone models can be “called” from different higher level contexts. We can always “ﬂatten” an HHMM to a regular HMM, but a factored representation is often easier to interpret, and allows for more efficient inference and model ﬁtting.\n",
      "\n",
      "HHMMs have been used in many application domains, e.g., speech recognition (Bilmes 2001), gene ﬁnding (Hu et al. 2000), plan recognition (Bui et al. 2002), monitoring transportation patterns (Liao et al. 2007), indoor robot localization (Theocharous et al. 2004), etc. HHMMs are less expressive than stochastic context free grammars (SCFGs), since they only allow hierarchies of bounded depth, but they support more efficient inference. In particular, inference in SCFGs (using the inside outside algorithm, (Jurafsky and Martin 2008)) takes O(T 3) whereas inference in an HHMM takes O(T ) time (Murphy and Paskin 2001).\n",
      "\n",
      "We can represent an HHMM as a directed graphical model as shown in Figure 17.17. Q(cid:8) t represents the state at time t and level (cid:2). A state transition at level (cid:2) is only “allowed” if the\n",
      "\n",
      "17.6. Generalizations of HMMs\n",
      "\n",
      "625\n",
      "\n",
      "F 1 1\n",
      "\n",
      "F 1 2\n",
      "\n",
      "F 1 3\n",
      "\n",
      "Q1 1\n",
      "\n",
      "Q1 2\n",
      "\n",
      "Q1 3\n",
      "\n",
      "F 2 1\n",
      "\n",
      "F 2 2\n",
      "\n",
      "F 2 3\n",
      "\n",
      "Q2 1\n",
      "\n",
      "Q2 2\n",
      "\n",
      "Q2 3\n",
      "\n",
      "F 3 1\n",
      "\n",
      "F 3 2\n",
      "\n",
      "F 3 3\n",
      "\n",
      "Q3 1\n",
      "\n",
      "Q3 2\n",
      "\n",
      "Q3 3\n",
      "\n",
      "Y1\n",
      "\n",
      "Y2\n",
      "\n",
      "Y3\n",
      "\n",
      "Figure 17.17 An HHMM represented as a DGM. Q(cid:12) level (cid:7) has ﬁnished (entered its exit state), otherwise F (cid:12) nodes are hidden. We may optionally clamp F (cid:12) to ensure all models have ﬁnished by the end of the sequence. 2001).\n",
      "\n",
      "t = 1 if the HMM at t = 0. Shaded nodes are observed; the remaining T = 1, where T is the length of the observation sequence, Source: Figure 2 of (Murphy and Paskin\n",
      "\n",
      "t is the state at time t, level (cid:7); F (cid:12)\n",
      "\n",
      "chain at the level below has “ﬁnished”, as determined by the F (cid:8)−1 (The chain below ﬁnishes when it chooses to enter its end state.) This mechanism ensures that higher level chains evolve more slowly than lower level chains, i.e., lower levels are nested within higher levels.\n",
      "\n",
      "t\n",
      "\n",
      "node.\n",
      "\n",
      "A variable duration HMM can be thought of as a special case of an HHMM, where the top level is a deterministic counter, and the bottom level is a regular HMM, which can only change states once the counter has “timed out”. See (Murphy and Paskin 2001) for further details.\n",
      "\n",
      "17.6.3\n",
      "\n",
      "Input-output HMMs\n",
      "\n",
      "It is straightforward to extend an HMM to handle inputs, as shown in Figure 17.18(a). This deﬁnes a conditional density model for sequences of the form\n",
      "\n",
      "p(y1:T , z1:T |u1:T , θ)\n",
      "\n",
      "(17.115)\n",
      "\n",
      "where ut is the input at time t; this is sometimes called a control signal. outputs are continuous, a typical parameterization would be\n",
      "\n",
      "If the inputs and\n",
      "\n",
      "p(zt|xt, zt−1 = i, θ) = Cat(zt|S(Wiut)) p(yt|xt, zt = j, θ) =N (yt|Vjut, Σj)\n",
      "\n",
      "(17.116)\n",
      "\n",
      "(17.117)\n",
      "\n",
      "Thus the transition matrix is a logistic regression model whose parameters depend on the previous state. The observation model is a Gaussian whose parameters depend on the current\n",
      "\n",
      "626\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "ut−1\n",
      "\n",
      "ut\n",
      "\n",
      "zt−1\n",
      "\n",
      "zt\n",
      "\n",
      "z1\n",
      "\n",
      "z2\n",
      "\n",
      "zT\n",
      "\n",
      "yt−1\n",
      "\n",
      "yt\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "xT\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 17.18 (a) Input-output HMM. (b) First-order auto-regressive HMM. (c) A second-order buried Markov model. Depending on the value of the hidden variables, the effective graph structure between the com- ponents of the observed variables (i.e., the non-zero elements of the regression matrix and the precision matrix) can change, although this is not shown.\n",
      "\n",
      "state. The whole model can be thought of as a hidden version of a maximum entropy Markov model (Section 19.6.1).\n",
      "\n",
      "backwards algorithm to estimate the hidden states. algorithm to estimate the parameters (see (Bengio and Frasconi 1996) for details).\n",
      "\n",
      "Conditional on the inputs u1:T and the parameters θ, one can apply the standard forwards- It is also straightforward to derive an EM\n",
      "\n",
      "17.6.4\n",
      "\n",
      "Auto-regressive and buried HMMs\n",
      "\n",
      "The standard HMM assumes the observations are conditionally independent given the hidden state. In practice this is often not the case. However, it is straightforward to have direct arcs from xt−1 to xt as well as from zt to xt, as in Figure 17.18(b). This is known as an auto-regressive HMM, or aregime switching Markov model. For continuous data, the observation model becomes\n",
      "\n",
      "p(xt|xt−1, zt = j, θ) = N (xt|Wjxt−1 + μj, Σj)\n",
      "\n",
      "(17.118)\n",
      "\n",
      "This is a linear regression model, where the parameters are chosen according to the current hidden state. We can also consider higher-order extensions, where we condition on the last L observations:\n",
      "\n",
      "p(xt|xt−L:t−1, zt = j, θ) = N (xt|\n",
      "\n",
      "L(cid:4)\n",
      "\n",
      "Wj,(cid:8)xt−(cid:8) + μj, Σj)\n",
      "\n",
      "(17.119)\n",
      "\n",
      "(cid:8)=1\n",
      "\n",
      "Such models are widely used in econometrics (Hamilton 1990). Similar models can be deﬁned for discrete observations.\n",
      "\n",
      "The AR-HMM essentially combines two Markov chains, one on the hidden variables, to capture long range dependencies, and one on the observed variables, to capture short range dependen- cies (Berchtold 1999). Since the X nodes are observed, the connections between them only\n",
      "\n",
      "17.6. Generalizations of HMMs\n",
      "\n",
      "627\n",
      "\n",
      "z1,1\n",
      "\n",
      "z1,2\n",
      "\n",
      "z1,3\n",
      "\n",
      "z11\n",
      "\n",
      "z12\n",
      "\n",
      "z13\n",
      "\n",
      "x11\n",
      "\n",
      "x12\n",
      "\n",
      "x13\n",
      "\n",
      "z2,1\n",
      "\n",
      "z2,2\n",
      "\n",
      "z2,3\n",
      "\n",
      "z21\n",
      "\n",
      "z22\n",
      "\n",
      "z23\n",
      "\n",
      "z3,1\n",
      "\n",
      "z3,2\n",
      "\n",
      "z3,3\n",
      "\n",
      "x21\n",
      "\n",
      "x22\n",
      "\n",
      "x23\n",
      "\n",
      "z31\n",
      "\n",
      "z32\n",
      "\n",
      "z33\n",
      "\n",
      "x1\n",
      "\n",
      "x2 (a)\n",
      "\n",
      "x3\n",
      "\n",
      "x31\n",
      "\n",
      "x32\n",
      "\n",
      "(b)\n",
      "\n",
      "x33\n",
      "\n",
      "Figure 17.19 (a) A factorial HMM with 3 chains. (b) A coupled HMM with 3 chains.\n",
      "\n",
      "change the computation of the local evidence; inference can still be performed using the stan- dard forwards-backwards algorithm. Parameter estimation using EM is also straightforward: the E step is unchanged, as is the M step for the transition matrix. If we assume scalar observations for notational simplicty, the M step involves minimizing (cid:18)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "t\n",
      "\n",
      "E\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "1 σ2(st)\n",
      "\n",
      "(yt − yT\n",
      "\n",
      "t−L:t−1w(st))2 + log σ2(st)\n",
      "\n",
      "(17.120)\n",
      "\n",
      "Focussing on the w terms, we see that this requires solving K weighted least squares problems:\n",
      "\n",
      "J(w1:K) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "j\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "t\n",
      "\n",
      "γt(j) σ2(j)\n",
      "\n",
      "(yt − yT\n",
      "\n",
      "t−L:t−1wj)2\n",
      "\n",
      "(17.121)\n",
      "\n",
      "where γt(j) =p( zt = k|x1:T ) is the smoothed posterior marginal. This is a weighted linear regression problem, where the design matrix has a Toeplitz form. This subproblem can be solved efficiently using the Levinson-Durbin method (Durbin and Koopman 2001).\n",
      "\n",
      "Buried Markov models generalize AR-HMMs by allowing the dependency structure between the observable nodes to change based on the hidden state, as in Figure 17.18(c). Such a model is called a dynamic Bayesian multi net, since it is a mixture of different networks. In the linear-Gaussian setting, we can change the structure of the of xt−1 → xt arcs by using sparse regression matrices, Wj, and we can change the structure of the connections within the components of xt by using sparse Gaussian graphical models, either directed or undirected. See (Bilmes 2000) for details.\n",
      "\n",
      "17.6.5\n",
      "\n",
      "Factorial HMM\n",
      "\n",
      "An HMM represents the hidden state using a single discrete random variable zt ∈ {1, . . . , K}. To represent 10 bits of information would require K = 210 = 1024 states. By contrast, consider a distributed representation of the hidden state, where each zc,t ∈ {0, 1} represents the c’th\n",
      "\n",
      "628\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "bit of the t’th hidden state. Now we can represent 10 bits using just 10 binary variables, as illustrated in Figure 17.19(a). This model is called a factorial HMM (Ghahramani and Jordan 1997). The hope is that this kind of model could capture different aspects of a signal, e.g., one chain would represent speaking style, another the words that are being spoken.\n",
      "\n",
      "Unfortunately, conditioned on xt, all the hidden variables are correlated (due to explaining away the common observed child xt). This make exact state estimation intractable. However, we can derive efficient approximate inference algorithms, as we discuss in Section 21.4.1.\n",
      "\n",
      "17.6.6\n",
      "\n",
      "Coupled HMM and the inﬂuence model\n",
      "\n",
      "If we have multiple related data streams, we can use a coupled HMM (Brand 1996), as illustrated in Figure 17.19(b). This is a series of HMMs where the state transitions depend on the states of neighboring chains. That is, we represent the joint conditional distribution as\n",
      "\n",
      "p(zt|zt−1) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "p(zct|zt−1)\n",
      "\n",
      "(17.122)\n",
      "\n",
      "c\n",
      "\n",
      "p(zct|zt−1) =p(\n",
      "\n",
      "zct|zc,t−1, zc−1,t−1, zc+1,t−1)\n",
      "\n",
      "(17.123)\n",
      "\n",
      "This has been used for various tasks, such as audio-visual speech recognition (Neﬁan et al. 2002) and modeling freeway traffic ﬂows (Kwon and Murphy 2000).\n",
      "\n",
      "The trouble with the above model is that it requires O(CK 4) parameters to specify, if there are C chains with K states per chain, because each state depends on its own past plus the past of its two neighbors. There is a closely related model, known as the inﬂuence model (Asavathiratham 2000), which uses fewer parameters. It models the joint conditional distribution as\n",
      "\n",
      "p(zct|zt−1) =\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "αc,c(cid:2) p(zct|zc(cid:2),t−1)\n",
      "\n",
      "(17.124)\n",
      "\n",
      "c(cid:2) αc,c(cid:2) = 1 for each c. That is, we use a convex combination of pairwise transition where matrices. The αc,c(cid:2) parameter speciﬁes how much inﬂuence chain c has on chain c(cid:2). This model only takes O(C 2 + CK 2) parameters to specify. Furthermore, it allows each chain to be inﬂuenced by all the other chains, not just its nearest neighbors. (Hence the corresponding graphical model is similar to Figure 17.19(b), except that each node has incoming edges from all the previous nodes.) This has been used for various tasks, such as modeling conversational interactions between people (Basu et al. 2001).\n",
      "\n",
      "Unfortunately, inference in both of these models takes O(T (K C)2) time, since all the chains become fully correlated even if the interaction graph is sparse. Various approximate inference methods can be applied, as we discuss later.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "c(cid:2)=1\n",
      "\n",
      "17.6.7\n",
      "\n",
      "Dynamic Bayesian networks (DBNs)\n",
      "\n",
      "A dynamic Bayesian network is just a way to represent a stochastic process using a directed graphical model.7 Note that the network is not dynamic (the structure and parameters are ﬁxed),\n",
      "\n",
      "7. The acronym DBN can stand for either “dynamic Bayesian network” or “deep belief network” (Section 28.1) depending on the context. Geoff Hinton (who invented the term “deep belief network”) has suggested the acronyms DyBN and DeeBN to avoid this ambiguity.\n",
      "\n",
      "17.6. Generalizations of HMMs\n",
      "\n",
      "629\n",
      "\n",
      "27\n",
      "\n",
      "LeftClr0\n",
      "\n",
      "LeftClr1\n",
      "\n",
      "LeftClrSens1\n",
      "\n",
      "28\n",
      "\n",
      "25\n",
      "\n",
      "RightClr0\n",
      "\n",
      "RightClr1\n",
      "\n",
      "RightClrSens1\n",
      "\n",
      "26\n",
      "\n",
      "21\n",
      "\n",
      "LatAction0\n",
      "\n",
      "LatAction1\n",
      "\n",
      "TurnSignal1\n",
      "\n",
      "22\n",
      "\n",
      "23\n",
      "\n",
      "Xdot0\n",
      "\n",
      "Xdot1\n",
      "\n",
      "6\n",
      "\n",
      "SensorValid1\n",
      "\n",
      "XdotSens1\n",
      "\n",
      "24\n",
      "\n",
      "20\n",
      "\n",
      "InLane0\n",
      "\n",
      "InLane1\n",
      "\n",
      "YdotSens1\n",
      "\n",
      "18\n",
      "\n",
      "12\n",
      "\n",
      "FYdotDiff1\n",
      "\n",
      "16\n",
      "\n",
      "FwdAction0\n",
      "\n",
      "FwdAction1\n",
      "\n",
      "FYdotDiffSens1\n",
      "\n",
      "15\n",
      "\n",
      "17\n",
      "\n",
      "Ydot0\n",
      "\n",
      "Ydot1\n",
      "\n",
      "13\n",
      "\n",
      "FcloseSlow1\n",
      "\n",
      "10 Fclr1\n",
      "\n",
      "FclrSens1\n",
      "\n",
      "11\n",
      "\n",
      "19\n",
      "\n",
      "Stopped0\n",
      "\n",
      "Stopped1\n",
      "\n",
      "8\n",
      "\n",
      "BXdot1\n",
      "\n",
      "BXdotSens1\n",
      "\n",
      "9\n",
      "\n",
      "7\n",
      "\n",
      "EngStatus0\n",
      "\n",
      "EngStatus1\n",
      "\n",
      "4\n",
      "\n",
      "BcloseFast1\n",
      "\n",
      "3\n",
      "\n",
      "Bclr1\n",
      "\n",
      "BclrSens1\n",
      "\n",
      "5\n",
      "\n",
      "14\n",
      "\n",
      "FrontBackStatus0\n",
      "\n",
      "FrontBackStatus1\n",
      "\n",
      "1\n",
      "\n",
      "BYdotDiff1\n",
      "\n",
      "BYdotDiffSens1\n",
      "\n",
      "2\n",
      "\n",
      "slice t\n",
      "\n",
      "slice t+1\n",
      "\n",
      "evidence\n",
      "\n",
      "Figure 17.20 The BATnet DBN. The transient nodes are only shown for the second slice, to minimize clutter. The dotted lines can be ignored. Used with kind permission of Daphne Koller.\n",
      "\n",
      "rather it is a network representation of a dynamical system. All of the HMM variants we have seen above could be considered to be DBNs. However, we prefer to reserve the term “DBN” for graph structures that are more “irregular” and problem-speciﬁc. An example is shown in Figure 17.20, which is a DBN designed to monitor the state of a simulated autonomous car known as the “Bayesian Automated Taxi”, or “BATmobile” (Forbes et al. 1995).\n",
      "\n",
      "Deﬁning DBNs is straightforward: you just need to specify the structure of the ﬁrst time-slice, the structure between two time-slices, and the form of the CPDs. Learning is also easy. The main problem is that exact inference can be computationally expensive, because all the hidden variables become correlated over time (this is known as entanglement — see e.g., (Koller and 15.2.4) for details). Thus a sparse graph does not necessarily result in Friedman 2009, Sec. tractable exact inference. However, later we will see algorithms that can exploit the graph structure for efficient approximate inference.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 17.1 Derivation of Q function for HMM Derive Equation 17.97.\n",
      "\n",
      "Exercise 17.2 Two ﬁlter approach to smoothing in HMMs Assuming that Πt(i) =p (St = i) > 0 for all i and t, derive a recursive algorithm for updating rt(i) = p(St = i|xt+1:T ). Hint: it should be very similar to the standard forwards algorithm, but using a time- reversed transition matrix. Then show how to compute the posterior marginals γt(i) =p (St = i|x1:T )\n",
      "\n",
      "630\n",
      "\n",
      "Chapter 17. Markov and hidden Markov models\n",
      "\n",
      "from the backwards ﬁltered messages rt(i), the forwards ﬁltered messages αt(i), and the stationary distribution Πt(i).\n",
      "\n",
      "Exercise 17.3 EM for for HMMs with mixture of Gaussian observations Consider an HMM where the observation model has the form\n",
      "\n",
      "p(xt|zt = j, θ) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "wjkN (xt|μjk, Σjk)\n",
      "\n",
      "(17.125)\n",
      "\n",
      "k\n",
      "\n",
      "Draw the DGM. • Derive the E step. • Derive the M step.\n",
      "\n",
      "Exercise 17.4 EM for for HMMs with tied mixtures In many applications, it is common that the observations are high-dimensional vectors (e.g., in speech recognition, xt is often a vector of cepstral coefficients and their derivatives, so xt ∈ R39), so estimating a full covariance matrix for KM values (where M is the number of mixture components per hidden state), as in Exercise 17.3, requires a lot of data. An alternative is to use just M Gaussians, rather than M K Gaussians, and to let the state inﬂuence the mixing weights but not the means and covariances. This is called a semi-continuous HMM or tied-mixture HMM.\n",
      "\n",
      "Draw the corresponding graphical model. • Derive the E step. • Derive the M step.\n",
      "\n",
      "18 State space models\n",
      "\n",
      "18.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "A state space model or SSM is just like an HMM, except the hidden states are continuous. The model can be written in the following generic form:\n",
      "\n",
      "zt = g(ut, zt−1, (cid:11)t) yt = h(zt, ut, δt)\n",
      "\n",
      "(18.1)\n",
      "\n",
      "(18.2)\n",
      "\n",
      "where zt is the hidden state, ut is an optional input or control signal, yt is the observation, g is the transition model, h is the observation model, (cid:11)t is the system noise at time t, and δt is the observation noise at time t. We assume that all parameters of the model, θ, are known; if not, they can be included into the hidden state, as we discuss below.\n",
      "\n",
      "(Note: we will often drop the conditioning on u and θ for brevity.) We will discuss algorithms for this later in this chapter. We will also discuss how to convert our beliefs about the hidden state into predictions about future observables by computing the posterior predictive p(yt+1|y1:t).\n",
      "\n",
      "One of the primary goals in using SSMs is to recursively estimate the belief state, p(zt|y1:t, u1:t, θ).\n",
      "\n",
      "An important special case of an SSM is where all the CPDs are linear-Gaussian.\n",
      "\n",
      "In other\n",
      "\n",
      "words, we assume\n",
      "\n",
      "The transition model is a linear function\n",
      "\n",
      "zt = Atzt−1 + Btut + (cid:11)t\n",
      "\n",
      "(18.3)\n",
      "\n",
      "The observation model is a linear function\n",
      "\n",
      "yt = Ctzt + Dtut + δt\n",
      "\n",
      "(18.4)\n",
      "\n",
      "The system noise is Gaussian\n",
      "\n",
      "(cid:11)t ∼ N (0, Qt)\n",
      "\n",
      "(18.5)\n",
      "\n",
      "The observation noise is Gaussian\n",
      "\n",
      "δt ∼ N (0, Rt)\n",
      "\n",
      "(18.6)\n",
      "\n",
      "This model is called a linear-Gaussian SSM (LG-SSM) or alinear dynamical system (LDS). If the parameters θt = (At, Bt, Ct, Dt, Qt, Rt) are independent of time, the model is called stationary.\n",
      "\n",
      "632\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "14\n",
      "\n",
      "observed truth\n",
      "\n",
      "16\n",
      "\n",
      "14\n",
      "\n",
      "observed filtered\n",
      "\n",
      "14\n",
      "\n",
      "observed smoothed\n",
      "\n",
      "12\n",
      "\n",
      "12\n",
      "\n",
      "12\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "18\n",
      "\n",
      "20\n",
      "\n",
      "22\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "18\n",
      "\n",
      "20\n",
      "\n",
      "22\n",
      "\n",
      "24\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 18.1 (a) Observations (green cirles) are generated Illustration of Kalman ﬁltering and smoothing. (b) Filtered estimated is shown by an object moving to the right (true location denoted by black squares). by dotted red line. Red cross is the posterior mean, blue circles are 95% conﬁdence ellipses derived from the posterior covariance. For clarity, we only plot the ellipses every other time step. (c) Same as (b), but using offline Kalman smoothing. Figure generated by kalmanTrackingDemo.\n",
      "\n",
      "In particular, if the initial belief state is Gaussian, p(z1) =N (μ1|0, Σ1|0), then all subsequent belief states will also be Gaussian; we will denote them by p(zt|y1:t) = N (μt|t, Σt|t). (The notation μt|τ denotes E [zt|y1:τ ], and similarly for Σt|t; thus μt|0 denotes the prior for z1 before we have seen any data. For brevity we will denote the posterior belief states using μt|t = μt and Σt|t = Σt.) We can compute these quantities efficiently using the celebrated Kalman ﬁlter, as we show in Section 18.3.1. But before discussing algorithms, we discuss some important applications.\n",
      "\n",
      "The LG-SSM is important because it supports exact inference, as we will see.\n",
      "\n",
      "18.2\n",
      "\n",
      "Applications of SSMs\n",
      "\n",
      "SSMs have many applications, some of which we discuss in the sections below. We mostly focus on LG-SSMs, for simplicity, although non-linear and/or non-Gaussian SSMs are even more widely used.\n",
      "\n",
      "18.2.1\n",
      "\n",
      "SSMs for object tracking\n",
      "\n",
      "One of the earliest applications of Kalman ﬁltering was for tracking objects, such as airplanes and missiles, from noisy measurements, such as radar. Here we give a simpliﬁed example to illustrate the key ideas. Consider an object moving in a 2D plane. Let z1t and z2t be the horizontal and vertical locations of the object, and ˙z1t and ˙z2t be the corresponding velocity. We can represent this as a state vector zt ∈ R4 as follows:\n",
      "\n",
      "zT t =\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "z1t\n",
      "\n",
      "z2t\n",
      "\n",
      "˙z1t\n",
      "\n",
      "˙z2t\n",
      "\n",
      "(cid:23)\n",
      "\n",
      ".\n",
      "\n",
      "(18.7)\n",
      "\n",
      "18.2. Applications of SSMs\n",
      "\n",
      "633\n",
      "\n",
      "Let us assume that the object is moving at constant velocity, but is “perturbed” by random Gaussian noise (e.g., due to the wind). Thus we can model the system dynamics as follows:\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "z1t z2t ˙z1t ˙z2t\n",
      "\n",
      "zt = Atzt−1 + (cid:11)t ⎛ ⎞ 1 0 0 0\n",
      "\n",
      "⎟ ⎟ ⎠ =\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "0 Δ 0 0 Δ 1 0 1 0 1 0 0\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "z1,t−1 z2,t−1 ˙z1,t−1 ˙z2,t−1\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠ +\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "(cid:16)1t (cid:16)2t (cid:16)3t (cid:16)4t\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠\n",
      "\n",
      "(18.8)\n",
      "\n",
      "(18.9)\n",
      "\n",
      "where (cid:11)t ∼ N (0, Q) is the system noise, and Δ is the sampling period. This says that the new location zj,t is the old location zj,t−1 plus Δ times the old velocity ˙zj,t−1, plus random noise, (cid:16)jt, for j = 1 : 2. Also, the new velocity ˙zj,t is the old velocity ˙zj,t−1 plus random noise, (cid:16)jt, for j = 3 : 4. This is called a random accelerations model, since the object moves according to Newton’s laws, but is subject to random changes in velocity.\n",
      "\n",
      "Now suppose that we can observe the location of the object but not its velocity. Let yt ∈ R2 represent our observation, which we assume is subject to Gaussian noise. We can model this as follows:\n",
      "\n",
      "yt = Ctzt + δt\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "y1t y2t\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 0 0 1\n",
      "\n",
      "0 0\n",
      "\n",
      "(cid:9) 0 0\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "z1t z2t ˙z1t ˙z2t\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠ +\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "δ1t δ2t δ3t δ4t\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠\n",
      "\n",
      "(18.10)\n",
      "\n",
      "(18.11)\n",
      "\n",
      "where δt ∼ N (0, R) is the measurement noise.\n",
      "\n",
      "Finally, we need to specify our initial (prior) beliefs about the state of the object, p(z1). We will assume this is a Gaussian, p(z1) = N (z1|μ1|0, Σ1|0). We can represent prior ignorance by making Σ1|0 suitably “broad”, e.g., Σ1|0 = ∞I. We have now fully speciﬁed the model and can perform sequential Bayesian updating to compute p(zt|y1:t) using an algorithm known as the Kalman ﬁlter, to be described in Section 18.3.1.\n",
      "\n",
      "Figure 18.1(a) gives an example. The object moves to the right and generates an observation at each time step (think of “blips” on a radar screen). We observe these blips and ﬁlter out the noise by using the Kalman ﬁlter. At every step, we have p(zt|y1:t), from which we can compute p(z1t, z2t|y1:t) by marginalizing out the dimensions corresponding to the velocities. (This is easy to do since the posterior is Gaussian.) Our “best guess” about the location of the object is the posterior mean, E[zt|y1:t], denoted as a red cross in Figure 18.1(b). Our uncertainty associated with this is represented as an ellipse, which contains 95% of the probability mass. We see that our uncertainty goes down over time, as the effects of the initial uncertainty get “washed out”. We also see that the estimated trajectory has “ﬁltered out” some of the noise. To obtain the much smoother plot in Figure 18.1(c), we need to use the Kalman smoother, which computes p(zt|y1:T ); this depends on “future” as well as “past” data, as discussed in Section 18.3.2.\n",
      "\n",
      "18.2.2\n",
      "\n",
      "Robotic SLAM\n",
      "\n",
      "Consider a robot moving around an unknown 2d world. It needs to learn a map and keep track of its location within that map. This problem is known as simultaneous localization and\n",
      "\n",
      "634\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "L1\n",
      "\n",
      "Y1\n",
      "\n",
      "Y3\n",
      "\n",
      "X1\n",
      "\n",
      "X2\n",
      "\n",
      "X3\n",
      "\n",
      ". . .\n",
      "\n",
      "XT\n",
      "\n",
      "Y1\n",
      "\n",
      "Y2\n",
      "\n",
      "YT\n",
      "\n",
      "L2\n",
      "\n",
      "Figure 18.2 Illustration of graphical model underlying SLAM. Li is the ﬁxed location of landmark i, xt is the location of the robot, and yt is the observation. In this trace, the robot sees landmarks 1 and 2 at time step 1, then just landmark 2, then just landmark 1, etc. Based on Figure 15.A.3 of (Koller and Friedman 2009).\n",
      "\n",
      "Robot pose\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 18.3 Illustration of the SLAM problem. (a) A robot starts at the top left and moves clockwise in a circle back to where it started. We see how the posterior uncertainty about the robot’s location increases and then decreases as it returns to a familar location, closing the loop. If we performed smoothing, this new information would propagate backwards in time to disambiguate the entire trajectory. (b) We show the precision matrix, representing sparse correlations between the landmarks, and between the landmarks and the robot’s position (pose). This sparse precision matrix can be visualized as a Gaussian graphical model, as shown. Source: Figure 15.A.3 of (Koller and Friedman 2009) . Used with kind permission of Daphne Koller.\n",
      "\n",
      "18.2. Applications of SSMs\n",
      "\n",
      "635\n",
      "\n",
      "mapping, or SLAM for short, and is widely used in mobile robotics, as well as other applications such as indoor navigation using cellphones (since GPS does not work inside buildings).\n",
      "\n",
      "Let us assume we can represent the map as the 2d locations of a ﬁxed set of K landmarks, denote them by L1, . . . , LK (each is a vector in R2). For simplicity, we will assume these are uniquely identiﬁable. Let xt represent the unknown location of the robot at time t. We deﬁne the state space to be zt = (xt, L1:K); we assume the landmarks are static, so their motion If yt measures the distance from xt to model is a constant, and they have no system noise. the set of closest landmarks, then the robot can update its estimate of the landmark locations based on what it sees. Figure 18.2 shows the corresponding graphical model for the case where K = 2, and where on the ﬁrst step it sees landmarks 1 and 2, then just landmark 2, then just landmark 1, etc.\n",
      "\n",
      "If we assume the observation model p(yt|zt, L) is linear-Gaussian, and we use a Gaussian motion model for p(xt|xt−1, ut), we can use a Kalman ﬁlter to maintain our belief state about the location of the robot and the location of the landmarks (Smith and Cheeseman 1986; Choset and Nagatani 2001).\n",
      "\n",
      "Over time, the uncertainty in the robot’s location will increase, due to wheel slippage etc., but when the robot returns to a familiar location, its uncertainty will decrease again. This is called closing the loop, and is illustrated in Figure 18.3(a), where we see the uncertainty ellipses, representing cov [xt|y1:t, u1:t], grow and then shrink. (Note that in this section, we assume that a human is joysticking the robot through the environment, so u1:t is given as input, i.e., we do not address the decision-theoretic issue of choosing where to explore.)\n",
      "\n",
      "Since the belief state is Gaussian, we can visualize the posterior covariance matrix Σt. Ac- tually, it is more interesting to visualize the posterior precision matrix, Λt = Σ−1 , since that is fairly sparse, as shown in Figure 18.3(b). The reason for this is that zeros in the precision matrix correspond to absent edges in the corresponding undirected Gaussian graphical model (see Section 19.4.4). Initially all the landmarks are uncorrelated (assuming we have a diagonal prior on L), so the GGM is a disconnected graph, and Λt is diagonal. However, as the robot moves about, it will induce correlation between nearby landmarks. Intuitively this is because the robot is estimating its position based on distance to the landmarks, but the landmarks’ locations are being estimated based on the robot’s position, so they all become inter-dependent. This can be seen more clearly from the graphical model in Figure 18.2: it is clear that L1 and L2 are not d-separated by y1:t, because there is a path between them via the unknown sequence of x1:t nodes. As a consequence of the precision matrix becoming denser, exact inference takes O(K 3) time. (This is an example of the entanglement problem for inference in DBNs.) This prevents the method from being applied to large maps.\n",
      "\n",
      "t\n",
      "\n",
      "There are two main solutions to this problem. The ﬁrst is to notice that the correlation pattern moves along with the location of the robot (see Figure 18.3(b)). The remaining correlations become weaker over time. Consequently we can dynamically “prune out” weak edges from the GGM using a technique called the thin junction tree ﬁlter (Paskin 2003) (junction trees are explained in Section 20.4).\n",
      "\n",
      "A second approach is to notice that, conditional on knowing the robot’s path, x1:t, the k=1 p(Lk|x1:t, y1:t). This landmark locations are independent. That is, p(L|x1:t, y1:t) = forms the basis of a method known as FastSLAM, which combines Kalman ﬁltering and particle ﬁltering, as discussed in Section 23.6.3.\n",
      "\n",
      "(cid:26)K\n",
      "\n",
      "(Thrun et al. 2006) provides a more detailed account of SLAM and mobile robotics.\n",
      "\n",
      "636\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "θt−1\n",
      "\n",
      "θt\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "online linear regression\n",
      "\n",
      "w0 w1 w0 batch w1 batch\n",
      "\n",
      "yt−1\n",
      "\n",
      "yt\n",
      "\n",
      "s t h g e w\n",
      "\n",
      "i\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "xt−1\n",
      "\n",
      "xt\n",
      "\n",
      "−8\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "time\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 18.4 (a) A dynamic generalization of linear regression. (b) Illustration of the recursive least squares algorithm applied to the model p(y|x, θ) =N (y|w0 + w1x, σ2). We plot the marginal posterior of w0 and w1 vs number of data points. var [wj|y1:t].) After seeing all (Error bars represent E [wj|y1:t] ± the data, we converge to the offline ML (least squares) solution, represented by the horizontal lines. Figure generated by linregOnlineDemoKalman.\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "18.2.3\n",
      "\n",
      "Online parameter learning using recursive least squares\n",
      "\n",
      "We can perform online Bayesian inference for the parameters of various statistical models using SSMs. in Section 18.5.3.2, we discuss logistic regression.\n",
      "\n",
      "In this section, we focus on linear regression;\n",
      "\n",
      "The basic idea is to let the hidden state represent the regression parameters, and to let the In more detail, deﬁne the (time-varying) observation model represent the current data vector. prior to be p(θ) =N (θ|θ0, Σ0). (If we want to do online ML estimation, we can just set Σ0 = ∞I.) Let the hidden state be zt = θ; if we assume the regression parameters do not change, we can set At = I and Qt = 0I, so\n",
      "\n",
      "p(θt|θt−1) = N (θt|θt−1, 0I) = δθt−1 (θt)\n",
      "\n",
      "(18.12)\n",
      "\n",
      "(If we do let the parameters change over time, we get a so-called dynamic linear model (Harvey 1990; West and Harrison 1997; Petris et al. 2009).) Let Ct = xT t , and Rt = σ2, so the (non-stationary) observation model has the form\n",
      "\n",
      "N (yt|Ctzt, Rt) = N (yt|xT\n",
      "\n",
      "t θt, σ2)\n",
      "\n",
      "(18.13)\n",
      "\n",
      "Applying the Kalman ﬁlter to this model provides a way to update our posterior beliefs about the parameters as the data streams in. This is known as the recursive least squares or RLS algorithm.\n",
      "\n",
      "We can derive an explicit form for the updates as follows. In Section 18.3.1, we show that the\n",
      "\n",
      "Kalman update for the posterior mean has the form\n",
      "\n",
      "μt = Atμt−1 + Kt(yt − CtAtμt−1)\n",
      "\n",
      "(18.14)\n",
      "\n",
      "18.2. Applications of SSMs\n",
      "\n",
      "637\n",
      "\n",
      "where Kt is known as the Kalman gain matrix. Based on Equation 18.39, one can show that t R−1 In this context, we have Kt = Σtxt/σ2. Hence the update for the Kt = ΣtCT . t parameters becomes\n",
      "\n",
      "ˆθt = ˆθt−1 +\n",
      "\n",
      "1 σ2\n",
      "\n",
      "Σt|t(yt − xT\n",
      "\n",
      "t ˆθt−1)xt\n",
      "\n",
      "(18.15)\n",
      "\n",
      "If we approximate 1 σ2 Σt|t−1 with ηtI, we recover the least mean squares or LMS algorithm, discussed in Section 8.5.3. In LMS, we need to specify how to adapt the update parameter ηt to ensure convergence to the MLE. Furthermore, the algorithm may take multiple passes through the data. By contrast, the RLS algorithm automatically performs step-size adaptation, and converges to the optimal posterior in one pass over the data. See Figure 18.4 for an example.\n",
      "\n",
      "18.2.4\n",
      "\n",
      "SSM for time series forecasting *\n",
      "\n",
      "SSMs are very well suited for time-series forecasting, as we explain below. We focus on the case of scalar (one dimensional) time series, for simplicity. Our presentation is based on (Varian 2011). See also (Aoki 1987; Harvey 1990; West and Harrison 1997; Durbin and Koopman 2001; Petris et al. 2009; Prado and West 2010) for good books on this topic.\n",
      "\n",
      "At ﬁrst sight, it might not be apparent why SSMs are useful, since the goal in forecasting is to predict future visible variables, not to estimate hidden states of some system. Indeed, most classical methods for time series forecasting are just functions of the form ˆyt+1 = f (y1:t, θ), where hidden variables play no role (see Section 18.2.4.4). The idea in the state-space approach to time series is to create a generative model of the data in terms of latent processes, which capture different aspects of the signal. We can then integrate out the hidden variables to compute the posterior predictive of the visibles.\n",
      "\n",
      "Since the model is linear-Gaussian, we can just add these processes together to explain the observed data. This is called a structural time series model. Below we explain some of the basic building blocks.\n",
      "\n",
      "18.2.4.1\n",
      "\n",
      "Local level model\n",
      "\n",
      "The simplest latent process is known as the local level model, which has the form\n",
      "\n",
      "(cid:16)y yt = at + (cid:16)y t , t ∼ N (0, R) (cid:16)a at = at−1 + (cid:16)a t ∼ N (0, Q) t ,\n",
      "\n",
      "(18.16)\n",
      "\n",
      "(18.17)\n",
      "\n",
      "where the hidden state is just zt = at. This model asserts that the observed data yt ∈ R is equal to some unknown level term at ∈ R, plus observation noise with variance R. In addition, the level at evolves over time subject to system noise with variance Q. See Figure 18.5 for some examples.\n",
      "\n",
      "638\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "at−1\n",
      "\n",
      "at\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "local level, a=1.000\n",
      "\n",
      "Q=0.0, R=0.1 Q=0.1, R=0.0 Q=0.1, R=0.1\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "yt\n",
      "\n",
      "−6\n",
      "\n",
      "−8 0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "120\n",
      "\n",
      "140\n",
      "\n",
      "160\n",
      "\n",
      "180\n",
      "\n",
      "200\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(b) Sample output, for a0 = 10. Black solid line: Q = 0, R = 1 Figure 18.5 (deterministic system, noisy observations). Red dotted line: Q = 0.1, R = 0 (noisy system, deterministic observation). Blue dot-dash line: Q = 0.1, R = 1 (noisy system and observations). Figure generated by ssmTimeSeriesSimple.\n",
      "\n",
      "(a) Local level model.\n",
      "\n",
      "at−1\n",
      "\n",
      "at\n",
      "\n",
      "200\n",
      "\n",
      "local trend, a=10.000, b=1.000\n",
      "\n",
      "0\n",
      "\n",
      "bt−1\n",
      "\n",
      "bt\n",
      "\n",
      "−200\n",
      "\n",
      "−400\n",
      "\n",
      "−600\n",
      "\n",
      "yt\n",
      "\n",
      "−800\n",
      "\n",
      "−1000 0\n",
      "\n",
      "Q=0.0, R=100.0 Q=1.0, R=0.0 Q=1.0, R=100.0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 18.6 (a) Local Trend. (b) Sample output, for a0 = 10, b0 = 1. Color code as in Figure 18.5. Figure generated by ssmTimeSeriesSimple.\n",
      "\n",
      "18.2.4.2\n",
      "\n",
      "Local linear trend\n",
      "\n",
      "Many time series exhibit linear trends upwards or downwards, at least locally. We can model this by letting the level at change by an amount bt at each step as follows:\n",
      "\n",
      "yt = at + (cid:16)y (cid:16)y t , t ∼ N (0, R) at = at−1 + bt−1 + (cid:16)a t , bt = bt−1 + (cid:16)b t,\n",
      "\n",
      "(cid:16)a t ∼ N (0, Qa) (cid:16)b t ∼ N (0, Qb)\n",
      "\n",
      "(18.18)\n",
      "\n",
      "(18.19)\n",
      "\n",
      "(18.20)\n",
      "\n",
      "See Figure 18.6(a). We can write this in standard form by deﬁning zt = (at, bt) and (cid:8)\n",
      "\n",
      "A =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 1 0 1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      ", C =\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "1\n",
      "\n",
      "(cid:23) 0\n",
      "\n",
      ", Q =\n",
      "\n",
      "Qa 0 0 Qb\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(18.21)\n",
      "\n",
      "When Qb = 0, we have bt = b0, which is some constant deﬁning the slope of the line. If in addition we have Qa = 0, we havea t = at−1 + b0t. Unrolling this, we have at = a0 + b0t, and\n",
      "\n",
      "18.2. Applications of SSMs\n",
      "\n",
      "639\n",
      "\n",
      "at−1\n",
      "\n",
      "at\n",
      "\n",
      "bt−1\n",
      "\n",
      "bt\n",
      "\n",
      "c3 t−1\n",
      "\n",
      "1\n",
      "\n",
      "c3 t\n",
      "\n",
      "c2 t−1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "c2 t\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "Q=0.0, R=1.0 Q=1.0, R=0.0 Q=1.0, R=1.0\n",
      "\n",
      "seasonal model, s=4, a=0.000, b=0.000\n",
      "\n",
      "c1 t−1\n",
      "\n",
      "c1 t\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "−10\n",
      "\n",
      "yt\n",
      "\n",
      "−20\n",
      "\n",
      "−30 0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "14\n",
      "\n",
      "16\n",
      "\n",
      "18\n",
      "\n",
      "20\n",
      "\n",
      "Figure 18.7 Color code as in Figure 18.5. Figure generated by ssmTimeSeriesSimple.\n",
      "\n",
      "(a) Seasonal model.\n",
      "\n",
      "(b) Sample output, for a0 = b0 = 0, c0 = (1, 1, 1), with a period of 4.\n",
      "\n",
      "hence E [yt|y1:t−1] = a0 + tb0. This is thus a generalization of the classic constant linear trend model, an example of which is shown in the black line of Figure 18.6(b).\n",
      "\n",
      "18.2.4.3\n",
      "\n",
      "Seasonality\n",
      "\n",
      "Many time series ﬂuctuate periodically, as illustrated in Figure 18.7(b). This can be modeled by adding a latent process consisting of a series offset terms, ct, which sum to zero (on average) over a complete cycle of S steps:\n",
      "\n",
      "ct = −\n",
      "\n",
      "S−1(cid:4)\n",
      "\n",
      "ct−s + (cid:16)c\n",
      "\n",
      "t , (cid:16)c\n",
      "\n",
      "t ∼ N (0, Qc)\n",
      "\n",
      "(18.22)\n",
      "\n",
      "s=1\n",
      "\n",
      "See Figure 18.7(a) for the graphical model for the case S = 4 (we only need 3 seasonal vari- able because of the sum-to-zero constraint). Writing this in standard LG-SSM form is left to Exercise 18.2.\n",
      "\n",
      "18.2.4.4\n",
      "\n",
      "ARMA models *\n",
      "\n",
      "The classical approach to time-series forecasting is based on ARMA models. “ARMA” stands for auto-regressive moving-average, and refers to a model of the form\n",
      "\n",
      "xt =\n",
      "\n",
      "p(cid:4)\n",
      "\n",
      "αixt−i +\n",
      "\n",
      "q(cid:4)\n",
      "\n",
      "βjwt−j + vt\n",
      "\n",
      "(18.23)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "If q = 0, we have a pure AR where vt, wt ∼ N (0, 1) are independent Gaussian noise terms. model, where xt ⊥ xi|xt−1:t−p, for i < t − p. For example, if p = 1, we have the AR(1) model\n",
      "\n",
      "640\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "w1\n",
      "\n",
      "w2\n",
      "\n",
      "w3\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "x4\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "x4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "w1\n",
      "\n",
      "w2\n",
      "\n",
      "w3\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "x4\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 18.8 (a) An AR(1) model. (b) An MA(1) model represented as a bi-directed graph. (c) An ARMA(1,1) model. Source: Figure 5.14 of (Choi 2011). Used with kind permission of Myung Choi.\n",
      "\n",
      "(The vt nodes are implicit in the Gaussian CPD for xt.) This is just a shown in Figure 18.8(a). If p = 0, we have a pure MA model, where xt ⊥ xi, for i < t − q. ﬁrst-order Markov chain. For example, if q = 1, we have the MA(1) model shown in Figure 18.8(b). Here the wt nodes are hidden common causes, which induces dependencies between adjacent time steps. This models short-range correlation. If p = q = 1, we get the ARMA(1,1) model shown in Figure 18.8(c), which captures correlation at short and long time scales.\n",
      "\n",
      "It turns out that ARMA models can be represented as SSMs, as explained in (Aoki 1987; Harvey 1990; West and Harrison 1997; Durbin and Koopman 2001; Petris et al. 2009; Prado and West 2010). However, the structural approach to time series is often easier to understand than the ARMA approach. In addition, it allows the parameters to evolve over time, which makes the models more adaptive to non-stationarity.\n",
      "\n",
      "18.3\n",
      "\n",
      "Inference in LG-SSM\n",
      "\n",
      "In this section, we discuss exact inference in LG-SSM models. We ﬁrst consider the online case, which is analogous to the forwards algorithm for HMMs. We then consider the offline case, which is analogous to the forwards-backwards algorithm for HMMs.\n",
      "\n",
      "18.3.1\n",
      "\n",
      "The Kalman ﬁltering algorithm\n",
      "\n",
      "The Kalman ﬁlter is an algorithm for exact Bayesian ﬁltering for linear-Gaussian state space models. We will represent the marginal posterior at time t by\n",
      "\n",
      "p(zt|y1:t, u1:t) = N (zt|μt, Σt)\n",
      "\n",
      "(18.24)\n",
      "\n",
      "Since everything is Gaussian, we can perform the prediction and update steps in closed form, as we explain below. The resulting algorithm is the Gaussian analog of the HMM ﬁlter in Section 17.4.2.\n",
      "\n",
      "18.3.\n",
      "\n",
      "Inference in LG-SSM\n",
      "\n",
      "641\n",
      "\n",
      "18.3.1.1\n",
      "\n",
      "Prediction step\n",
      "\n",
      "The prediction step is straightforward to derive:\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(zt|y1:t−1, u1:t) =\n",
      "\n",
      "N (zt|Atzt−1 + Btut, Qt)N (zt−1|μt−1, Σt−1)dzt−1\n",
      "\n",
      "(18.25)\n",
      "\n",
      "= N (zt|μt|t−1, Σt|t−1)\n",
      "\n",
      "(18.26)\n",
      "\n",
      "μt|t−1 (cid:2) Atμt−1 + Btut Σt|t−1 (cid:2) AtΣt−1AT\n",
      "\n",
      "t + Qt\n",
      "\n",
      "(18.27)\n",
      "\n",
      "(18.28)\n",
      "\n",
      "18.3.1.2 Measurement step\n",
      "\n",
      "The measurement step can be computed using Bayes rule, as follows\n",
      "\n",
      "p(zt|yt, y1:t−1, u1:t) ∝ p(yt|zt, ut)p(zt|y1:t−1, u1:t)\n",
      "\n",
      "(18.29)\n",
      "\n",
      "In Section 18.3.1.6, we show that this is given by\n",
      "\n",
      "p(zt|y1:t, ut) =N (zt|μt, Σt) μt = μt|t−1 + Ktrt Σt = (I − KtCt)Σt|t−1\n",
      "\n",
      "(18.30)\n",
      "\n",
      "(18.31)\n",
      "\n",
      "(18.32)\n",
      "\n",
      "where rt is the residual or innovation, given by the difference between our predicted observa- tion and the actual observation:\n",
      "\n",
      "rt (cid:2) yt − ˆyt ˆyt (cid:2) E [yt|y1:t−1, u1:t] = Ctμt|t−1 + Dtut\n",
      "\n",
      "(18.33)\n",
      "\n",
      "(18.34)\n",
      "\n",
      "and Kt is the Kalman gain matrix, given by\n",
      "\n",
      "Kt (cid:2) Σt|t−1CT\n",
      "\n",
      "t S−1 t\n",
      "\n",
      "(18.35)\n",
      "\n",
      "where\n",
      "\n",
      "St (cid:2) cov [rt|y1:t−1, u1:t]\n",
      "\n",
      "= E = CtΣt|t−1CT\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(Ctzt + δt − ˆyt)(Ctzt + δt − ˆyt)T |y1:t−1, u1:t\n",
      "\n",
      "t + Rt\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(18.36) (18.37)\n",
      "\n",
      "(18.38)\n",
      "\n",
      "where δt ∼ N (0, Rt) is an observation noise term which is independent of all other noise sources. Note that by using the matrix inversion lemma, the Kalman gain matrix can also be written as\n",
      "\n",
      "Kt = Σt|t−1CT (CΣt|t−1CT + R)−1 = (Σ−1\n",
      "\n",
      "t|t−1 + CT RC)−1CT R−1\n",
      "\n",
      "(18.39)\n",
      "\n",
      "We now have all the quantities we need to implement the algorithm; see kalmanFilter for some Matlab code.\n",
      "\n",
      "In particular, consider the equation for the mean update: μt = μt|t−1 + Ktrt. This says that the new mean is the old mean plus a\n",
      "\n",
      "Let us try to make sense of these equations.\n",
      "\n",
      "642\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "correction factor, which is Kt times the error signal rt. The amount of weight placed on the error signal depends on the Kalman gain matrix. , which is the ratio between the covariance of the prior (from the dynamic model) and the covariance If we have a strong prior and/or very noisy sensors, |Kt| will be of the measurement error. small, and we will place little weight on the correction term. Conversely, if we have a weak prior and/or high precision sensors, then |Kt| will be large, and we will place a lot of weight on the correction term.\n",
      "\n",
      "If Ct = I, then Kt = Σt|t−1S−1\n",
      "\n",
      "t\n",
      "\n",
      "18.3.1.3 Marginal likelihood\n",
      "\n",
      "As a byproduct of the algorithm, we can also compute the log-likelihood of the sequence using\n",
      "\n",
      "log p(y1:T |u1:T ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "log p(yt|y1:t−1, u1:t)\n",
      "\n",
      "(18.40)\n",
      "\n",
      "t\n",
      "\n",
      "where\n",
      "\n",
      "p(yt|y1:t−1, u1:t) = N (yt|Ctμt|t−1, St)\n",
      "\n",
      "(18.41)\n",
      "\n",
      "18.3.1.4\n",
      "\n",
      "Posterior predictive\n",
      "\n",
      "The one-step-ahead posterior predictive density for the observations can be computed as follows\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(yt|y1:t−1, u1:t) =\n",
      "\n",
      "N (yt|Czt, R)N (zt|μt|t−1, Σt|t−1)dzt\n",
      "\n",
      "(18.42)\n",
      "\n",
      "= N (yt|Cμt|t−1, CΣt|t−1CT + R)\n",
      "\n",
      "(18.43)\n",
      "\n",
      "This is useful for time series forecasting.\n",
      "\n",
      "18.3.1.5\n",
      "\n",
      "Computational issues\n",
      "\n",
      "There are two dominant costs in the Kalman ﬁlter: the matrix inversion to compute the Kalman gain matrix, Kt, which takes O(|yt|3) time; and the matrix-matrix multiply to compute Σt, which takes O(|zt|2) time. In some applications (e.g., robotic mapping), we have |zt| (cid:19) |yt|, so the latter cost dominates. However, in such cases, we can sometimes use sparse approximations (see (Thrun et al. 2006)).\n",
      "\n",
      "In cases where |yt| (cid:19) |zt|, we can precompute Kt, since, suprisingly, it does not depend on the actual observations y1:t (an unusual property that is speciﬁc to linear Gaussian systems). The iterative equations for updating Σt are called the Ricatti equations, and for time invariant systems (i.e., where θt = θ), they converge to a ﬁxed point. This steady state solution can then be used instead of using a time-speciﬁc gain matrix.\n",
      "\n",
      "In practice, more sophisticated implementations of the Kalman ﬁlter should be used, for rea- sons of numerical stability. One approach is the information ﬁlter, which recursively updates the canonical parameters of the Gaussian, Λt = Σ−1 and ηt = Λtμt, instead of the moment parameters. Another approach is the square root ﬁlter, which works with the Cholesky de- composition or the UtDtUt decomposition of Σt. This is much more numerically stable than directly updating Σt. Further details can be found at http://www.cs.unc.edu/~welch/kal man/ and in various books, such as (Simon 2006).\n",
      "\n",
      "t\n",
      "\n",
      "18.3.\n",
      "\n",
      "Inference in LG-SSM\n",
      "\n",
      "643\n",
      "\n",
      "18.3.1.6\n",
      "\n",
      "Derivation *\n",
      "\n",
      "We now derive the Kalman ﬁlter equations. For notational simplicity, we will ignore the input terms u1:t. From Bayes rule for Gaussians (Equation 4.125), we have that the posterior precision is given by\n",
      "\n",
      "Σ−1 t\n",
      "\n",
      "= Σ−1\n",
      "\n",
      "t|t−1 + CT\n",
      "\n",
      "t R−1\n",
      "\n",
      "t Ct\n",
      "\n",
      "(18.44)\n",
      "\n",
      "From the matrix inversion lemma (Equation 4.106) we can rewrite this as\n",
      "\n",
      "Σt = Σt|t−1 − Σt|t−1CT = (I − KtCt)Σt|t−1\n",
      "\n",
      "t (Rt + CtΣt|t−1CT\n",
      "\n",
      "t )−1CtΣt|t−1\n",
      "\n",
      "(18.45)\n",
      "\n",
      "(18.46)\n",
      "\n",
      "From Bayes rule for Gaussians (Equation 4.125), the posterior mean is given by\n",
      "\n",
      "μt = ΣtCtR−1\n",
      "\n",
      "t yt + ΣtΣ−1\n",
      "\n",
      "t|t−1μt|t−1\n",
      "\n",
      "(18.47)\n",
      "\n",
      "We will now massage this into the form stated earlier. Applying the second matrix inversion lemma (Equation 4.107) to the ﬁrst term of Equation 18.47 we have t|t−1 + CT\n",
      "\n",
      "ΣtCtR−1\n",
      "\n",
      "t yt = (Σ−1\n",
      "\n",
      "= Σt|t−1CT\n",
      "\n",
      "t (Rt + CtΣt|t−1CT\n",
      "\n",
      "t R−1\n",
      "\n",
      "t Ct)−1CtR−1\n",
      "\n",
      "t yt t )−1yt = Ktyt\n",
      "\n",
      "(18.48)\n",
      "\n",
      "(18.49)\n",
      "\n",
      "Now applying the matrix inversion lemma (Equation 4.106) to the second term of Equation 18.47 we have\n",
      "\n",
      "ΣtΣ−1\n",
      "\n",
      "t|t−1μt|t−1 = (Σ−1 (cid:13)\n",
      "\n",
      "= = (Σt|t−1 − KtCT = μt|t−1 − KtCT\n",
      "\n",
      "t Ct)−1Σ−1 t R−1 Σt|t−1 − Σt|t−1CT (Rt + CT t Σt|t−1)Σ−1 t μt|t−1\n",
      "\n",
      "t|t−1 + CT\n",
      "\n",
      "t|t−1μt|t−1\n",
      "\n",
      "t|t−1μt|t−1\n",
      "\n",
      "t Σt|t−1CT\n",
      "\n",
      "t )CtΣt|t−1\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "Σ−1\n",
      "\n",
      "t|t−1μt|t−1\n",
      "\n",
      "(18.50)\n",
      "\n",
      "(18.51)\n",
      "\n",
      "(18.52)\n",
      "\n",
      "(18.53)\n",
      "\n",
      "(18.54)\n",
      "\n",
      "Putting the two together we get\n",
      "\n",
      "μt = μt|t−1 + Kt(yt − Ctμt|t−1)\n",
      "\n",
      "(18.55)\n",
      "\n",
      "18.3.2\n",
      "\n",
      "The Kalman smoothing algorithm\n",
      "\n",
      "In Section 18.3.1, we described the Kalman ﬁlter, which sequentially computes p(zt|y1:t) for each t. This is useful for online inference problems, such as tracking. However, in an offline setting, we can wait until all the data has arrived, and then compute p(zt|y1:T ). By conditioning on past and future data, our uncertainty will be signiﬁcantly reduced. This is illustrated in Figure 18.1(c), where we see that the posterior covariance ellipsoids are smaller for the smoothed trajectory than for the ﬁltered trajectory. (The ellipsoids are larger at the beginning and end of the trajectory, since states near the boundary do not have as many useful neighbors from which to borrow information.)\n",
      "\n",
      "644\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "We now explain how to compute the smoothed estimates, using an algorithm called the RTS smoother, named after its inventors, Rauch, Tung and Striebel (Rauch et al. 1965). It is also known as the Kalman smoothing algorithm. The algorithm is analogous to the forwards- backwards algorithm for HMMs, although there are some small differences which we discuss below.\n",
      "\n",
      "18.3.2.1\n",
      "\n",
      "Algorithm\n",
      "\n",
      "Kalman ﬁltering can be regarded as message passing on a graph, from left to right. When the messages have reached the end of the graph, we have successfully computed p(zT |y1:T ). Now we work backwards, from right to left, sending information from the future back to the past, and them combining the two information sources. The question is: how do we compute these backwards equations? We ﬁrst give the equations, then the derivation.\n",
      "\n",
      "We have\n",
      "\n",
      "p(zt|y1:T ) =N (μt|T , Σt|T )\n",
      "\n",
      "(18.56)\n",
      "\n",
      "μt|T = μt|t + Jt(μt+1|T − μt+1|t) Σt|T = Σt|t + Jt(Σt+1|T − Σt+1|t)JT t t+1Σ−1\n",
      "\n",
      "Jt (cid:2) Σt|tAT\n",
      "\n",
      "t+1|t\n",
      "\n",
      "(18.57)\n",
      "\n",
      "(18.58)\n",
      "\n",
      "(18.59)\n",
      "\n",
      "where Jt is the backwards Kalman gain matrix. The algorithm can be initialized from μT |T and ΣT |T from the Kalman ﬁlter. Note that this backwards pass does not need access to the data, that is, it does not need y1:T . This allows us to “throw away” potentially high dimensional observation vectors, and just keep the ﬁltered belief states, which usually requires less memory.\n",
      "\n",
      "18.3.2.2\n",
      "\n",
      "Derivation *\n",
      "\n",
      "We now derive the Kalman smoother, following the presentation of (Jordan 2007, sec 15.7).\n",
      "\n",
      "The key idea is to leverage the Markov property, which says that zt is independent of future data, yt+1:T , as long as zt+1 is known. Of course, zt+1 is not known, but we have a distribution over it. So we condition on zt+1 and then integrate it out, as follows.\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(zt|y1:T ) =\n",
      "\n",
      "p(zt|y1:T , zt+1)p(zt+1|y1:T )dzt+1\n",
      "\n",
      "(18.60)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "=\n",
      "\n",
      "p(zt|y1:t,(cid:3)(cid:3)(cid:3)yt+1:T , zt+1)p(zt+1|y1:T )dzt+1\n",
      "\n",
      "(18.61)\n",
      "\n",
      "By induction, assume we have already computed the smoothed distribution for t + 1:\n",
      "\n",
      "p(zt+1|y1:T ) = N (zt+1|μt+1|T , Σt+1|T )\n",
      "\n",
      "(18.62)\n",
      "\n",
      "The question is: how do we perform the integration?\n",
      "\n",
      "First, we compute the ﬁltered two-slice distribution p(zt, zt+1|y1:t) as follows:\n",
      "\n",
      "p(zt, zt+1|y1:t) = N\n",
      "\n",
      "(cid:8)(cid:8)\n",
      "\n",
      "zt zt+1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "|\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "μt|t μt+1|t\n",
      "\n",
      "(cid:9) (cid:8)\n",
      "\n",
      "Σt|t At+1Σt|t\n",
      "\n",
      "Σt|tAT Σt+1|t\n",
      "\n",
      "t+1\n",
      "\n",
      "(cid:9)(cid:9)\n",
      "\n",
      "(18.63)\n",
      "\n",
      "18.3.\n",
      "\n",
      "Inference in LG-SSM\n",
      "\n",
      "645\n",
      "\n",
      "Now we use Gaussian conditioning to compute p(zt|zt+1, y1:t) as follows:\n",
      "\n",
      "p(zt|zt+1, y1:t) =N (zt|μt|t + Jt(zt+1 − μt+1|t), Σt|t − JtΣt+1|tJT t ) (18.64) We can compute the smoothed distribution for t using the rules of iterated expectation and\n",
      "\n",
      "iterated covariance. First, the mean: (cid:13) E [zt|zt+1, y1:T ] E [zt|zt+1, y1:t] (cid:15)\n",
      "\n",
      "μt|T = E = E\n",
      "\n",
      "μt|t + Jt(zt+1 − μt+1|t) = μt|t + Jt(μt+1|T − μt+1|t)\n",
      "\n",
      "= E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "y1:T (cid:14) y1:T\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "y1:T\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(18.65)\n",
      "\n",
      "(18.66)\n",
      "\n",
      "(18.67)\n",
      "\n",
      "(18.68)\n",
      "\n",
      "Now the covariance: (cid:13) Σt|T = cov\n",
      "\n",
      "= cov\n",
      "\n",
      "E [zt|zt+1, y1:T ] = cov [E [zt|zt+1, y1:t] |y1:T ] +E\n",
      "\n",
      "= Jtcov\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "μt|t + Jt(zt+1 − μt+1|t)|y1:T (cid:15)\n",
      "\n",
      "zt+1 − μt+1|t|y1:T\n",
      "\n",
      "y1:T\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "JT t + Σt|t − JtΣt+1|tJT t\n",
      "\n",
      "+ E (cid:13)\n",
      "\n",
      "cov [zt|zt+1, y1:t] (cid:13)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "cov [zt|zt+1, y1:T ]\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "+ E\n",
      "\n",
      "y1:T Σt|t − JtΣt+1|tJT\n",
      "\n",
      "y1:T (cid:14)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "t |y1:T\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(18.69)\n",
      "\n",
      "(18.70)\n",
      "\n",
      "(18.71)\n",
      "\n",
      "(18.72)\n",
      "\n",
      "(18.74) The algorithm can be initialized from μT |T and ΣT |T from the last step of the ﬁltering algo- rithm.\n",
      "\n",
      "= JtΣt+1|T JT = Σt|t + Jt(Σt+1|T − Σt+1|t)JT t\n",
      "\n",
      "t + Σt|t − JtΣt+1|tJT t\n",
      "\n",
      "(18.73)\n",
      "\n",
      "18.3.2.3\n",
      "\n",
      "Comparison to the forwards-backwards algorithm for HMMs *\n",
      "\n",
      "Note that in both the forwards and backwards passes for LDS, we always worked with normalized distributions, either conditioned on the past data or conditioned on all the data. Furthermore, the backwards pass depends on the results of the forwards pass. This is different from the usual presentation of forwards-backwards for HMMs, where the backwards pass can be computed independently of the forwards pass (see Section 17.4.3).\n",
      "\n",
      "It turns out that we can rewrite the Kalman smoother in a modiﬁed form which makes it\n",
      "\n",
      "more similar to forwards-backwards for HMMs. In particular, we have\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(zt|y1:T ) =\n",
      "\n",
      "p(zt|y1:t, zt+1)p(zt+1|y1:T )dzt+1\n",
      "\n",
      "(18.75)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(zt, zt+1|y1:t)\n",
      "\n",
      "p(zt+1|y1:T ) p(zt+1|y1:t)\n",
      "\n",
      "dzt+1\n",
      "\n",
      "(18.76)\n",
      "\n",
      "Now\n",
      "\n",
      "p(zt+1|y1:T ) =\n",
      "\n",
      "p(yt+1:T |zt+1,(cid:2)(cid:2)y1:t)p(zt+1|y1:t) p(yt+1:T |y1:t)\n",
      "\n",
      "(18.77)\n",
      "\n",
      "so\n",
      "\n",
      "p(zt+1|y1:T ) p(zt+1|y1:t)\n",
      "\n",
      "=\n",
      "\n",
      "p(zt+1|y1:t)p(yt+1:T |zt+1) p(zt+1|y1:t)p(yt+1:T |y1:t)\n",
      "\n",
      "∝ p(yt+1:T |zt+1)\n",
      "\n",
      "(18.78)\n",
      "\n",
      "646\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "which is the conditional likelihood of the future data. This backwards message can be computed independently of the forwards message. However, this approach has several disadvantages: (1) it needs access to the original observation sequence; (2) the backwards message is a likelihood, not a posterior, so it need not to integrate to 1 over zt – in fact, it may not always be possible to represent p(yt+1:T |zt+1) as a Gaussian with positive deﬁnite covariance (this problem does not arise in discrete state-spaces, as used in HMMs); (3) when exact inference is not possible, it makes more sense to try to approximate the smoothed distribution rather than the backwards likelihood term (see Section 22.5).\n",
      "\n",
      "There is yet another variant, known as two-ﬁlter smoothing, whereby we compute p(zt|y1:t) in the forwards pass as usual, and the ﬁltered posterior p(zt|yt+1:T ) in the backwards pass. These can then be easily combined to compute p(zt|y1:T ). See (Kitagawa 2004; Briers et al. 2010) for details.\n",
      "\n",
      "18.4\n",
      "\n",
      "Learning for LG-SSM\n",
      "\n",
      "In this section, we brieﬂy discuss how to estimate the parameters of an LG-SSM. In the control theory community, this is known as systems identiﬁcation (Ljung 1987).\n",
      "\n",
      "When using SSMs for time series forecasting, and also in some physical state estimation problems, the observation matrix C and the transition matrix A are both known and ﬁxed, by deﬁnition of the model. In such cases, all that needs to be learned are the noise covariances Q and R. (The initial state estimate μ0 is often less important, since it will get “washed away” by the data after a few time steps. This can be encouraged by setting the initial state covariance to be large, representing a weak prior.) Although we can estimate Q and R offline, using the methods described below, it is also possible to derive a recursive procedure to exactly compute the posterior p(zt, R, Q|y1:t), which has the form of a Normal-inverse-Wishart; see (West and Harrison 1997; Prado and West 2010) for details.\n",
      "\n",
      "18.4.1\n",
      "\n",
      "Identiﬁability and numerical stability\n",
      "\n",
      "In the more general setting, where the hidden states have no pre-speciﬁed meaning, we need to learn A and C. However, in this case we can set Q = I without loss of generality, since an arbitrary noise covariance can be modeled by appropriately modifying A. Also, by analogy with factor analysis, we can require R to be diagonal without loss of generality. Doing this reduces the number of free parameters and improves numerical stability.\n",
      "\n",
      "To see why this is important, consider the case of no system noise. state at time t is given by\n",
      "\n",
      "Another constraint that is useful to impose is on the eigenvalues of the dynamics matrix A. In this case, the hidden\n",
      "\n",
      "zt = Atz1 = UΛtU−1z1\n",
      "\n",
      "(18.79)\n",
      "\n",
      "where U is the matrix of eigenvectors for A, and Λ = diag(λi) contains the eigenvalues. If any λi > 1, then for large t, zt will blow up in magnitude. Consequently, to ensure stability, it is useful to require that all the eigenvalues are less than 1 (Siddiqi et al. 2007). Of course, if all the eigenvalues are less than 1, then E [zt] =0 for large t, so the state will return to the origin. Fortunately, when we add noise, the state become non-zero, so the model does not degenerate.\n",
      "\n",
      "18.5. Approximate online inference for non-linear, non-Gaussian SSMs\n",
      "\n",
      "647\n",
      "\n",
      "Below we discuss how to estimate the parameters. However, for simplicity of presentation, we\n",
      "\n",
      "do not impose any of the constraints mentioned above.\n",
      "\n",
      "18.4.2\n",
      "\n",
      "Training with fully observed data\n",
      "\n",
      "If we observe the hidden state sequences, we can ﬁt the model by computing the MLEs (or even the full posteriors) for the parameters by solving a multivariate linear regression problem for zt−1 → zt and for zt → yt. That is, we can estimate A by solving the least squares problem t=1(zt −Azt−1)2, and similarly for C. We can estimate the system noise covariance J(A) = Q from the residuals in predicting zt from zt−1, and estimate the observation noise covariance R from the residuals in predicting yt from zt.\n",
      "\n",
      "(cid:7)2\n",
      "\n",
      "18.4.3\n",
      "\n",
      "EM for LG-SSM\n",
      "\n",
      "If we only observe the output sequence, we can compute ML or MAP estimates of the parameters using EM. The method is conceptually quite similar to the Baum-Welch algorithm for HMMs (Section 17.5), except we use Kalman smoothing instead of forwards-backwards in the E step, and use different calculations in the M step. We leave the details to Exercise 18.1.\n",
      "\n",
      "18.4.4\n",
      "\n",
      "Subspace methods\n",
      "\n",
      "EM does not always give satisfactory results, because it is sensitive to the initial parameter estimates. One way to avoid this is to use a different approach known as a subspace method (Overschee and Moor 1996; Katayama 2005).\n",
      "\n",
      "To understand this approach, let us initially assume there is no observation noise and no In this case, we have zt = Azt−1 and yt = Czt, and hence yt = CAt−1z1. system noise. Consequently all the observations must be generated from a dim(zt)-dimensional linear mani- fold or subspace. We can identify this subspace using PCA (see the above references for details). Once we have an estimate of the zt’s, we can ﬁt the model as if it were fully observed. We can either use these estimates in their own right, or use them to initialize EM.\n",
      "\n",
      "18.4.5\n",
      "\n",
      "Bayesian methods for “ﬁtting” LG-SSMs\n",
      "\n",
      "There are various offline Bayesian alternatives to the EM algorithm, including variational Bayes EM (Beal 2003; Barber and Chiappa 2007) and blocked Gibbs sampling (Carter and Kohn 1994; Cappe et al. 2005; Fruhwirth-Schnatter 2007). The Bayesian approach can also be used to perform online learning, as we discussed in Section 18.2.3. Unfortunately, once we add the SSM parameters to the state space, the model is generally no longer linear Gaussian. Consequently we must use some of the approximate online inference methods to be discussed below.\n",
      "\n",
      "18.5\n",
      "\n",
      "Approximate online inference for non-linear, non-Gaussian SSMs\n",
      "\n",
      "In Section 18.3.1, we discussed how to perform exact online inference for LG-SSMs. However, many models are non linear. For example, most moving objects do not move in straight lines. And even if they did, if we assume the parameters of the model are unknown and add them\n",
      "\n",
      "648\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "to the state space, the model becomes nonlinear. Furthermore, non-Gaussian noise is also very common, e.g., due to outliers, or when inferring parameters for GLMs instead of just linear regression. For these more general models, we need to use approximate inference.\n",
      "\n",
      "The approximate inference algorithms we discuss below approximate the posterior by a Gaus- In general, if Y = f (X), where X has a Gaussian distribution and f is a non-linear sian. function, there are two main ways to approximate p(Y ) by a Gaussian. The ﬁrst is to use a ﬁrst-order approximation of f . The second is to use the exact f , but to project f (X) onto the space of Gaussians by moment matching. We discuss each of these methods in turn. (See also Section 23.5, where we discuss particle ﬁltering, which is a stochastic algorithm for approximate online inference, which uses a non-parametric approximation to the posterior, which is often more accurate but slower to compute.)\n",
      "\n",
      "18.5.1\n",
      "\n",
      "Extended Kalman ﬁlter (EKF)\n",
      "\n",
      "In this section, we focus on non-linear models, but we assume the noise is Gaussian. That is, we consider models of the form\n",
      "\n",
      "(18.81) where the transition model g and the observation model h are nonlinear but differentiable functions. Furthermore, we focus on the case where we approximate the posterior by a single Gaussian. (The simplest way to handle more general posteriors (e.g., multi-modal, discrete, etc). is to use particle ﬁltering, which we discuss in Section 23.5.)\n",
      "\n",
      "zt = g(ut, zt−1) +N (0, Qt) yt = h(zt) +N (0, Rt)\n",
      "\n",
      "(18.80)\n",
      "\n",
      "The extended Kalman ﬁlter or EKF can be applied to nonlinear Gaussian dynamical systems of this form. The basic idea is to linearize g and h about the previous state estimate using a ﬁrst order Taylor series expansion, and then to apply the standard Kalman ﬁlter equations. (The noise variance in the equations (Q and R) is not changed, i.e., the additional error due to linearization is not modeled.) Thus we approximate the stationary non-linear dynamical system with a non-stationary linear dynamical system.\n",
      "\n",
      "The intuition behind the approach is shown in Figure 18.9, which shows what happens when we pass a Gaussian distribution p(x), shown on the bottom right, through a nonlinear function y = g(x), shown on the top right. The resulting distribution (approximated by Monte Carlo) is shown in the shaded gray area in the top left corner. The best Gaussian approximation to this, computed from E [g(x)] and var [g(x)] by Monte Carlo, is shown by the solid black line. The EKF approximates this Gaussian as follows: it linearizes the g function at the current mode, μ, and then passes the Gaussian distribution p(x) through this linearized function. In this example, the result is quite a good approximation to the ﬁrst and second moments of p(y), for much less cost than an MC approximation.\n",
      "\n",
      "In more detail, the method works as follows. We approximate the measurement model using p(yt|zt) ≈ N (yt|h(μt|t−1) +H t(yt − μt|t−1), Rt)\n",
      "\n",
      "(18.82)\n",
      "\n",
      "where Ht is the Jacobian matrix of h evaluated at the prior mode:\n",
      "\n",
      "Hij (cid:2)\n",
      "\n",
      "∂hi(z) ∂zj\n",
      "\n",
      "(18.83)\n",
      "\n",
      "Ht (cid:2) H|z=μt|t−1\n",
      "\n",
      "(18.84)\n",
      "\n",
      "18.5. Approximate online inference for non-linear, non-Gaussian SSMs\n",
      "\n",
      "649\n",
      "\n",
      "p(y) Gaussian of p(y) Mean of p(y) EKF Gaussian Mean of EKF\n",
      "\n",
      "Function g(x) Taylor approx. Mean μ g(μ)\n",
      "\n",
      "y\n",
      "\n",
      ") x ( g = y\n",
      "\n",
      "p(y)\n",
      "\n",
      "x\n",
      "\n",
      "p(x) Mean μ\n",
      "\n",
      ") x ( p\n",
      "\n",
      "x\n",
      "\n",
      "Figure 18.9 Nonlinear transformation of a Gaussian random variable. The prior p(x) is shown on the bottom right. The function y = g(x) is shown on the top right. The transformed distribution p(y) is shown in the top left. A linear function induces a Gaussian distribution, but a non-linear function induces a complex distribution. The solid line is the best Gaussian approximation to this; the dotted line is the EKF approximation to this. Source: Figure 3.4 of (Thrun et al. 2006). Used with kind permission of Sebastian Thrun.\n",
      "\n",
      "Similarly, we approximate the system model using\n",
      "\n",
      "p(zt|zt−1, ut) ≈ N (zt|g(ut, μt−1) +G t(zt−1 − μt−1), Qt)\n",
      "\n",
      "(18.85)\n",
      "\n",
      "where\n",
      "\n",
      "Gij(u) (cid:2)\n",
      "\n",
      "∂gi(u, z) ∂zj\n",
      "\n",
      "(18.86)\n",
      "\n",
      "Gt (cid:2) G(ut)|z=μt−1\n",
      "\n",
      "(18.87)\n",
      "\n",
      "so G is the Jacobian matrix of g evaluated at the prior mode.\n",
      "\n",
      "Given this, we can then apply the Kalman ﬁlter to compute the posterior as follows:\n",
      "\n",
      "μt|t−1 = g(ut, μt−1) Vt|t−1 = GtVt−1GT Kt = Vt|t−1HT μt = μt|t−1 + Kt(yt − h(μt|t−1)) Vt = (I − KtHt)Vt|t−1\n",
      "\n",
      "t (HtVt|t−1HT\n",
      "\n",
      "t + Qt\n",
      "\n",
      "t + Rt)−1\n",
      "\n",
      "(18.88)\n",
      "\n",
      "(18.89)\n",
      "\n",
      "(18.90)\n",
      "\n",
      "(18.91)\n",
      "\n",
      "(18.92)\n",
      "\n",
      "650\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "Actual (sampling)\n",
      "\n",
      "Sigma-Point\n",
      "\n",
      "Linearized (EKF)\n",
      "\n",
      "covariance\n",
      "\n",
      "sigma points\n",
      "\n",
      "mean\n",
      "\n",
      "( ) =y f x\n",
      "\n",
      "ϒ = f\n",
      "\n",
      "i\n",
      "\n",
      "(\n",
      "\n",
      "χ i\n",
      "\n",
      ")\n",
      "\n",
      "y\n",
      "\n",
      "=\n",
      "\n",
      "( ) y A f x P\n",
      "\n",
      "=\n",
      "\n",
      "AT P x\n",
      "\n",
      "true mean\n",
      "\n",
      "S-P mean\n",
      "\n",
      "true covariance\n",
      "\n",
      "S-P covariance\n",
      "\n",
      "transformed sigma points\n",
      "\n",
      "A\n",
      "\n",
      "AT xP\n",
      "\n",
      "f x( )\n",
      "\n",
      "Figure 18.10 An example of the unscented transform in two dimensions. Source: (Wan and der Merwe 2001). Used with kind permission of Eric Wan.\n",
      "\n",
      "We see that the only difference from the regular Kalman ﬁlter is that, when we compute the state prediction, we use g(ut, μt−1) instead of Atμt−1 + Btut, and when we compute the measurement update we use h(μt|t−1) instead of Ctμt|t−1.\n",
      "\n",
      "It is possible to improve performance by repeatedly re-linearizing the equations around μt instead of μt|t−1; this is called the iterated EKF, and yields better results, although it is of course slower.\n",
      "\n",
      "There are two cases when the EKF works poorly. The ﬁrst is when the prior covariance is large. In this case, the prior distribution is broad, so we end up sending a lot of probability mass through different parts of the function that are far from the mean, where the function has been linearized. The other setting where the EKF works poorly is when the function is highly nonlinear near the current mean. In Section 18.5.2, we will discuss an algorithm called the UKF which works better than the EKF in both of these settings.\n",
      "\n",
      "18.5.2\n",
      "\n",
      "Unscented Kalman ﬁlter (UKF)\n",
      "\n",
      "The unscented Kalman ﬁlter (UKF) is a better version of the EKF (Julier and Uhlmann 1997). (Apparently it is so-called because it “doesn’t stink”!) The key intuition is this: it is easier to approximate a Gaussian than to approximate a function. So instead of performing a linear approximation to the function, and passing a Gaussian through it, instead pass a deterministically chosen set of points, known as sigma points, through the function, and ﬁt a Gaussian to the resulting transformed points. This is known as the unscented transform, and is sketched in Figure 18.10. (We explain this ﬁgure in detail below.)\n",
      "\n",
      "18.5. Approximate online inference for non-linear, non-Gaussian SSMs\n",
      "\n",
      "651\n",
      "\n",
      "The UKF basically uses the unscented transform twice, once to approximate passing through the system model g, and once to approximate passing through the measurement model h. We give the details below. Note that the UKF and EKF both perform O(d3) operations per time step where d is the size of the latent state-space. However, the UKF is accurate to at least second order, whereas the EKF is only a ﬁrst order approximation (although both the EKF and UKF can be extended to capture higher order terms). Furthermore, the unscented transform does not require the analytic evaluation of any derivatives or Jacobians (a so-called derivative free ﬁlter), making it simpler to implement and more widely applicable.\n",
      "\n",
      "18.5.2.1\n",
      "\n",
      "The unscented transform\n",
      "\n",
      "Before explaining the UKF, we ﬁrst explain the unscented transform. Assume p(x) = N (x|μ, Σ), and consider estimating p(y), where y = f (x) for some nonlinear function f . The unscented transform does this as follows. First we create a set of 2d + 1 sigma points xi, given by\n",
      "\n",
      "x =\n",
      "\n",
      "!\n",
      "\n",
      "μ, {μ + (\n",
      "\n",
      "\"\n",
      "\n",
      "(d + λ)Σ):i}d\n",
      "\n",
      "i=1, {μ − (\n",
      "\n",
      "\"\n",
      "\n",
      "(d + λ)Σ):i}d\n",
      "\n",
      "i=1\n",
      "\n",
      "#\n",
      "\n",
      "(18.93)\n",
      "\n",
      "where λ = α2(d + κ) − d is a scaling parameter to be speciﬁed below, and the notation M:i means the i’th column of matrix M.\n",
      "\n",
      "the mean and covariance for y is computed as follows:\n",
      "\n",
      "These sigma points are propagated through the nonlinear function to yield yi = f (xi), and\n",
      "\n",
      "μy =\n",
      "\n",
      "2d(cid:4)\n",
      "\n",
      "wi\n",
      "\n",
      "myi\n",
      "\n",
      "(18.94)\n",
      "\n",
      "Σy =\n",
      "\n",
      "i=0 2d(cid:4)\n",
      "\n",
      "wi\n",
      "\n",
      "c(yi − μy)(yi − μy)T\n",
      "\n",
      "(18.95)\n",
      "\n",
      "i=0\n",
      "\n",
      "where the w’s are weighting terms, given by\n",
      "\n",
      "wi\n",
      "\n",
      "m =\n",
      "\n",
      "wi\n",
      "\n",
      "c =\n",
      "\n",
      "λ d + λ λ d + λ\n",
      "\n",
      "+ (1− α2 + β)\n",
      "\n",
      "(18.96)\n",
      "\n",
      "(18.97)\n",
      "\n",
      "wi\n",
      "\n",
      "m = wi\n",
      "\n",
      "c =\n",
      "\n",
      "1 2(d + λ)\n",
      "\n",
      "(18.98)\n",
      "\n",
      "See Figure 18.10 for an illustration.\n",
      "\n",
      "are α = 1, β = 0, κ = 2. Thus in the 1d case, λ = 2, so the 3 sigma points are μ, μ + and μ −\n",
      "\n",
      "In general, the optimal values of α, β and κ are problem dependent, but when d = 1, they 3σ\n",
      "\n",
      "√\n",
      "\n",
      "3σ.\n",
      "\n",
      "√\n",
      "\n",
      "18.5.2.2\n",
      "\n",
      "The UKF algorithm\n",
      "\n",
      "The UKF algorithm is simply two applications of the unscented tranform, one to compute p(zt|y1:t−1, u1:t) and the other to compute p(zt|y1:t, u1:t). We give the details below.\n",
      "\n",
      "652\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "passing the old belief state N (zt−1|μt−1, Σt−1) through the system model g as follows:\n",
      "\n",
      "The ﬁrst step is to approximate the predictive density p(zt|y1:t−1, u1:t) ≈ N (zt|μt, Σt) by\n",
      "\n",
      "z0 t−1 = t = g(ut, z0i z∗i 2d(cid:4)\n",
      "\n",
      "μt =\n",
      "\n",
      "!\n",
      "\n",
      "μt−1, {μt−1 + γ(\n",
      "\n",
      "wi\n",
      "\n",
      "mz∗i t\n",
      "\n",
      "t−1)\n",
      "\n",
      "\"\n",
      "\n",
      "Σt−1):i}d\n",
      "\n",
      "i=1, {μt−1 − γ(\n",
      "\n",
      "\"\n",
      "\n",
      "Σt−1):i}d\n",
      "\n",
      "i=1\n",
      "\n",
      "#\n",
      "\n",
      "(18.100)\n",
      "\n",
      "(18.101)\n",
      "\n",
      "(18.99)\n",
      "\n",
      "Σt =\n",
      "\n",
      "i=0 2d(cid:4)\n",
      "\n",
      "wi\n",
      "\n",
      "c(z∗i\n",
      "\n",
      "t − μt)(z∗i\n",
      "\n",
      "t − μt) +Q t\n",
      "\n",
      "(18.102)\n",
      "\n",
      "where γ =\n",
      "\n",
      "prior N (zt|μt, Σt) through the observation model h: $\n",
      "\n",
      "The second step is to approximate the likelihood p(yt|zt) ≈ N (yt|ˆyt, St) by passing the\n",
      "\n",
      "√\n",
      "\n",
      "d + λ.\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "i=0\n",
      "\n",
      "$\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "z0 t =\n",
      "\n",
      "μt, {μt + γ(\n",
      "\n",
      "Σt):i}d\n",
      "\n",
      "i=1, {μt − γ(\n",
      "\n",
      "Σt):i}d\n",
      "\n",
      "i=1\n",
      "\n",
      "(18.103)\n",
      "\n",
      "t = h(z0i y∗i t )\n",
      "\n",
      "(18.104)\n",
      "\n",
      "ˆyt =\n",
      "\n",
      "2d(cid:4)\n",
      "\n",
      "wi\n",
      "\n",
      "my∗i t\n",
      "\n",
      "(18.105)\n",
      "\n",
      "St =\n",
      "\n",
      "i=0 2d(cid:4)\n",
      "\n",
      "wi\n",
      "\n",
      "c(y∗i\n",
      "\n",
      "t − ˆyt)(y∗i\n",
      "\n",
      "t − ˆyt)T + Rt\n",
      "\n",
      "(18.106)\n",
      "\n",
      "i=0\n",
      "\n",
      "Finally, we use Bayes rule for Gaussians to get the posterior p(zt|y1:t, u1:t) ≈ N (zt|μt, Σt):\n",
      "\n",
      "Σz,y t\n",
      "\n",
      "=\n",
      "\n",
      "2d(cid:4)\n",
      "\n",
      "wi\n",
      "\n",
      "c(z∗i\n",
      "\n",
      "t − μt)(y∗i\n",
      "\n",
      "t − ˆyt)T\n",
      "\n",
      "(18.107)\n",
      "\n",
      "i=0\n",
      "\n",
      "Kt = Σz,y t S−1 t μt = μt + Kt(yt − ˆyt) Σt = Σt − KtStKT t\n",
      "\n",
      "(18.108)\n",
      "\n",
      "(18.109)\n",
      "\n",
      "(18.110)\n",
      "\n",
      "18.5.3\n",
      "\n",
      "Assumed density ﬁltering (ADF)\n",
      "\n",
      "In this section, we discuss inference where we perform an exact update step, but then approx- imate the posterior by a distribution of a certain convenient form, such as a Gaussian. More precisely, let the unknowns that we want to infer be denoted by θt. Suppose that Q is a set of tractable distributions, e.g., Gaussians with a diagonal covariance matrix, or a product of discrete distributions. Suppose that we have an approximate prior qt−1(θt−1) ≈ p(θt−1|y1:t−1), where qt−1 ∈ Q. We can update this with the new measurement to get the approximate posterior\n",
      "\n",
      "ˆp(θt) =\n",
      "\n",
      "1 Zt\n",
      "\n",
      "p(yt|θt)qt|t−1(θt)\n",
      "\n",
      "(18.111)\n",
      "\n",
      "18.5. Approximate online inference for non-linear, non-Gaussian SSMs\n",
      "\n",
      "653\n",
      "\n",
      "Predict\n",
      "\n",
      "U pdate qt|t−1\n",
      "\n",
      "ˆpt\n",
      "\n",
      "P r o j e c t\n",
      "\n",
      "Predict\n",
      "\n",
      "U pdate qt+1|t\n",
      "\n",
      "ˆpt+1\n",
      "\n",
      "P r o j e c t\n",
      "\n",
      "θt−1\n",
      "\n",
      "st−1\n",
      "\n",
      "yt−1\n",
      "\n",
      "st\n",
      "\n",
      "θt\n",
      "\n",
      "yt\n",
      "\n",
      "qt−1\n",
      "\n",
      "qt\n",
      "\n",
      "qt+1\n",
      "\n",
      "xt−1\n",
      "\n",
      "xt\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 18.11 ical logistic regression model. Compare to Figure 18.4(a).\n",
      "\n",
      "(a) Illustration of the predict-update-project cycle of assumed density ﬁltering. (b) A dynam-\n",
      "\n",
      "where\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "Zt =\n",
      "\n",
      "p(yt|θt)qt|t−1(θt)dθt\n",
      "\n",
      "(18.112)\n",
      "\n",
      "is the normalization constant and (cid:12)\n",
      "\n",
      "qt|t−1(θt) =\n",
      "\n",
      "p(θt|θt−1)qt−1(θt−1)dθt−1\n",
      "\n",
      "(18.113)\n",
      "\n",
      "is the one step ahead predictive distribution. If the prior is from a suitably restricted family, this one-step update process is usually tractable. However, we often ﬁnd that the resulting posterior is no longer in our tractable family, ˆp(θt) (cid:4)∈ Q. So after updating we seek the best tractable approximation by computing\n",
      "\n",
      "q(θt) = argmin\n",
      "\n",
      "q∈Q\n",
      "\n",
      "KL (ˆp(θt)||q(θt))\n",
      "\n",
      "(18.114)\n",
      "\n",
      "This minimizes the the Kullback-Leibler divergence (Section 2.8.2) from the approximation q(θt) to the “exact” posterior ˆp(θt), and can be thought of as projecting ˆp onto the space of tractable distributions. The whole algorithm consists of predict-update-project cycles. This is known as assumed density ﬁltering or ADF (Maybeck 1979). See Figure 18.11(a) for a sketch.\n",
      "\n",
      "If q is in the exponential family, one can show that this KL minimization can be done by\n",
      "\n",
      "moment matching. We give some examples of this below.\n",
      "\n",
      "18.5.3.1\n",
      "\n",
      "Boyen-Koller algorithm for online inference in DBNs\n",
      "\n",
      "If we are performing inference in a discrete-state dynamic Bayes net (Section 17.6.7), where θtj is the j’th hidden variable at time t, then the exact posterior p(θt) becomes intractable to compute because of the entanglement problem. Suppose we use a fully factored approximation of the form q(θt) = j=1 Cat(θt,j|πt,j), where πtjk = q(θt,j = k) is the probability variable j is in state k, and D is the number of variables. In this case, the moment matching operation becomes\n",
      "\n",
      "(cid:26)D\n",
      "\n",
      "πtjk = ˆp(θt,j = k)\n",
      "\n",
      "(18.115)\n",
      "\n",
      "654\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "This can be computed by performing a predict-update step using the factored prior, and then computing the posterior marginals. This is known as the Boyen-Koller algorithm, named after the authors of (Boyen and Koller 1998), who demonstrated that the error incurred by this series of repeated approximations remains bounded (under certain assumptions about the stochasticity of the system).\n",
      "\n",
      "18.5.3.2\n",
      "\n",
      "Gaussian approximation for online inference in GLMs\n",
      "\n",
      "Now suppose q(θt) = parameters of the tractable approximation to the posterior are\n",
      "\n",
      "(cid:26)D\n",
      "\n",
      "j=1 N (θt,j|μt,j, τt,j), where τt,j is the variance. Then the optimal\n",
      "\n",
      "μt,j = E ˆp [θt,j] , τt,j = var ˆp [θt,j]\n",
      "\n",
      "(18.116)\n",
      "\n",
      "This method can be used to do online inference for the parameters of many statistical models. For example, theTrueSkill system, used in Microsoft’s Xbox to rank players over time, uses this form of approximation (Herbrich et al. 2007). We can also apply this method to simpler models, such as GLM, which have the advantage that the posterior is log-concave. Below we explain how to do this for binary logistic regression, following the presentation of (Zoeter 2007).\n",
      "\n",
      "The model has the form\n",
      "\n",
      "p(yt|xt, θt) = Ber(yt|sigm(xT p(θt|θt−1) =N (θt|θt−1, σ2I)\n",
      "\n",
      "t θt))\n",
      "\n",
      "(18.117)\n",
      "\n",
      "(18.118)\n",
      "\n",
      "where σ2 is some process noise which allows the parameters to change slowly over time. (This can be set to 0, as in the recursive least squares method (Section 18.2.3), if desired.) We will assume qt−1(θt−1) = j N (θt−1,j|μt−1,j, τt−1,j) is the tractable prior. We can compute the one-step-ahead predictive density qt|t−1(θt) using the standard linear-Gaussian update. So now we concentrate on the measurement update step. Deﬁne the deterministic quantity st = θT If qt|t−1(θt) = (cid:26) j N (θt,j|μt|t−1,j, τt|t−1,j), then we can compute the predictive distribution for st as follows:\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "t xt, as shown in Figure 18.11(b).\n",
      "\n",
      "qt|t−1(st) =N (st|mt|t−1, vt|t−1) (cid:4)\n",
      "\n",
      "mt|t−1 =\n",
      "\n",
      "xt,jμt|t−1,j\n",
      "\n",
      "(18.119)\n",
      "\n",
      "(18.120)\n",
      "\n",
      "vt|t−1 =\n",
      "\n",
      "j (cid:4)\n",
      "\n",
      "x2 t,jτt|t−1,j\n",
      "\n",
      "(18.121)\n",
      "\n",
      "j\n",
      "\n",
      "The posterior for st is given by\n",
      "\n",
      "qt(st) =N (st|mt, vt) (cid:12)\n",
      "\n",
      "mt =\n",
      "\n",
      "vt =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "st\n",
      "\n",
      "s2 t\n",
      "\n",
      "1 Zt 1 Zt\n",
      "\n",
      "p(yt|st)qt|t−1(st)dst\n",
      "\n",
      "p(yt|st)qt|t−1(st)dst − m2 t\n",
      "\n",
      "(18.122)\n",
      "\n",
      "(18.123)\n",
      "\n",
      "(18.124)\n",
      "\n",
      "Zt =\n",
      "\n",
      "p(yt|st)qt|t−1(st)dst\n",
      "\n",
      "(18.125)\n",
      "\n",
      "18.6. Hybrid discrete/continuous SSMs\n",
      "\n",
      "655\n",
      "\n",
      "where p(yt|st) = Ber(yt|st). These integrals are one dimensional, and so can be computed using Gaussian quadrature (see (Zoeter 2007) for details). This is the same as one step of the UKF algorithm.\n",
      "\n",
      "as the change in the mean of st and δv as the change in the variance:\n",
      "\n",
      "Having inferred q(st), we need to compute q(θ|st). This can be done as follows. Deﬁne δm\n",
      "\n",
      "mt = mt|t−1 + δm, vt = vt|t−1 + δv\n",
      "\n",
      "(18.126)\n",
      "\n",
      "Then one can show that the new factored posterior over the model parameters is given by\n",
      "\n",
      "q(θt,j) =N (θt,j|μt,j, τt,j) μt,j = μt|t−1,j + ajδm τt,j = τt|t−1,j + a2 j δv xt,jτt|t−1,j t,j(cid:2) τ 2 j(cid:2) x2\n",
      "\n",
      "aj (cid:2)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "t|t−1,j\n",
      "\n",
      "(18.127)\n",
      "\n",
      "(18.128)\n",
      "\n",
      "(18.129)\n",
      "\n",
      "(18.130)\n",
      "\n",
      "Thus we see that the parameters which correspond to inputs with larger magnitude (big |xt,j|) or larger uncertainty (big τt|t−1,j) get updated most, which makes intuitive sense.\n",
      "\n",
      "In (Opper 1998) a version of this algorithm is derived using a probit likelihood (see Section 9.4). In this case, the measurement update can be done in closed form, without the need for numerical In either case, the algorithm only takes O(D) operations per time step, so it can integration. be applied to models with large numbers of parameters. And since it is an online algorithm, it can also handle massive datasets. For example (Zhang et al. 2010) use a version of this algorithm to ﬁt a multi-class classiﬁer online to very large datasets. They beat alternative (non Bayesian) online learning algorithms, and sometimes even outperform state of the art batch (offline) learning methods such as SVMs (described in Section 14.5).\n",
      "\n",
      "18.6 Hybrid discrete/continuous SSMs\n",
      "\n",
      "Many systems contain both discrete and continuous hidden variables; these are known as hybrid systems. For example, the discrete variables may indicate whether a measurement sensor is faulty or not, or which “regime” the system is in. We will see some other examples below.\n",
      "\n",
      "A special case of a hybrid system is when we combine an HMM and an LG-SSM. This is called a switching linear dynamical system (SLDS), a jump Markov linear system (JMLS), or a switching state space model (SSSM). More precisely, we have a discrete latent variable, qt ∈ {1, . . . , K}, a continuous latent variable, zt ∈ RL, an continuous observed response yt ∈ RD and an optional continuous observed input or control ut ∈ RU . We then assume that the continuous variables have linear Gaussian CPDs, conditional on the discrete states:\n",
      "\n",
      "p(qt = k|qt−1 = j, θ) =A ij\n",
      "\n",
      "(18.131)\n",
      "\n",
      "p(zt|zt−1, qt = k, ut, θ) =N (zt|Akzt−1 + Bkut, Qk) p(yt|zt, qt = k, ut, θ) =N (yt|Ckzt + Dkut, Rk)\n",
      "\n",
      "(18.132)\n",
      "\n",
      "(18.133)\n",
      "\n",
      "See Figure 18.12(a) for the DGM representation.\n",
      "\n",
      "656\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "ut−1\n",
      "\n",
      "ut\n",
      "\n",
      "qt−1\n",
      "\n",
      "qt\n",
      "\n",
      "zt−1\n",
      "\n",
      "zt\n",
      "\n",
      "yt−1\n",
      "\n",
      "yt\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 18.12 A switching linear dynamical system. (a) Squares represent discrete nodes, circles represent continuous nodes. (b) Illustration of how the number of modes in the belief state grows exponentially over time. We assume there are two binary states.\n",
      "\n",
      "18.6.1\n",
      "\n",
      "Inference\n",
      "\n",
      "Unfortunately inference (i.e., state estimation) in hybrid models, including the switching LG- is intractable. To see why, suppose qt is binary, but that only the dynamics SSM model, A depend on qt, not the observation matrix. Our initial belief state will be a mixture of 2 Gaussians, corresponding to p(z1|y1, q1 = 1) and p(z1|y1, q1 = 2). The one-step-ahead predictive density will be a mixture of 4 Gaussians p(z2|y1, q1 = 1, q2 = 1), p(z2|y1, q1 = 1, q2 = 2), p(z2|y1, q1 = 2, q2 = 1), and p(z2|y1, q1 = 2, q2 = 2), obtained by passing each of the prior modes through the 2 possible transition models. The belief state at step 2 will also be a mixture of 4 Gaussians, obtained by updating each of the above distributions with y2. At step 3, the belief state will be a mixture of 8 Gaussians. And so on. So we see there is an exponential explosion in the number of modes (see Figure 18.12(b)).\n",
      "\n",
      "Various approximate inference methods have been proposed for this model, such as the\n",
      "\n",
      "following:\n",
      "\n",
      "Prune off low probability trajectories in the discrete tree;\n",
      "\n",
      "this is the basis of multiple\n",
      "\n",
      "hypothesis tracking (Bar-Shalom and Fortmann 1988; Bar-Shalom and Li 1993).\n",
      "\n",
      "Use Monte Carlo. Essentially we just sample discrete trajectories, and apply an analytical ﬁlter to the continuous variables conditional on a trajectory. See Section 23.6 for details.\n",
      "\n",
      "Use ADF, where we approximate the exponentially large mixture of Gaussians with a smaller\n",
      "\n",
      "mixture of Gaussians. See Section 18.6.1.1 for details.\n",
      "\n",
      "18.6.1.1\n",
      "\n",
      "A Gaussian sum ﬁlter for switching SSMs\n",
      "\n",
      "A Gaussian sum ﬁlter (Sorenson and Alspach 1971) approximates the belief state at each step by a mixture of K Gaussians. This can be implemented by running K Kalman ﬁlters in\n",
      "\n",
      "18.6. Hybrid discrete/continuous SSMs\n",
      "\n",
      "657\n",
      "\n",
      "b1 t−1\n",
      "\n",
      "b2 t−1\n",
      "\n",
      "(cid:5) (cid:7)\n",
      "\n",
      "(cid:5)(cid:5)(cid:6)\n",
      "\n",
      "(cid:7)(cid:7)(cid:8)\n",
      "\n",
      "(cid:5) (cid:7)\n",
      "\n",
      "(cid:5)(cid:5)(cid:6)\n",
      "\n",
      "(cid:7)(cid:7)(cid:8)\n",
      "\n",
      "Filter 1\n",
      "\n",
      "Filter 2\n",
      "\n",
      "Filter 1\n",
      "\n",
      "Filter 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "b1,1 t\n",
      "\n",
      "b1,2 t (cid:10)\n",
      "\n",
      "b2,1 (cid:12) t\n",
      "\n",
      "b2,2 t\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "(cid:10) (cid:12) (cid:10) (cid:12)\n",
      "\n",
      "(cid:7)(cid:7)(cid:8) (cid:12)(cid:12)(cid:13) (cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:10) (cid:10)(cid:10)(cid:11) (cid:5)(cid:5)(cid:6)\n",
      "\n",
      "Merge\n",
      "\n",
      "Merge\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "b1 t\n",
      "\n",
      "b2 t\n",
      "\n",
      "(a)\n",
      "\n",
      "b1 t−1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "˜b1 t−1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "Filter 1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "b1 t\n",
      "\n",
      "Merge\n",
      "\n",
      "b2 t−1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "˜b2 t−1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "Filter 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "b2 t\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 18.13 ADF for a switching linear dynamical system. for details.\n",
      "\n",
      "(a) GPB2 method.\n",
      "\n",
      "(b) IMM method. See text\n",
      "\n",
      "parallel. This is particularly well suited to switching SSMs. We now describe one version of this algorithm, known as the “second order generalized pseudo Bayes ﬁlter” (GPB2) (Bar-Shalom and Fortmann 1988). We assume that the prior belief state bt−1 is a mixture of K Gaussians, one per discrete state:\n",
      "\n",
      "bi t−1 (cid:2) p(zt−1, qt−1 = i|y1:t−1) = πt−1,iN (zt−1|μt−1,i, Σt−1,i)\n",
      "\n",
      "(18.134)\n",
      "\n",
      "We then pass this through the K different linear models to get\n",
      "\n",
      "bij t (cid:2) p(zt, qt−1 = i, qt = j|y1:t) = πtijN (zt|μt,ij, Σt,ij)\n",
      "\n",
      "(18.135)\n",
      "\n",
      "where πtij = πt−1,ip(qt = j|qt−1 = i). Finally, for each value of j, we collapse the K Gaussian mixtures down to a single mixture to give\n",
      "\n",
      "bj t (cid:2) p(zt, qt = j|y1:t) = πtjN (zt|μt,j, Σt,j)\n",
      "\n",
      "(18.136)\n",
      "\n",
      "658\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "See Figure 18.13(a) for a sketch.\n",
      "\n",
      "q = arg minq KL (q||p), where p(z) = be solved by moment matching, that is,\n",
      "\n",
      "The optimal way to approximate a mixture of Gaussians with a single Gaussian is given by k πkN (z|μk, Σk) and q(z) = N (z|μ, Σ). This can\n",
      "\n",
      "μ = E [z] =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "πkμk\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(18.137)\n",
      "\n",
      "k (cid:4) Σ = cov [z] =\n",
      "\n",
      "πk\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "Σk + (μk − μ)(μk − μ)T\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(18.138)\n",
      "\n",
      "k\n",
      "\n",
      "In the graphical model literature, this is called weak marginalization (Lauritzen 1992), since it preserves the ﬁrst two moments. Applying these equations to our model, we can go from bij to t bj t as follows (where we drop the t subscript for brevity):\n",
      "\n",
      "πj =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "πij\n",
      "\n",
      "(18.139)\n",
      "\n",
      "πj|i =\n",
      "\n",
      "μj =\n",
      "\n",
      "πij(cid:7) (cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "j(cid:2) πij(cid:2) πj|iμij\n",
      "\n",
      "(18.140)\n",
      "\n",
      "(18.141)\n",
      "\n",
      "Σj =\n",
      "\n",
      "i (cid:4)\n",
      "\n",
      "πj|i\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "Σij + (μij − μj)(μij − μj)T\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(18.142)\n",
      "\n",
      "i\n",
      "\n",
      "This algorithm requires running K 2 ﬁlters at each step. A cheaper alternative is to represent the belief state by a single Gaussian, marginalizing over the discrete switch at each step. This is a straightforward application of ADF. An offline extension to this method, called expectation correction, is described in (Barber 2006; Mesot and Barber 2009).\n",
      "\n",
      "Another heuristic approach, known as interactive multiple models or IMM (Bar-Shalom and Fortmann 1988), can be obtained by ﬁrst collapsing the prior to a single Gaussian (by moment matching), and then updating it using K different Kalman ﬁlters, one per value of qt. See Figure 18.13(b) for a sketch.\n",
      "\n",
      "18.6.2\n",
      "\n",
      "Application: data association and multi-target tracking Suppose we are tracking K objects, such as airplanes, and at time t, we observe K (cid:2) detection events, e.g., “blips” on a radar screen. We can have K (cid:2) < K due to occlusion or missed detections. We can have K (cid:2) > K due to clutter or false alarms. Or we can have K (cid:2) = K. In any case, we need to ﬁgure out the correspondence between the K (cid:2) detections ytk and the K objects ztj. This is called the problem of data association, and it arises in many application domains.\n",
      "\n",
      "Figure 18.14 gives an example in which we are tracking K = 2 objects. At each time step, qt is the unknown mapping which speciﬁes which objects caused which observations. It speciﬁes the “wiring diagram” for time slice t. The standard way to solve this problem is to compute a weight which measures the “compatibility” between object j and measurement k, typically based on how close k is to where the model thinks j should be (the so-called nearest neighbor data association heuristic). This gives us a K × K (cid:2) weight matrix. We can make this into a\n",
      "\n",
      "18.6. Hybrid discrete/continuous SSMs\n",
      "\n",
      "659\n",
      "\n",
      "zt−1,1 zt−1,2\n",
      "\n",
      "zt,1 zt,2\n",
      "\n",
      "zt+1,1 zt+1,2\n",
      "\n",
      "yt−1,1 yt−1,2 yt−1,3\n",
      "\n",
      "yt,1\n",
      "\n",
      "yt+1,1 yt+1,2\n",
      "\n",
      "qt−1\n",
      "\n",
      "qt\n",
      "\n",
      "qt+1\n",
      "\n",
      "Figure 18.14 A model for tracking two objects in the presence of data-assocation ambiguity. We observe 3, 1 and 2 detections in the ﬁrst three time steps.\n",
      "\n",
      "square matrix of size N × N , whereN = max(K, K (cid:2)), by adding dummy background objects, which can explain all the false alarms, and adding dummy observations, which can explain all the missed detections. We can then compute the maximal weight bipartite matching using the Hungarian algorithm, which takes O(N 3) time (see e.g., (Burkard et al. 2009)). Conditional on this, we can perform a Kalman ﬁlter update, where objects that are assigned to dummy observations do not perform a measurement update.\n",
      "\n",
      "An extension of this method, to handle a variable and/or unknown number of objects, is known as multi-target tracking. This requires dealing with a variable-sized state space. There are many ways to do this, but perhaps the simplest and most robust methods are based on sequential Monte Carlo (e.g., (Ristic et al. 2004)) or MCMC (e.g., (Khan et al. 2006; Oh et al. 2009)).\n",
      "\n",
      "18.6.3\n",
      "\n",
      "Application: fault diagnosis\n",
      "\n",
      "Consider the model in Figure 18.15(a). This represents an industrial plant consisting of various tanks of liquid, interconnected by pipes. In this example, we just have two tanks, for simplicity. We want to estimate the pressure inside each tank, based on a noisy measurement of the ﬂow into and out of each tank. However, the measurement devices can sometimes fail. Furthermore, pipes can burst or get blocked; we call this a “resistance failure”. This model is widely used as a benchmark in the fault diagnosis community (Mosterman and Biswas 1999).\n",
      "\n",
      "We can create a probabilistic model of the system as shown in Figure 18.15(b). The square nodes represent discrete variables, such as measurement failures and resistance failures. The remaining variables are continuous. A variety of approximate inference algorithms can be applied to this model. See (Koller and Lerner 2001) for one approach, based on Rao-Blackwellized particle ﬁltering (which is explained in Section 23.6).\n",
      "\n",
      "660\n",
      "\n",
      "Chapter 18. State space models\n",
      "\n",
      "RF 1 1\n",
      "\n",
      "RF 1 2\n",
      "\n",
      "R1 1\n",
      "\n",
      "R1 2\n",
      "\n",
      "M F 1 1\n",
      "\n",
      "M F 1 2\n",
      "\n",
      "F 1 1\n",
      "\n",
      "M 1 1\n",
      "\n",
      "F 1 2\n",
      "\n",
      "M 1 2\n",
      "\n",
      "P 1 1\n",
      "\n",
      "P 1 2\n",
      "\n",
      "M F 12\n",
      "\n",
      "1\n",
      "\n",
      "M F 12\n",
      "\n",
      "2\n",
      "\n",
      "F 12 1\n",
      "\n",
      "M 12 1\n",
      "\n",
      "F 12 2\n",
      "\n",
      "M 12 2\n",
      "\n",
      "R12 1\n",
      "\n",
      "R12 2\n",
      "\n",
      "RF 12 1\n",
      "\n",
      "RF 12 2\n",
      "\n",
      "P 2 1\n",
      "\n",
      "P 2 2\n",
      "\n",
      "F 2 1\n",
      "\n",
      "M 2 1\n",
      "\n",
      "F 2 2\n",
      "\n",
      "M 2 2\n",
      "\n",
      "M F 2 1\n",
      "\n",
      "M F 2 2\n",
      "\n",
      "R2 1\n",
      "\n",
      "R2 2\n",
      "\n",
      "RF 2 1\n",
      "\n",
      "RF 2 2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 18.15 (a) The two-tank system. The goal is to infer when pipes are blocked or have burst, or sensors have broken, from (noisy) observations of the ﬂow out of tank 1, F 1o, out of tank 2, F 2o, or between tanks 1 and 2, F 12. R1o is a hidden variable representing the resistance of the pipe out of tank 1, P 1 is a hidden variable representing the pressure in tank 1, etc. Source: Figure 11 of (Koller and Lerner 2001) . Used with kind permission of Daphne Koller. (b) Dynamic Bayes net representation of the two-tank system. Discrete nodes are squares, continuous nodes are circles. Abbreviations: R = resistance, P = pressure, F = ﬂow, M = measurement, RF = resistance failure, MF = measurement failure. Based on Figure 12 of (Koller and Lerner 2001).\n",
      "\n",
      "18.6.4\n",
      "\n",
      "Application: econometric forecasting\n",
      "\n",
      "The switching LG-SSM model is widely used in econometric forecasting, where it is called a regime switching model. For example, we can combine two linear trend models (see Sec- tion 18.2.4.2), one in which bt > 0 reﬂects a growing economy, and one in which bt < 0 reﬂects a shrinking economy. See (West and Harrison 1997) for further details.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 18.1 Derivation of EM for LG-SSM Derive the E and M steps for computing a (locally optimal) MLE for an LG-SSM model. Hint: the results are in (Ghahramani and Hinton 1996b); your task is to derive these results.\n",
      "\n",
      "Exercise 18.2 Seasonal LG-SSM model in standard form Write the seasonal model in Figure 18.7(a) as an LG-SSM. Deﬁne the matrices A, C, Q and R.\n",
      "\n",
      "19 Undirected graphical models (Markov\n",
      "\n",
      "random ﬁelds)\n",
      "\n",
      "19.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In Chapter 10, we discussed directed graphical models (DGMs), commonly known as Bayes nets. However, for some domains, being forced to choose a direction for the edges, as required by a DGM, is rather awkward. For example, consider modeling an image. We might suppose that the intensity values of neighboring pixels are correlated. We can create a DAG model with a 2d lattice topology as shown in Figure 19.1(a). This is known as a causal MRF or a Markov mesh (Abend et al. 1965). However, its conditional independence properties are rather unnatural. In particular, the Markov blanket (deﬁned in Section 10.5) of the node X8 in the middle is the other colored nodes (3, 4, 7, 9, 12 and 13) rather than just its 4 nearest neighbors as one might expect. An alternative is to use an undirected graphical model (UGM), also called a Markov random ﬁeld (MRF) or Markov network. These do not require us to specify edge orientations, and are much more natural for some problems such as image analysis and spatial statistics. For example, an undirected 2d lattice is shown in Figure 19.1(b); now the Markov blanket of each node is just its nearest neighbors, as we show in Section 19.2.\n",
      "\n",
      "Roughly speaking, the main advantages of UGMs over DGMs are: (1) they are symmetric and therefore more “natural” for certain domains, such as spatial or relational data; and (2) discrimi- nativel UGMs (aka conditional random ﬁelds, or CRFs), which deﬁne conditional densities of the form p(y|x), work better than discriminative DGMs, for reasons we explain in Section 19.6.1. The (1) the parameters are less interpretable main disadvantages of UGMs compared to DGMs are: and less modular, for reasons we explain in Section 19.3; and (2) parameter estimation is com- putationally more expensive, for reasons we explain in Section 19.5. See (Domke et al. 2008) for an empirical comparison of the two approaches for an image processing task.\n",
      "\n",
      "19.2\n",
      "\n",
      "Conditional independence properties of UGMs\n",
      "\n",
      "19.2.1\n",
      "\n",
      "Key properties\n",
      "\n",
      "UGMs deﬁne CI relationships via simple graph separation as follows: for sets of nodes A, B, and C, we say xA ⊥G xB|xC iff C separates A from B in the graph G. This means that, when we remove all the nodes in C, if there are no paths connecting any node in A to any node in B, then the CI property holds. This is called the global Markov property for UGMs. For example, in Figure 19.2(b), we have that {1, 2} ⊥ {6, 7}|{3, 4, 5}.\n",
      "\n",
      "662\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "X1\n",
      "\n",
      "X2\n",
      "\n",
      "X3\n",
      "\n",
      "X4\n",
      "\n",
      "X5\n",
      "\n",
      "X1\n",
      "\n",
      "X2\n",
      "\n",
      "X3\n",
      "\n",
      "X4\n",
      "\n",
      "X5\n",
      "\n",
      "X6\n",
      "\n",
      "X7\n",
      "\n",
      "X8\n",
      "\n",
      "X9\n",
      "\n",
      "X10\n",
      "\n",
      "X6\n",
      "\n",
      "X7\n",
      "\n",
      "X8\n",
      "\n",
      "X9\n",
      "\n",
      "X10\n",
      "\n",
      "X11\n",
      "\n",
      "X12\n",
      "\n",
      "X13\n",
      "\n",
      "X14\n",
      "\n",
      "X15\n",
      "\n",
      "X11\n",
      "\n",
      "X12\n",
      "\n",
      "X13\n",
      "\n",
      "X14\n",
      "\n",
      "X15\n",
      "\n",
      "X16\n",
      "\n",
      "X17\n",
      "\n",
      "X18\n",
      "\n",
      "X19\n",
      "\n",
      "X20\n",
      "\n",
      "X16\n",
      "\n",
      "X17\n",
      "\n",
      "X18\n",
      "\n",
      "X19\n",
      "\n",
      "X20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(a) A 2d lattice represented as a DAG. The dotted red node X8 is independent of all other Figure 19.1 nodes (black) given its Markov blanket, which include its parents (blue), children (green) and co-parents (orange). (b) The same model represented as a UGM. The red node X8 is independent of the other black nodes given its neighbors (blue nodes).\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "1\n",
      "\n",
      "5\n",
      "\n",
      "7\n",
      "\n",
      "1\n",
      "\n",
      "5\n",
      "\n",
      "7\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "\n",
      "3\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 19.2 (a) A DGM. (b) Its moralized version, represented as a UGM.\n",
      "\n",
      "The set of nodes that renders a node t conditionally independent of all the other nodes in the graph is called t’s Markov blanket; we will denote this by mb(t). Formally, the Markov blanket satisﬁes the following property:\n",
      "\n",
      "t ⊥ V \\ cl(t)|mb(t)\n",
      "\n",
      "(19.1)\n",
      "\n",
      "where cl(t) (cid:2) mb(t) ∪ {t} is the closure of node t. One can show that, in a UGM, a node’s Markov blanket is its set of immediate neighbors. This is called the undirected local Markov property. For example, in Figure 19.2(b), we have mb(5) = {2, 3, 4, 6}.\n",
      "\n",
      "From the local Markov property, we can easily see that two nodes are conditionally indepen- dent given the rest if there is no direct edge between them. This is called the pairwise Markov property. In symbols, this is written as\n",
      "\n",
      "s ⊥ t|V \\ {s, t} ⇐⇒G st = 0\n",
      "\n",
      "(19.2)\n",
      "\n",
      "Using the three Markov properties we have discussed, we can derive the following CI properties\n",
      "\n",
      "(amongst others) from the UGM in Figure 19.2(b):\n",
      "\n",
      "Pairwise 1 ⊥ 7|rest • Local 1 ⊥ rest|2, 3\n",
      "\n",
      "19.2. Conditional independence properties of UGMs\n",
      "\n",
      "663\n",
      "\n",
      "G\n",
      "\n",
      "L\n",
      "\n",
      "P\n",
      "\n",
      "p(x) > 0\n",
      "\n",
      "Figure 19.3 Relationship between Markov properties of UGMs.\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "1\n",
      "\n",
      "5\n",
      "\n",
      "1\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 19.4 moralized version of (a).\n",
      "\n",
      "(a) The ancestral graph induced by the DAG in Figure 19.2(a) wrt U = {2, 4, 5}.\n",
      "\n",
      "(b) The\n",
      "\n",
      "Global 1, 2 ⊥ 6, 7|3, 4, 5\n",
      "\n",
      "It is obvious that global Markov implies local Markov which implies pairwise Markov. What is less obvious, but nevertheless true (assuming p(x) > 0 for all x, i.e., that p is a positive density), is that pairwise implies global, and hence that all these Markov properties are the same, as illustrated in Figure 19.3 (see e.g., (Koller and Friedman 2009, p119) for a proof).1 The importance of this result is that it is usually easier to empirically assess pairwise conditional independence; such pairwise CI statements can be used to construct a graph from which global CI statements can be extracted.\n",
      "\n",
      "19.2.2\n",
      "\n",
      "An undirected alternative to d-separation\n",
      "\n",
      "We have seen that determinining CI relationships in UGMs is much easier than in DGMs, because we do not have to worry about the directionality of the edges. In this section, we show how to determine CI relationships for a DGM using a UGM.\n",
      "\n",
      "It is tempting to simply convert the DGM to a UGM by dropping the orientation of the edges, but this is clearly incorrect, since a v-structure A → B ← C has quite different CI properties than the corresponding undirected chain A − B − C. The latter graph incorrectly states that A ⊥ C|B. To avoid such incorrect CI statements, we can add edges between the “unmarried” parents A and C, and then drop the arrows from the edges, forming (in this case) a fully connected undirected graph. This process is called moralization. Figure 19.2(b) gives a larger\n",
      "\n",
      "1. The restriction to positive densities arises because deterministic constraints can result in independencies present in the distribution that are not explicitly represented in the graph. See e.g., (Koller and Friedman 2009, p120) for some examples. Distributions with non-graphical CI properties are said to be unfaithful to the graph, so I(p) (cid:3)= I(G).\n",
      "\n",
      "664\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "(cid:51)(cid:85)(cid:82)(cid:69)(cid:68)(cid:69)(cid:76)(cid:79)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:86)\n",
      "\n",
      "(cid:42)(cid:85)(cid:68)(cid:83)(cid:75)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)(cid:86)\n",
      "\n",
      "(cid:39)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)\n",
      "\n",
      "(cid:38)(cid:75)(cid:82)(cid:85)(cid:71)(cid:68)(cid:79)\n",
      "\n",
      "(cid:56)(cid:81)(cid:71)(cid:76)(cid:85)(cid:72)(cid:70)(cid:87)(cid:72)(cid:71)\n",
      "\n",
      "Figure 19.5 DGMs and UGMs can perfectly represent different sets of distributions. Some distributions can be perfectly represented by either DGMs or UGMs; the corresponding graph must be chordal.\n",
      "\n",
      "example of moralization: we interconnect 2 and 3, since they have a common child 5, and we interconnect 4, 5 and 6, since they have a common child 7.\n",
      "\n",
      "Unfortunately, moralization loses some CI information, and therefore we cannot use the moralized UGM to determine CI properties of the DGM. For example, in Figure 19.2(a), using d-separation, we see that 4 ⊥ 5|2. Adding a moralization arc 4 − 5 would lose this fact (see Figure 19.2(b)). However, notice that the 4-5 moralization edge, due to the common child 7, is not needed if we do not observe 7 or any of its descendants. This suggests the following approach to determining if A ⊥ B|C. First we form the ancestral graph of DAG G with respect to U = A ∪ B ∪ C. This means we remove all nodes from G that are not in U or are not ancestors of U . We then moralize this ancestral graph, and apply the simple graph separation rules for UGMs. For example, in Figure 19.4(a), we show the ancestral graph for Figure 19.2(a) using U = {2, 4, 5}. In Figure 19.4(b), we show the moralized version of this graph. It is clear that we now correctly conclude that 4 ⊥ 5|2.\n",
      "\n",
      "19.2.3\n",
      "\n",
      "Comparing directed and undirected graphical models\n",
      "\n",
      "Which model has more “expressive power”, a DGM or a UGM? To formalize this question, recall that we say that G is an I-map of a distribution p if I(G) ⊆ I(p). Now deﬁne G to be perfect map of p if I(G) =I (p), in other words, the graph can represent all (and only) the CI properties of the distribution. It turns out that DGMs and UGMs are perfect maps for different sets of distributions (see Figure 19.5). In this sense, neither is more powerful than the other as a representation language.\n",
      "\n",
      "As an example of some CI relationships that can be perfectly modeled by a DGM but not a UGM, consider a v-structure A → C ← B. This asserts that A ⊥ B, and A (cid:4)⊥ B|C. If we drop the arrows, we get A − C − B, which asserts A ⊥ B|C and A (cid:4)⊥ B, which is incorrect. In fact, there is no UGM that can precisely represent all and only the two CI statements encoded by a v- structure. In general, CI properties in UGMs are monotonic, in the following sense: if A ⊥ B|C, then A ⊥ B|(C ∪ D). But in DGMs, CI properties can be non-monotonic, since conditioning\n",
      "\n",
      "19.3. Parameterization of MRFs\n",
      "\n",
      "665\n",
      "\n",
      "A\n",
      "\n",
      "A\n",
      "\n",
      "D\n",
      "\n",
      "B\n",
      "\n",
      "D\n",
      "\n",
      "B\n",
      "\n",
      "D\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "C\n",
      "\n",
      "C\n",
      "\n",
      "A\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 19.6 A UGM and two failed attempts to represent it as a DGM. Source: Figure 3.10 of (Koller and Friedman 2009). Used with kind permission of Daphne Koller.\n",
      "\n",
      "on extra variables can eliminate conditional independencies due to explaining away.\n",
      "\n",
      "As an example of some CI relationships that can be perfectly modeled by a UGM but not a DGM, consider the 4-cycle shown in Figure 19.6(a). One attempt to model this with a DGM is shown in Figure 19.6(b). This correctly asserts that A ⊥ C|B, D. However, it incorrectly asserts that B ⊥ D|A. Figure 19.6(c) is another incorrect DGM: it correctly encodes A ⊥ C|B, D, but incorrectly encodes B ⊥ D. In fact there is no DGM that can precisely represent all and only the CI statements encoded by this UGM.\n",
      "\n",
      "Some distributions can be perfectly modeled by either a DGM or a UGM; the resulting graphs are called decomposable or chordal. Roughly speaking, this means the following: if we collapse together all the variables in each maximal clique, to make “mega-variables”, the resulting graph will be a tree. Of course, if the graph is already a tree (which includes chains as a special case), it will be chordal. See Section 20.4.1 for further details.\n",
      "\n",
      "19.3\n",
      "\n",
      "Parameterization of MRFs\n",
      "\n",
      "Although the CI properties of UGM are simpler and more natural than for DGMs, representing the joint distribution for a UGM is less natural than for a DGM, as we see below.\n",
      "\n",
      "19.3.1\n",
      "\n",
      "The Hammersley-Clifford theorem\n",
      "\n",
      "Since there is no topological ordering associated with an undirected graph, we can’t use the chain rule to represent p(y). So instead of associating CPDs with each node, we associate potential functions orfactors with each maximal clique in the graph. We will denote the potential function for clique c by ψc(yc|θc). A potential function can be any non-negative function of its arguments. The joint distribution is then deﬁned to be proportional to the product of clique potentials. Rather surprisingly, one can show that any positive distribution whose CI properties can be represented by a UGM can be represented in this way. We state this result more formally below.\n",
      "\n",
      "666\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "Theorem 19.3.1 (Hammersley-Clifford). A positive distribution p(y) > 0 satisﬁes the CI prop- erties of an undirected graph G iff p can be represented as a product of factors, one per maximal clique, i.e.,\n",
      "\n",
      "p(y|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "c∈C\n",
      "\n",
      "ψc(yc|θc)\n",
      "\n",
      "(19.3)\n",
      "\n",
      "where C is the set of all the (maximal) cliques of G, and Z(θ) is the partition function given by\n",
      "\n",
      "Z(θ) (cid:2)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ψc(yc|θc)\n",
      "\n",
      "(19.4)\n",
      "\n",
      "x\n",
      "\n",
      "c∈C\n",
      "\n",
      "Note that the partition function is what ensures the overall distribution sums to 1.2\n",
      "\n",
      "then we can write p as follows:\n",
      "\n",
      "The proof was never published, but can be found in e.g., (Koller and Friedman 2009). For example, consider the MRF in Figure 10.1(b). If p satisﬁes the CI properties of this graph\n",
      "\n",
      "p(y|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "ψ123(y1, y2, y3)ψ234(y2, y3, y4)ψ35(y3, y5)\n",
      "\n",
      "(19.5)\n",
      "\n",
      "where\n",
      "\n",
      "Z =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψ123(y1, y2, y3)ψ234(y2, y3, y4)ψ35(y3, y5)\n",
      "\n",
      "(19.6)\n",
      "\n",
      "y\n",
      "\n",
      "There is a deep connection between UGMs and statistical physics. model known as the Gibbs distribution, which can be written as follows:\n",
      "\n",
      "In particular, there is a\n",
      "\n",
      "p(y|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "exp(−\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "c\n",
      "\n",
      "E(yc|θc))\n",
      "\n",
      "(19.7)\n",
      "\n",
      "where E(yc) > 0 is the energy associated with the variables in clique c. We can convert this to a UGM by deﬁning\n",
      "\n",
      "ψc(yc|θc) = exp(−E(yc|θc))\n",
      "\n",
      "(19.8)\n",
      "\n",
      "We see that high probability states correspond to low energy conﬁgurations. Models of this form are known as energy based models, and are commonly used in physics and biochemistry, as well as some branches of machine learning (LeCun et al. 2006).\n",
      "\n",
      "Note that we are free to restrict the parameterization to the edges of the graph, rather than\n",
      "\n",
      "the maximal cliques. This is called a pairwise MRF. In Figure 10.1(b), we get\n",
      "\n",
      "p(y|θ) ∝ ψ12(y1, y2)ψ13(y1, y3)ψ23(y2, y3)ψ24(y2, y4)ψ34(y3, y4)ψ35(y3, y5)\n",
      "\n",
      "∝\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ψst(ys, yt)\n",
      "\n",
      "(19.9)\n",
      "\n",
      "(19.10)\n",
      "\n",
      "s∼t\n",
      "\n",
      "This form is widely used due to its simplicity, although it is not as general.\n",
      "\n",
      "2. The partition function is denoted by Z because of the German word zustandssumme, which means “sum over states”. This reﬂects the fact that a lot of pioneering working in statistical physics was done by Germans.\n",
      "\n",
      "19.3. Parameterization of MRFs\n",
      "\n",
      "667\n",
      "\n",
      "19.3.2\n",
      "\n",
      "Representing potential functions\n",
      "\n",
      "If the variables are discrete, we can represent the potential or energy functions as tables of (non-negative) numbers, just as we did with CPTs. However, the potentials are not probabilities. Rather, they represent the relative “compatibility” between the different assignments to the potential. We will see some examples of this below.\n",
      "\n",
      "A more general approach is to deﬁne the log potentials as a linear function of the parameters: log ψc(yc) (cid:2) φc(yc)T θc\n",
      "\n",
      "(19.11)\n",
      "\n",
      "where φc(xc) is a feature vector derived from the values of the variables yc. The resulting log probability has the form\n",
      "\n",
      "log p(y|θ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "φc(yc)T θc − Z(θ)\n",
      "\n",
      "(19.12)\n",
      "\n",
      "c\n",
      "\n",
      "This is also known as a maximum entropy or a log-linear model.\n",
      "\n",
      "length K 2 as follows:\n",
      "\n",
      "For example, consider a pairwise MRF, where for each edge, we associate a feature vector of\n",
      "\n",
      "φst(ys, yt) = [. . . ,I (ys = j, yt = k), . . .]\n",
      "\n",
      "(19.13)\n",
      "\n",
      "If we have a weight for each feature, we can convert this into a K × K potential function as follows:\n",
      "\n",
      "ψst(ys = j, yt = k) = exp([θT\n",
      "\n",
      "stφst]jk) = exp(θst(j, k))\n",
      "\n",
      "(19.14)\n",
      "\n",
      "So we see that we can easily represent tabular potentials using a log-linear form. But the log-linear form is more general.\n",
      "\n",
      "To see why this is useful, suppose we are interested in making a probabilistic model of English spelling. Since certain letter combinations occur together quite frequently (e.g., “ing”), we will need higher order factors to capture this. Suppose we limit ourselves to letter trigrams. A tabular potential still has 263 = 17, 576 parameters in it. However, most of these triples will never occur.\n",
      "\n",
      "An alternative approach is to deﬁne indicator functions that look for certain “special” triples,\n",
      "\n",
      "such as “ing”, “qu-”, etc. Then we can deﬁne the potential on each trigram as follows:\n",
      "\n",
      "ψ(yt−1, yt, yt+1) = exp(\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θkφk(yt−1, yt, yt+1))\n",
      "\n",
      "(19.15)\n",
      "\n",
      "k\n",
      "\n",
      "where k indexes the different features, corresponding to “ing”, “qu-”, etc., and φk is the corre- sponding binary feature function. By tying the parameters across locations, we can deﬁne the probability of a word of any length using (cid:4)\n",
      "\n",
      "p(y|θ) ∝ exp(\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θkφk(yt−1, yt, yt+1))\n",
      "\n",
      "(19.16)\n",
      "\n",
      "t\n",
      "\n",
      "k\n",
      "\n",
      "This raises the question of where these feature functions come from. In many applications, they are created by hand to reﬂect domain knowledge (we will see examples later), but it is also possible to learn them from data, as we discuss in Section 19.5.6.\n",
      "\n",
      "668\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "19.4\n",
      "\n",
      "Examples of MRFs\n",
      "\n",
      "In this section, we show how several popular probability models can be conveniently expressed as UGMs.\n",
      "\n",
      "19.4.1\n",
      "\n",
      "Ising model\n",
      "\n",
      "The Ising model is an example of an MRF that arose from statistical physics.3 It was originally used for modeling the behavior of magnets. In particular, let ys ∈ {−1, +1} represent the spin In some magnets, called ferro-magnets, of an atom, which can either be spin down or up. neighboring spins tend to line up in the same direction, whereas in other kinds of magnets, called anti-ferromagnets, the spins “want” to be different from their neighbors.\n",
      "\n",
      "We can model this as an MRF as follows. We create a graph in the form of a 2D or 3D lattice, and connect neighboring variables, as in Figure 19.1(b). We then deﬁne the following pairwise clique potential:\n",
      "\n",
      "ψst(ys, yt) =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "ewst e−wst\n",
      "\n",
      "e−wst ewst\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(19.17)\n",
      "\n",
      "Here wst is the coupling strength between nodes s and t. If two nodes are not connected in the graph, we set wst = 0. We assume that the weight matrix W is symmetric, so wst = wts. Often we assume all edges have the same strength, so wst = J (assuming wst (cid:4)= 0).\n",
      "\n",
      "If all the weights are positive, J > 0, then neighboring spins are likely to be in the same state; this can be used to model ferromagnets, and is an example of an associative Markov network. If the weights are sufficiently strong, the corresponding probability distribution will have two modes, corresponding to the all +1’s state and the all -1’s state. These are called the ground states of the system.\n",
      "\n",
      "If all of the weights are negative, J < 0, then the spins want to be different from their neighbors; this can be used to model an anti-ferromagnet, and results in a frustrated system, in which not all the constraints can be satisﬁed at the same time. The corresponding probability distribution will have multiple modes. Interestingly, computing the partition function Z(J) can be done in polynomial time for associative Markov networks, but is NP-hard in general (Cipra 2000).\n",
      "\n",
      "There is an interesting analogy between Ising models and Gaussian graphical models. First, assuming yt ∈ {−1, +1}, we can write the unnormalized log probability of an Ising model as follows:\n",
      "\n",
      "log ˜p(y) = −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "s∼t\n",
      "\n",
      "yswstyt = − 1 2\n",
      "\n",
      "yT Wy\n",
      "\n",
      "(19.18)\n",
      "\n",
      "(The factor of 1 (and hence high probability) if neighboring states agree.\n",
      "\n",
      "2 arises because we sum each edge twice.) If wst = J > 0, we get a low energy\n",
      "\n",
      "Sometimes there is an external ﬁeld, which is an energy term which is added to each spin. This can be modelled using a local energy term of the form −bT y, where b is sometimes called\n",
      "\n",
      "3. Ernst Ising was a German-American physicist, 1900–1998.\n",
      "\n",
      "19.4. Examples of MRFs\n",
      "\n",
      "669\n",
      "\n",
      "a bias term. The modiﬁed distribution is given by\n",
      "\n",
      "log ˜p(y) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "s∼t\n",
      "\n",
      "wstysyt +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "s\n",
      "\n",
      "bsys =\n",
      "\n",
      "1 2\n",
      "\n",
      "yT Wy + bT y\n",
      "\n",
      "(19.19)\n",
      "\n",
      "where θ = (W, b).\n",
      "\n",
      "If we deﬁne μ (cid:2) − 1\n",
      "\n",
      "2 Σ−1b, Σ−1 = −W, and c (cid:2) 1\n",
      "\n",
      "2 μT Σ−1μ, we can rewrite this in a form\n",
      "\n",
      "that looks similar to a Gaussian:\n",
      "\n",
      "˜p(y) ∝ exp(− 1 2\n",
      "\n",
      "(y − μ)T Σ−1(y − μ) +c )\n",
      "\n",
      "(19.20)\n",
      "\n",
      "One very important difference is that, in the case of Gaussians, the normalization constant, Z = |2πΣ|, requires the computation of a matrix determinant, which can be computed in O(D3) time, whereas in the case of the Ising model, the normalization constant requires summing over all 2D bit vectors; this is equivalent to computing the matrix permanent, which is NP-hard in general (Jerrum et al. 2004).\n",
      "\n",
      "19.4.2\n",
      "\n",
      "Hopﬁeld networks\n",
      "\n",
      "A Hopﬁeld network (Hopﬁeld 1982) is a fully connected Ising model with a symmetric weight matrix, W = WT . These weights, plus the bias terms b, can be learned from training data using (approximate) maximum likelihood, as described in Section 19.5.4\n",
      "\n",
      "The main application of Hopﬁeld networks is as an associative memory or content ad- dressable memory. The idea is this: suppose we train on a set of fully observed bit vectors, corresponding to patterns we want to memorize. Then, at test time, we present a partial pattern to the network. We would like to estimate the missing variables; this is called pattern com- pletion. See Figure 19.7 for an example. This can be thought of as retrieving an example from memory based on a piece of the example itself, hence the term “associative memory”.\n",
      "\n",
      "Since exact inference is intractable in this model, it is standard to use a coordinate descent algorithm known as iterative conditional modes (ICM), which just sets each node to its most likely (lowest energy) state, given all its neighbors. The full conditional can be shown to be\n",
      "\n",
      "Picking the most probable state amounts to using the rule y∗ y∗ s = 0 otherwise. (Much better inference algorithms will be discussed later in this book.) Since inference is deterministic, it is also possible to interpret this model as a recurrent neural network. (This is quite different from the feedforward neural nets studied in Section 16.5; they are univariate conditional density models of the form p(y|x, θ) which can only be used for supervised learning.) See Hertz et al. (1991) for further details on Hopﬁeld networks.\n",
      "\n",
      "p(ys = 1|y−s, θ) = sigm(wT\n",
      "\n",
      "s,:y−s + bs)\n",
      "\n",
      "s = 1 if\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "t wstyt > bs and using\n",
      "\n",
      "(19.21)\n",
      "\n",
      "A Boltzmann machine generalizes the Hopﬁeld / Ising model by including some hidden Inference in such models nodes, which makes the model representationally more powerful. often uses Gibbs sampling, which is a stochastic version of ICM (see Section 24.2 for details).\n",
      "\n",
      "4. ML estimation works much better than the outer product rule proposed in in (Hopﬁeld 1982), because it not only lowers the energy of the observed patterns, but it also raises the energy of the non-observed patterns, in order to make the distribution sum to one (Hillar et al. 2012).\n",
      "\n",
      "670\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "Figure 19.7 Examples of how an associative memory can reconstruct images. These are binary images of size 50 × 50 pixels. Top: training images. Row 2: partially visible test images. Row 3: estimate after 5 iterations. Bottom: ﬁnal state estimate. Based on Figure 2.1 of Hertz et al. (1991). Figure generated by hopfieldDemo.\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 19.8 Visualizing a sample from a 10-state Potts model of size 128 × 128 for different association (a) J = 1.42, (b) J = 1.44, (c) J = 1.46. The regions are labeled according to size: blue is strengths: largest, red is smallest. Used with kind permission of Erik Sudderth. See gibbsDemoIsing for Matlab code to produce a similar plot for the Ising model.\n",
      "\n",
      "However, we could equally well apply Gibbs to a Hopﬁeld net and ICM to a Boltzmann machine: the inference algorithm is not part of the model deﬁnition. See Section 27.7 for further details on Boltzmann machines.\n",
      "\n",
      "19.4. Examples of MRFs\n",
      "\n",
      "671\n",
      "\n",
      "ys\n",
      "\n",
      "yt\n",
      "\n",
      "xs\n",
      "\n",
      "xt\n",
      "\n",
      "Figure 19.9 A grid-structured MRF with local evidence nodes.\n",
      "\n",
      "19.4.3\n",
      "\n",
      "Potts model\n",
      "\n",
      "It is easy to generalize the Ising model to multiple discrete states, yt ∈ {1, 2, . . . , K}. common to use a potential function of the following form:\n",
      "\n",
      "ψst(ys, yt) =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "eJ 0 0\n",
      "\n",
      "0 eJ 0\n",
      "\n",
      "0 0 eJ\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "(19.22)\n",
      "\n",
      "It is\n",
      "\n",
      "This is called the Potts model.5 If J > 0, then neighboring nodes are encouraged to have the same label. Some samples from this model are shown in Figure 19.8. We see that for J > 1.44, large clusters occur, for J < 1.44, many small clusters occur, and at the critical value of K = 1.44, there is a mix of small and large clusters. This rapid change in behavior as we vary a parameter of the system is called a phase transition, and has been widely studied in the physics community. An analogous phenomenon occurs in the Ising model; see (MacKay 2003, ch 31) for details.\n",
      "\n",
      "The Potts model can be used as a prior for image segmentation, since it says that neighboring pixels are likely to have the same discrete label and hence belong to the same segment. We can combine this prior with a likelihood term as follows:\n",
      "\n",
      "p(y, x|θ) = p(y|J)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "t\n",
      "\n",
      "p(xt|yt, θ) =\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "1 Z(J)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "s∼t\n",
      "\n",
      "ψ(ys, yt; J)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "t\n",
      "\n",
      "p(xt|yt, θ)\n",
      "\n",
      "(19.23)\n",
      "\n",
      "where p(xt|yt = k, θ) is the probability of observing pixel xt given that the corresponding segment belongs to class k. This observation model can be modeled using a Gaussian or a non-parametric density. (Note that we label the hidden nodes yt and the observed nodes xt, to be compatible with Section 19.6.)\n",
      "\n",
      "The corresponding graphical model is a mix of undirected and directed edges, as shown in Figure 19.9. The undirected 2d lattice represents the prior p(y); in addition, there are directed edge from each yt to its corresponding xt, representing the local evidence. Technically speak- ing, this combination of an undirected and directed graph is called a chain graph. However,\n",
      "\n",
      "5. Renfrey Potts was an Australian mathematician, 1925–2005.\n",
      "\n",
      "672\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "since the xt nodes are observed, they can be “absorbed” into the model, thus leaving behind an undirected “backbone”.\n",
      "\n",
      "This model is a 2d analog of an HMM, and could be called a partially observed MRF. As in an HMM, the goal is to perform posterior inference, i.e., to compute (some function of) p(y|x, θ). Unfortunately, the 2d case is provably much harder than the 1d case, and we must resort to approximate methods, as we discuss in later chapters.\n",
      "\n",
      "Although the Potts prior is adequate for regularizing supervised learning problems, it is not sufficiently accurate to perform image segmentation in an unsupervised way, since the segments produced by this model do not accurately represent the kinds of segments one sees in natural images (Morris et al. 1996).6 For the unsupervised case, one needs to use more sophisticated priors, such as the truncated Gaussian process prior of (Sudderth and Jordan 2008).\n",
      "\n",
      "19.4.4\n",
      "\n",
      "Gaussian MRFs\n",
      "\n",
      "An undirected GGM, also called a Gaussian MRF (see e.g., (Rue and Held 2005)), is a pairwise MRF of the following form:\n",
      "\n",
      "p(y|θ) ∝\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ψst(ys, yt)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ψt(yt)\n",
      "\n",
      "(19.24)\n",
      "\n",
      "s∼t\n",
      "\n",
      "t\n",
      "\n",
      "ψst(ys, yt) = exp(− 1 2 ψt(yt) = exp(− 1 2\n",
      "\n",
      "ysΛstyt)\n",
      "\n",
      "Λtty2\n",
      "\n",
      "t + ηtyt)\n",
      "\n",
      "(19.25)\n",
      "\n",
      "(19.26)\n",
      "\n",
      "(Note that we could easily absorb the node potentials ψt into the edge potentials, but we have kept them separate for clarity.) The joint distribution can be written as follows:\n",
      "\n",
      "p(y|θ) ∝ exp[ηT y − 1 2\n",
      "\n",
      "yT Λy]\n",
      "\n",
      "(19.27)\n",
      "\n",
      "We recognize this as a multivariate Gaussian written in information form where Λ = Σ−1 and η = Λμ.\n",
      "\n",
      "If Λst = 0 , then there is no pairwise term connecting s and t, so by the factorization theorem\n",
      "\n",
      "(Theorem 2.2.1), we conclude that\n",
      "\n",
      "ys ⊥ yt|y−(st) ⇐⇒ Λst = 0\n",
      "\n",
      "(19.28)\n",
      "\n",
      "The zero entries in Λ are called structural zeros, since they represent the absent edges in the graph. Thus undirected GGMs correspond to sparse precision matrices, a fact which we exploit in Section 26.7.2 to efficiently learn the structure of the graph.\n",
      "\n",
      "19.4.4.1\n",
      "\n",
      "Comparing Gaussian DGMs and UGMs *\n",
      "\n",
      "In Section 10.2.5, we saw that directed GGMs correspond to sparse regression matrices, and hence sparse Cholesky factorizations of covariance matrices, whereas undirected GGMs correspond to\n",
      "\n",
      "6. An inﬂuential paper (Geman and Geman 1984), which introduced the idea of a Gibbs sampler (Section 24.2), proposed using the Potts model as a prior for image segmentation, but the results in their paper are misleading because they did not run their Gibbs sampler for long enough. See Figure 24.10 for a vivid illustration of this point.\n",
      "\n",
      "19.4. Examples of MRFs\n",
      "\n",
      "673\n",
      "\n",
      "Figure 19.10 A VAR(2) process represented as a dynamic chain graph. Source: 2000). Used with kind permission of Rainer Dahlhaus and Oxford University Press.\n",
      "\n",
      "(Dahlhaus and Eichler\n",
      "\n",
      "sparse precision matrices. The advantage of the DAG formulation is that we can make the regression weights W, and hence Σ, be conditional on covariate information (Pourahmadi 2004), without worrying about positive deﬁnite constraints. The disadavantage of the DAG formulation is its dependence on the order, although in certain domains, such as time series, there is already a natural ordering of the variables.\n",
      "\n",
      "It is actually possible to combine both representations, resulting in a Gaussian chain graph. For example, consider a a discrete-time, second-order Markov chain in which the states are continuous, yt ∈ RD. The transition function can be represented as a (vector-valued) linear- Gaussian CPD:\n",
      "\n",
      "p(yt|yt−1, yt−2, θ) = N (yt|A1yt−1 + A2yt−2, Σ)\n",
      "\n",
      "(19.29)\n",
      "\n",
      "This is called vector auto-regressive or VAR process of order 2. Such models are widely used in econometrics for time-series forecasting.\n",
      "\n",
      "The time series aspect is most naturally modeled using a DGM. However, if Σ−1 is sparse, then the correlation amongst the components within a time slice is most naturally modeled using a UGM. For example, suppose we have ⎛ 0 ⎜ 0 ⎜ ⎜ 0 ⎜ ⎝ 0 0\n",
      "\n",
      "A1 =\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "⎛\n",
      "\n",
      "3 5 0 2 5 0 0\n",
      "\n",
      "0 3 5 1 3 0 0\n",
      "\n",
      "1 0 5 0 − 1 5 3 0 5 0 − 1 2 1 0 5\n",
      "\n",
      "0 0 0 1 5 2 5\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎟ ⎠\n",
      "\n",
      ", A2 =\n",
      "\n",
      "0 − 1 5 0 0 0 0 1 0 5 0 0\n",
      "\n",
      "0 0 0 0 0 0 1 0 3 0 − 1 5\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "(19.30)\n",
      "\n",
      "and\n",
      "\n",
      "Σ =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "1 1 2 1\n",
      "\n",
      "3 − 1 3 0 0 0 0\n",
      "\n",
      "1 1 2 3 1 − 1 3 1 0 0\n",
      "\n",
      "0 0 ⎟ 0 0 ⎟ ⎟ 0 0 ⎟ ⎠ 1 0 0 1\n",
      "\n",
      "⎞\n",
      "\n",
      ", Σ−1 =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "2.13 −1.47 −1.2 1.2 2.13 −1.47 1.8 1.2 −1.2 0 0 0 0 0 0\n",
      "\n",
      "0 0 0 1 0\n",
      "\n",
      "0 ⎟ 0 ⎟ ⎟ 0 ⎟ ⎠ 0 1\n",
      "\n",
      "⎞\n",
      "\n",
      "(19.31)\n",
      "\n",
      "674\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "(a)\n",
      "\n",
      "w1\n",
      "\n",
      "w2\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 19.11 Based on Figures 5.12-5.13 of (Choi 2011). Used with kind permission of Myung Choi.\n",
      "\n",
      "(a) A bi-directed graph.\n",
      "\n",
      "(b) The equivalent DAG. Here the w nodes are latent confounders.\n",
      "\n",
      "The resulting graphical model is illustrated in Figure 19.10. Zeros in the transition matrices A1 and A2 correspond to absent directed arcs from yt−1 and yt−2 into yt. Zeros in the precision matrix Σ−1 correspond to absent undirected arcs between nodes in yt.\n",
      "\n",
      "Sometimes we have a sparse covariance matrix rather than a sparse precision matrix. This can be represented using a bi-directed graph, where each edge has arrows in both directions, as in Figure 19.11(a). Here nodes that are not connected are unconditionally independent. For example in Figure 19.11(a) we see that Y1 ⊥ Y3. In the Gaussian case, this means Σ1,3 = Σ3,1 = 0. (A graph representing a sparse covariance matrix is called a covariance graph.) By contrast, if this were an undirected model, we would have that Y1 ⊥ Y3|Y2, and Λ1,3 = Λ3,1 = 0, where Λ = Σ−1.\n",
      "\n",
      "A bidirected graph can be converted to a DAG with latent variables, where each bidirected edge is replaced with a hidden variable representing a hidden common cause, or confounder, as illustrated in Figure 19.11(b). The relevant CI properties can then be determined using d- separation.\n",
      "\n",
      "We can combine bidirected and directed edges to get a directed mixed graphical model. This is useful for representing a variety of models, such as ARMA models (Section 18.2.4.4), structural equation models (Section 26.5.5), etc.\n",
      "\n",
      "19.4.5 Markov logic networks *\n",
      "\n",
      "In Section 10.2.2, we saw how we could “unroll” Markov models and HMMs for an arbitrary number of time steps in order to model variable-length sequences. Similarly, in Section 19.4.1, we saw how we could expand a lattice UGM to model images of any size. What about more complex domains, where we have a variable number of objects and relationships between them? Creating models for such scenarios is often done using ﬁrst-order logic (see e.g., (Russell and Norvig 2010)). For example, consider the sentences “Smoking causes cancer” and “If two people are friends, and one smokes, then so does the other”. We can write these sentences in ﬁrst-order\n",
      "\n",
      "19.4. Examples of MRFs\n",
      "\n",
      "675\n",
      "\n",
      "Friends(A,B)\n",
      "\n",
      "Friends(A,A)\n",
      "\n",
      "Smokes(A)\n",
      "\n",
      "Smokes(B)\n",
      "\n",
      "Friends(B,B)\n",
      "\n",
      "Cancer(A)\n",
      "\n",
      "Cancer(B)\n",
      "\n",
      "Friends(B,A)\n",
      "\n",
      "Figure 19.12 An example of a ground Markov logic network represented as a pairwise MRF for 2 people. Based on Figure 2.1 from (Domingos and Lowd 2009). Used with kind permission of Pedro Domingos.\n",
      "\n",
      "logic as follows:\n",
      "\n",
      "∀x.Sm(x) =⇒ Ca(x) ∀x.∀y.F r(x, y) ∧ Sm(x) =⇒ Sm(y)\n",
      "\n",
      "(19.32)\n",
      "\n",
      "(19.33)\n",
      "\n",
      "where Sm and Ca are predicates, and F r is a relation.7\n",
      "\n",
      "Indeed, this brittleness is the main reason why logical approaches to AI are no longer widely used, at least not in their pure form. There have been a variety of attempts to combine ﬁrst order logic with probability theory, an area known as statistical relational AI or probabilistic relational modeling (Kersting et al. 2011). One simple approach is to take logical rules and attach weights (known as certainty factors) to them, and then to interpret them as conditional probability distributions. For example, we might say p(Ca(x) = 1|Sm(x) = 1) = 0.9. Unfortunately, the rule does not say what to predict if Sm(x) = 0. Furthermore, combining CPDs in this way is not guaranteed to deﬁne a consistent joint distribution, because the resulting graph may not be a DAG.\n",
      "\n",
      "Of course, such rules are not always true.\n",
      "\n",
      "An alternative approach is to treat these rules as a way of deﬁning potential functions in an unrolled UGM. The result is known as a Markov logic network (Domingos and Lowd 2009). To specify the network, we ﬁrst rewrite all the rules in conjunctive normal form (CNF), also known as clausal form. In this case, we get\n",
      "\n",
      "¬Sm(x) ∨ Ca(x) ¬F r(x, y) ∨ ¬Sm(x) ∨ Sm(y)\n",
      "\n",
      "(19.34) (19.35)\n",
      "\n",
      "The ﬁrst clause can be read as “Either x does not smoke or he has cancer”, which is logically equivalent to Equation 19.32. (Note that in a clause, any unbound variable, such as x, is assumed to be universally quantiﬁed.)\n",
      "\n",
      "7. A predicate is just a function of one argument, known as an object, that evaluates to true or false, depending on whether the property holds or not of that object. A (logical) relation is just a function of two or more arguments (objects) that evaluates to true or false, depending on whether the relationship holds between that set of objects or not.\n",
      "\n",
      "676\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "Inference in ﬁrst-order logic is only semi-decidable, so it is common to use a restricted subset. A common approach (as used in Prolog) is to restrict the language to Horn clauses, which are clauses that contain at most one positive literal. Essentially this means the model is a series of if-then rules, where the right hand side of the rules (the “then” part, or consequence) has only a single term.\n",
      "\n",
      "Once we have encoded our knowledge base as a set of clauses, we can attach weights to each one; these weights are the parameter of the model, and they deﬁne the clique potentials as follows:\n",
      "\n",
      "ψc(xc) = exp(wcφc(xc))\n",
      "\n",
      "(19.36)\n",
      "\n",
      "where φc(xc) is a logical expression which evaluates clause c applied to the variables xc, and wc is the weight we attach to this clause. Roughly speaking, the weight of a clause speciﬁes the probability of a world in which this clause is satsiﬁed relative to a world in which it is not satisﬁed.\n",
      "\n",
      "Now suppose there are two objects (people) in the world, Anna and Bob, which we will denote by constant symbols A and B. We can make a ground network from the above clauses by creating binary random variables Sx, Cx, and Fx,y for x, y ∈ {A, B}, and then “wiring these up” according to the clauses above. The result is the UGM in Figure 19.12 with 8 binary nodes. Note that we have not encoded the fact that F r is a symmetric relation, so F r(A, B) and F r(B, A) might have different values. Similarly, we have the “degenerate” nodes F r(A, A) and F r(B, B), since we did not enforce x (cid:4)= y in Equation 19.33. (If we add such constraints, then the model compiler, which generates the ground network, could avoid creating redundant nodes.)\n",
      "\n",
      "In summary, we can think of MLNs as a convenient way of specifying a UGM template, that can get unrolled to handle data of arbitrary size. There are several other ways to deﬁne relational probabilistic models; see e.g., (Koller and Friedman 2009; Kersting et al. 2011) for details. In some cases, there is uncertainty about the number or existence of objects or relations (the so-called open universe problem). Section 18.6.2 gives a concrete example in the context of multi-object tracking. See e.g., (Russell and Norvig 2010; Kersting et al. 2011) and references therein for further details.\n",
      "\n",
      "19.5\n",
      "\n",
      "Learning\n",
      "\n",
      "In this section, we discuss how to perform ML and MAP parameter estimation for MRFs. We will see that this is quite computationally expensive. For this reason, it is rare to perform Bayesian inference for the parameters of MRFs (although see (Qi et al. 2005)).\n",
      "\n",
      "19.5.1\n",
      "\n",
      "Training maxent models using gradient methods\n",
      "\n",
      "Consider an MRF in log-linear form:\n",
      "\n",
      "p(y|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "c\n",
      "\n",
      "θT c φc(y)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(19.37)\n",
      "\n",
      "19.5. Learning\n",
      "\n",
      "677\n",
      "\n",
      "where c indexes the cliques. The scaled log-likelihood is given by\n",
      "\n",
      "(cid:2)(θ) (cid:2) 1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "log p(yi|θ) =\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "c\n",
      "\n",
      "θT c φc(yi) − log Z(θ)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(19.38)\n",
      "\n",
      "Since MRFs are in the exponential family, we know that this function is convex in θ (see Section 9.2.3), so it has a unique global maximum which we can ﬁnd using gradient-based optimizers. In particular, the derivative for the weights of a particular clique, c, is given by\n",
      "\n",
      "∂(cid:2) ∂θc\n",
      "\n",
      "=\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "φc(yi) −\n",
      "\n",
      "∂ ∂θc\n",
      "\n",
      "log Z(θ)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(19.39)\n",
      "\n",
      "Exercise 19.1 asks you to show that the derivative of the log partition function wrt θc is the expectation of the c’th feature under the model, i.e.,\n",
      "\n",
      "∂ log Z(θ) ∂θc\n",
      "\n",
      "= E [φc(y)|θ] =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "y\n",
      "\n",
      "φc(y)p(y|θ)\n",
      "\n",
      "(19.40)\n",
      "\n",
      "Hence the gradient of the log likelihood is\n",
      "\n",
      "∂(cid:2) ∂θc\n",
      "\n",
      "=\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "φc(yi)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "− E [φc(y)]\n",
      "\n",
      "(19.41)\n",
      "\n",
      "In the ﬁrst term, we ﬁx y to its observed values; this is sometimes called the clamped term. In the second term, y is free; this is sometimes called the unclamped term or contrastive term. Note that computing the unclamped term requires inference in the model, and this must be done once per gradient step. This makes UGM training much slower than DGM training.\n",
      "\n",
      "The gradient of the log likelihood can be rewritten as the expected feature vector according\n",
      "\n",
      "to the empirical distribution minus the model’s expectation of the feature vector:\n",
      "\n",
      "∂(cid:2) ∂θc\n",
      "\n",
      "= Epemp [φc(y)] − Ep(·|θ) [φc(y)]\n",
      "\n",
      "(19.42)\n",
      "\n",
      "At the optimum, the gradient will be zero, so the empirical distribution of the features will match the model’s predictions:\n",
      "\n",
      "Epemp [φc(y)] = Ep(·|θ) [φc(y)]\n",
      "\n",
      "(19.43)\n",
      "\n",
      "This is called moment matching. This observation motivates a different optimization algorithm which we discuss in Section 19.5.7.\n",
      "\n",
      "19.5.2\n",
      "\n",
      "Training partially observed maxent models\n",
      "\n",
      "Suppose we have missing data and/or hidden variables in our model. represent such models as follows:\n",
      "\n",
      "In general, we can\n",
      "\n",
      "p(y, h|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "exp(\n",
      "\n",
      "c\n",
      "\n",
      "θT c φc(h, y))\n",
      "\n",
      "(19.44)\n",
      "\n",
      "678\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "The log likelihood has the form\n",
      "\n",
      "(cid:2)(θ) =\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "log\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "hi\n",
      "\n",
      "p(yi, hi|θ)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "=\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "log\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "hi\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "˜p(yi, hi|θ)\n",
      "\n",
      "(19.45)\n",
      "\n",
      "where\n",
      "\n",
      "˜p(y, h|θ) (cid:2) exp\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "θT c φc(h, y)\n",
      "\n",
      "(19.46)\n",
      "\n",
      "hi ˜p(yi, hi|θ) is the same as the partition function is the unnormalized distribution. The term for the whole model, except that y is ﬁxed at yi. Hence the gradient is just the expected features where we clamp yi, but average over h:\n",
      "\n",
      "∂ ∂θc\n",
      "\n",
      "log\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "hi\n",
      "\n",
      "˜p(yi, hi|θ)\n",
      "\n",
      "c\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "= E [φc(h, yi)|θ]\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(19.47)\n",
      "\n",
      "So the overall gradient is given by\n",
      "\n",
      "∂(cid:2) ∂θc\n",
      "\n",
      "=\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "{E [φc(h, yi)|θ] − E [φc(h, y)|θ]}\n",
      "\n",
      "(19.48)\n",
      "\n",
      "The ﬁrst set of expectations are computed by “clamping” the visible nodes to their observed values, and the second set are computed by letting the visible nodes be free. In both cases, we marginalize over hi.\n",
      "\n",
      "An alternative approach is to use generalized EM, where we use gradient methods in the M\n",
      "\n",
      "step. See (Koller and Friedman 2009, p956) for details.\n",
      "\n",
      "19.5.3\n",
      "\n",
      "Approximate methods for computing the MLEs of MRFs\n",
      "\n",
      "When ﬁtting a UGM there is (in general) no closed form solution for the ML or the MAP estimate of the parameters, so we need to use gradient-based optimizers. This gradient requires inference. In models where inference is intractable, learning also becomes intractable. This has motivated various computationally faster alternatives to ML/MAP estimation, which we list in Table 19.1. We dicsuss some of these alternatives below, and defer others to later sections.\n",
      "\n",
      "19.5.4\n",
      "\n",
      "Pseudo likelihood\n",
      "\n",
      "One alternative to MLE is to maximize the pseudo likelihood (Besag 1975), deﬁned as follows:\n",
      "\n",
      "(cid:2)P L(θ) (cid:2)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "y\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "d=1\n",
      "\n",
      "pemp(y log p(yd|y−d) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "d=1\n",
      "\n",
      "log p(yid|yi,−d, θ)\n",
      "\n",
      "(19.49)\n",
      "\n",
      "That is, we optimize the product of the full conditionals, also known as the composite likeli- hood (Lindsay 1988), Compare this to the objective for maximum likelihood:\n",
      "\n",
      "(cid:2)M L(θ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "pemp(y log p(y|θ) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "log p(yi|θ)\n",
      "\n",
      "(19.50)\n",
      "\n",
      "y,x\n",
      "\n",
      "i=1\n",
      "\n",
      "19.5. Learning\n",
      "\n",
      "679\n",
      "\n",
      "Method Closed form IPF Gradient-based optimization Max-margin training Pseudo-likelihood Stochastic ML Contrastive divergence Minimum probability ﬂow\n",
      "\n",
      "Restriction Only Chordal MRF Only Tabular / Gaussian MRF Low tree width Only CRFs No hidden variables - - Can integrate out the hiddens\n",
      "\n",
      "Exact MLE? Exact Exact Exact N/A Approximate Exact (up to MC error) Approximate Approximate\n",
      "\n",
      "Section Section 19.5.7.4 Section 19.5.7 Section 19.5.1 Section 19.7 Section 19.5.4 Section 19.5.5 Section 27.7.2.4 Sohl-Dickstein et al. (2011)\n",
      "\n",
      "Table 19.1 Some methods that can be used to compute approximate ML/ MAP parameter estimates for MRFs/ CRFs. Low tree-width means that, in order for the method to be efficient, the graph must “tree-like”; see Section 20.5 for details.\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 19.13 (a) A small 2d lattice. observed neighbors. Based on Figure 2.2 of (Carbonetto 2003).\n",
      "\n",
      "(b) The representation used by pseudo likelihood. Solid nodes are\n",
      "\n",
      "In the case of Gaussian MRFs, PL is equivalent to ML (Besag 1975), but this is not true in general (Liang and Jordan 2008).\n",
      "\n",
      "The PL approach is illustrated in Figure 19.13 for a 2d grid. We learn to predict each node, given all of its neighbors. This objective is generally fast to compute since each full conditional p(yid|yi,−d, θ) only requires summing over the states of a single node, yid, in order to compute the local normalization constant. The PL approach is similar to ﬁtting each full conditional separately (which is the method used to train dependency networks, discussed in Section 26.2.2), except that the parameters are tied between adjacent nodes.\n",
      "\n",
      "One problem with PL is that it is hard to apply to models with hidden variables (Parise and Welling 2005). Another more subtle problem is that each node assumes that its neighbors have If node t ∈ nbr(s) is a perfect predictor for node s, then s will learn to rely known values. completely on node t, even at the expense of ignoring other potentially useful information, such as its local evidence.\n",
      "\n",
      "However, experiments in (Parise and Welling 2005; Hoeﬂing and Tibshirani 2009) suggest that PL works as well as exact ML for fully observed Ising models, and of course PL is much faster.\n",
      "\n",
      "19.5.5\n",
      "\n",
      "Stochastic maximum likelihood\n",
      "\n",
      "Recall that the gradient of the log-likelihood for a fully observed MRF is given by\n",
      "\n",
      "∇θ(cid:2)(θ) =\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "[φ(yi) − E [φ(y)]]\n",
      "\n",
      "(19.51)\n",
      "\n",
      "680\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "The gradient for a partially observed MRF is similar. In both cases, we can approximate the model expectations using Monte Carlo sampling. We can combine this with stochastic gradient descent (Section 8.5.2), which takes samples from the empirical distribution. Pseudocode for the resulting method is shown in Algorithm 3.\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "13\n",
      "\n",
      "11\n",
      "\n",
      "Algorithm 19.1: Stochastic maximum likelihood for ﬁtting an MRF 1 Initialize weights θ randomly; 2 k = 0, η = 1 ; 3 for each epoch do 4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "for each minibatch of size B do\n",
      "\n",
      "for each sample s = 1 : S do Sample ys,k ∼ p(y|θk) ; (cid:7)S s=1 φ(ys,k);\n",
      "\n",
      "ˆE(φ(y)) = 1 S for each training case i in minibatch do\n",
      "\n",
      "gk = 1 i∈B gik; B θk+1 = θk − ηgk; k = k + 1; Decrease step size η;\n",
      "\n",
      "gik = φ(yi) − ˆE(φ(y)) ;\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Typically we use MCMC to generate the samples. Of course, running MCMC to convergence at each step of the inner loop would be extremely slow. Fortunately, it was shown in (Younes 1989) that we can start the MCMC chain at its previous value, and just take a few steps. In otherwords, we sample ys,k by initializing the MCMC chain at ys,k−1, and then run for a few iterations. This is valid since p(y|θk) is likely to be close to p(y|θk−1), since we only changed the parameters a small amount. We call this algorithm stochastic maximum likelihood or SML. (There is a closely related algorithm called persistent contrastive divergence which we discuss in Section 27.7.2.5.)\n",
      "\n",
      "19.5.6\n",
      "\n",
      "Feature induction for maxent models *\n",
      "\n",
      "MRFs require a good set of features. One unsupervised way to learn such features, known as feature induction, is to start with a base set of features, and then to continually create new feature combinations out of old ones, greedily adding the best ones to the model. This approach was ﬁrst proposed in (Pietra et al. 1997; Zhu et al. 1997), and was later extended to the CRF case in (McCallum 2003).\n",
      "\n",
      "To illustrate the basic idea, we present an example from (Pietra et al. 1997), which described how to build unconditional probabilistic models to represent English spelling. Initially the model has no features, which represents the uniform distribution. The algorithm starts by choosing to add the feature (cid:4)\n",
      "\n",
      "φ1(y) =\n",
      "\n",
      "I(yt ∈ {a, . . . , z})\n",
      "\n",
      "(19.52)\n",
      "\n",
      "t\n",
      "\n",
      "19.5. Learning\n",
      "\n",
      "681\n",
      "\n",
      "which checks if any letter is lower case or not. After the feature is added, the parameters are (re)-ﬁt by maximum likelihood. For this feature, it turns out that ˆθ1 = 1.944, which means that a word with a lowercase letter in any position is about e1.944 ≈ 7 times more likely than the same word without a lowercase letter in that position. Some samples from this model, generated using (annealed) Gibbs sampling (Section 24.2), are shown below.8\n",
      "\n",
      "m, r, xevo, ijjiir, b, to, jz, gsr, wq, vf, x, ga, msmGh, pcp, d, oziVlal, hzagh, yzop, io, advzmxnv, ijv_bolft, x, emx, kayerf, mlj, rawzyb, jp, ag, ctdnnnbg, wgdw, t, kguv, cy, spxcq, uzflbbf, dxtkkn, cxwx, jpd, ztzh, lv, zhpkvnu, l^, r, qee, nynrx, atze4n, ik, se, w, lrh, hp+, yrqyka’h, zcngotcnx, igcump, zjcjs, lqpWiqu, cefmfhc, o, lb, fdcY, tzby, yopxmvk, by, fz„ t, govyccm, ijyiduwfzo, 6xr, duh, ejv, pk, pjw, l, fl, w\n",
      "\n",
      "The second feature added by the algorithm checks if two adjacent characters are lower case:\n",
      "\n",
      "φ2(y) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "I(ys ∈ {a, . . . , z}, yt ∈ {a, . . . , z})\n",
      "\n",
      "(19.53)\n",
      "\n",
      "s∼t\n",
      "\n",
      "Now the model has the form\n",
      "\n",
      "p(y) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "exp(θ1φ1(y) +θ 2φ2(y))\n",
      "\n",
      "(19.54)\n",
      "\n",
      "Continuing in this way, the algorithm adds features for the strings s> and ing>, where > represents the end of word, and for various regular expressions such as [0-9], etc. Some samples from the model with 1000 features, generated using (annealed) Gibbs sampling, are shown below.\n",
      "\n",
      "was, reaser, in, there, to, will, „ was, by, homes, thing, be, reloverated, ther, which, conists, at, fores, anditing, with, Mr., proveral, the, „ ***, on’t, prolling, prothere, „ mento, at, yaou, 1, chestraing, for, have, to, intrally, of, qut, ., best, compers, ***, cluseliment, uster, of, is, deveral, this, thise, of, offect, inatever, thifer, constranded, stater, vill, in, thase, in, youse, menttering, and, ., of, in, verate, of, to\n",
      "\n",
      "This approach of feature learning can be thought of as a form of graphical model structure learning (Chapter 26), except it is more ﬁne-grained: we add features that are useful, regardless of the resulting graph structure. However, the resulting graphs can become densely connected, which makes inference (and hence parameter estimation) intractable.\n",
      "\n",
      "19.5.7\n",
      "\n",
      "Iterative proportional ﬁtting (IPF) *\n",
      "\n",
      "Consider a pairwise MRF where the potentials are represented as tables, with one parameter per variable setting. We can represent this in log-linear form using\n",
      "\n",
      "ψst(ys, yt) = exp\n",
      "\n",
      "!\n",
      "\n",
      "θT st[I(ys = 1, yt = 1), . . . , I(ys = K, yt = K)]\n",
      "\n",
      "#\n",
      "\n",
      "(19.55)\n",
      "\n",
      "and similarly for ψt(yt). Thus the feature vectors are just indicator functions.\n",
      "\n",
      "8. We thank John Lafferty for sharing this example.\n",
      "\n",
      "682\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "From Equation 19.43, we have that, at the maximum of the likelihood, the empirical expectation\n",
      "\n",
      "of the features equals the model’s expectation:\n",
      "\n",
      "Epemp [I(ys = j, yt = k)] = Ep(·|θ) [I(ys = j, yt = k)]\n",
      "\n",
      "(19.56)\n",
      "\n",
      "pemp(ys = j, yt = k) =p(\n",
      "\n",
      "ys = j, yt = k|θ)\n",
      "\n",
      "(19.57)\n",
      "\n",
      "where pemp is the empirical probability:\n",
      "\n",
      "pemp(ys = j, yt = k) =\n",
      "\n",
      "Nst,jk N\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "n=1 I(yns = j, ynt = k) N\n",
      "\n",
      "(19.58)\n",
      "\n",
      "For a general graph, the condition that must hold at the optimum is\n",
      "\n",
      "pemp(yc) = p(yc|θ)\n",
      "\n",
      "(19.59)\n",
      "\n",
      "For a special family of graphs known as decomposable graphs (deﬁned in Section 20.4.1), one can show that p(yc|θ) =ψ c(yc). However, even if the graph is not decomposable, we can imagine trying to enforce this condition. This suggests an iterative coordinate ascent scheme where at each step we compute\n",
      "\n",
      "ψt+1 c\n",
      "\n",
      "(yc) = ψt\n",
      "\n",
      "c(yc) ×\n",
      "\n",
      "pemp(yc) p(yc|ψt)\n",
      "\n",
      "(19.60)\n",
      "\n",
      "where the multiplication is elementwise. This is known as iterative proportional ﬁtting or IPF (Fienberg 1970; Bishop et al. 1975). See Algorithm 7 for the pseudocode.\n",
      "\n",
      "Algorithm 19.2: Iterative Proportional Fitting algorithm for tabular MRFs 1 Initialize ψc = 1 for c = 1 :C ; 2 repeat 3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "for c = 1 :C do pc = p(yc|ψ); ˆpc = pemp(yc); ψc = ψc ∗ ˆpc pc ;\n",
      "\n",
      "7 until converged;\n",
      "\n",
      "19.5.7.1\n",
      "\n",
      "Example\n",
      "\n",
      "Let us consider a simple example from http://en.wikipedia.org/wiki/Iterative_propo rtional_fitting. We have two binary variables, Y1 and Y2, where Yn1 = 1 if man n is left handed, and Yn1 = 0 otherwise; similarly, Yn2 = 1 if woman n is left handed, and Yn2 = 0 otherwise. We can summarize the data using the following 2 × 2 contingency table: right-handed 43 44 87\n",
      "\n",
      "male female Total\n",
      "\n",
      "left-handed 9 4 13\n",
      "\n",
      "Total 52 48 100\n",
      "\n",
      "19.5. Learning\n",
      "\n",
      "683\n",
      "\n",
      "Suppose we want to ﬁt a disconnected graphical model containing nodes Y1 and Y2 but with no edge between them. That is, we want to ﬁnd vectors ψ1 and ψ2 such that M (cid:2) ψ1ψT 2 ≈ C, where M are the model’s expected counts, and C are the empirical counts. By moment matching, we ﬁnd that the row and column sums of the model must exactly match the row and column sums of the data. One possible solution is to use ψ1 = [0.5200, 0.4800] and ψ2 = [87, 13]. Below we show the model’s predictions, M = ψ1ψT 2 . Total 52 48 100\n",
      "\n",
      "male female Total\n",
      "\n",
      "right-handed 45.24 41.76 87\n",
      "\n",
      "left-handed 6.76 6.24 13\n",
      "\n",
      "It is easy to see that this matches the required constraints. See IPFdemo2x2 for some Matlab\n",
      "\n",
      "code that computes these numbers. This method is easily to generalized to arbitrary graphs.\n",
      "\n",
      "19.5.7.2\n",
      "\n",
      "Speed of IPF\n",
      "\n",
      "IPF is a ﬁxed point algorithm for enforcing the moment matching constraints and is guaranteed to converge to the global optimum (Bishop et al. 1975). The number of iterations depends on the form of the model. If the graph is decomposable, then IPF converges in a single iteration, but in general, IPF may require many iterations.\n",
      "\n",
      "It is clear that the dominant cost of IPF is computing the required marginals under the model. Efficient methods, such as the junction tree algorithm (Section 20.4), can be used, resulting in something called efficient IPF (Jirousek and Preucil 1995).\n",
      "\n",
      "Nevertheless, coordinate descent can be slow. An alternative method is to update all the parameters at once, by simply following the gradient of the likelihood. This gradient approach has the further signiﬁcant advantage that it works for models in which the clique potentials may not be fully parameterized, i.e., the features may not consist of all possible indicators for each clique, but instead can be arbitrary. Although it is possible to adapt IPF to this setting of general features, resulting in a method known as iterative scaling, in practice the gradient method is much faster (Malouf 2002; Minka 2003).\n",
      "\n",
      "19.5.7.3\n",
      "\n",
      "Generalizations of IPF\n",
      "\n",
      "We can use IPF to ﬁt Gaussian graphical models: instead of working with empirical counts, we work with empirical means and covariances (Speed and Kiiveri 1986). It is also possible to create a Bayesian IPF algorithm for sampling from the posterior of the model’s parameters (see e.g., (Dobra and Massam 2010)).\n",
      "\n",
      "19.5.7.4\n",
      "\n",
      "IPF for decomposable graphical models\n",
      "\n",
      "There is a special family of undirected graphical models known as decomposable graphical models. This is formally deﬁned in Section 20.4.1, but the basic idea is that it contains graphs which are “tree-like”. Such graphs can be represented by UGMs or DGMs without any loss of information.\n",
      "\n",
      "In the case of decomposable graphical models, IPF converges in one iteration.\n",
      "\n",
      "In fact, the\n",
      "\n",
      "684\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "MLE has a closed form solution (Lauritzen 1996). In particular, for tabular potentials we have\n",
      "\n",
      "ˆψc(yc = k) =\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 I(yi,c = k) N\n",
      "\n",
      "(19.61)\n",
      "\n",
      "and for Gaussian potentials, we have\n",
      "\n",
      "ˆμc =\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 yic N\n",
      "\n",
      ", ˆΣc =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "i(yic − ˆμc)(xic − ˆμc)T N\n",
      "\n",
      "(19.62)\n",
      "\n",
      "By using conjugate priors, we can also easily compute the full posterior over the model pa- rameters in the decomposable case, just as we did in the DGM case. See (Lauritzen 1996) for details.\n",
      "\n",
      "19.6\n",
      "\n",
      "Conditional random ﬁelds (CRFs)\n",
      "\n",
      "A conditional random ﬁeld or CRF (Lafferty et al. 2001), sometimes a discriminative random ﬁeld (Kumar and Hebert 2003), is just a version of an MRF where all the clique potentials are conditioned on input features: (cid:20)\n",
      "\n",
      "p(y|x, w) =\n",
      "\n",
      "1 Z(x, w)\n",
      "\n",
      "c\n",
      "\n",
      "ψc(yc|x, w)\n",
      "\n",
      "(19.63)\n",
      "\n",
      "A CRF can be thought of as a structured output extension of logistic regression. We will usually assume a log-linear representation of the potentials:\n",
      "\n",
      "ψc(yc|x, w) = exp(wT\n",
      "\n",
      "c φ(x, yc))\n",
      "\n",
      "(19.64)\n",
      "\n",
      "where φ(x, yc) is a feature vector derived from the global inputs x and the local set of labels yc. We will give some examples below which will make this notation clearer.\n",
      "\n",
      "The advantage of a CRF over an MRF is analogous to the advantage of a discriminative classiﬁer over a generative classiﬁer (see Section 8.6), namely, we don’t need to “waste resources” modeling things that we always observe. Instead we can focus our attention on modeling what we care about, namely the distribution of labels given the data.\n",
      "\n",
      "Another important advantage of CRFs is that we can make the potentials (or factors) of the model be data-dependent. For example, in image processing applications, we may “turn off” the label smoothing between two neighboring nodes s and t if there is an observed discontinuity in the image intensity between pixels s and t. Similarly, in natural language processing problems, we can make the latent labels depend on global properties of the sentence, such as which language it is written in. It is hard to incorporate global features into generative models.\n",
      "\n",
      "The disadvantage of CRFs over MRFs is that they require labeled training data, and they are slower to train, as we explain in Section 19.6.3. This is analogous to the strengths and weaknesses of logistic regression vs naive Bayes, discussed in Section 8.6.\n",
      "\n",
      "19.6.1\n",
      "\n",
      "Chain-structured CRFs, MEMMs and the label-bias problem\n",
      "\n",
      "The most widely used kind of CRF uses a chain-structured graph to model correlation amongst neighboring labels. Such models are useful for a variety of sequence labeling tasks (see Sec- tion 19.6.2).\n",
      "\n",
      "19.6. Conditional random ﬁelds (CRFs)\n",
      "\n",
      "685\n",
      "\n",
      "yt−1\n",
      "\n",
      "yt\n",
      "\n",
      "yt+1\n",
      "\n",
      "xg\n",
      "\n",
      "xg\n",
      "\n",
      "yt−1\n",
      "\n",
      "yt\n",
      "\n",
      "yt+1\n",
      "\n",
      "yt−1\n",
      "\n",
      "yt\n",
      "\n",
      "yt+1\n",
      "\n",
      "xt−1\n",
      "\n",
      "xt\n",
      "\n",
      "xt+1\n",
      "\n",
      "xt−1\n",
      "\n",
      "xt\n",
      "\n",
      "xt+1\n",
      "\n",
      "xt−1\n",
      "\n",
      "xt\n",
      "\n",
      "xt+1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 19.14 Various models for sequential data. directed MEMM. (c) A discriminative undirected CRF.\n",
      "\n",
      "(a) A generative directed HMM. (b) A discriminative\n",
      "\n",
      "Traditionally, HMMs (discussed in detail in Chapter 17) have been used for such tasks. These\n",
      "\n",
      "are joint density models of the form\n",
      "\n",
      "p(x, y|w) =\n",
      "\n",
      "T(cid:20)\n",
      "\n",
      "p(yt|yt−1, w)p(xt|yt, w)\n",
      "\n",
      "(19.65)\n",
      "\n",
      "t=1\n",
      "\n",
      "where we have dropped the initial p(y1) term for simplicity. See Figure 19.14(a). If we observe both xt and yt for all t, it is very easy to train such models, using techniques described in Section 17.5.1.\n",
      "\n",
      "An HMM requires specifying a generative observation model, p(xt|yt, w), which can be difficult. Furthemore, each xt is required to be local, since it is hard to deﬁne a generative model for the whole stream of observations, x = x1:T .\n",
      "\n",
      "yt to xt, as in Figure 19.14(b). This deﬁnes a directed discriminative model of the form\n",
      "\n",
      "An obvious way to make a discriminative version of an HMM is to “reverse the arrows” from\n",
      "\n",
      "p(y|x, w) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "p(yt|yt−1, x, w)\n",
      "\n",
      "(19.66)\n",
      "\n",
      "t\n",
      "\n",
      "where x = (x1:T , xg), xg are global features, and xt are features speciﬁc to node t. (This partition into local and global is not necessary, but helps when comparing to HMMs.) This is called a maximum entropy Markov model or MEMM (McCallum et al. 2000; Kakade et al. 2002).\n",
      "\n",
      "An MEMM is simply a Markov chain in which the state transition probabilities are conditioned (It is therefore a special case of an input-output HMM, discussed in on the input features. Section 17.6.3.) This seems like the natural generalization of logistic regression to the structured- output setting, but it suffers from a subtle problem known (rather obscurely) as the label bias problem (Lafferty et al. 2001). The problem is that local features at time t do not inﬂuence states prior to time t. This follows by examining the DAG, which shows that xt is d-separated from yt−1 (and all earlier time points) by the v-structure at yt, which is a hidden child, thus blocking the information ﬂow.\n",
      "\n",
      "To understand what this means in practice, consider the part of speech (POS) tagging task. Suppose we see the word “banks”; this could be a verb (as in “he banks at BoA”), or a noun (as in “the river banks were overﬂowing”). Locally the POS tag for the word is ambiguous. However,\n",
      "\n",
      "686\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(e)\n",
      "\n",
      "Figure 19.15 Example of handwritten letter recognition. In the word ’brace’, the ’r’ and the ’c’ look very similar, but can be disambiguated using context. Source: (Taskar et al. 2003) . Used with kind permission of Ben Taskar.\n",
      "\n",
      "suppose that later in the sentence, we see the word “ﬁshing”; this gives us enough context to infer that the sense of “banks” is “river banks”. However, in an MEMM (unlike in an HMM and CRF), the “ﬁshing” evidence will not ﬂow backwards, so we will not be able to disambiguate “banks”.\n",
      "\n",
      "Now consider a chain-structured CRF. This model has the form\n",
      "\n",
      "p(y|x, w) =\n",
      "\n",
      "1 Z(x, w)\n",
      "\n",
      "T(cid:20)\n",
      "\n",
      "t=1\n",
      "\n",
      "ψ(yt|x, w)\n",
      "\n",
      "T −1(cid:20)\n",
      "\n",
      "t=1\n",
      "\n",
      "ψ(yt, yt+1|x, w)\n",
      "\n",
      "(19.67)\n",
      "\n",
      "From the graph in Figure 19.14(c), we see that the label bias problem no longer exists, since yt does not block the information from xt from reaching other yt(cid:2) nodes.\n",
      "\n",
      "The label bias problem in MEMMs occurs because directed models are locally normalized, meaning each CPD sums to 1. By contrast, MRFs and CRFs are globally normalized, which means that local factors do not need to sum to 1, since the partition function Z, which sums over all joint conﬁgurations, will ensure the model deﬁnes a valid distribution. However, this solution comes at a price: we do not get a valid probability distribution over y until we have seen the whole sentence, since only then can we normalize over all conﬁgurations. Consequently, CRFs are not as useful as DGMs (whether discriminative or generative) for online or real-time inference. Furthermore, the fact that Z depends on all the nodes, and hence all their parameters, makes CRFs much slower to train than DGMs, as we will see in Section 19.6.3.\n",
      "\n",
      "19.6.2\n",
      "\n",
      "Applications of CRFs\n",
      "\n",
      "CRFs have been applied to many interesting problems; we give a representative sample below. These applications illustrate several useful modeling tricks, and will also provide motivation for some of the inference techniques we will discuss in Chapter 20.\n",
      "\n",
      "19.6.2.1\n",
      "\n",
      "Handwriting recognition\n",
      "\n",
      "A natural application of CRFs is to classify hand-written digit strings, as illustrated in Figure 19.15. The key observation is that locally a letter may be ambiguous, but by depending on the (un- known) labels of one’s neighbors, it is possible to use context to reduce the error rate. Note that the node potential, ψt(yt|xt), is often taken to be a probabilistic discriminative classiﬁer,\n",
      "\n",
      "19.6. Conditional random ﬁelds (CRFs)\n",
      "\n",
      "687\n",
      "\n",
      "B\n",
      "\n",
      "I\n",
      "\n",
      "O\n",
      "\n",
      "O\n",
      "\n",
      "O\n",
      "\n",
      "B\n",
      "\n",
      "I\n",
      "\n",
      "O\n",
      "\n",
      "B\n",
      "\n",
      "I\n",
      "\n",
      "I\n",
      "\n",
      "NP\n",
      "\n",
      "ADJ\n",
      "\n",
      "N\n",
      "\n",
      "V\n",
      "\n",
      "IN\n",
      "\n",
      "V\n",
      "\n",
      "PRP\n",
      "\n",
      "N\n",
      "\n",
      "IN\n",
      "\n",
      "DT\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "POS\n",
      "\n",
      "British\n",
      "\n",
      "Airways\n",
      "\n",
      "rose\n",
      "\n",
      "after\n",
      "\n",
      "announcing\n",
      "\n",
      "its\n",
      "\n",
      "withdrawal\n",
      "\n",
      "from\n",
      "\n",
      "the\n",
      "\n",
      "UAL\n",
      "\n",
      "deal\n",
      "\n",
      "KEY\n",
      "\n",
      "B I O N ADJ\n",
      "\n",
      "Begin noun phrase Within noun phrase Not a noun phrase Noun Adjective\n",
      "\n",
      "V IN PRP DT\n",
      "\n",
      "Verb Preposition Possesive pronoun Determiner (e.g., a, an, the)\n",
      "\n",
      "Figure 19.16 A CRF for joint POS tagging and NP segmentation. Friedman 2009). Used with kind permission of Daphne Koller.\n",
      "\n",
      "Source: Figure 4.E.1 of (Koller and\n",
      "\n",
      "such as a neural network or RVM, that is trained on isolated letters, and the edge potentials, ψst(ys, yt), are often taken to be a language bigram model. Later we will discuss how to train all the potentials jointly.\n",
      "\n",
      "19.6.2.2\n",
      "\n",
      "Noun phrase chunking\n",
      "\n",
      "One common NLP task is noun phrase chunking, which refers to the task of segmenting a sentence into its distinct noun phrases (NPs). This is a simple example of a technique known as shallow parsing.\n",
      "\n",
      "In more detail, we tag each word in the sentence with B (meaning beginning of a new NP), I (meaning inside a NP), or O (meaning outside an NP). This is called BIO notation. For example, in the following sentence, the NPs are marked with brackets:\n",
      "\n",
      "B\n",
      "\n",
      "I\n",
      "\n",
      "O\n",
      "\n",
      "O\n",
      "\n",
      "O\n",
      "\n",
      "B\n",
      "\n",
      "I\n",
      "\n",
      "O\n",
      "\n",
      "B\n",
      "\n",
      "I\n",
      "\n",
      "I\n",
      "\n",
      "(British Airways) rose after announcing (its withdrawl) from (the UAI deal)\n",
      "\n",
      "(We need the B symbol so that we can distinguish I I, meaning two words within a single NP, from B B, meaning two separate NPs.)\n",
      "\n",
      "A standard approach to this problem would ﬁrst convert the string of words into a string of POS tags, and then convert the POS tags to a string of BIOs. However, such a pipeline method can propagate errors. A more robust approach is to build a joint probabilistic model of the form p(NP1:T , POS1:T |words1:T ). One way to do this is to use the CRF in Figure 19.16. The connections between adjacent labels encode the probability of transitioning between the B, I and O states, and can enforce constraints such as the fact that B must preceed I. The features are usually hand engineered and include things like: does this word begin with a capital letter, is this word followed by a full stop, is this word a noun, etc. Typically there are ∼ 1, 000 − 10, 000 features per node.\n",
      "\n",
      "observed and do not need to be summed over.\n",
      "\n",
      "The number of features has minimal impact on the inference time, since the features are increase in the cost of\n",
      "\n",
      "(There is a small\n",
      "\n",
      "688\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "B-PER\n",
      "\n",
      "I-PER\n",
      "\n",
      "OTH\n",
      "\n",
      "OTH\n",
      "\n",
      "OTH\n",
      "\n",
      "B-LOC\n",
      "\n",
      "I-LOC\n",
      "\n",
      "B-PER\n",
      "\n",
      "OTH\n",
      "\n",
      "OTH\n",
      "\n",
      "OTH\n",
      "\n",
      "OTH\n",
      "\n",
      "Mrs.\n",
      "\n",
      "Green\n",
      "\n",
      "spoke\n",
      "\n",
      "today\n",
      "\n",
      "in\n",
      "\n",
      "New\n",
      "\n",
      "York\n",
      "\n",
      "Green\n",
      "\n",
      "chairs\n",
      "\n",
      "the\n",
      "\n",
      "ﬁnance committee\n",
      "\n",
      "KEY\n",
      "\n",
      "B-PER I-PER B-LOC\n",
      "\n",
      "Begin person name Within person name Begin location name\n",
      "\n",
      "I-LOC OTH\n",
      "\n",
      "Within location name Not an entitiy\n",
      "\n",
      "( )\n",
      "\n",
      "Figure 19.17 A skip-chain CRF for named entity recognition. Source: Figure 4.E.1 of (Koller and Friedman 2009). Used with kind permission of Daphne Koller.\n",
      "\n",
      "evaluating potential functions with many features, but this is usually negligible; if not, one can use (cid:2)1 regularization to prune out irrelevant features.) However, the graph structure can have a dramatic effect on inference time. The model in Figure 19.16 is tractable, since it is essentially a “fat chain”, so we can use the forwards-backwards algorithm (Section 17.4.3) for exact inference in O(T |POS|2|NP|2) time, where |POS| is the number of POS tags, and |NP| is the number of NP tags. However, the seemingly similar graph in Figure 19.17, to be explained below, is computationally intractable.\n",
      "\n",
      "19.6.2.3\n",
      "\n",
      "Named entity recognition\n",
      "\n",
      "A task that is related to NP chunking is named entity extraction. Instead of just segmenting out noun phrases, we can segment out phrases to do with people and locations. Similar techniques are used to automatically populate your calendar from your email messages; this is called information extraction.\n",
      "\n",
      "A simple approach to this is to use a chain-structured CRF, but to expand the state space from BIO to B-Per, I-Per, B-Loc, I-Loc, and Other. However, sometimes it is ambiguous whether a word is a person, location, or something else. (Proper nouns are particularly difficult to deal with because they belong to an open class, that is, there is an unbounded number of possible names, unlike the set of nouns and verbs, which is large but essentially ﬁxed.) We can get better performance by considering long-range correlations between words. For example, we might add a link between all occurrences of the same word, and force the word to have the same tag in each occurence. (The same technique can also be helpful for resolving the identity of pronouns.) This is known as a skip-chain CRF. See Figure 19.17 for an illustration.\n",
      "\n",
      "We see that the graph structure itself changes depending on the input, which is an additional advantage of CRFs over generative models. Unfortunately, inference in this model is gener- ally more expensive than in a simple chain with local connections, for reasons explained in Section 20.5.\n",
      "\n",
      "19.6. Conditional random ﬁelds (CRFs)\n",
      "\n",
      "689\n",
      "\n",
      "Figure 19.18 Illustration of a simple parse tree based on a context free grammar in Chomsky normal form. The feature vector φ(x, y) = Ψ(x, y) counts the number of times each production rule was used. Source: Figure 5.2 of (Altun et al. 2006) . Used with kind permission of Yasemin Altun.\n",
      "\n",
      "19.6.2.4\n",
      "\n",
      "Natural language parsing\n",
      "\n",
      "A generalization of chain-structured models for language is to use probabilistic grammars. In particular, a probabilistic context free grammar or PCFG is a set of re-write or production rules of the form σ → σ(cid:2)σ(cid:2)(cid:2) or σ → x, where σ, σ(cid:2), σ(cid:2)(cid:2) ∈ Σ are non-terminals (analogous to parts of speech), and x ∈ X are terminals, i.e., words. See Figure 19.18 for an example. Each such rule has an associated probability. The resulting model deﬁnes a probability distribution over sequences of words. We can compute the probability of observing a particular sequence x = x1 . . . xT by summing over all trees that generate it. This can be done in O(T 3) time using the inside-outside algorithm; see e.g., (Jurafsky and Martin 2008; Manning and Schuetze 1999) for details.\n",
      "\n",
      "It is possible to make discriminative versions which encode the probability of a labeled tree, y, given a sequence of words, x, by using a CRF of the form p(y|x) ∝ exp(wT φ(x, y)). For example, we might deﬁne φ(x, y) to count the number of times each production rule was used (which is analogous to the number of state transitions in a chain-structured model). See e.g., (Taskar et al. 2004) for details.\n",
      "\n",
      "PCFGs are generative models.\n",
      "\n",
      "19.6.2.5\n",
      "\n",
      "Hierarchical classiﬁcation\n",
      "\n",
      "Suppose we are performing multi-class classiﬁcation, where we have a label taxonomy, which groups the classes into a hierarchy. We can encode the position of y within this hierarchy by deﬁning a binary vector φ(y), where we turn on the bit for component y and for all its children. This can be combined with input features φ(x) using a tensor product, φ(x, y) = φ(x) ⊗ φ(y). See Figure 19.19 for an example.\n",
      "\n",
      "This method is widely used for text classiﬁcation, where manually constructed taxnomies (such as the Open Directory Project at www.dmoz.org) are quite common. The beneﬁt is that information can be shared between the parameters for nearby categories, enabling generalization across classes.\n",
      "\n",
      "690\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "(cid:4)w, Ψ(x, 2)(cid:5) = (cid:4)w2, x(cid:5) + (cid:4)w6, x(cid:5) + (cid:4)w9, x(cid:5)\n",
      "\n",
      "Figure 19.19 Illustration of a simple label taxonomy, and how it can be used to compute a distributed representation for the label for class 2. In this ﬁgure, φ(x) = x, φ(y = 2) = Λ(2), φ(x, y) is denoted by Ψ(x, 2), and wT φ(x, y) is denoted by (cid:4)w, Ψ(x, 2)(cid:5). Source: Figure 5.1 of (Altun et al. 2006) . Used with kind permission of Yasemin Altun.\n",
      "\n",
      "19.6.2.6\n",
      "\n",
      "Protein side-chain prediction\n",
      "\n",
      "An interesting analog to the skip-chain model arises in the problem of predicting the structure of protein side chains. Each residue in the side chain has 4 dihedral angles, which are usually discretized into 3 values called rotamers. The goal is to predict this discrete sequence of angles, y, from the discrete sequence of amino acids, x.\n",
      "\n",
      "We can deﬁne an energy function E(x, y), where we include various pairwise interaction terms between nearby residues (elements of the y vector). This energy is usually deﬁned as a weighted sum of individual energy terms, E(x, y|w) = j=1 θjEj(x, y), where the Ej are energy contribution due to various electrostatic charges, hydrogen bonding potentials, etc, and w are the parameters of the model. See (Yanover et al. 2007) for details.\n",
      "\n",
      "Given the model, we can compute the most probable side chain conﬁguration using y∗ = argmin E(x, y|w). In general, this problem is NP-hard, depending on the nature of the graph induced by the Ej terms, due to long-range connections between the variables. Nevertheless, some special cases can be efficiently handled, using methods discussed in Section 22.6.\n",
      "\n",
      "(cid:7)D\n",
      "\n",
      "19.6.2.7\n",
      "\n",
      "Stereo vision\n",
      "\n",
      "Low-level vision problems are problems where the input is an image (or set of images), and the output is a processed version of the image. In such cases, it is common to use 2d lattice- structured models; the models are similar to Figure 19.9, except that the features can be global, and are not generated by the model. We will assume a pairwise CRF.\n",
      "\n",
      "A classic low-level vision problem is dense stereo reconstruction, where the goal is to estimate the depth of every pixel given two images taken from slightly different angles. In this section (based on (Sudderth and Freeman 2008)), we give a sketch of how a simple CRF can be used to solve this task. See e.g., (Sun et al. 2003) for a more sophisticated model.\n",
      "\n",
      "By using some standard preprocessing techniques, one can convert depth estimation into a\n",
      "\n",
      "19.6. Conditional random ﬁelds (CRFs)\n",
      "\n",
      "691\n",
      "\n",
      "problem of estimating the disparity ys between the pixel at location (is, js) in the left image and the corresponding pixel at location (is + ys, js) in the right image. We typically assume that corresponding pixels have similar intensity, so we deﬁne a local node potential of the form\n",
      "\n",
      "ψs(ys|x) ∝ exp\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "− 1 2σ2 (xL(is, js) − xR(is + ys, js))\n",
      "\n",
      "2\n",
      "\n",
      "%\n",
      "\n",
      "(19.68)\n",
      "\n",
      "where xL is the left image and xR is the right image. This equation can be generalized to model the intensity of small windows around each location. In highly textured regions, it is usually possible to ﬁnd the corresponding patch using cross correlation, but in regions of low texture, there will be considerable ambiguity about the correct value of ys.\n",
      "\n",
      "that neighboring disparities ys, yt should be similar, as follows:\n",
      "\n",
      "− 1 2γ2 (ys − yt)2 The resulting model is a Gaussian CRF.\n",
      "\n",
      "We can easily add a Gaussian prior on the edges of the MRF that encodes the assumption\n",
      "\n",
      "ψst(ys, yt) ∝ exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(19.69)\n",
      "\n",
      "However, using Gaussian edge-potentials will oversmooth the estimate, since this prior fails to account for the occasional large changes in disparity that occur between neighboring pixels which are on different sides of an occlusion boundary. One gets much better results using a truncated Gaussian potential of the form (cid:19)\n",
      "\n",
      "ψst(ys, yt) ∝ exp\n",
      "\n",
      "− 1\n",
      "\n",
      "2γ2 min\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(ys − yt)2, δ2 0\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "%\n",
      "\n",
      "(19.70)\n",
      "\n",
      "where γ encodes the expected smoothness, and δ0 encodes the maximum penalty that will be imposed if disparities are signiﬁcantly different. This is called a discontinuity preserving potential; note that such penalties are not convex. The local evidence potential can be made robust in a similar way, in order to handle outliers due to specularities, occlusions, etc.\n",
      "\n",
      "Figure 19.20 illustrates the difference between these two forms of prior. On the top left is an image from the standard Middlebury stereo benchmark dataset (Scharstein and Szeliski 2002). On the bottom left is the corresponding true disparity values. The remaining columns represent the estimated disparity after 0, 1 and an “inﬁnite” number of rounds of loopy belief propagation (see Section 22.2), where by “inﬁnite” we mean the results at convergence. The top row shows the results using a Gaussian edge potential, and the bottom row shows the results using the truncated potential. The latter is clearly better.\n",
      "\n",
      "Unfortunately, performing inference with real-valued variables is computationally difficult, unless the model is jointly Gaussian. Consequently, it is common to discretize the variables. (For example, Figure 19.20(bottom) used 50 states.) The edge potentials still have the form given in Equation 19.69. The resulting model is called a metric CRF, since the potentials form a metric. Inference in metric CRFs is more efficient than in CRFs where the discrete labels have no natural ordering, as we explain in Section 22.6.3.3. See Section 22.6.4 for a comparison of various approximate inference methods applied to low-level CRFs, and see (Blake et al. 2011; Prince 2012) for more details on probabilistic models for computer vision.\n",
      "\n",
      "9\n",
      "\n",
      "9. A function f is said to be a metric if it satisﬁes the following three properties: Reﬂexivity: f (a, b) = 0 iff a = b; If f satisﬁes only the ﬁrst two Symmetry: f (a, b) =f (b, a); and Triangle inequality: f (a, b) + f (b, c) ≥ f (a, c). properties, it is called a semi-metric.\n",
      "\n",
      "692\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:5)(cid:10)(cid:14)(cid:10)(cid:7)(cid:11)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "Figure 19.20 Illustration of belief propagation for stereo depth estimation. Left column: image and true disparities. Remaining columns: initial estimate, estimate after 1 iteration, and estimate at convergence. Top row: Gaussian edge potentials. Bottom row: robust edge potentials. Source: Figure 4 of (Sudderth and Freeman 2008). Used with kind permission of Erik Sudderth.\n",
      "\n",
      "19.6.3\n",
      "\n",
      "CRF training\n",
      "\n",
      "We can modify the gradient based optimization of MRFs described in Section 19.5.1 to the CRF case in a straightforward way. In particular, the scaled log-likelihood becomes\n",
      "\n",
      "(cid:2)(w) (cid:2) 1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "log p(yi|xi, w) =\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "c\n",
      "\n",
      "wT\n",
      "\n",
      "c φc(yi, xi) − log Z(w, xi)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(19.71)\n",
      "\n",
      "and the gradient becomes\n",
      "\n",
      "∂(cid:2) ∂wc\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "1 N\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i (cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "[φc(yi, xi) − E [φc(y, xi)]]\n",
      "\n",
      "φc(yi, xi) −\n",
      "\n",
      "∂ ∂wc\n",
      "\n",
      "log Z(w, xi)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(19.72)\n",
      "\n",
      "(19.73)\n",
      "\n",
      "Note that we now have to perform inference for every single training case inside each gradient step, which is O(N ) times slower than the MRF case. This is because the partition function depends on the inputs xi.\n",
      "\n",
      "In most applications of CRFs (and some applications of MRFs), the size of the graph structure can vary. Hence we need to use parameter tying to ensure we can deﬁne a distribution of arbitrary size. In the pairwise case, we can write the model as follows:\n",
      "\n",
      "p(y|x, w) =\n",
      "\n",
      "1 Z(w, x)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:22) wT φ(y, x)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(19.74)\n",
      "\n",
      "19.7. Structural SVMs\n",
      "\n",
      "693\n",
      "\n",
      "where w = [wn, we] are the node and edge parameters, and (cid:4)\n",
      "\n",
      "φ(y, x) (cid:2) [\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "φt(yt, x),\n",
      "\n",
      "φst(ys, yt, x)]\n",
      "\n",
      "(19.75)\n",
      "\n",
      "t\n",
      "\n",
      "s∼t\n",
      "\n",
      "are the summed node and edge features (these are the sufficient statistics). The gradient expression is easily modiﬁed to handle this case.\n",
      "\n",
      "In practice, it is important to use a prior/ regularization to prevent overﬁtting.\n",
      "\n",
      "If we use a\n",
      "\n",
      "Gaussian prior, the new objective becomes\n",
      "\n",
      "(cid:2)(cid:2)(w) (cid:2) 1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "log p(yi|xi, w) − λ||w||2 2\n",
      "\n",
      "(19.76)\n",
      "\n",
      "It is simple to modify the gradient expression.\n",
      "\n",
      "Alternatively, we can use (cid:2)1 regularization. For example, we could use (cid:2)1 for the edge weights we to learn a sparse graph structure, and (cid:2)2 for the node weights wn, as in (Schmidt et al. 2008). In other words, the objective becomes\n",
      "\n",
      "(cid:2)(cid:2)(w) (cid:2) 1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "log p(yi|xi, w) − λ1||we||1 − λ2||wn||2 2\n",
      "\n",
      "(19.77)\n",
      "\n",
      "Unfortunately, the optimization algorithms are more complicated when we use (cid:2)1 (see Sec- tion 13.4), although the problem is still convex.\n",
      "\n",
      "To handle large datasets, we can use stochastic gradient descent (SGD), as described in\n",
      "\n",
      "Section 8.5.2.\n",
      "\n",
      "It is possible (and useful) to deﬁne CRFs with hidden variables, for example to allow for an unknown alignment between the visible features and the hidden labels (see e.g., (Schnitzspan et al. 2010)). In this case, the objective function is no longer convex. Nevertheless, we can ﬁnd a locally optimal ML or MAP parameter estimate using EM and/ or gradient methods.\n",
      "\n",
      "19.7\n",
      "\n",
      "Structural SVMs\n",
      "\n",
      "We have seen that training a CRF requires inference, in order to compute the expected sufficient statistics needed to evaluate the gradient. For certain models, computing a joint MAP estimate of the states is provably simpler than computing marginals, as we discuss in Section 22.6. In this section, we discuss a way to train structured output classiﬁers that that leverages the existence of fast MAP solvers. (To avoid confusion with MAP estimation of parameters, we will often refer to MAP estimation of states as decoding.) These methods are known as structural support vector machines or SSVMs (Tsochantaridis et al. 2005). (There is also a very similar class of methods known as max margin Markov networks or M3nets (Taskar et al. 2003); see Section 19.7.2 for a discussion of the differences.)\n",
      "\n",
      "19.7.1\n",
      "\n",
      "SSVMs: a probabilistic view\n",
      "\n",
      "In this book, we have mostly concentrated on ﬁtting models using MAP parameter estimation, i.e., by minimizing functions of the form\n",
      "\n",
      "RM AP (w) = − log p(w) −\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "log p(yi|xi, w)\n",
      "\n",
      "(19.78)\n",
      "\n",
      "i=1\n",
      "\n",
      "694\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "However, at test time, we pick the label so as to minimize the posterior expected loss (deﬁned in Section 5.7):\n",
      "\n",
      "ˆy(x|w) = argmin\n",
      "\n",
      "ˆy\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "y\n",
      "\n",
      "L(ˆy, y)p(y|x, w)\n",
      "\n",
      "(19.79)\n",
      "\n",
      "where L(y∗, ˆy) is the loss we incur when we estimate ˆy but the truth is y∗. It therefore seems reasonable to take the loss function into account when performing parameter estimation.10 So, following (Yuille and He 2011), let us instead minimized the posterior expected loss on the training set:\n",
      "\n",
      "REL(w) (cid:2) − log p(w) +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "log\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "L(yi, y)p(y|xi, w)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(19.80)\n",
      "\n",
      "i=1\n",
      "\n",
      "y\n",
      "\n",
      "In the special case of 0-1 loss, L(yi, y) = 1 − δy,yi , this reduces to RM AP .\n",
      "\n",
      "We will assume that we can write our model in the following form:\n",
      "\n",
      "where Z(x, w) = this, we can rewrite our objective as follows:\n",
      "\n",
      "p(y|x, w) =\n",
      "\n",
      "REL(w) =− log p(w) +\n",
      "\n",
      "p(w) =\n",
      "\n",
      "= E(w) +\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "exp(wT φ(x, y)) Z(x, w) exp(−E(w)) Z\n",
      "\n",
      "y exp(wT φ(x, y)). Also, let us deﬁne L(yi, y) = exp ˜L(yi, y). With\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "− log Z(xi, w) + log\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "log\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "y\n",
      "\n",
      "exp ˜L(yi, y)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "exp\n",
      "\n",
      "exp(wT φ(x, y)) Z(x, w)\n",
      "\n",
      "!\n",
      "\n",
      "˜L(vyi, y) +w T φ(xi, y)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "# (19.84)\n",
      "\n",
      "(19.82)\n",
      "\n",
      "(19.83)\n",
      "\n",
      "(19.81)\n",
      "\n",
      "i\n",
      "\n",
      "y\n",
      "\n",
      "any function f (y) we have\n",
      "\n",
      "We will now consider various bounds in order to simplify this objective. First note that for\n",
      "\n",
      "max y∈Y\n",
      "\n",
      "f (y) ≤ log\n",
      "\n",
      "y∈Y\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "exp[f (y)] ≤ log\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "|Y| exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "max y\n",
      "\n",
      "f (y)\n",
      "\n",
      "(cid:9)(cid:18)\n",
      "\n",
      "= log |Y| + max\n",
      "\n",
      "y\n",
      "\n",
      "f (y) (19.85)\n",
      "\n",
      "For example, suppose Y = {0, 1, 2} and f (y) = y. Then we have\n",
      "\n",
      "2 = log[exp(2)] ≤ log[exp(0) + exp(1) + exp(2)] ≤ log[3 × exp(2)] = log(3) + 2 (19.86) We can ignore the log |Y| term, which is independent of y, and treat maxy∈Y f (y) as both a lower and upper bound. Hence we see that\n",
      "\n",
      "REL(w) ∼ E(w) +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "max y\n",
      "\n",
      "&\n",
      "\n",
      "˜L(yi, y) +w T φ(xi, y)\n",
      "\n",
      "’\n",
      "\n",
      "− max y\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "wT φ(xi, y)\n",
      "\n",
      "(19.87)\n",
      "\n",
      "10. Note that this violates the fundamental Bayesian distinction between inference and decision making. However, performing these tasks separately will only result in an optimal decision if we can compute the exact posterior. In most cases, this is intractable, so we need to perform loss-calibrated inference (Lacoste-Julien et al. 2011). In this section, (See also (Stoyanov et al. we just perform loss-calibrated MAP parameter estimation, which is computationally simpler. 2011).)\n",
      "\n",
      "19.7. Structural SVMs\n",
      "\n",
      "695\n",
      "\n",
      "where x ∼ y means c1 + x ≤ y + c2 for some constants c1, c2. Unfortunately, this objective is not convex in w. However, we can devise a convex upper bound by exploiting the following looser lower bound on the log-sum-exp function:\n",
      "\n",
      "f (y(cid:2)) ≤ log\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "exp[f (y)]\n",
      "\n",
      "(19.88)\n",
      "\n",
      "y\n",
      "\n",
      "for any y(cid:2) ∈ Y. Applying this equation to our earlier example, for f (y) = y and y(cid:2) = 1, we get 1 = log[exp(1)] ≤ log[exp(0) + exp(1) + exp(2)]. And applying this bound to REL we get\n",
      "\n",
      "REL(w) ≤ E(w) +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "max y\n",
      "\n",
      "&\n",
      "\n",
      "˜L(yi, y) +w T φ(xi, y)\n",
      "\n",
      "’\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "− wT φ(xi, yi)\n",
      "\n",
      "(19.89)\n",
      "\n",
      "If we set E(w) = − 1\n",
      "\n",
      "2C ||w||2\n",
      "\n",
      "2 (corresponding to a spherical Gaussian prior), we get\n",
      "\n",
      "RSSV M (w) (cid:2) 1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "max y\n",
      "\n",
      "&\n",
      "\n",
      "˜L(yi, y) +w T φ(xi, y)\n",
      "\n",
      "’\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "− wT φ(xi, yi)\n",
      "\n",
      "(19.90)\n",
      "\n",
      "This is the same objective as used in the SSVM approach of (Tsochantaridis et al. 2005). In the special case that Y = {−1, +1} L(y∗, y) = 1 − δy,y∗ , and φ(x, y) = 1 criterion reduces to the following (by considering the two cases that y = yi and y (cid:8)= yi):\n",
      "\n",
      "2 yx, this\n",
      "\n",
      "RSV M (w) (cid:2) 1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "max{0, 1 − yiwT xi}\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(19.91)\n",
      "\n",
      "which is the standard binary SVM objective (see Equation 14.57).\n",
      "\n",
      "So we see that the SSVM criterion can be seen as optimizing an upper bound on the Bayesian objective, a result ﬁrst shown in (Yuille and He 2011). This bound will be tight (and hence the approximation will be a good one) when ||w|| is large, since in that case, p(y|x, w) will concentrate its mass on argmaxy p(y|x, w). Unfortunately, a large ||w|| corresponds to a model that is likely to overﬁt, so it is unlikely that we will be working in this regime (because we will tune the strength of the regularizer to avoid this situation). An alternative justiﬁcation for the SVM criterion is that it focusses effort on ﬁtting parameters that affect the decision boundary. This is a better use of computational resources than ﬁtting the full distribution, especially when the model is wrong.\n",
      "\n",
      "19.7.2\n",
      "\n",
      "SSVMs: a non-probabilistic view\n",
      "\n",
      "We now present SSVMs in a more traditional (non-probabilistic) way, following (Tsochantaridis et al. 2005). The resulting objective will be the same as the one above. However, this derivation will set the stage for the algorithms we discuss below.\n",
      "\n",
      "Let f (x; w) = argmaxy∈Y wT φ(x, y) be the prediction function. We can obtain zero loss\n",
      "\n",
      "on the training set using this predictor if\n",
      "\n",
      "∀i. max y∈Y\\yi\n",
      "\n",
      "wT φ(xi, y) ≤ wT φ(xi, yi)\n",
      "\n",
      "(19.92)\n",
      "\n",
      "696\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "Each one of these nonlinear inequalities can be equivalently replaced by |Y| − 1 linear inequal- ities, resulting in a total of N |Y| − N linear constraints of the following form:\n",
      "\n",
      "∀i.∀y ∈ Y \\ yi.wT φ(xi, yi) − wT φ(xi, y) ≥ 0\n",
      "\n",
      "(19.93)\n",
      "\n",
      "For brevity, we introduce the notation\n",
      "\n",
      "δi(y) (cid:2) φ(xi, yi) − φ(xi, y)\n",
      "\n",
      "(19.94)\n",
      "\n",
      "so we can rewrite these constraints as wT δi(y) ≥ 0.\n",
      "\n",
      "If we can achieve zero loss, there will typically be multiple solution vectors w. We pick the\n",
      "\n",
      "one that maximizes the margin, deﬁned as\n",
      "\n",
      "γ (cid:2) min\n",
      "\n",
      "i\n",
      "\n",
      "f (x, yi; w) − max y(cid:2)∈Y\\y\n",
      "\n",
      "f (x, y(cid:2); w)\n",
      "\n",
      "(19.95)\n",
      "\n",
      "Since the margin can be made arbitrarily large by rescaling w, we ﬁx its norm to be 1, resulting in the optimization problem\n",
      "\n",
      "max γ,w:||w||=1\n",
      "\n",
      "s.t. ∀i.∀y ∈ Y \\ yi. wT δi(y) ≥ γ\n",
      "\n",
      "(19.96)\n",
      "\n",
      "Equivalently, we can write\n",
      "\n",
      "min w\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2\n",
      "\n",
      "s.t. ∀i.∀y ∈ Y \\ yi. wT δi(y) ≥ 1\n",
      "\n",
      "(19.97)\n",
      "\n",
      "To allow for the case where zero loss cannot be achieved (equivalent to the data being inseparable in the case of binary classiﬁcation), we relax the constraints by introducing slack terms ξi, one per data case. This yields\n",
      "\n",
      "min w,ξ\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "ξi\n",
      "\n",
      "s.t. ∀i.∀y ∈ Y \\ yi. wT δi(y) ≥ 1 − ξi, ξi ≥ 0\n",
      "\n",
      "(19.98)\n",
      "\n",
      "In the case of structured outputs, we don’t want to treat all constraint violations equally. For example, in a segmentation problem, getting one position wrong should be punished less than getting many positions wrong. One way to achieve this is to divide the slack variable by the size of the loss (this is called slack re-scaling). This yields\n",
      "\n",
      "min w,ξ\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "ξi\n",
      "\n",
      "s.t. ∀i.∀y ∈ Y \\ yi. wT δi(y) ≥ 1 −\n",
      "\n",
      "ξi L(yi, y)\n",
      "\n",
      ", ξi ≥ 0 (19.99)\n",
      "\n",
      "Alternatively, we can deﬁne the margin to be proportional to the loss (this is called margin re-rescaling). This yields\n",
      "\n",
      "min w,ξ\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "ξi\n",
      "\n",
      "s.t. ∀i.∀y ∈ Y \\ yi. wT δi(y) ≥ L(yi, y) − ξi, ξi ≥ 0(19.100)\n",
      "\n",
      "(In fact, we can write ∀y ∈ Y instead of ∀y ∈ Y \\ yi, since if y = yi, then wT δi(y) = 0 and ξi = 0. By using the simpler notation, which doesn’t exclude yi, we add an extra but redundant constraint.) This latter approach is used in M3nets.\n",
      "\n",
      "19.7. Structural SVMs\n",
      "\n",
      "697\n",
      "\n",
      "For future reference, note that we can solve for the ξ∗\n",
      "\n",
      "ξ∗ i (w) = max{0, max\n",
      "\n",
      "y\n",
      "\n",
      "(L(yi, y) − wT δi))} = max\n",
      "\n",
      "y\n",
      "\n",
      "i terms as follows: (L(yi, y) − wT δi))\n",
      "\n",
      "(19.101)\n",
      "\n",
      "Substituting in, and dropping the constraints, we get the following equivalent problem:\n",
      "\n",
      "min w\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "max y\n",
      "\n",
      "(\n",
      "\n",
      "L(yi, y) +w T φ(xi, y)\n",
      "\n",
      ")\n",
      "\n",
      "− wT φ(xi, yi)\n",
      "\n",
      "(19.102)\n",
      "\n",
      "19.7.2.1\n",
      "\n",
      "Empirical risk minimization\n",
      "\n",
      "Let us pause and consider whether the above objective is reasonable. Recall that in the frequen- tist approach to machine learning (Section 6.5), the goal is to minimize the regularized empirical risk, deﬁned by\n",
      "\n",
      "R(w) +\n",
      "\n",
      "C N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(yi, f (xi, w))\n",
      "\n",
      "(19.103)\n",
      "\n",
      "where R(w) is the regularizer, and f (xi, w) = argmaxy wT φ(xi, y) = ˆyi is the prediction. Since this objective is hard to optimize, because the loss is not differentiable, we will construct a convex upper bound instead.\n",
      "\n",
      "We can show that (cid:4)\n",
      "\n",
      "R(w) +\n",
      "\n",
      "C N\n",
      "\n",
      "i\n",
      "\n",
      "max y\n",
      "\n",
      "(L(yi, y) − wT δi))\n",
      "\n",
      "(19.104)\n",
      "\n",
      "is such a convex upper bound. To see this, note that\n",
      "\n",
      "L(yi, f (xi, w)) ≤ L(yi, f (xi, w)) − wT φ(xi, yi) +w T φ(xi, ˆyi)\n",
      "\n",
      "(19.105)\n",
      "\n",
      "≤ max\n",
      "\n",
      "y\n",
      "\n",
      "L(yi, y) − wT φ(xi, yi) +w T φ(xi, y)\n",
      "\n",
      "(19.106)\n",
      "\n",
      "Using this bound and R(w) = 1\n",
      "\n",
      "2 ||w||2 yields Equation 19.102.\n",
      "\n",
      "19.7.2.2\n",
      "\n",
      "Computational issues\n",
      "\n",
      "Although the above objectives are simple quadratic programs (QP), they have O(N |Y|) con- straints. This is intractable, since Y is usually exponentially large. In the case of the margin rescaling formulation, it is possible to reduce the exponential number of constraints to a poly- nomial number, provided the loss function and the feature vector decompose according to a graphical model. This is the approach used in M3nets (Taskar et al. 2003).\n",
      "\n",
      "An alternative approach is to work directly with the exponentially sized QP. This allows for the use of more general loss functions. There are several possible methods to make this feasible. One is to use cutting plane methods. Another is to use stochastic subgradient methods. We discuss both of these below.\n",
      "\n",
      "698\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "Fi\n",
      "\n",
      "C I Illustration of the cutting plane algorithm in 2d. We start with the estimate w = w0 = 0. Figure 19.21 (a) We add the ﬁrst constraint; the shaded region is the new feasible set. The new minimum norm solution is w1. (c) We add a third constraint. Source: Figure 5.3 of (Altun et al. 2006) . Used with kind permission of Yasemin Altun.\n",
      "\n",
      "(b) We add another constraint; the dark shaded region is the new feasible set.\n",
      "\n",
      "l\n",
      "\n",
      "l\n",
      "\n",
      "h\n",
      "\n",
      "S\n",
      "\n",
      "f h\n",
      "\n",
      "l\n",
      "\n",
      "l\n",
      "\n",
      "h\n",
      "\n",
      "19.7.3\n",
      "\n",
      "Cutting plane methods for ﬁtting SSVMs\n",
      "\n",
      "In this section, we discuss an efficient algorithm for ﬁtting SSVMs due to (Joachims et al. 2009). This method can handle general loss functions, and is implemented in the popular SVMstruct package11. The method is based on the cutting plane method from convex optimization (Kelley 1960).\n",
      "\n",
      "The basic idea is as follows. We start with an initial guess w and no constraints. At each iteration, we then do the following: for each example i, we ﬁnd the “most violated” constraint involving xi and ˆyi. If the loss-augmented margin violation exceeds the current value of ξi by more than (cid:16), we add ˆyi to the working set of constraints for this training case, Wi, and then solve the resulting new QP to ﬁnd the new w, ξ. See Figure 19.21 for a sketch, and Algorithm 11 for the pseudo code. (Since at each step we only add one new constraint, we can warm-start the QP solver.) We can can easily modify the algorithm to optimize the slack rescaling version by replacing the expression L(yi, y) − wT δi(ˆyi) with L(yi, y)(1 − wT δi(ˆyi)).\n",
      "\n",
      "The key to the efficiency of this method is that only polynomially many constraints need to be added, and as soon as they are, the exponential number of other constraints are guaranteed to also be satisﬁed to within a tolerance of (cid:16) (see (Tsochantaridis et al. 2005) for the proof).\n",
      "\n",
      "19.7.3.1\n",
      "\n",
      "Loss-augmented decoding\n",
      "\n",
      "The other key to efficiency is the ability to ﬁnd the most violated constraint in line 5 of the algorithm, i.e., to compute\n",
      "\n",
      "argmax y∈Y\n",
      "\n",
      "L(yi, y) − wT δi(y) = argmax\n",
      "\n",
      "y∈Y\n",
      "\n",
      "L(yi, y) + wT φ(xi, y)\n",
      "\n",
      "(19.107)\n",
      "\n",
      "11. http://svmlight.joachims.org/svm_struct.html\n",
      "\n",
      "19.7. Structural SVMs\n",
      "\n",
      "699\n",
      "\n",
      "Algorithm 19.3: Cutting plane algorithm for SSVMs (margin rescaling, N -slack version) 1 Input D = {(x1, y1), . . . , (xN , yn)}, C, (cid:16) ; 2 Wi = ∅, ξi = 0 for i = 1 : N ; 3 repeat 4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "for i = 1 : N do\n",
      "\n",
      "ˆyi = argmaxˆyi∈Y L(yi, y) − wT δi(ˆyi) ; if L(yi, y) − wT δi(ˆyi) > ξi + (cid:16) then\n",
      "\n",
      "Wi = Wi ∪ {ˆyi} ; (w, ξ) = argminw,ξ≥0\n",
      "\n",
      "s.t. ∀i = 1 :N, ∀y(cid:2) ∈ Wi : wT δi(ˆyi) ≥ L(yi, y(cid:2)) − ξi ;\n",
      "\n",
      "1\n",
      "\n",
      "2 ||w||2\n",
      "\n",
      "2 + C\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 ξi ;\n",
      "\n",
      "10 until no Wi has changed; 11 Return (w, ξ)\n",
      "\n",
      "We call this process loss-augmented decoding. (In (Joachims et al. 2009), this procedure is called the separation oracle.) If the loss function has an additive decomposition of the same form as the features, then we can fold the loss into the weight vector, i.e., we can ﬁnd a new set of parameters w(cid:2) such that (w(cid:2))T δi(y) = wT δi(y). We can then use a standard decoding algorithm, such as Viterbi, on the model p(y|x, w(cid:2)).\n",
      "\n",
      "In the special case of 0-1 loss, the optimum will either be the best solution, argmaxy wT φ(xi, y),\n",
      "\n",
      "with a value of of 0 − wT δi(ˆy), or it will be the second best solution, i.e.,\n",
      "\n",
      "˜y = argmax\n",
      "\n",
      "y(cid:5)=ˆy\n",
      "\n",
      "wT φ(xi, y)\n",
      "\n",
      "(19.108)\n",
      "\n",
      "which achieves an overall value of 1 − wT δi(˜y). For chain structured CRFs, we can use the Viterbi algorithm to do decoding; the second best path will differ from the best path in a single position, which can be obtained by changing the variable whose max marginal is closest to its decision boundary to its second best value. We can generalize this (with a bit more work) to ﬁnd the N -best list (Schwarz and Chow 1990; Nilsson and Goldberger 2001). t I(y∗\n",
      "\n",
      "t (cid:8)= yt), and for the F1 score (deﬁned in Section 5.7.2.3), we can devise a dynamic programming algorithm to compute Equation 19.107. See (Altun et al. 2006) for details. Other models and loss function combinations will require different methods.\n",
      "\n",
      "For Hamming loss, L(y∗, y) =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "19.7.3.2\n",
      "\n",
      "A linear time algorithm\n",
      "\n",
      "Although the above algorithm takes polynomial time, we can do better, and devise an algorithm that runs in linear time, assuming we use a linear kernel (i.e., we work with the original features φ(x, y) and do not apply the kernel trick). The basic idea, as explained in (Joachims et al. 2009), is to have a single slack variable, ξ, instead of N , but to use |Y|N constraints, instead of\n",
      "\n",
      "700\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "just N |Y|. Speciﬁcally, we optimize the following (assuming the margin rescaling formulation):\n",
      "\n",
      "min w,ξ≥0\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2\n",
      "\n",
      "2 + Cξ\n",
      "\n",
      "s.t. ∀(y1, . . . , yN ) ∈ Y N :\n",
      "\n",
      "1 N\n",
      "\n",
      "wT\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "δi(yi) ≥ 1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "L(yi, yi) − ξ(19.109)\n",
      "\n",
      "Compare this to the original version, which was\n",
      "\n",
      "min w,ξ≥0\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2\n",
      "\n",
      "2 +\n",
      "\n",
      "C N\n",
      "\n",
      "ξ\n",
      "\n",
      "s.t. ∀i = 1 :N, ∀y ∈ Y : wT δi(y) ≥ L(yi, yi) − ξi\n",
      "\n",
      "(19.110)\n",
      "\n",
      "One can show that any solution w∗ of Equation 19.109 is also a solution of Equation 19.110 and vice versa, with ξ∗ = 1\n",
      "\n",
      "N ξ∗ i .\n",
      "\n",
      "Algorithm 19.4: Cutting plane algorithm for SSVMs (margin rescaling, 1-slack version) 1 Input D = {(x1, y1), . . . , (xN , yn)}, C, (cid:16) ; 2 W = ∅; 3 repeat\n",
      "\n",
      "6\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "7\n",
      "\n",
      "for i = 1 : N do\n",
      "\n",
      "(w, ξ) = argminw,ξ≥0\n",
      "\n",
      "ˆyi = argmaxˆyi∈Y L(yi, ˆyi) +w T φ(xi, ˆyi)\n",
      "\n",
      "2 + C s.t. ∀(y1, . . . , yN ) ∈ W : 1\n",
      "\n",
      "1\n",
      "\n",
      "2 ||w||2\n",
      "\n",
      "N wT\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 ξ ; (cid:7)N\n",
      "\n",
      "i=1 δi(yi) ≥ 1 N\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 L(yi, yi) − ξ;\n",
      "\n",
      "9 until 10 Return (w, ξ)\n",
      "\n",
      "8\n",
      "\n",
      "W = W ∪ {(ˆy1, . . . , ˆyN )};\n",
      "\n",
      "1 N\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 L(yi, ˆyi) − 1\n",
      "\n",
      "N wT\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 δi(ˆyi) ≤ ξ + (cid:16);\n",
      "\n",
      "(This is what is implemented in SVMstruct.) The inner QP in line 4 can be solved in O(N ) time using the method of (Joachims 2006). In line 7 we make N calls to the loss-augmented decoder. Finally, it can be shown that the number of iterations is a constant independent on N . Thus the overall running time is linear.\n",
      "\n",
      "We can optimize Equation 19.109 using the cutting plane algorithm in Algorithm 10.\n",
      "\n",
      "19.7.4\n",
      "\n",
      "Online algorithms for ﬁtting SSVMs\n",
      "\n",
      "Although the cutting plane algorithm can be made to run in time linear in the number of data points, that can still be slow if we have a large dataset. In such cases, it is preferable to use online learning. We brieﬂy mention a few possible algorithms below.\n",
      "\n",
      "19.7.4.1\n",
      "\n",
      "The structured perceptron algorithm\n",
      "\n",
      "A very simple algorithm for ﬁtting SSVMs is the structured perceptron algorithm (Collins 2002). This method is an extension of the regular perceptron algorithm of Section 8.5.4. At each\n",
      "\n",
      "19.7. Structural SVMs\n",
      "\n",
      "701\n",
      "\n",
      "step, we compute ˆy = argmax p(y|x) (e.g., using the Viterbi algorithm) for the current training sample x. If ˆy = y, we do nothing, otherwise we update the weight vector using\n",
      "\n",
      "wk+1 = wk + φ(y, x) − φ(ˆy, x)\n",
      "\n",
      "(19.111)\n",
      "\n",
      "To get good performance, it is necessary to average the parameters over the last few updates (see Section 8.5.2 for details), rather than using the most recent value.\n",
      "\n",
      "19.7.4.2\n",
      "\n",
      "Stochastic subgradient descent\n",
      "\n",
      "The disadvantage of the structured perceptron algorithm is that it implicitly assumes 0-1 loss, and it does not enforce any kind of margin. An alternative approach is to perform stochastic subgradient descent. A speciﬁc instance of this the Pegasos algorithm (Shalev-Shwartz et al. 2007), which stands for “primal estimated sub-gradient solver for SVM”. Pegasos was designed for binary SVMs, but can be extended to SSVMS.\n",
      "\n",
      "Let us start by considering the objective function:\n",
      "\n",
      "f (w) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "max ˆyi\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "L(yi, ˆyi) +w T φ(xi, ˆyi)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "− wT φ(xi, yi) +λ||w ||2\n",
      "\n",
      "(19.112)\n",
      "\n",
      "Letting ˆyi be the argmax of this max. Then the subgradient of this objective function is\n",
      "\n",
      "g(w) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "φ(xi, ˆyi) − φ(xi, yi) + 2λw\n",
      "\n",
      "(19.113)\n",
      "\n",
      "i=1\n",
      "\n",
      "In stochastic subgradient descent, we approximate this gradient with a single term, i, and then perform an update:\n",
      "\n",
      "wk+1 = wk − ηkgi(wk) = wk − ηk[φ(xi, ˆyi) − φ(xi, yi) + (2/N )λw]\n",
      "\n",
      "(19.114)\n",
      "\n",
      "where ηk is the step size parameter, which should satisfy the Robbins-Monro conditions (Sec- (Notice that the perceptron algorithm is just a special case where λ = 0 and tion 8.5.2.1). ηk = 1.) To ensure that w has unit norm, we can project it onto the (cid:2)2 ball after each update.\n",
      "\n",
      "19.7.5\n",
      "\n",
      "Latent structural SVMs\n",
      "\n",
      "In many applications of interest, we have latent or hidden variables h. For example, in object detections problems, we may be told that the image contains an object, so y = 1, but we may not know where it is. The location of the object, or its pose, can be considered a hidden variable. Or in machine translation, we may know the source text x (say English) and the target text y (say French), but we typically do not know the alignment between the words.\n",
      "\n",
      "We will extend our model as follows, to get a latent CRF:\n",
      "\n",
      "p(y, h|x, w) =\n",
      "\n",
      "Z(x, w) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "exp(wT φ(x, y, h)) Z(x, w)\n",
      "\n",
      "exp(wT φ(x, y, h))\n",
      "\n",
      "(19.115)\n",
      "\n",
      "(19.116)\n",
      "\n",
      "y,h\n",
      "\n",
      "702\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "In addition, we introduce the loss function L(y∗, y, h); this measures the loss when the “action” that we take is to predict y using latent variables h. We could just use L(y∗, y) as before, since h is usually a nuisance variable and not of direct interest. However, h can sometimes play a useful role in deﬁning a loss function.12\n",
      "\n",
      "Given the loss function, we deﬁne our objective as\n",
      "\n",
      "⎡\n",
      "\n",
      "⎤\n",
      "\n",
      "REL(w) =− log p(w) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "log\n",
      "\n",
      "⎣\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "y,h\n",
      "\n",
      "exp ˜L(yi, y, h)\n",
      "\n",
      "exp(wT φ(x, y, h)) Z(x, w)\n",
      "\n",
      "⎦\n",
      "\n",
      "(19.117)\n",
      "\n",
      "Using the same loose lower bound as before, we get\n",
      "\n",
      "REL(w) ≤ E(w) +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "max y,h\n",
      "\n",
      "&\n",
      "\n",
      "˜L(yi, y, h) +w T φ(xi, y, h)\n",
      "\n",
      "’\n",
      "\n",
      "−\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "max h\n",
      "\n",
      "wT φ(xi, yi, h)\n",
      "\n",
      "(19.118)\n",
      "\n",
      "If we set E(w) =− 1 and Joachims 2009).\n",
      "\n",
      "2C ||w||2\n",
      "\n",
      "2, we get the same objective as is optimized in latent SVMs (Yu\n",
      "\n",
      "it is a difference of convex functions, and hence can be solved efficiently using the CCCP or concave-convex procedure (Yuille and Rangarajan 2003). This is a method for minimizing functions of the form f (w) − g(w), where f and g are convex. The method alternates between ﬁnding a linear upper bound u on −g, and then minimizing the convex function f (w) +u( w); see Algorithm 6 for the pseudocode. CCCP is guaranteed to decrease the objective at every iteration, and to converge to a local minimum or a saddle point.\n",
      "\n",
      "Unfortunately, this objective is no longer convex. However,\n",
      "\n",
      "Algorithm 19.5: Concave-Convex Procedure (CCCP) 1 Set t = 0 and initialize w0 ; 2 repeat 3\n",
      "\n",
      "Find hyperplane vt such that −g(w) ≤ −g(wt) + (w − wt)T vt for all w ; Solve wt+1 = argminw f (w) +w T vt ; Set t = t + 1 5 6 until converged;\n",
      "\n",
      "4\n",
      "\n",
      "When applied to latent SSVMs, CCCP is very similar to (hard) EM. In the “E step”, we compute\n",
      "\n",
      "12. For example, consider the problem of learning to classify a set of documents as relevant or not to a query. That is, given n documents x1, . . . ,x n for a single query q, we want to produce a labeling yj ∈ {−1, +1}, representing whether document j is relevant to q or not. Suppose our goal is to maximize the precision at k, which is a metric widely used in ranking (see Section 9.7.4). We will introduce a latent variable for each document hj representing its degree of relevance. This corresponds to a latent total ordering, that has to be consistent with the observed partial ordering y. Given this, we can deﬁne the following loss function: L(y, ˆy, ˆh) = min{1, n(y) j=1 I(yhj = 1), where k n(y) is the total number of relevant documents. This loss is essentially just 1 minus the precision@k, except we replace 1 with n(y)/k so that the loss will have a minimum of zero. See (Yu and Joachims 2009) for details.\n",
      "\n",
      "} − 1 k\n",
      "\n",
      "(cid:2)k\n",
      "\n",
      "19.7. Structural SVMs\n",
      "\n",
      "703\n",
      "\n",
      "the linear upper bound by setting vt = −C\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "i=1 φ(xi, yi, h∗\n",
      "\n",
      "i ), where\n",
      "\n",
      "hi = argmax\n",
      "\n",
      "h\n",
      "\n",
      "wT\n",
      "\n",
      "t φ(xi, yi, h)\n",
      "\n",
      "(19.119)\n",
      "\n",
      "In the “M step”, we estimate w using techniques for solving fully visible SSVMs. Speciﬁcally, we minimize\n",
      "\n",
      "1 2\n",
      "\n",
      "||w||2 + C\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "max y,h\n",
      "\n",
      "(\n",
      "\n",
      "L(yi, y, h) +w T φ(xi, y, h)\n",
      "\n",
      ")\n",
      "\n",
      "− C\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "wT φ(xi, yi, h∗ i )\n",
      "\n",
      "(19.120)\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 19.1 Derivative of the log partition function Derive Equation 19.40.\n",
      "\n",
      "Exercise 19.2 CI properties of Gaussian graphical models (Source: Jordan.) In this question, we study the relationship between sparse matrices and sparse graphs for Gaussian graphical models. Consider a multivariate Gaussian N (x|μ, Σ) in 3 dimensions. Suppose μ = (0, 0, 0)T throughout. Recall that for jointly Gaussian random variables, we know that Xi and Xj are independent iff they are uncorrelated, ie. Σij = 0. (This is not true in general, or even if Xi and Xj are Gaussian but not jointly Gaussian.) Also, Xi is conditionally independent of Xj given all the other variables iff Σ−1\n",
      "\n",
      "ij = 0.\n",
      "\n",
      "a. Suppose\n",
      "\n",
      "Σ =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "0.75 0.5 0.25\n",
      "\n",
      "0.5 1.0 0.5\n",
      "\n",
      "0.25 0.5 0.75\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "Are there any marginal independencies amongst X1, X2 and X3? What about conditional indepen- dencies? Hint: compute Σ−1 and expand out xT Σ−1x: which pairwise terms xixj are missing? Draw an undirected graphical model that captures as many of these independence statements (marginal and conditional) as possible, but does not make any false independence assertions.\n",
      "\n",
      "b. Suppose\n",
      "\n",
      "Σ =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "2 1 0\n",
      "\n",
      "1 2 1\n",
      "\n",
      "0 1 2\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "Are there any marginal independencies amongst X1, X2 and X3? Are there any conditional inde- pendencies amongst X1, X2 and X3? Draw an undirected graphical model that captures as many of these independence statements (marginal and conditional) as possible, but does not make any false independence assertions.\n",
      "\n",
      "c. Now suppose the distribution on X can be represented by the following DAG:\n",
      "\n",
      "X1 → X2 → X3\n",
      "\n",
      "Let the CPDs be as follows:\n",
      "\n",
      "P (X1) = N (X1; 0, 1), P (X2|x1) = N (X2; x1, 1), P (X3|x2) = N (X3; x2, 1)\n",
      "\n",
      "(19.121)\n",
      "\n",
      "Multiply these 3 CPDs together and complete the square (Bishop p101) to ﬁnd the corresponding joint distribution N (X1:3|μ, Σ). (You may ﬁnd it easier to solve for Σ−1 rather than Σ.)\n",
      "\n",
      "704\n",
      "\n",
      "Chapter 19. Undirected graphical models (Markov random ﬁelds)\n",
      "\n",
      "d. For the DAG model in the previous question: Are there any marginal independencies amongst X1, X2 and X3? What about conditional independencies? Draw an undirected graphical model that captures as many of these independence statements as possible, but does not make any false independence assertions (either marginal or conditional).\n",
      "\n",
      "Exercise 19.3 Independencies in Gaussian graphical models (Source: MacKay.)\n",
      "\n",
      "a. Consider the DAG X1 ← X2 → X3. Assume that all the CPDs are linear-Gaussian. Which of the\n",
      "\n",
      "following matrices could be the covariance matrix?\n",
      "\n",
      "A =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "9 3 1\n",
      "\n",
      "3 9 3\n",
      "\n",
      "1 3 9\n",
      "\n",
      "⎠ , B =\n",
      "\n",
      "⎞\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "8 −3 −3 1 −3\n",
      "\n",
      "9−3\n",
      "\n",
      "1\n",
      "\n",
      "8\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠ , C =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "9 3 0\n",
      "\n",
      "3 9 3\n",
      "\n",
      "0 3 9\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠ , D =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "9 −3 −3 0 −3\n",
      "\n",
      "10−3\n",
      "\n",
      "0\n",
      "\n",
      "9\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠(19.122)\n",
      "\n",
      "b. Which of the above matrices could be inverse covariance matrix? c. Consider the DAG X1 → X2 ← X3. Assume that all the CPDs are linear-Gaussian. Which of the\n",
      "\n",
      "above matrices could be the covariance matrix?\n",
      "\n",
      "d. Which of the above matrices could be the inverse covariance matrix? e. Let three variables x1, x2, x4 have covariance matrix Σ(1:3) and precision matrix Ω(1:3) = Σ−1\n",
      "\n",
      "(1:3) as\n",
      "\n",
      "follows\n",
      "\n",
      "Σ(1:3) =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "1 0.5 0\n",
      "\n",
      "0.5 1 0.5\n",
      "\n",
      "0 0.5 1\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠ , Ω(1:3) =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "1.5 −1 −1 0.5 −1\n",
      "\n",
      "2−1\n",
      "\n",
      "0.5\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "1.5\n",
      "\n",
      "(19.123)\n",
      "\n",
      "Now focus on x1 and x2. Which of the following statements about their covariance matrix Σ(1:2) and precision matrix Ω(1:2) are true?\n",
      "\n",
      "A : Σ(1:2) =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "1 0.5\n",
      "\n",
      "0.5 1\n",
      "\n",
      "(cid:4)\n",
      "\n",
      ", B : Ω(1:2) =\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "1.5 −1 −1 2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(19.124)\n",
      "\n",
      "Exercise 19.4 Cost of training MRFs and CRFs (Source: Koller.) Consider the process of gradient-ascent training for a log-linear model with k features, given a data set with N training instances. Assume for simplicity that the cost of computing a single feature over a single instance in our data set is constant, as is the cost of computing the expected value of each feature once we compute a marginal over the variables in its scope. Assume that it takes c time to compute all the marginals for each data case. Also, assume that we need r iterations for the gradient process to converge.\n",
      "\n",
      "Using this notation, what is the time required to train an MRF in big-O notation?\n",
      "\n",
      "Using this notation, what is the time required to train a CRF in big-O notation?\n",
      "\n",
      "Exercise 19.5 Full conditional in an Ising model Consider an Ising model\n",
      "\n",
      "p(x1, . . . , xn|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "<ij>\n",
      "\n",
      "exp(Jijxixj)\n",
      "\n",
      "n(cid:13)\n",
      "\n",
      "i=1\n",
      "\n",
      "exp(hixi)\n",
      "\n",
      "(19.125)\n",
      "\n",
      "where < ij > denotes all unique pairs (i.e., all edges), Jij ∈ R is the coupling strength (weight) on edge i − j, hi ∈ R is the local evidence (bias term), and θ = (J, h) are all the parameters.\n",
      "\n",
      "19.7. Structural SVMs\n",
      "\n",
      "705\n",
      "\n",
      "If xi ∈ {0, 1}, derive an expression for the full conditional\n",
      "\n",
      "p(xi = 1|x−i, θ) = p(xi = 1|xnbi , θ)\n",
      "\n",
      "(19.126)\n",
      "\n",
      "where x−i are all nodes except i, and nbi are the neighbors of i in the graph. Hint: you answer should use the sigmoid/ logistic function σ(z) = 1/(1 + e−z). Now suppose xi ∈ {−1, +1}. Derive a related expression for p(xi|x−i, θ) in this case. (This result can be used when applying Gibbs sampling to the model.)\n",
      "\n",
      "20 Exact inference for graphical models\n",
      "\n",
      "20.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In Section 17.4.3, we discussed the forwards-backwards algorithm, which can exactly compute the posterior marginals p(xt|v, θ) in any chain-structured graphical model, where x are the hidden variables (assumed discrete) and v are the visible variables. This algorithm can be modiﬁed to compute the posterior mode and posterior samples. A similar algorithm for linear-Gaussian chains, known as the Kalman smoother, was discussed in Section 18.3.2. Our goal in this chapter is to generalize these exact inference algorithms to arbitrary graphs. The resulting methods apply to both directed and undirected graphical models. We will describe a variety of algorithms, but we omit their derivations for brevity. See e.g., (Darwiche 2009; Koller and Friedman 2009) for a detailed exposition of exact inference techniques for discrete directed graphical models.\n",
      "\n",
      "20.2\n",
      "\n",
      "Belief propagation for trees\n",
      "\n",
      "In this section, we generalize the forwards-backwards algorithm from chains to trees. The resulting algorithm is known as belief propagation (BP) (Pearl 1988), or the sum-product algorithm.\n",
      "\n",
      "20.2.1\n",
      "\n",
      "Serial protocol\n",
      "\n",
      "We initially assume (for notational simplicity) that the model is a pairwise MRF (or CRF), i.e.,\n",
      "\n",
      "p(x|v) =\n",
      "\n",
      "1 Z(v)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "s∈V\n",
      "\n",
      "ψs(xs)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "ψs,t(xs, xt)\n",
      "\n",
      "(20.1)\n",
      "\n",
      "where ψs is the local evidence for node s, and ψst is the potential for edge s − t. We will consider the case of models with higher order cliques (such as directed trees) later on.\n",
      "\n",
      "One way to implement BP for undirected trees is as follows. Pick an arbitrary node and call it the root, r. Now orient all edges away from r (intuitively, we can imagine “picking up the graph” at node r and letting all the edges “dangle” down). This gives us a well-deﬁned notion of parent and child. Now we send messages up from the leaves to the root (the collect evidence phase) and then back down from the root (the distribute evidence phase), in a manner analogous to forwards-backwards on chains.\n",
      "\n",
      "708\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "root\n",
      "\n",
      "v+ st\n",
      "\n",
      "root\n",
      "\n",
      "v− st\n",
      "\n",
      "t\n",
      "\n",
      "t\n",
      "\n",
      "s\n",
      "\n",
      "u\n",
      "\n",
      "s\n",
      "\n",
      "u\n",
      "\n",
      "s1\n",
      "\n",
      "s2\n",
      "\n",
      "u1\n",
      "\n",
      "u2\n",
      "\n",
      "s1\n",
      "\n",
      "s2\n",
      "\n",
      "u1\n",
      "\n",
      "u2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 20.1 Message passing on a tree. (a) Collect-to-root phase. (b) Distribute-from-root phase.\n",
      "\n",
      "To explain the process in more detail, consider the example in Figure 20.1. Suppose we want to compute the belief state at node t. We will initially condition the belief only on evidence that is at or below t in the graph, i.e., we want to compute bel− t ). We will call this a “bottom-up belief state”. Suppose, by induction, that we have computed “messages” from t’s two children, summarizing what they think t should know about the evidence in their subtrees, i.e., we have computed m− st), where v− st is all the evidence on the downstream side of the s − t edge (see Figure 20.1(a)), and similarly we have computed mu→t(xt). Then we can compute the bottom-up belief state at t as follows:\n",
      "\n",
      "s→t(xt) =p( xt|v−\n",
      "\n",
      "t (xt) (cid:2) p(xt|v−\n",
      "\n",
      "bel−\n",
      "\n",
      "t (xt) (cid:2) p(xt|v−\n",
      "\n",
      "t ) =\n",
      "\n",
      "1 Zt\n",
      "\n",
      "ψt(xt)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "c∈ch(t)\n",
      "\n",
      "m−\n",
      "\n",
      "c→t(xt)\n",
      "\n",
      "(20.2)\n",
      "\n",
      "where ψt(xt) ∝ p(xt|vt) is the local evidence for node t, and Zt is the local normalization In words, we multiply all the incoming messages from our children, as well as the constant. incoming message from our local evidence, and then normalize.\n",
      "\n",
      "How do we compute the messages themselves? Consider computing m− of t’s children. Assume, by recursion, that we have computed bel− can compute the message as follows: (cid:4)\n",
      "\n",
      "We have explained how to compute the bottom-up belief states from the bottom-up messages. s→t(xt), where s is one s ). Then we\n",
      "\n",
      "m−\n",
      "\n",
      "s→t(xt) =\n",
      "\n",
      "ψst(xs, xt)bel−\n",
      "\n",
      "s (xs)\n",
      "\n",
      "s (xs) = p(xs|v−\n",
      "\n",
      "(20.3)\n",
      "\n",
      "xs\n",
      "\n",
      "Essentially we convert beliefs about xs into beliefs about xt by using the edge potential ψst.\n",
      "\n",
      "We continue in this way up the tree until we reach the root. Once at the root, we have “seen”\n",
      "\n",
      "all the evidence in the tree, so we can compute our local belief state at the root using\n",
      "\n",
      "belr(xr) (cid:2) p(xr|v) = p(xt|v−\n",
      "\n",
      "r ) ∝ ψr(xr)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "m−\n",
      "\n",
      "c→r(xr)\n",
      "\n",
      "(20.4)\n",
      "\n",
      "c∈ch(r)\n",
      "\n",
      "This completes the end of the upwards pass, which is analogous to the forwards pass in an HMM. As a “side effect”, we can compute the probability of the evidence by collecting the\n",
      "\n",
      "20.2. Belief propagation for trees\n",
      "\n",
      "709\n",
      "\n",
      "normalization constants: (cid:20)\n",
      "\n",
      "p(v) =\n",
      "\n",
      "Zt\n",
      "\n",
      "(20.5)\n",
      "\n",
      "t\n",
      "\n",
      "We can now pass messages down from the root. For example, consider node s, with parent t, as shown in Figure 20.1(b). To compute the belief state for s, we need to combine the bottom-up belief for s together with a top-down message from t, which summarizes all the information in the rest of the graph, m+ t→s(xs) (cid:2) p(xt|v+ st is all the evidence on the upstream (root) side of the s − t edge, as shown in Figure 20.1(b). We then have\n",
      "\n",
      "bels(xs) (cid:2) p(xs|v) ∝ bel−\n",
      "\n",
      "s (xs)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "m+\n",
      "\n",
      "st), where v+\n",
      "\n",
      "t→s(xt)\n",
      "\n",
      "(20.6)\n",
      "\n",
      "t∈pa(s)\n",
      "\n",
      "How do we compute these downward messages? For example, consider the message from t to s. Suppose t’s parent is r, and t’s children are s and u, as shown in Figure 20.1(b). We want to include in m+ t→s all the information that t has received, except for the information that s sent it:\n",
      "\n",
      "m+\n",
      "\n",
      "t→s(xs) (cid:2) p(xs|v+\n",
      "\n",
      "st) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "xt\n",
      "\n",
      "ψst(xs, xt)\n",
      "\n",
      "belt(xt) m− s→t(xt)\n",
      "\n",
      "(20.7)\n",
      "\n",
      "Rather than dividing out the message sent up to t, we can plug in the equation of belt to get\n",
      "\n",
      "m+\n",
      "\n",
      "t→s(xs) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψst(xs, xt)ψt(xt)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "m−\n",
      "\n",
      "c→t(xt)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "m+\n",
      "\n",
      "p→t(xt)\n",
      "\n",
      "(20.8)\n",
      "\n",
      "xt\n",
      "\n",
      "c∈ch(t),c(cid:5)=s\n",
      "\n",
      "p∈pa(t)\n",
      "\n",
      "In other words, we multiply together all the messages coming into t from all nodes except for the recipient s, combine together, and then pass through the edge potential ψst. In the case of a chain, t only has one child s and one parent p, so the above simpliﬁes to\n",
      "\n",
      "m+\n",
      "\n",
      "t→s(xs) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψst(xs, xt)ψt(xt)m+\n",
      "\n",
      "p→t(xt)\n",
      "\n",
      "(20.9)\n",
      "\n",
      "xt\n",
      "\n",
      "The version of BP in which we use division is called belief updating, and the version in which we multiply all-but-one of the messages is called sum-product. The belief updating the top- version is analogous to how we formulated the Kalman smoother in Section 18.3.2: down messages depend on the bottom-up messages. This means they can be interpreted as conditional posterior probabilities. The sum-product version is analogous to how we formulated the backwards algorithm in Section 17.4.3: the top-down messages are completely independent of the bottom-up messages, which means they can only be interpreted as conditional likelihoods. See Section 18.3.2.3 for a more detailed discussion of this subtle difference.\n",
      "\n",
      "20.2.2\n",
      "\n",
      "Parallel protocol\n",
      "\n",
      "So far, we have presented a serial version of the algorithm, in which we send messages up to the root and back. This is the optimal approach for a tree, and is a natural extension of forwards-backwards on chains. However, as a prelude to handling general graphs with loops, we now consider a parallel version of BP. This gives equivalent results to the serial version but is less efficient when implemented on a serial machine.\n",
      "\n",
      "710\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "The basic idea is that all nodes receive messages from their neighbors in parallel, they then updates their belief states, and ﬁnally they send new messages back out to their neighbors. This process repeats until convergence. This kind of computing architecture is called a systolic array, due to its resemblance to a beating heart.\n",
      "\n",
      "More precisely, we initialize all messages to the all 1’s vector. Then, in parallel, each node\n",
      "\n",
      "absorbs messages from all its neighbors using\n",
      "\n",
      "bels(xs) ∝ ψs(xs)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "mt→s(xs)\n",
      "\n",
      "(20.10)\n",
      "\n",
      "t∈nbrs\n",
      "\n",
      "Then, in parallel, each node sends messages to each of its neighbors:\n",
      "\n",
      "ms→t(xt) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝ψs(xs)ψst(xs, xt)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "mu→s(xs)\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "(20.11)\n",
      "\n",
      "xs\n",
      "\n",
      "u∈nbrs\\t\n",
      "\n",
      "The ms→t message is computed by multiplying together all incoming messages, except the one sent by the recipient, and then passing through the ψst potential.\n",
      "\n",
      "At iteration T of the algorithm, bels(xs) represents the posterior belief of xs conditioned on the evidence that is T steps away in the graph. After D(G) steps, where D(G) is the diameter of the graph (the largest distance between any two pairs of nodes), every node has obtained information from all the other nodes. Its local belief state is then the correct posterior marginal. Since the diameter of a tree is at most |V| − 1, the algorithm converges in a linear number of steps.\n",
      "\n",
      "We can actually derive the up-down version of the algorithm by imposing the condition that a node can only send a message once it has received messages from all its other neighbors. This means we must start with the leaf nodes, which only have one neighbor. The messages then propagate up to the root and back. We can also update the nodes in a random order. The only requirement is that each node get updated D(G) times. This is just enough time for information to spread throughout the whole tree.\n",
      "\n",
      "Similar parallel, distributed algorithms for solving linear systems of equations are discussed in (Bertsekas 1997). In particular, the Gauss-Seidel algorithm is analogous to the serial up-down version of BP, and the Jacobi algorithm is analogous to the parallel version of BP.\n",
      "\n",
      "20.2.3\n",
      "\n",
      "Gaussian BP *\n",
      "\n",
      "Now consider the case where p(x|v) is jointly Gaussian, so it can be represented as a Gaussian pairwise MRF, as in Section 19.4.4. We now present the belief propagation algorithm for this class of models, follow the presentation of (Bickson 2009) (see also (Malioutov et al. 2006)). We will assume the following node and edge potentials:\n",
      "\n",
      "ψt(xt) = exp(− 1 2 ψst(xs, xt) = exp(− 1 2\n",
      "\n",
      "Attx2\n",
      "\n",
      "t + btxt)\n",
      "\n",
      "xsAstxt)\n",
      "\n",
      "(20.12)\n",
      "\n",
      "(20.13)\n",
      "\n",
      "so the overall model has the form\n",
      "\n",
      "p(x|v) ∝ exp(− 1 2\n",
      "\n",
      "xT Ax + bT x)\n",
      "\n",
      "(20.14)\n",
      "\n",
      "20.2. Belief propagation for trees\n",
      "\n",
      "711\n",
      "\n",
      "This is the information form of the MVN (see Exercise 9.2), where A is the precision matrix. Note that by completing the square, the local evidence can be rewritten as a Gaussian:\n",
      "\n",
      "ψt(xt) ∝ N (bt/Att, A−1\n",
      "\n",
      "tt ) (cid:2) N (mt, (cid:2)−1 t )\n",
      "\n",
      "(20.15)\n",
      "\n",
      "Below we describe how to use BP to compute the posterior node marginals,\n",
      "\n",
      "p(xt|v) = N (μt, λ−1 t )\n",
      "\n",
      "(20.16)\n",
      "\n",
      "If the graph is a tree, the method is exact. If the graph is loopy, the posterior means may still be exact, but the posterior variances are often too small (Weiss and Freeman 1999).\n",
      "\n",
      "Although the precision matrix A is often sparse, computing the posterior mean requires inverting it, since μ = A−1b. BP provides a way to exploit graph structure to perform this computation in O(D) time instead of O(D3). This is related to various methods from linear algebra, as discussed in (Bickson 2009).\n",
      "\n",
      "Since the model is jointly Gaussian, all marginals and all messages will be Gaussian. The key operations we need are to multiply together two Gaussian factors, and to marginalize out a variable from a joint Gaussian factor.\n",
      "\n",
      "For multiplication, we can use the fact that the product of two Gaussians is Gaussian: 2 ) =C N (x|μ, λ−1)\n",
      "\n",
      "N (x|μ1, λ−1\n",
      "\n",
      "1 ) × N (x|μ2, λ−1\n",
      "\n",
      "(20.17)\n",
      "\n",
      "λ = λ1 + λ2 μ = λ−1(μ1λ1 + μ2λ2)\n",
      "\n",
      "(20.18)\n",
      "\n",
      "(20.19)\n",
      "\n",
      "where\n",
      "\n",
      "C =\n",
      "\n",
      ".\n",
      "\n",
      "λ λ1λ2\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 2\n",
      "\n",
      "(λ1μ2\n",
      "\n",
      "1(λ−1λ1 − 1) + λ2μ2\n",
      "\n",
      "2(λ−1λ2 − 1) + 2λ−1λ1λ2μ1μ2)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(20.20)\n",
      "\n",
      "See Exercise 20.2 for the proof.\n",
      "\n",
      "For marginalization, we have the following result: (cid:12)\n",
      "\n",
      "exp(−ax2 + bx)dx =\n",
      "\n",
      "\"\n",
      "\n",
      "π/a exp(b2/4a)\n",
      "\n",
      "(20.21)\n",
      "\n",
      "which follows from the normalization constant of a Gaussian (Exercise 2.11).\n",
      "\n",
      "We now have all the pieces we need. In particular, let the message ms→t(xt) be a Gaussian with mean μst and precision λst. From Equation 20.10, the belief at node s is given by the product of incoming messages times the local evidence (Equation 20.15) and hence\n",
      "\n",
      "bels(xs) =ψ s(xs)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "mts(xs) = N (xs|μs, λ−1 s )\n",
      "\n",
      "(20.22)\n",
      "\n",
      "λs = (cid:2)s +\n",
      "\n",
      "t∈nbr(s) (cid:4)\n",
      "\n",
      "λts\n",
      "\n",
      "(20.23)\n",
      "\n",
      "t∈nbr(s) ⎛\n",
      "\n",
      "⎞\n",
      "\n",
      "μs = λ−1\n",
      "\n",
      "s\n",
      "\n",
      "⎝(cid:2)sms +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "λtsμts\n",
      "\n",
      "⎠\n",
      "\n",
      "(20.24)\n",
      "\n",
      "t∈nbr(s)\n",
      "\n",
      "712\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "To compute the messages themselves, we use Equation 20.11, which is given by\n",
      "\n",
      "ms→t(xt) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "xs\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝ψst(xs, xt)ψs(xs)\n",
      "\n",
      "u∈nbrs\\t\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "mu→s(xs)\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠ dxs\n",
      "\n",
      "(20.25)\n",
      "\n",
      "=\n",
      "\n",
      "xs\n",
      "\n",
      "ψst(xs, xt)fs\\t(xs)dxs\n",
      "\n",
      "(20.26)\n",
      "\n",
      "where fs\\t(xs) is the product of the local evidence and all incoming messages excluding the message from t:\n",
      "\n",
      "fs\\t(xs) (cid:2) ψs(xs)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "mu→s(xs)\n",
      "\n",
      "(20.27)\n",
      "\n",
      "λs\\t (cid:2) (cid:2)s +\n",
      "\n",
      "u∈nbrs\\t = N (xs|μs\\t, λ−1 s\\t)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "λus\n",
      "\n",
      "(20.28)\n",
      "\n",
      "(20.29)\n",
      "\n",
      "u∈nbr(s)\\t ⎛\n",
      "\n",
      "⎞\n",
      "\n",
      "μs\\t (cid:2) λ−1 s\\t\n",
      "\n",
      "⎝(cid:2)sms +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "λusμus\n",
      "\n",
      "⎠\n",
      "\n",
      "(20.30)\n",
      "\n",
      "u∈nbr(s)\\t\n",
      "\n",
      "Returning to Equation 20.26 we have\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "ms→t(xt) =\n",
      "\n",
      "=\n",
      "\n",
      "xs ∝ exp ∝ N (μst, λ−1 st )\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "xs\n",
      "\n",
      "exp(−xsAstxt) / 2 01 ψst(xs,xt) (cid:22)\n",
      "\n",
      "exp (cid:22)\n",
      "\n",
      "(λs\\tμs\\t − Astxt)2/(2λs\\t)\n",
      "\n",
      "(−λs\\tx2\n",
      "\n",
      "s/2) + (λs\\tμs\\t − Astxt)xs\n",
      "\n",
      "exp(−λs\\t/2(xs − μs\\t)2) / 2 01 fs\\t(xs)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "dxs\n",
      "\n",
      "dxs + const\n",
      "\n",
      "(20.31)\n",
      "\n",
      "(20.32)\n",
      "\n",
      "(20.33)\n",
      "\n",
      "(20.34)\n",
      "\n",
      "λst = A2 μst = Astμs\\t/λst\n",
      "\n",
      "st/λs\\t\n",
      "\n",
      "(20.35)\n",
      "\n",
      "(20.36)\n",
      "\n",
      "One can generalize these equations to the case where each node is a vector, and the messages become small MVNs instead of scalar Gaussians (Alag and Agogino 1996). If we apply the resulting algorithm to a linear dynamical system, we recover the Kalman smoothing algorithm of Section 18.3.2.\n",
      "\n",
      "To perform message passing in models with non-Gaussian potentials, one can use sampling methods to approximate the relevant integrals. This is called non-parametric BP (Sudderth et al. 2003; Isard 2003; Sudderth et al. 2010).\n",
      "\n",
      "20.2.4\n",
      "\n",
      "Other BP variants *\n",
      "\n",
      "In this section, we brieﬂy discuss several variants of the main algorithm.\n",
      "\n",
      "20.2. Belief propagation for trees\n",
      "\n",
      "713\n",
      "\n",
      "ft\n",
      "\n",
      "xt\n",
      "\n",
      "ψt,t+1\n",
      "\n",
      "xt+1\n",
      "\n",
      "βt+1\n",
      "\n",
      "ψt\n",
      "\n",
      "ψt+1\n",
      "\n",
      "vt\n",
      "\n",
      "vt+1\n",
      "\n",
      "Figure 20.2 Illustration of how to compute the two-slice distribution for an HMM. The ψt and ψt+1 terms are the local evidence messages from the visible nodes vt, vt+1 to the hidde nodes xt, xt+1 respectively; ft is the forwards message from xt−1 and βt+1 is the backwards message from xt+2.\n",
      "\n",
      "20.2.4.1 Max-product algorithm\n",
      "\n",
      "It is possible to devise a max-product version of the BP algorithm, by replacing the operator with the max operator. We can then compute the local MAP marginal of each node. However, if there are ties, this might not be globally consistent, as discussed in Section 17.4.4. Fortunately, we can generalize the Viterbi algorithm to trees, where we use max and argmax in the collect- to-root phase, and perform traceback in the distribute-from-root phase. See (Dawid 1992) for details.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "20.2.4.2\n",
      "\n",
      "Sampling from a tree\n",
      "\n",
      "It is possible to draw samples from a tree structured model by generalizing the forwards ﬁltering / backwards sampling algorithm discussed in Section 17.4.5. See (Dawid 1992) for details.\n",
      "\n",
      "20.2.4.3\n",
      "\n",
      "Computing posteriors on sets of variables\n",
      "\n",
      "In Section 17.4.3.2, we explained how to compute the “two-slice” distribution ξt,t+1(i, j) = p(xt = i, xt+1 = j|v) in an HMM, namely by using\n",
      "\n",
      "ξt,t+1(i, j) =α t(i)ψt+1(j)βt+1(j)ψt,t+1(i, j)\n",
      "\n",
      "(20.37)\n",
      "\n",
      "Since αt(i) ∝ ψt(i)ft(i), where ft = p(xt|v1:t−1) is the forwards message, we can think of this as sending messages ft and ψt into xt, βt+1 and φt+1 into xt+1, and then combining them with the Ψ matrix, as shown in Figure 20.2. This is like treating xt and xt+1 as a single “mega node”, and then multiplying all the incoming messages as well as all the local factors (here, ψt,t+1).\n",
      "\n",
      "714\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "Coherence\n",
      "\n",
      "Coherence\n",
      "\n",
      "Diﬃculty\n",
      "\n",
      "Intelligence\n",
      "\n",
      "Diﬃculty\n",
      "\n",
      "Intelligence\n",
      "\n",
      "Grade\n",
      "\n",
      "SAT\n",
      "\n",
      "Grade\n",
      "\n",
      "SAT\n",
      "\n",
      "Letter\n",
      "\n",
      "Letter\n",
      "\n",
      "Job\n",
      "\n",
      "Job\n",
      "\n",
      "Happy\n",
      "\n",
      "Happy\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 20.3 Left: The “student” DGM. Right: the equivalent UGM. We add moralization arcs D-I, G-J and L-S. Based on Figure 9.8 of (Koller and Friedman 2009).\n",
      "\n",
      "20.3\n",
      "\n",
      "The variable elimination algorithm\n",
      "\n",
      "We have seen how to use BP to compute exact marginals on chains and trees. In this section, we discuss an algorithm to compute p(xq|xv) for any kind of graph.\n",
      "\n",
      "We will explain the algorithm by example. Consider the DGM in Figure 20.3(a). This model, from (Koller and Friedman 2009), is a hypothetical model relating various variables pertaining to a typical student. The corresponding joint has the following form:\n",
      "\n",
      "P (C, D, I, G, S, L, J, H)\n",
      "\n",
      "(20.38)\n",
      "\n",
      "= P (C)P (D|C)P (I)P (G|I, D)P (S|I)P (L|G)P (J|L, S)P (H|G, J)\n",
      "\n",
      "(20.39)\n",
      "\n",
      "Note that the forms of the CPDs do not matter, since all our calculations will be symbolic. However, for illustration purposes, we will assume all variables are binary.\n",
      "\n",
      "Before proceeding, we convert our model to undirected form. This is not required, but it makes for a more uniﬁed presentation, since the resulting method can then be applied to both DGMs and UGMs (and, as we will see in Section 20.3.1, to a variety of other problems that have nothing to do with graphical models). Since the computational complexity of inference in DGMs and UGMs is, generally speaking, the same, nothing is lost in this transformation from a computational point of view.1\n",
      "\n",
      "To convert the DGM to a UGM, we simply deﬁne a potential or factor for every CPD, yielding p(C, D, I, G, S, L, J, H)\n",
      "\n",
      "(20.40) = ψC(C)ψD(D, C)ψI (I)ψG(G, I, D)ψS(S, I)ψL(L, G)ψJ (J, L, S)ψH (H, G, J)(20.41)\n",
      "\n",
      "1. There are a few “tricks” one can exploit in the directed case that cannot easily be exploited in the undirected case. One important example is barren node removal. To explain this, consider a naive Bayes classiﬁer, as in Figure 10.2. Suppose we want to infer y and we observe x1 and x2, but not x3 and x4. It is clear that we can safely remove x3 and x4, since In general, once we have removed hidden leaves, we can apply this process recursively. Since potential functions do not necessary sum to one, we cannot use this trick in the undirected case. See (Koller and Friedman 2009) for a variety of other speedup tricks.\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "x3\n",
      "\n",
      "p(x3|y) = 1, and similarly for x4.\n",
      "\n",
      "20.3. The variable elimination algorithm\n",
      "\n",
      "715\n",
      "\n",
      "Since all the potentials are locally normalized, since they are CPDs, there is no need for a global normalization constant, so Z = 1. The corresponding undirected graph is shown in Figure 20.3(b). Note that it has more edges than the DAG. In particular, any “unmarried” nodes that share a child must get “married”, by adding an edge between them; this process is known as moralization. Only then can the arrows be dropped. In this example, we added D-I, G-J, and L-S moralization arcs. The reason this operation is required is to ensure that the CI properties of the UGM match those of the DGM, as explained in Section 19.2.2. It also ensures there is a clique that can “store” the CPDs of each family.\n",
      "\n",
      "Now suppose we want to compute p(J = 1), the marginal probability that a person will get a job. Since we have 8 binary variables, we could simply enumerate over all possible assignments to all the variables (except for J), adding up the probability of each joint instantiation:\n",
      "\n",
      "p(J) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(C, D, I, G, S, L, J, H)\n",
      "\n",
      "(20.42)\n",
      "\n",
      "L\n",
      "\n",
      "S\n",
      "\n",
      "G\n",
      "\n",
      "H\n",
      "\n",
      "I\n",
      "\n",
      "D\n",
      "\n",
      "C\n",
      "\n",
      "However, this would take O(27) time. We can be smarter by pushing sums inside products. This is the key idea behind the variable elimination algorithm (Zhang and Poole 1996), also called bucket elimination (Dechter 1996), or, in the context of genetic pedigree trees, the peeling algorithm (Cannings et al. 1978). In our example, we get\n",
      "\n",
      "p(J) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(C, D, I, G, S, L, J, H)\n",
      "\n",
      "=\n",
      "\n",
      "L,S,G,H,I,D,C (cid:4)\n",
      "\n",
      "ψC(C)ψD(D, C)ψI (I)ψG(G, I, D)ψS(S, I)ψL(L, G)\n",
      "\n",
      "=\n",
      "\n",
      "L,S,G,H,I,D,C ×ψJ (J, L, S)ψH (H, G, J) (cid:4)\n",
      "\n",
      "ψJ (J, L, S)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψL(L, G)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψH (H, G, J)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψS(S, I)ψI (I)\n",
      "\n",
      "×\n",
      "\n",
      "L,S\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "G\n",
      "\n",
      "ψG(G, I, D)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "H\n",
      "\n",
      "ψC(C)ψD(D, C)\n",
      "\n",
      "I\n",
      "\n",
      "D\n",
      "\n",
      "C\n",
      "\n",
      "We now evaluate this expression, working right to left as shown in Table 20.1. First we multiply together all the terms in the scope of the\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "C operator to create the temporary factor\n",
      "\n",
      "τ (cid:2) 1(C, D) = ψC(C)ψD(D, C)\n",
      "\n",
      "(20.43)\n",
      "\n",
      "Then we marginalize out C to get the new factor\n",
      "\n",
      "τ1(D) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "τ (cid:2) 1(C, D)\n",
      "\n",
      "(20.44)\n",
      "\n",
      "C\n",
      "\n",
      "Next we multiply together all the terms in the scope of the out to create\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "D operator and then marginalize\n",
      "\n",
      "τ (cid:2) 2(G, I, D) =ψ G(G, I, D)τ1(D) (cid:4)\n",
      "\n",
      "τ2(G, I) =\n",
      "\n",
      "τ (cid:2) 2(G, I, D)\n",
      "\n",
      "(20.45)\n",
      "\n",
      "(20.46)\n",
      "\n",
      "D\n",
      "\n",
      "716\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψJ (J, L, S)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψL(L, G)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψH (H, G, J)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψS (S, I)ψI (I)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψG(G, I, D)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψC (C)ψD(D, C)\n",
      "\n",
      "L\n",
      "\n",
      "S\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "G\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "H\n",
      "\n",
      "ψJ (J, L, S)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψL(L, G)\n",
      "\n",
      "I\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψH (H, G, J)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "D\n",
      "\n",
      "ψS (S, I)ψI (I)\n",
      "\n",
      "C (cid:8)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:9)(cid:10) τ1(D)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ψG(G, I, D)τ1(D)\n",
      "\n",
      "L\n",
      "\n",
      "S\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "G\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψJ (J, L, S)\n",
      "\n",
      "H\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψL(L, G)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "I\n",
      "\n",
      "ψH (H, G, J)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "D (cid:8)\n",
      "\n",
      "(cid:9)(cid:10) τ2(G,I)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ψS (S, I)ψI (I)τ2(G, I)\n",
      "\n",
      "L\n",
      "\n",
      "S\n",
      "\n",
      "G\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "H\n",
      "\n",
      "ψJ (J, L, S)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "I\n",
      "\n",
      "ψL(L, G)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:9)(cid:10) τ3(G,S)\n",
      "\n",
      "ψH (H, G, J)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "τ3(G, S)\n",
      "\n",
      "L\n",
      "\n",
      "S\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "G\n",
      "\n",
      "ψJ (J, L, S)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "H (cid:8)\n",
      "\n",
      "(cid:9)(cid:10) τ4(G,J)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ψL(L, G)τ4(G, J)τ3(G, S)\n",
      "\n",
      "L\n",
      "\n",
      "S\n",
      "\n",
      "G (cid:8)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:9)(cid:10) τ5(J,L,S)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ψJ (J, L, S)τ5(J, L, S)\n",
      "\n",
      "L\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "S\n",
      "\n",
      "(cid:9)(cid:10) τ6(J,L)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "τ6(J, L)\n",
      "\n",
      "L (cid:8)\n",
      "\n",
      "(cid:9)(cid:10) τ7(J)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "Table 20.1 Eliminating variables from Figure 20.3 in the order C, D, I, H, G, S, L to compute P (J).\n",
      "\n",
      "Next we multiply together all the terms in the scope of the out to create\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "I operator and then marginalize\n",
      "\n",
      "τ (cid:2) 3(G, I, S) =ψ S(S, I)ψI (I)τ2(G, I) (cid:4) τ3(G, S) =\n",
      "\n",
      "τ (cid:2) 3(G, I, S)\n",
      "\n",
      "(20.47)\n",
      "\n",
      "(20.48)\n",
      "\n",
      "I\n",
      "\n",
      "And so on.\n",
      "\n",
      "The above technique can be used to compute any marginal of interest, such as p(J) or p(J, H). To compute a conditional, we can take a ratio of two marginals, where the visible variables have been clamped to their known values (and hence don’t need to be summed over). For example,\n",
      "\n",
      "p(J = j|I = 1, H = 0) =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "p(J = j, I = 1, H = 0) j(cid:2) p(J = j(cid:2), I = 1, H = 0)\n",
      "\n",
      "(20.49)\n",
      "\n",
      "In general, we can write\n",
      "\n",
      "p(xq|xv) =\n",
      "\n",
      "p(xq, xv) p(xv)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "xh\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "xh (cid:7)\n",
      "\n",
      "p(xh, xq, xv) p(xh, x(cid:2)\n",
      "\n",
      "x(cid:2) q\n",
      "\n",
      "q, xv)\n",
      "\n",
      "(20.50)\n",
      "\n",
      "20.3. The variable elimination algorithm\n",
      "\n",
      "717\n",
      "\n",
      "The normalization constant in the denominator, p(xv), is called the probability of the evi- dence.\n",
      "\n",
      "See variableElimination for a simple Matlab implementation of this algorithm, which works for arbitrary graphs, and arbitrary discrete factors. But before you go too crazy, please read Section 20.3.2, which points out that VE can be exponentially slow in the worst case.\n",
      "\n",
      "20.3.1\n",
      "\n",
      "The generalized distributive law *\n",
      "\n",
      "Abstractly, VE can be thought of as computing the following expression:\n",
      "\n",
      "p(xq|xv) ∝\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ψc(xc)\n",
      "\n",
      "(20.51)\n",
      "\n",
      "x\n",
      "\n",
      "c\n",
      "\n",
      "It is understood that the visible variables xv are clamped, and not summed over. VE uses non-serial dynamic programming (Bertele and Brioschi 1972), caching intermediate results to avoid redundant computation.\n",
      "\n",
      "However, there are other tasks we might like to solve for any given graphical model. For\n",
      "\n",
      "example, we might want the MAP estimate:\n",
      "\n",
      "x∗ = argmax\n",
      "\n",
      "x\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "c\n",
      "\n",
      "ψc(xc)\n",
      "\n",
      "(20.52)\n",
      "\n",
      "Fortunately, essentially the same algorithm can also be used to solve this task: we just replace sum with max. (We also need a traceback step, which actually recovers the argmax, as opposed to just the value of max; these details are explained in Section 17.4.4.)\n",
      "\n",
      "two binary operations called “+” and “×”, which satisfy the following three axioms:\n",
      "\n",
      "In general, VE can be applied to any commutative semi-ring. This is a set K, together with\n",
      "\n",
      "1. The operation “+” is associative and commutative, and there is an additive identity element\n",
      "\n",
      "called “0” such that k + 0 = k for all k ∈ K.\n",
      "\n",
      "2. The operation “×” is associative and commutative, and there is a multiplicative identity\n",
      "\n",
      "element called “1” such that k × 1 =k for all k ∈ K.\n",
      "\n",
      "3. The distributive law holds, i.e.,\n",
      "\n",
      "(a × b) + (a × c) = a × (b + c)\n",
      "\n",
      "(20.53)\n",
      "\n",
      "for all triples (a, b, c) from K.\n",
      "\n",
      "This framework covers an extremely wide range of important applications, including constraint satisfaction problems (Bistarelli et al. 1997; Dechter 2003), the fast Fourier transform (Aji and McEliece 2000), etc. See Table 20.2 for some examples.\n",
      "\n",
      "20.3.2\n",
      "\n",
      "Computational complexity of VE\n",
      "\n",
      "The running time of VE is clearly exponential in the size of the largest factor, since we have sum over all of the corresponding variables. Some of the factors come from the original model (and are thus unavoidable), but new factors are created in the process of summing out. For example,\n",
      "\n",
      "718\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "Domain [0, ∞) [0, ∞) (−∞, ∞] {T, F }\n",
      "\n",
      "+ (+, 0) (max, 0) (min, ∞) (∨, F )\n",
      "\n",
      "× Name (×, 1) sum-product (×, 1) max-product (+, 0) min-sum (∧, T )\n",
      "\n",
      "Boolean satisﬁability\n",
      "\n",
      "Table 20.2 Some commutative semirings.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψD(D, C)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψJ (J, L, S)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψI (I)ψS (S, I)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψG(G, I, D)ψL(L, )ψH (H, G, J)\n",
      "\n",
      "D\n",
      "\n",
      "C\n",
      "\n",
      "H\n",
      "\n",
      "L\n",
      "\n",
      "S\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "ψD(D, C)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "I\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "G (cid:8)\n",
      "\n",
      "ψJ (J, L, S)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:9)(cid:10) τ1(I,D,L,J,H)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ψI (I)ψS (S, I)τ1(I, D, L, J, H)\n",
      "\n",
      "D\n",
      "\n",
      "C\n",
      "\n",
      "H\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "L\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "S\n",
      "\n",
      "ψD(D, C)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "I\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:9)(cid:10) τ2(D,L,S,J,H)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ψJ (J, L, S)τ2(D, L, S, J, H)\n",
      "\n",
      "D\n",
      "\n",
      "C\n",
      "\n",
      "H\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "L\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "S\n",
      "\n",
      "ψD(D, C)\n",
      "\n",
      "(cid:9)(cid:10) τ3(D,L,J,H) (cid:7) (cid:7)\n",
      "\n",
      "τ3(D, L, J, H)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "D\n",
      "\n",
      "C\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "H\n",
      "\n",
      "L (cid:8)\n",
      "\n",
      "ψD(D, C)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:9)(cid:10) τ4(D,J,H)\n",
      "\n",
      "τ4(D, J, H)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "D\n",
      "\n",
      "C\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "H (cid:8)\n",
      "\n",
      "(cid:9)(cid:10) τ5(D,J)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "ψD(D, C)τ5(D, J)\n",
      "\n",
      "D\n",
      "\n",
      "C (cid:8)\n",
      "\n",
      "(cid:9)(cid:10) τ6(D,J) (cid:7)\n",
      "\n",
      "τ6(D, J)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "D (cid:8)\n",
      "\n",
      "(cid:9)(cid:10) τ7(J)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "Table 20.3 Eliminating variables from Figure 20.3 in the order G, I, S, L, H, C, D.\n",
      "\n",
      "in Equation 20.47, we created a factor involving G, I and S; but these nodes were not originally present together in any factor.\n",
      "\n",
      "The order in which we perform the summation is known as the elimination order. This can have a large impact on the size of the intermediate factors that are created. For example, consider the ordering in Table 20.1: the largest created factor (beyond the original ones in the model) has size 3, corresponding to τ5(J, L, S). Now consider the ordering in Table 20.3: now the largest factors are τ1(I, D, L, J, H) and τ2(D, L, S, J, H), which are much bigger.\n",
      "\n",
      "We can determine the size of the largest factor graphically, without worrying about the actual numerical values of the factors. When we eliminate a variable Xt, we connect it to all variables\n",
      "\n",
      "20.3. The variable elimination algorithm\n",
      "\n",
      "719\n",
      "\n",
      "Coherence\n",
      "\n",
      "Coherence\n",
      "\n",
      "Coherence\n",
      "\n",
      "Diﬃculty\n",
      "\n",
      "Intelligence\n",
      "\n",
      "Diﬃculty\n",
      "\n",
      "Intelligence\n",
      "\n",
      "Diﬃculty\n",
      "\n",
      "Intelligence\n",
      "\n",
      "Grade\n",
      "\n",
      "SAT\n",
      "\n",
      "Grade\n",
      "\n",
      "SAT\n",
      "\n",
      "Grade\n",
      "\n",
      "SAT\n",
      "\n",
      "Letter\n",
      "\n",
      "Letter\n",
      "\n",
      "Letter\n",
      "\n",
      "Job\n",
      "\n",
      "Job\n",
      "\n",
      "Job\n",
      "\n",
      "Happy\n",
      "\n",
      "Happy\n",
      "\n",
      "Happy\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 20.4 Example of the elimination process, in the order C, D, I, etc. When we eliminate I (ﬁgure c), we add a ﬁll-in edge between G and S, since they are not connected. Based on Figure 9.10 of (Koller and Friedman 2009).\n",
      "\n",
      "that share a factor with Xt (to reﬂect the new temporary factor τ (cid:2) t). The edges created by this process are called ﬁll-in edges. For example, Figure 20.4 shows the ﬁll-in edges introduced when we eliminate in the order C, D, I, . . .. The ﬁrst two steps do not introduce any ﬁll-ins, but when we eliminate I, we connect G and S, since they co-occur in Equation 20.48.\n",
      "\n",
      "Let G(≺) be the (undirected) graph induced by applying variable elimination to G using elimination ordering ≺. The temporary factors generated by VE correspond to maximal cliques in the graph G(≺). For example, with ordering (C, D, I, H, G, S, L), the maximal cliques are as follows:\n",
      "\n",
      "{C, D}, {D, I, G}, {G, L, S, J}, {G, J, H}, {G, I, S}\n",
      "\n",
      "(20.54)\n",
      "\n",
      "It is clear that the time complexity of VE is\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "K |c|\n",
      "\n",
      "(20.55)\n",
      "\n",
      "c∈C(G(≺))\n",
      "\n",
      "where C are the cliques that are created, |c| is the size of the clique c, and we assume for notational simplicity that all the variables have K states each.\n",
      "\n",
      "Let us deﬁne the induced width of a graph given elimination ordering ≺, denoted w(≺), as the size of the largest factor (i.e., the largest clique in the induced graph ) minus 1. Then it is easy to see that the complexity of VE with ordering ≺ is O(Kw(≺)+1).\n",
      "\n",
      "Obviously we would like to minimize the running time, and hence the induced width. Let us\n",
      "\n",
      "deﬁne the treewidth of a graph as the minimal induced width.\n",
      "\n",
      "w (cid:2) min ≺\n",
      "\n",
      "max c∈G(≺)\n",
      "\n",
      "|c| −1\n",
      "\n",
      "(20.56)\n",
      "\n",
      "Then clearly the best possible running time for VE is O(DK w+1). Unfortunately, one can show that for arbitrary graphs, ﬁnding an elimination ordering ≺ that minimizes w(≺) is NP-hard (Arnborg et al. 1987). In practice greedy search techniques are used to ﬁnd reasonable orderings (Kjaerulff 1990), although people have tried other heuristic methods for discrete optimization,\n",
      "\n",
      "720\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "such as genetic algorithms (Larranaga et al. 1997). algorithms with provable performance guarantees (Amir 2010).\n",
      "\n",
      "It is also possible to derive approximate\n",
      "\n",
      "In some cases, the optimal elimination ordering is clear. For example, for chains, we should work forwards or backwards in time. For trees, we should work from the leaves to the root. These orderings do not introduce any ﬁll-in edges, so w = 1. Consequently, inference in chains and trees takes O(V K 2) time. This is one reason why Markov chains and Markov trees are so widely used.\n",
      "\n",
      "Unfortunately, for other graphs, the treewidth is large. For example, for an m × n 2d lattice, the treewidth is O(min{m, n}) (Lipton and Tarjan 1979). So VE on a 100 × 100 Ising model would take O(2100) time.\n",
      "\n",
      "Of course, just because VE is slow doesn’t mean that there isn’t some smarter algorithm out\n",
      "\n",
      "there. We discuss this issue in Section 20.5.\n",
      "\n",
      "20.3.3\n",
      "\n",
      "A weakness of VE\n",
      "\n",
      "The main disadvantage of the variable elimination algorithm (apart from its exponential depen- dence on treewidth) is that it is inefficient if we want to compute multiple queries conditioned on the same evidence. For example, consider computing all the marginals in a chain-structured graphical model such as an HMM. We can easily compute the ﬁnal marginal p(xT |v) by elimi- nating all the nodes x1 to xT −1 in order. This is equivalent to the forwards algorithm, and takes O(K 2T ) time. But now suppose we want to compute p(xT −1|v). We have to run VE again, at a cost of O(K 2T ) time. So the total cost to compute all the marginals is O(K 2T 2). However, we know that we can solve this problem in O(K 2T ) using forwards-backwards. The difference is that FB caches the messages computed on the forwards pass, so it can reuse them later.\n",
      "\n",
      "The same argument holds for BP on trees. For example, consider the 4-node tree in Fig- ure 20.5. We can compute p(x1|v) by eliminating x2:4; this is equivalent to sending messages up to x1 (the messages correspond to the τ factors created by VE). Similarly we can compute p(x2|v), p(x3|v) and then p(x4|v). We see that some of the messages used to compute the marginal on one node can be re-used to compute the marginals on the other nodes. By storing the messages for later re-use, we can compute all the marginals in O(DK 2) time. This is what the up-down (collect-distribute) algorithm on trees does.\n",
      "\n",
      "The question is: how can we combine the efficiency of BP on trees with the generality of VE?\n",
      "\n",
      "The answer is given in Section 20.4.\n",
      "\n",
      "20.4\n",
      "\n",
      "The junction tree algorithm *\n",
      "\n",
      "The junction tree algorithm or JTA generalizes BP from trees to arbitrary graphs. We sketch the basic idea below; for details, see e.g., (Koller and Friedman 2009).\n",
      "\n",
      "20.4.1\n",
      "\n",
      "Creating a junction tree\n",
      "\n",
      "The basic idea behind the JTA is this. We ﬁrst run the VE algorithm “symbolically”, adding ﬁll-in edges as we go, according to a given elimination ordering. The resulting graph will be a chordal graph, which means that every undirected cycle X1 − X2 · · ·X k − X1 of length k ≥ 4 has a\n",
      "\n",
      "20.4. The junction tree algorithm *\n",
      "\n",
      "721\n",
      "\n",
      "(cid:59) (cid:20)\n",
      "\n",
      "(cid:59) (cid:20)\n",
      "\n",
      "(cid:80)(cid:21)(cid:20)(cid:11)(cid:91)(cid:20)(cid:12)\n",
      "\n",
      "(cid:80)(cid:22)(cid:21)(cid:11)(cid:91)(cid:21)(cid:12)\n",
      "\n",
      "(cid:59) (cid:21)\n",
      "\n",
      "(cid:80)(cid:23)(cid:21)(cid:11)(cid:91)(cid:21)(cid:12)\n",
      "\n",
      "(cid:80)(cid:20)(cid:21)(cid:11)(cid:91)(cid:21)(cid:12)\n",
      "\n",
      "(cid:80)(cid:22)(cid:21)(cid:11)(cid:91)(cid:21)(cid:12)\n",
      "\n",
      "(cid:59) (cid:21)\n",
      "\n",
      "(cid:80)(cid:23)(cid:21)(cid:11)(cid:91)(cid:23)(cid:12)\n",
      "\n",
      "(cid:59) (cid:22)\n",
      "\n",
      "(cid:11)(cid:68)(cid:12)\n",
      "\n",
      "(cid:59) (cid:23)\n",
      "\n",
      "(cid:59) (cid:22)\n",
      "\n",
      "(cid:11)(cid:69)(cid:12)\n",
      "\n",
      "(cid:59) (cid:23)\n",
      "\n",
      "(cid:59) (cid:20)\n",
      "\n",
      "(cid:59) (cid:20)\n",
      "\n",
      "(cid:80)(cid:20)(cid:21)(cid:11)(cid:91)(cid:21)(cid:12)\n",
      "\n",
      "(cid:80)(cid:22)(cid:21)(cid:11)(cid:91)(cid:21)(cid:12)\n",
      "\n",
      "(cid:59) (cid:21)\n",
      "\n",
      "(cid:59) (cid:22)\n",
      "\n",
      "(cid:80)(cid:21)(cid:23)(cid:11)(cid:91)(cid:23)(cid:12)\n",
      "\n",
      "(cid:59) (cid:23)\n",
      "\n",
      "(cid:80)(cid:20)(cid:21)(cid:11)(cid:91)(cid:21)(cid:12)\n",
      "\n",
      "(cid:80)(cid:22)(cid:21)(cid:11)(cid:91)(cid:21)(cid:12)\n",
      "\n",
      "(cid:59) (cid:22)\n",
      "\n",
      "(cid:59) (cid:21)\n",
      "\n",
      "(cid:80)(cid:21)(cid:20)(cid:11)(cid:91)(cid:20)(cid:12)\n",
      "\n",
      "(cid:80)(cid:23)(cid:21)(cid:11)(cid:91)(cid:21)(cid:12) (cid:59) (cid:23)\n",
      "\n",
      "(cid:80)(cid:21)(cid:22)(cid:11)(cid:91)(cid:22)(cid:12)\n",
      "\n",
      "(cid:80)(cid:21)(cid:23)(cid:11)(cid:91)(cid:23)(cid:12)\n",
      "\n",
      "Figure 20.5 of the messages needed to compute all singleton marginals. Based on Figure 4.3 of (Jordan 2007).\n",
      "\n",
      "Sending multiple messages along a tree. (a) X1 is root. (b) X2 is root. (c) X4 is root. (d) All\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 20.6 Left: this graph is not triangulated, despite appearances, since it contains a chordless 5-cycle 1-2-3-4-5-1. Right: one possible triangulation, by adding the 1-3 and 1-4 ﬁll-in edges. Based on (Armstrong 2005, p46)\n",
      "\n",
      "722\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "chord, i.e., an edge connects Xi, Xj for all non-adjacent nodes i,j in the cycle.2\n",
      "\n",
      "Having created a chordal graph, we can extract its maximal cliques. In general, ﬁnding max cliques is computationally hard, but it turns out that it can be done efficiently from this special kind of graph. Figure 20.7(b) gives an example, where the max cliques are as follows:\n",
      "\n",
      "{C, D}, {G, I, D}, {G, S, I}, {G, J, S, L}, {H, G, J}\n",
      "\n",
      "(20.57)\n",
      "\n",
      "Note that if the original graphical model was already chordal, the elimination process would not add any extra ﬁll-in edges (assuming the optimal elimination ordering was used). We call such models decomposable, since they break into little pieces deﬁned by the cliques.\n",
      "\n",
      "It turns out that the cliques of a chordal graph can be arranged into a special kind of tree known as a junction tree. This enjoys the running intersection property (RIP), which means that any subset of nodes containing a given variable forms a connected component. Figure 20.7(c) gives an example of such a tree. We see that the node I occurs in two adjacent tree nodes, so they can share information about this variable. A similar situation holds for all the other variables.\n",
      "\n",
      "One can show that if a tree that satisﬁes the running intersection property, then applying BP to this tree (as we explain below) will return the exact values of p(xc|v) for each node c in the tree (i.e., clique in the induced graph). From this, we can easily extract the node and edge marginals, p(xt|v) and p(xs, xt|v) from the original model, by marginalizing the clique distributions.3\n",
      "\n",
      "20.4.2 Message passing on a junction tree\n",
      "\n",
      "Having constructed a junction tree, we can use it for inference. The process is very similar to belief propagation on a tree. As in Section 20.2, there are two versions: the sum-product form, also known as the Shafer-Shenoy algorithm, named after (Shafer and Shenoy 1990); and the belief updating form (which involves division), also known as the Hugin (named after a company) or the Lauritzen-Spiegelhalter algorithm (named after (Lauritzen and Spiegelhalter 1988)). See (Lepar and Shenoy 1998) for a detailed comparison of these methods. Below we sketch how the Hugin algorithm works.\n",
      "\n",
      "We assume the original model has the following form:\n",
      "\n",
      "p(x) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "c∈C(G)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ψc(xc)\n",
      "\n",
      "(20.58)\n",
      "\n",
      "where C(G) are the cliques of the original graph. On the other hand, the tree deﬁnes a distribution of the following form:\n",
      "\n",
      "p(x) =\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "c∈C(T ) ψc(xc) s∈S(T ) ψs(xs)\n",
      "\n",
      "(20.59)\n",
      "\n",
      "2. The largest loop in a chordal graph is length 3. Consequently chordal graphs are sometimes called triangulated. However, it is not enough for the graph to look like it is made of little triangles. For example, Figure 20.6(a) is not chordal, even though it is made of little triangles, since it contains the chordless 5-cycle 1-2-3-4-5-1. 3. If we want the joint distribution of some variables that are not in the same clique — a so-called out-of-clique query — we can adapt the technique described in Section 20.2.4.3 as follows: create a mega node containing the query variables and any other nuisance variables that lie on the path between them, multiply in messages onto the boundary of the mega node, and then marginalize out the internal nuisance variables. This internal marginalization may require the use of BP or VE. See (Koller and Friedman 2009) for details.\n",
      "\n",
      "20.4. The junction tree algorithm *\n",
      "\n",
      "723\n",
      "\n",
      "Coherence\n",
      "\n",
      "Coherence\n",
      "\n",
      "Difﬁculty\n",
      "\n",
      "Intelligence\n",
      "\n",
      "Difﬁculty\n",
      "\n",
      "Intelligence\n",
      "\n",
      "Grade\n",
      "\n",
      "SAT\n",
      "\n",
      "Grade\n",
      "\n",
      "SAT\n",
      "\n",
      "Letter\n",
      "\n",
      "Letter\n",
      "\n",
      "Job\n",
      "\n",
      "Job\n",
      "\n",
      "Happy\n",
      "\n",
      "Happy\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "C, D\n",
      "\n",
      "G, I, D\n",
      "\n",
      "G, S, I\n",
      "\n",
      "G, J, S, L\n",
      "\n",
      "H, G, J\n",
      "\n",
      "D\n",
      "\n",
      "G,I\n",
      "\n",
      "G, S\n",
      "\n",
      "G,J\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 20.7 (c) The junction tree. An edge between nodes s and t is labeled by the intersection of the sets on nodes s and t; this is called the separating set. From Figure 9.11 of (Koller and Friedman 2009). Used with kind permission of Daphne Koller.\n",
      "\n",
      "(a) The student graph with ﬁll-in edges added.\n",
      "\n",
      "(b) The maximal cliques.\n",
      "\n",
      "where C(T ) are the nodes of the junction tree (which are the cliques of the chordal graph), and S(T ) are the separators of the tree. To make these equal, we initialize by deﬁning ψs = 1 for all separators and ψc = 1 for all cliques. Then, for each clique in the original model, c ∈ C(G), we ﬁnd a clique in the tree c(cid:2) ∈ C(T ) which contains it, c(cid:2) ⊇ c. We then multiply ψc onto ψc(cid:2) by computing ψc(cid:2) = ψc(cid:2) ψc. After doing this for all the cliques in the original graph, we have\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ψc(xc) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ψc(xc)\n",
      "\n",
      "(20.60)\n",
      "\n",
      "c∈C(T )\n",
      "\n",
      "c∈C(G)\n",
      "\n",
      "As in Section 20.2.1, we now send messages from the leaves to the root and back, as sketched in Figure 20.1. In the upwards pass, also known as the collect-to-root phase, node i sends to its parent j the following message:\n",
      "\n",
      "mi→j(Sij) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψi(Ci)\n",
      "\n",
      "(20.61)\n",
      "\n",
      "Ci\\Sij\n",
      "\n",
      "That is, we marginalize out the variables that node i “knows about” which are irrelevant to j, and then we send what is left over. Once a node has received messages from all its children, it updates its belief state using\n",
      "\n",
      "ψi(Ci) ∝ ψi(Ci)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "mj→i(Sij)\n",
      "\n",
      "(20.62)\n",
      "\n",
      "j∈chi\n",
      "\n",
      "724\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "At the root, ψr(Cr) represents p(xCr |v), which is the posterior over the nodes in clique Cr conditioned on all the evidence. Its normalization constant is p(v)/Z0, where Z0 is the normalization constant for the unconditional prior, p(x). (We have Z0 = 1 if the original model was a DGM.)\n",
      "\n",
      "children j the following message: ψi(Ci)\n",
      "\n",
      "In the downwards pass, also known as the distribute-from-root phase, node i sends to its\n",
      "\n",
      "mi→j(Sij) =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Ci\\Sij mj→i(Sij)\n",
      "\n",
      "(20.63)\n",
      "\n",
      "We divide out by what j sent to i to avoid double counting the evidence. This requires that we store the messages from the upwards pass. Once a node has received a top-down message from its parent, it can compute its ﬁnal belief state using\n",
      "\n",
      "ψj(Cj) ∝ ψj(Cj)mi→j(Sij)\n",
      "\n",
      "(20.64)\n",
      "\n",
      "An equivalent way to present this algorithm is based on storing the messages inside the separator potentials. So on the way up, sending from i to j we compute the separator potential\n",
      "\n",
      "ψ∗\n",
      "\n",
      "ij(Sij) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψi(Ci)\n",
      "\n",
      "(20.65)\n",
      "\n",
      "Ci\\Sij\n",
      "\n",
      "and then update the recipient potential:\n",
      "\n",
      "ψ∗\n",
      "\n",
      "j (Cj) ∝ ψj(Cj)\n",
      "\n",
      "ψ∗ ij(Sij) ψij(Sij)\n",
      "\n",
      "(20.66)\n",
      "\n",
      "(Recall that we initialize ψij(Sij) = 1.) This is sometimes called passing a ﬂow from i to j. On the way down, from i to j, we compute the separator potential\n",
      "\n",
      "ψ∗∗\n",
      "\n",
      "ij (Sij) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψ∗\n",
      "\n",
      "i (Ci)\n",
      "\n",
      "(20.67)\n",
      "\n",
      "Ci\\Sij\n",
      "\n",
      "and then update the recipient potential:\n",
      "\n",
      "j (Cj) ∝ ψ∗ ψ∗∗\n",
      "\n",
      "j (Cj)\n",
      "\n",
      "ψ∗∗ ij (Sij) ψ∗ ij(Sij)\n",
      "\n",
      "(20.68)\n",
      "\n",
      "This process is known as junction tree calibration. See Figure 20.1 for an illustration. Its correctness follows from the fact that each edge partitions the evidence into two distinct groups, plus the fact that the tree satisﬁes RIP, which ensures that no information is lost by only performing local computations.\n",
      "\n",
      "20.4.2.1\n",
      "\n",
      "Example: jtree algorithm on a chain\n",
      "\n",
      "It is interesting to see what happens if we apply this process to a chain structured graph such as an HMM. A detailed discussion can be found in (Smyth et al. 1997), but the basic idea is this. The cliques are the edges, and the separators are the nodes, as shown in Figure 20.8. We initialize the potentials as follows: we set ψs = 1 for all the separators, we set ψc(xt−1, xt) = p(xt|xt−1) for clique c = (Xt−1, Xt) , and we set ψc(xt, yt) = p(yt|xt) for clique c = (Xt, Yt).\n",
      "\n",
      "20.4. The junction tree algorithm *\n",
      "\n",
      "725\n",
      "\n",
      "X1 X2\n",
      "\n",
      "X2\n",
      "\n",
      "X2 X3\n",
      "\n",
      "X3\n",
      "\n",
      "X3 X4\n",
      "\n",
      "X1\n",
      "\n",
      "X2\n",
      "\n",
      "X3\n",
      "\n",
      "X4\n",
      "\n",
      "X1 Y1\n",
      "\n",
      "X2 Y2\n",
      "\n",
      "X3 Y3\n",
      "\n",
      "X4 Y4\n",
      "\n",
      "Figure 20.8 The junction tree derived from an HMM/SSM of length T = 4.\n",
      "\n",
      "Next we send messages from left to right. Consider clique (Xt−1, Xt) with potential p(Xt|Xt−1). It receives a message from clique (Xt−2, Xt−1) via separator Xt−1 of the form (cid:7) p(Xt−2, Xt−1|v1:t−1) =p( Xt−1|v1:t−1). When combined with the clique potential,\n",
      "\n",
      "xt−2\n",
      "\n",
      "this becomes the two-slice predictive density\n",
      "\n",
      "p(Xt|Xt−1)p(Xt−1|v1:t−1) = p(Xt−1, Xt|v1:t−1\n",
      "\n",
      "(20.69)\n",
      "\n",
      "The clique (Xt−1, Xt) also receives a message from (Xt, Yt) via separator Xt of the form p(yt|Xt), which corresponds to its local evidence. When combined with the updated clique potential, this becomes the two-slice ﬁltered posterior\n",
      "\n",
      "p(Xt−1, Xt|v1:t−1)p(vt|Xt) = p(Xt−1, Xt|v1:t)\n",
      "\n",
      "(20.70)\n",
      "\n",
      "Thus the messages in the forwards pass are the ﬁltered belief states αt, and the clique potentials γt αt , are the two-slice distributions. In the backwards pass, the messages are the update factors where γt(k) =p( xt = k|v1:T ) and αt(k) =p( xt = k|v1:t). By multiplying by this message, we “swap out” the old αt message and “swap in” the new γt message. We see that the backwards pass involves working with posterior beliefs, not conditional likelihoods. See Section 18.3.2.3 for further discussion of this difference.\n",
      "\n",
      "20.4.3\n",
      "\n",
      "Computational complexity of JTA If all nodes are discrete with K states each, it is clear that the JTA takes O(|C|K w+1) time and space, where |C| is the number of cliques and w is the treewidth of the graph, i.e., the size of the largest clique minus 1. Unfortunately, choosing a triangulation so as to minimize the treewidth is NP-hard, as explained in Section 20.3.2.\n",
      "\n",
      "The JTA can be modiﬁed to handle the case of Gaussian graphical models. The graph-theoretic steps remain unchanged. Only the message computation differs. We just need to deﬁne how to multiply, divide, and marginalize Gaussian potential functions. This is most easily done in information form. See e.g., (Lauritzen 1992; Murphy 1998; Cemgil 2001) for the details. The algorithm takes O(|C|w3) time and O(|C|w2) space. When applied to a chain structured graph, the algorithm is equivalent to the Kalman smoother in Section 18.3.2.\n",
      "\n",
      "726\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "Q1\n",
      "\n",
      "Q2\n",
      "\n",
      "Q3\n",
      "\n",
      "Q4\n",
      "\n",
      "Qn\n",
      "\n",
      "C1\n",
      "\n",
      "C2\n",
      "\n",
      "C3\n",
      "\n",
      ". . .\n",
      "\n",
      "Cm –1\n",
      "\n",
      "Cm\n",
      "\n",
      "A1\n",
      "\n",
      "A2\n",
      "\n",
      ". . .\n",
      "\n",
      "Am–2\n",
      "\n",
      "X\n",
      "\n",
      "Figure 20.9 Encoding a 3-SAT problem on n variables and m clauses as a DGM. The Qs variables are binary random variables. The Ct variables are deterministic functions of the Qs’s, and compute the truth value of each clause. The At nodes are a chain of AND gates, to ensure that the CPT for the ﬁnal x node has bounded size. The double rings denote nodes with deterministic CPDs. Source: Figure 9.1 of (Koller and Friedman 2009). Used with kind permission of Daphne Koller.\n",
      "\n",
      "20.4.4\n",
      "\n",
      "JTA generalizations *\n",
      "\n",
      "We have seen how to use the JTA algorithm to compute posterior marginals in a graphical model. There are several possible generalizations of this algorithm, some of which we mention below. All of these exploit graph decomposition in some form or other. They only differ in terms of how they deﬁne/ compute messages and “beliefs”. The key requirement is that the operators which compute messages form a commutative semiring (see Section 20.3.1).\n",
      "\n",
      "Computing the MAP estimate. We just replace the sum-product with max-product in the collect phase, and use traceback in the distribute phase, as in the Viterbi algorithm (Sec- tion 17.4.4). See (Dawid 1992) for details.\n",
      "\n",
      "Computing the N-most probable conﬁgurations (Nilsson 1998). • Computing posterior samples. The collect pass is the same as usual, but in the distribute pass, we sample variables given the values higher up in the tree, thus generalizing forwards- ﬁltering backwards-sampling for HMMs described in Section 17.4.5. See (Dawid 1992) for details.\n",
      "\n",
      "Solving constraint satisfaction problems (Dechter 2003). • Solving logical reasoning problems (Amir and McIlraith 2005).\n",
      "\n",
      "20.5\n",
      "\n",
      "Computational intractability of exact inference in the worst case\n",
      "\n",
      "As we saw in Sections 20.3.2 and 20.4.3, VE and JTA take time that is exponential in the treewidth of a graph. Since the treewidth can be O(number of nodes) in the worst case, this means these algorithms can be exponential in the problem size.\n",
      "\n",
      "Of course, just because VE and JTA are slow doesn’t mean that there isn’t some smarter algo- rithm out there. Unfortunately, this seems unlikely, since it is easy to show that exact inference is NP-hard (Dagum and Luby 1993). The proof is a simple reduction from the satisﬁability prob-\n",
      "\n",
      "20.5. Computational intractability of exact inference in the worst case\n",
      "\n",
      "727\n",
      "\n",
      "Method Forwards-backwards Belief propagation Variable elimination Junction tree algorithm Loopy belief propagation Convex belief propagation Mean ﬁeld Gibbs sampling\n",
      "\n",
      "Restriction Chains, D or LG Trees, D or LG Low treewidth, D or LG, single query Low treewidth, D or LG Approximate, D or LG Approximate, D or LG Approximate, C-E Approximate\n",
      "\n",
      "Section Section 17.4.3 Section 20.2 Section 20.3 Section 20.4 Section 22.2 Section 22.4.2 Section 21.3 Section 24.2\n",
      "\n",
      "Table 20.4 Summary of some methods that can be used for inference in graphical models. “D” means that all the hidden variables must be discrete. “L-G” means that all the factors must be linear-Gaussian. The term “single query” refers to the restriction that VE only computes one marginal p(xq|xv) at a time. See Section 20.3.3 for a discussion of this point. “C-E” stands for “conjugate exponential”; this means that variational mean ﬁeld only applies to models where the likelihood is in the exponential family, and the prior is conjugate. This includes the D and LG case, but many others as well, as we will see in Section 21.5.\n",
      "\n",
      "In particular, note that we can encode any 3-SAT problem4 as a DGM with deterministic lem. links, as shown in Figure 20.9. We clamp the ﬁnal node, x, to be on, and we arrange the CPTs so that p(x = 1) > 0 iff there a satisfying assignment. Computing any posterior marginal requires evaluating the normalization constant p(x = 1), which represents the probability of the evidence, so inference in this model implicitly solves the SAT problem.\n",
      "\n",
      "(See e.g., (Arora and Barak 2009) for deﬁnitions of these terms.) The intuitive reason for this is that to compute the normalizing constant Z, we have tocount how many satisfying assignments there are. By contrast, MAP estimation is provably easier for some model classes (Greig et al. 1989), since, intuitively speaking, it only requires ﬁnding one satisfying assignment, not counting all of them.\n",
      "\n",
      "In fact, exact inference is #P-hard (Roth 1996), which is even harder than NP-hard.\n",
      "\n",
      "20.5.1\n",
      "\n",
      "Approximate inference\n",
      "\n",
      "Many popular probabilistic models support efficient exact inference, since they are based on chains, trees or low treewidth graphs. But there are many other models for which exact In fact, even simple two node models of the form θ → x may not inference is intractable. support exact inference if the prior on θ is not conjugate to the likelihood p(x|θ).5\n",
      "\n",
      "Therefore we will need to turn to approximate inference methods. See Table 20.4 for a summary of coming attractions. For the most part, these methods do not come with any guarantee as to their accuracy or running time. Theoretical computer scientists would therefore describe them as heuristics rather than approximation algorithms. In fact, one can prove that\n",
      "\n",
      "4. A 3-SAT problem is a logical expression of the form (Q1 ∧ Q2 ∧ ¬Q3) ∨ (Q1 ∧ ¬Q4 ∧ Q5) · · ·, where the Qi are binary variables, and each clause consists of the conjunction of three variables (or their negation). The goal is to ﬁnd a satisfying assignment, which is a set of values for the Qi variables such that the expression evaluates to true. 5. For discrete random variables, conjugacy is not a concern, since discrete distributions are always closed under conditioning and marginalization. Consequently, graph-theoretic considerations are of more importance when discussing inference in models with discrete hidden states.\n",
      "\n",
      "728\n",
      "\n",
      "Chapter 20. Exact inference for graphical models\n",
      "\n",
      "it is not possible to construct polynomial time approximation schemes for inference in general discrete GMs (Dagum and Luby 1993; Roth 1996). Fortunately, we will see that for many of these heuristic methods often perform well in practice.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 20.1 Variable elimination Consider the MRF in Figure 10.14(b).\n",
      "\n",
      "a. Suppose we want to compute the partition function using the elimination ordering ≺= (1, 2, 3, 4, 5, 6),\n",
      "\n",
      "i.e., (cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "ψ12(x1, x2)ψ13(x1, x3)ψ24(x2, x4)ψ34(x3, x4)ψ45(x4, x5)ψ56(x5, x6)(20.71)\n",
      "\n",
      "x6\n",
      "\n",
      "x5\n",
      "\n",
      "x4\n",
      "\n",
      "x3\n",
      "\n",
      "x2\n",
      "\n",
      "x1\n",
      "\n",
      "If we use the variable elimination algorithm, we will create new intermediate factors. What is the largest intermediate factor?\n",
      "\n",
      "b. Add an edge to the original MRF between every pair of variables that end up in the same factor. (These are called ﬁll in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this graph?\n",
      "\n",
      "c. Now consider elimination ordering ≺= (4, 1, 2, 3, 5, 6), i.e.,\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "ψ12(x1, x2)ψ13(x1, x3)ψ24(x2, x4)ψ34(x3, x4)ψ45(x4, x5)ψ56(x5, x6)(20.72)\n",
      "\n",
      "x6\n",
      "\n",
      "x5\n",
      "\n",
      "x3\n",
      "\n",
      "x2\n",
      "\n",
      "x1\n",
      "\n",
      "x4\n",
      "\n",
      "If we use the variable elimination algorithm, we will create new intermediate factors. What is the largest intermediate factor?\n",
      "\n",
      "d. Add an edge to the original MRF between every pair of variables that end up in the same factor. (These are called ﬁll in edges.) Draw the resulting MRF. What is the size of the largest maximal clique in this graph?\n",
      "\n",
      "Exercise 20.2 Gaussian times Gaussian is Gaussian Prove Equation 20.17. Hint: use completing the square.\n",
      "\n",
      "Exercise 20.3 Message passing on a tree Consider the DGM in Figure 20.10 which represents the following ﬁctitious biological model. Each Gi represents the genotype of a person: Gi = 1 if they have a healthy gene and Gi = 2 if they have an unhealthy gene. G2 and G3 may inherit the unhealthy gene from their parent G1. Xi ∈ R is a continuous measure of blood pressure, which is low if you are healthy and high if you are unhealthy. We deﬁne the CPDs as follows\n",
      "\n",
      "p(G2|G1) =\n",
      "\n",
      "p(G3|G1) =\n",
      "\n",
      "p(G1) = [0.5, 0.5]\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "0.9 0.1 0.9 0.1\n",
      "\n",
      "0.1 0.9 0.1 0.9\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(20.73)\n",
      "\n",
      "(20.74)\n",
      "\n",
      "(20.75)\n",
      "\n",
      "p(Xi|Gi = 1) = N (Xi|μ = 50, σ2 = 10) p(Xi|Gi = 2) = N (Xi|μ = 60, σ2 = 10)\n",
      "\n",
      "(20.76)\n",
      "\n",
      "(20.77)\n",
      "\n",
      "The meaning of the matrix for p(G2|G1) is that p(G2 = 1|G1 = 1) = 0.9, p(G2 = 1|G1 = 2) = 0.1, etc.\n",
      "\n",
      "20.5. Computational intractability of exact inference in the worst case\n",
      "\n",
      "729\n",
      "\n",
      "X1\n",
      "\n",
      "G1\n",
      "\n",
      "G2\n",
      "\n",
      "G3\n",
      "\n",
      "X2\n",
      "\n",
      "X3\n",
      "\n",
      "Figure 20.10 A simple DAG representing inherited diseases.\n",
      "\n",
      "a. Suppose you observe X2 = 50, and X1 is unobserved. What is the posterior belief on G1,\n",
      "\n",
      "p(G1|X2 = 50)?\n",
      "\n",
      "i.e.,\n",
      "\n",
      "b. Now suppose you observe X2 = 50 amd X3 = 50. What is p(G1|X2, X3)? Explain your answer\n",
      "\n",
      "intuitively.\n",
      "\n",
      "c. Now suppose X2 = 60, X3 = 60. What is p(G1|X2, X3)? Explain your answer intuitively. d. Now suppose X2 = 50, X3 = 60. What is p(G1|X1, X2)? Explain your answer intuitively.\n",
      "\n",
      "Exercise 20.4 Inference in 2D lattice MRFs Consider an MRF with a 2D m × n lattice graph structure, so each hidden node, Xij, is connected to its 4 nearest neighbors, as in an Ising model. In addition, each hidden node has its own local evidence, Yij. Assume all hidden nodes have K > 2 states. In general, exact inference in such models is intractable, because the maximum cliques of the corresponding triangulated graph have size O(max{m, n}). Suppose m (cid:8) n i.e., the lattice is short and fat.\n",
      "\n",
      "a. How can one efficiently perform exact inference (using a deterministic algorithm) in such models? (By exact inference, I mean computing marginal probabilities P (Xij|(cid:22)y) exactly, where (cid:22)y is all the evidence.) Give a brief description of your method.\n",
      "\n",
      "b. What is the asymptotic complexity (running time) of your algorithm? c. Now suppose the lattice is large and square, so m = n, but all hidden states are binary (ie K = 2). In this case, how can one efficiently exactly compute (using a deterministic algorithm) the MAP estimate arg maxx P (x|y), wherex is the joint assignment to all hidden nodes?\n",
      "\n",
      "21 Variational inference\n",
      "\n",
      "21.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "We have now seen several algorithms for computing (functions of) a posterior distribution. For discrete graphical models, we can use the junction tree algorithm to perform exact inference, as explained in Section 20.4. However, this takes time exponential in the treewidth of the graph, rendering exact inference often impractical. For the case of Gaussian graphical models, exact inference is cubic in the treewidth. However, even this can be too slow if we have many variables. In addition, the JTA does not work for continuous random variables outside of the Gaussian case, nor for mixed discrete-continuous variables, outside of the conditionally Gaussian case.\n",
      "\n",
      "For some simple two node graphical models, of the form x → D, we can compute the exact posterior p(x|D) in closed form, provided the prior p(x) is conjugate to the likelihood, p(D|x) (which means the likelihood must be in the exponential family). See Chapter 5 for some (Note that in this chapter, x represent the unknown variables, whereas in examples of this. Chapter 5, we used θ to represent the unknowns.)\n",
      "\n",
      "In Section 8.4.1, we discussed the Gaussian approximation, which is useful for inference in two node models of the form x → D, where the prior is not conjugate. (For example, Section 8.4.3 applied the method to logistic regression.)\n",
      "\n",
      "In more general settings, we must use approximate inference methods.\n",
      "\n",
      "The Gaussian approximation is simple. However, some posteriors are not naturally modelled using Gaussians. For example, when inferring multinomial parameters, a Dirichlet distribution is a better choice, and when inferring states in a discrete graphical model, a categorical distribution is a better choice.\n",
      "\n",
      "In this chapter, we will study a more general class of deterministic approximate inference algorithms based on variational inference (Jordan et al. 1998; Jaakkola and Jordan 2000; Jaakkola 2001; Wainwright and Jordan 2008a). The basic idea is to pick an approximation q(x) to the distribution from some tractable family, and then to try to make this approximation as close as possible to the true posterior, p∗(x) (cid:2) p(x|D). This reduces inference to an optimization problem. By relaxing the constraints and/or approximating the objective, we can trade accuracy for speed. The bottom line is that variational inference often gives us the speed beneﬁts of MAP estimation but the statistical beneﬁts of the Bayesian approach.\n",
      "\n",
      "732\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "21.2\n",
      "\n",
      "Variational inference\n",
      "\n",
      "Suppose p∗(x) is our true but intractable distribution and q(x) is some approximation, chosen from some tractable family, such as a multivariate Gaussian or a factored distribution. We assume q has some free parameters which we want to optimize so as to make q “similar to” p∗.\n",
      "\n",
      "An obvious cost function to try to minimize is the KL divergence: p∗(x) q(x)\n",
      "\n",
      "KL (p∗||q) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "x\n",
      "\n",
      "p∗(x) log\n",
      "\n",
      "(21.1)\n",
      "\n",
      "However, this is hard to compute, since taking expectations wrt p∗ is assumed to be intractable. A natural alternative is the reverse KL divergence:\n",
      "\n",
      "KL (q||p∗) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "x\n",
      "\n",
      "q(x) log\n",
      "\n",
      "q(x) p∗(x)\n",
      "\n",
      "(21.2)\n",
      "\n",
      "The main advantage of this objective is that computing expectations wrt q is tractable (by choos- ing a suitable form for q). We discuss the statistical differences between these two objectives in Section 21.2.2.\n",
      "\n",
      "Unfortunately, Equation 21.2 is still not tractable as written, since even evaluating p∗(x) = p(x|D) pointwise is hard, since it requires evaluating the intractable normalization constant Z = p(D). However, usually the unnormalized distribution ˜p(x) (cid:2) p(x, D) =p ∗(x)Z is tractable to compute. We therefore deﬁne our new objective function as follows:\n",
      "\n",
      "(21.3) where we are slightly abusing notation, since ˜p is not a normalized distribution. Plugging in the deﬁnition of KL, we get\n",
      "\n",
      "J(q) (cid:2) KL (q||˜p)\n",
      "\n",
      "J(q) =\n",
      "\n",
      "=\n",
      "\n",
      "q(x) Zp∗(x) q(x) p∗(x) = KL (q||p∗) − log Z\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "x (cid:4)\n",
      "\n",
      "x (cid:4)\n",
      "\n",
      "x\n",
      "\n",
      "q(x) log\n",
      "\n",
      "q(x) log\n",
      "\n",
      "q(x) log\n",
      "\n",
      "q(x) ˜p(x)\n",
      "\n",
      "− log Z\n",
      "\n",
      "(21.4)\n",
      "\n",
      "(21.6)\n",
      "\n",
      "(21.5)\n",
      "\n",
      "(21.7)\n",
      "\n",
      "Since Z is a constant, by minimizing J(q), we will force q to become close to p∗.\n",
      "\n",
      "Since KL divergence is always non-negative, we see that J(q) is an upper bound on the NLL\n",
      "\n",
      "(negative log likelihood):\n",
      "\n",
      "J(q) = KL (q||p∗) − log Z ≥ − log Z = − log p(D)\n",
      "\n",
      "(21.8)\n",
      "\n",
      "Alternatively, we can try to maximize the following quantity (in (Koller and Friedman 2009), this is referred to as the energy functional), which is a lower bound on the log likelihood of the data:\n",
      "\n",
      "(21.9) Since this bound is tight when q = p∗, we see that variational inference is closely related to EM (see Section 11.4.7).\n",
      "\n",
      "L(q) (cid:2) −J(q) = −KL (q||p∗) + log Z ≤ log Z = log p(D)\n",
      "\n",
      "21.2. Variational inference\n",
      "\n",
      "733\n",
      "\n",
      "21.2.1\n",
      "\n",
      "Alternative interpretations of the variational objective\n",
      "\n",
      "There are several equivalent ways of writing this objective that provide different insights. One formulation is as follows:\n",
      "\n",
      "J(q) = Eq [log q(x)] + Eq [− log ˜p(x)] = −H (q) +E q [E(x)]\n",
      "\n",
      "(21.10)\n",
      "\n",
      "which is the expected energy (recall E(x) = − log ˜p(x)) minus the entropy of the system. statistical physics, J(q) is called the variational free energy or the Helmholtz free energy.1\n",
      "\n",
      "In\n",
      "\n",
      "Another formulation of the objective is as follows:\n",
      "\n",
      "J(q) =E q [log q(x) − log p(x)p(D|x)]\n",
      "\n",
      "(21.11)\n",
      "\n",
      "= Eq [log q(x) − log p(x) − log p(D|x)] = Eq [− log p(D|x)] + KL (q(x)||p(x))\n",
      "\n",
      "(21.12)\n",
      "\n",
      "(21.13)\n",
      "\n",
      "This is the expected NLL, plus a penalty term that measures how far the approximate posterior is from the exact prior.\n",
      "\n",
      "We can also interpret the variational objective from the point of view of information theory (the so-called bits-back argument). See (Hinton and Camp 1993; Honkela and Valpola 2004), for details.\n",
      "\n",
      "21.2.2\n",
      "\n",
      "Forward or reverse KL? *\n",
      "\n",
      "Since the KL divergence is not symmetric in its arguments, minimizing KL (q||p) wrt q will give different behavior than minimizing KL (p||q). Below we discuss these two different methods.\n",
      "\n",
      "First, consider the reverse KL, KL (q||p), also known as an I-projection or information\n",
      "\n",
      "projection. By deﬁnition, we have (cid:4)\n",
      "\n",
      "KL (q||p) =\n",
      "\n",
      "x\n",
      "\n",
      "q(x) ln\n",
      "\n",
      "q(x) p(x)\n",
      "\n",
      "(21.14)\n",
      "\n",
      "This is inﬁnite if p(x) = 0 and q(x) > 0. Thus if p(x) = 0 we must ensure q(x) = 0. We say that the reverse KL is zero forcing for q. Hence q will typically under-estimate the support of p.\n",
      "\n",
      "Now consider the forwards KL, also known as an M-projection or moment projection:\n",
      "\n",
      "KL (p||q) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "x\n",
      "\n",
      "p(x) ln\n",
      "\n",
      "p(x) q(x)\n",
      "\n",
      "(21.15)\n",
      "\n",
      "This is inﬁnite if q(x) = 0 and p(x) > 0. So if p(x) > 0 we must ensure q(x) > 0. We say that the forwards KL is zero avoiding for q. Hence q will typically over-estimate the support of p.\n",
      "\n",
      "The difference between these methods is illustrated in Figure 21.1. We see that when the true distribution is multimodal, using the forwards KL is a bad idea (assuming q is constrained to be unimodal), since the resulting posterior mode/mean will be in a region of low density, right between the two peaks. In such contexts, the reverse KL is not only more tractable to compute, but also more sensible statistically.\n",
      "\n",
      "734\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 21.1 Illustrating forwards vs reverse KL on a bimodal distribution. The blue curves are the contours of the true distribution p. The red curves are the contours of the unimodal approximation q. (a) Minimizing forwards KL: q tends to “cover” p. (b-c) Minimizing reverse KL: q locks on to one of the two modes. Based on Figure 10.3 of (Bishop 2006b). Figure generated by KLfwdReverseMixGauss.\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.8\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 21.2 Illustrating forwards vs reverse KL on a symmetric Gaussian. The blue curves are the contours of the true distribution p. The red curves are the contours of a factorized approximation q. (a) Minimizing KL (q||p). (b) Minimizing KL (p||q). Based on Figure 10.2 of (Bishop 2006b). Figure generated by KLpqGauss.\n",
      "\n",
      "Another example of the difference is shown in Figure 21.2, where the target distribution is an elongated 2d Gaussian and the approximating distribution is a product of two 1d Gaussians. That is, p(x) = N (x|μ, Λ−1), where (cid:9)\n",
      "\n",
      "μ =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "μ1 μ2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      ", Λ =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "Λ11 Λ12 Λ21 Λ22\n",
      "\n",
      "(21.16)\n",
      "\n",
      "In Figure 21.2(a) we show the result of minimizing KL (q||p). In this simple example, one can show that the solution has the form\n",
      "\n",
      "q(x) =N (x1|m1, Λ−1 m1 = μ1 − Λ−1 m2 = μ2 − Λ−1\n",
      "\n",
      "11 Λ12(m2 − μ2) 22 Λ21(m1 − μ1)\n",
      "\n",
      "11 )N (x2|m2, Λ−1 22 )\n",
      "\n",
      "(21.17) (21.18)\n",
      "\n",
      "(21.19)\n",
      "\n",
      "1. It is called “free” because the variables x are free to vary, rather than being ﬁxed. The variational free energy is a function of the distribution q, whereas the regular energy is a function of the state vector x.\n",
      "\n",
      "21.3. The mean ﬁeld method\n",
      "\n",
      "735\n",
      "\n",
      "Figure 21.2(a) shows that we have correctly captured the mean, but the approximation is too compact: In fact, it is often the case (although not always (Turner et al. 2008)) that minimizing KL (q||p), where q is factorized, results in an approximation that is overconﬁdent.\n",
      "\n",
      "In Figure 21.2(b), we show the result of minimizing KL (p||q). As we show in Exercise 21.7, the optimal solution when minimizing the forward KL wrt a factored approximation is to set q to be the product of marginals. Thus the solution has the form 11 )N (x2|μ2, Λ−1 22 )\n",
      "\n",
      "q(x) = N (x1|μ1, Λ−1\n",
      "\n",
      "its variance is controlled by the direction of smallest variance of p.\n",
      "\n",
      "(21.20)\n",
      "\n",
      "Figure 21.2(b) shows that this is too broad, since it is an over-estimate of the support of p.\n",
      "\n",
      "For the rest of this chapter, and for most of the next, we will focus on minimizing KL (q||p). In Section 22.5, when we discuss expectation proagation, we will discuss ways to locally optimize KL (p||q).\n",
      "\n",
      "One can create a family of divergence measures indexed by a parameter α ∈ R by deﬁning\n",
      "\n",
      "the alpha divergence as follows:\n",
      "\n",
      "Dα(p||q) (cid:2)\n",
      "\n",
      "4 1 − α2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 −\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(x)(1+α)/2q(x)(1−α)/2dx\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(21.21)\n",
      "\n",
      "This measure satisﬁes Dα(p||q) = 0 iff p = q, but is obviously not symmetric, and hence is not a metric. KL (p||q) corresponds to the limit α → 1, whereas KL (q||p) corresponds to the limit α → −1. When α = 0, we get a symmetric divergence measure that is linearly related to the Hellinger distance, deﬁned by (cid:12) !\n",
      "\n",
      "DH (p||q) (cid:2)\n",
      "\n",
      "p(x)\n",
      "\n",
      "1\n",
      "\n",
      "2 − q(x)\n",
      "\n",
      "1 2\n",
      "\n",
      "#2\n",
      "\n",
      "dx\n",
      "\n",
      "(21.22)\n",
      "\n",
      "DH (p||q) is a valid distance metric, that is, Note that satisﬁes the triangle inequality. See (Minka 2005) for details.\n",
      "\n",
      "\"\n",
      "\n",
      "it is symmetric, non-negative and\n",
      "\n",
      "21.3\n",
      "\n",
      "The mean ﬁeld method\n",
      "\n",
      "One of the most popular forms of variational inference is called the mean ﬁeld approxima- tion (Opper and Saad 2001). In this approach, we assume the posterior is a fully factorized approximation of the form (cid:20)\n",
      "\n",
      "q(x) =\n",
      "\n",
      "qi(xi)\n",
      "\n",
      "(21.23)\n",
      "\n",
      "i\n",
      "\n",
      "Our goal is to solve this optimization problem:\n",
      "\n",
      "min q1,...,qD\n",
      "\n",
      "KL (q||p)\n",
      "\n",
      "(21.24)\n",
      "\n",
      "where we optimize over the parameters of each marginal distribution qi. derive a coordinate descent method, where at each step we make the following update:\n",
      "\n",
      "In Section 21.3.1, we\n",
      "\n",
      "log qj(xj) = E−qj [log ˜p(x)] + const\n",
      "\n",
      "(21.25)\n",
      "\n",
      "736\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "Model Ising model Factorial HMM Univariate Gaussian Linear regression Logistic regression Mixtures of Gaussians Latent Dirichlet allocation\n",
      "\n",
      "Section Section 21.3.2 Section 21.4.1 Section 21.5.1 Section 21.5.2 Section 21.8.1.1 Section 21.6.1 Section 27.3.6.3\n",
      "\n",
      "Table 21.1 algorithm.\n",
      "\n",
      "Some models in this book for which we provide detailed derivations of the mean ﬁeld inference\n",
      "\n",
      "where ˜p(x) =p( x, D) is the unnormalized posterior and the notation E−qj [f (x)] means to take the expectation over f (x) with respect to all the variables except for xj. For example, if we have three variables, then\n",
      "\n",
      "E−q2 [f (x)] =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "q(x1)q3(x3)f (x1, x2, x3)\n",
      "\n",
      "(21.26)\n",
      "\n",
      "x1\n",
      "\n",
      "x3\n",
      "\n",
      "where sums get replaced by integrals where necessary.\n",
      "\n",
      "When updating qj, we only need to reason about the variables which share a factor with xj, i.e., the terms in j’s Markov blanket (see Section 10.5.3); the other terms get absorbed into the constant term. Since we are replacing the neighboring values by their mean value, the method is known as mean ﬁeld. This is very similar to Gibbs sampling (Section 24.2), except instead of sending sampled values between neighboring nodes, we send mean values between nodes. This tends to be more efficient, since the mean can be used as a proxy for a large number of samples. (On the other hand, mean ﬁeld messages are dense, whereas samples are sparse; this can make sampling more scalable to very large models.)\n",
      "\n",
      "Of course, updating one distribution at a time can be slow, since it is a form of coordinate descent. Several methods have been proposed to speed up this basic approach, including using pattern search (Honkela et al. 2003), and techniques based on parameter expansion (Qi and Jaakkola 2008). However, we will not consider these methods in this chapter.\n",
      "\n",
      "It is important to note that the mean ﬁeld method can be used to infer discrete or continuous latent quantities, using a variety of parametric forms for qi, as we will see below. This is in contrast to some of the other variational methods we will encounter later, which are more restricted in their applicability. Table 21.1 lists some of the examples of mean ﬁeld that we cover in this book.\n",
      "\n",
      "21.3.1\n",
      "\n",
      "Derivation of the mean ﬁeld update equations\n",
      "\n",
      "Recall that the goal of variational inference is to minimize the upper bound J(q) ≥ − log p(D). Equivalently, we can try to maximize the lower bound\n",
      "\n",
      "L(q) (cid:2) −J(q) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "x\n",
      "\n",
      "q(x) log\n",
      "\n",
      "˜p(x) q(x)\n",
      "\n",
      "≤ log p(D)\n",
      "\n",
      "(21.27)\n",
      "\n",
      "We will do this one term at a time.\n",
      "\n",
      "21.3. The mean ﬁeld method\n",
      "\n",
      "737\n",
      "\n",
      "If we write the objective singling out the terms that involve qj, and regarding all the other\n",
      "\n",
      "terms as constants, we get\n",
      "\n",
      "L(qj) =\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "x (cid:4)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "i (cid:4)\n",
      "\n",
      "qi(xi)\n",
      "\n",
      "qj(xj)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "log ˜p(x) − (cid:24)\n",
      "\n",
      "qi(xi)\n",
      "\n",
      "log ˜p(x) −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "k\n",
      "\n",
      "log qk(xk)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "log qk(xk)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(21.28)\n",
      "\n",
      "(21.29)\n",
      "\n",
      "=\n",
      "\n",
      "xj (cid:4)\n",
      "\n",
      "x−j\n",
      "\n",
      "qj(xj)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i(cid:5)=j (cid:20)\n",
      "\n",
      "qi(xi) log ˜p(x)\n",
      "\n",
      "k\n",
      "\n",
      "xj\n",
      "\n",
      "x−j\n",
      "\n",
      "i(cid:5)=j\n",
      "\n",
      "⎡\n",
      "\n",
      "⎤\n",
      "\n",
      "−\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "qj(xj)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "qi(xi)\n",
      "\n",
      "⎣\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "log qk(xk) +q j(xj)\n",
      "\n",
      "⎦\n",
      "\n",
      "(21.30)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "xj\n",
      "\n",
      "qj(xj) log fj(xj) −\n",
      "\n",
      "x−j\n",
      "\n",
      "i(cid:5)=j\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "k(cid:5)=j\n",
      "\n",
      "qj(xj) log qj(xj) + const\n",
      "\n",
      "(21.31)\n",
      "\n",
      "xj\n",
      "\n",
      "xj\n",
      "\n",
      "where\n",
      "\n",
      "log fj(xj) (cid:2)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "qi(xi) log ˜p(x) = E−qj [log ˜p(x)]\n",
      "\n",
      "(21.32)\n",
      "\n",
      "x−j\n",
      "\n",
      "i(cid:5)=j\n",
      "\n",
      "So we average out all the hidden variables except for xj. Thus we can rewrite L(qj) as follows:\n",
      "\n",
      "L(qj) = −KL (qj||fj)\n",
      "\n",
      "(21.33)\n",
      "\n",
      "We can maximize L by minimizing this KL, which we can do by setting qj = fj, as follows:\n",
      "\n",
      "qj(xj) =\n",
      "\n",
      "1 Zj\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "E−qj [log ˜p(x)]\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(21.34)\n",
      "\n",
      "We can usually ignore the local normalization constant Zj, since we know qj must be a normalized distribution. Hence we usually work with the form\n",
      "\n",
      "log qj(xj) = E−qj [log ˜p(x)] + const\n",
      "\n",
      "(21.35)\n",
      "\n",
      "The functional form of the qj distributions will be determined by the type of variables xj, as If xj is a well as the form of the model. discrete random variable, then qj will be a discrete distribution; if xj is a continuous random variable, then qj will be some kind of pdf. We will see examples of this below.\n",
      "\n",
      "(This is sometimes called free-form optimization.)\n",
      "\n",
      "21.3.2\n",
      "\n",
      "Example: mean ﬁeld for the Ising model\n",
      "\n",
      "Consider the image denoising example from Section 19.4.1, where xi ∈ {−1, +1} are the hidden pixel values of the “clean” image. We have a joint model of the form\n",
      "\n",
      "p(x, y) =p( x)p(y|x)\n",
      "\n",
      "(21.36)\n",
      "\n",
      "738\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "where the prior has the form\n",
      "\n",
      "p(x) =\n",
      "\n",
      "1 Z0\n",
      "\n",
      "exp(−E0(x))\n",
      "\n",
      "(21.37)\n",
      "\n",
      "E0(x) =−\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Wijxixj\n",
      "\n",
      "(21.38)\n",
      "\n",
      "i=1\n",
      "\n",
      "j∈nbri\n",
      "\n",
      "and the likelihood has the form (cid:4) (cid:20)\n",
      "\n",
      "p(y|x) =\n",
      "\n",
      "p(yi|xi) =\n",
      "\n",
      "exp(−Li(xi))\n",
      "\n",
      "(21.39)\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "Therefore the posterior has the form\n",
      "\n",
      "p(x|y) =\n",
      "\n",
      "E(x) =E 0(x) −\n",
      "\n",
      "1 Z\n",
      "\n",
      "exp(−E(x))\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Li(xi)\n",
      "\n",
      "(21.40)\n",
      "\n",
      "(21.41)\n",
      "\n",
      "i\n",
      "\n",
      "We will now approximate this by a fully factored approximation\n",
      "\n",
      "q(x) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "q(xi, μi)\n",
      "\n",
      "(21.42)\n",
      "\n",
      "i\n",
      "\n",
      "where μi is the mean value of node i. To derive the update for the variational parameter μi, we ﬁrst write out log ˜p(x) = −E(x), dropping terms that do not involve xi:\n",
      "\n",
      "log ˜p(x) = xi\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Wijxj + Li(xi) + const\n",
      "\n",
      "(21.43)\n",
      "\n",
      "j∈nbri\n",
      "\n",
      "This only depends on the states of the neighboring nodes. Now we take expectations of this wrt (cid:26)\n",
      "\n",
      "j(cid:5)=i qj(xj) to get ⎛\n",
      "\n",
      "qi(xi) ∝ exp\n",
      "\n",
      "⎝xi\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Wijμj + Li(xi)\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "(21.44)\n",
      "\n",
      "j∈nbri\n",
      "\n",
      "Thus we replace the states of the neighbors by their average values. Let\n",
      "\n",
      "mi =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Wijμj\n",
      "\n",
      "(21.45)\n",
      "\n",
      "j∈nbri\n",
      "\n",
      "be the mean ﬁeld inﬂuence on node i. Also, approximate marginal posterior is given by\n",
      "\n",
      "let L+\n",
      "\n",
      "i (cid:2) Li(+1) and L−\n",
      "\n",
      "i (cid:2) Li(−1). The\n",
      "\n",
      "qi(xi = 1) =\n",
      "\n",
      "ai (cid:2) mi + 0.5(L+\n",
      "\n",
      "emi+L+\n",
      "\n",
      "emi+L+ i + e−mi+L− i − L− i )\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "=\n",
      "\n",
      "1\n",
      "\n",
      "1 + e−2mi+L−\n",
      "\n",
      "i −L+\n",
      "\n",
      "i\n",
      "\n",
      "= sigm(2ai)\n",
      "\n",
      "(21.46)\n",
      "\n",
      "(21.47)\n",
      "\n",
      "21.4. Structured mean ﬁeld *\n",
      "\n",
      "739\n",
      "\n",
      "sample 1, meanfieldH\n",
      "\n",
      "sample 3, meanfieldH\n",
      "\n",
      "mean after 15 sweeps of meanfieldH\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.2\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.4\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.6\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.8\n",
      "\n",
      "−0.8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 21.3 Example of image denoising using mean ﬁeld (with parallel updates and a damping factor of 0.5). We use an Ising prior with Wij = 1 and a Gaussian noise model with σ = 2. We show the results after 1, 3 and 15 iterations across the image. Compare to Figure 24.1. Figure generated by isingImageDenoiseDemo.\n",
      "\n",
      "Similarly, we have qi(xi = −1) = sigm(−2ai). From this we can compute the new mean for site i:\n",
      "\n",
      "μi = Eqi [xi] = qi(xi = +1) · (+1) + qi(xi = −1) · (−1)\n",
      "\n",
      "(21.48)\n",
      "\n",
      "1 1 + e−2ai Hence the update equation becomes\n",
      "\n",
      "μi = tanh\n",
      "\n",
      "=\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Wijμj + 0.5(L+\n",
      "\n",
      "−\n",
      "\n",
      "1 1 + e2ai\n",
      "\n",
      "=\n",
      "\n",
      "i − L− i )\n",
      "\n",
      "eai eai + e−ai\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "−\n",
      "\n",
      "e−ai e−ai + eai\n",
      "\n",
      "= tanh(ai)\n",
      "\n",
      "(21.49)\n",
      "\n",
      "(21.50)\n",
      "\n",
      "j∈nbri\n",
      "\n",
      "See also Exercise 21.6 for an alternative derivation of these equations.\n",
      "\n",
      "We can turn the above equations in to a ﬁxed point algorithm by writing\n",
      "\n",
      "μt\n",
      "\n",
      "i = tanh\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Wijμt−1\n",
      "\n",
      "j + 0.5(L+\n",
      "\n",
      "i − L− i )\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "(21.51)\n",
      "\n",
      "j∈nbri\n",
      "\n",
      "It is usually better to use damped updates of the form\n",
      "\n",
      "⎛\n",
      "\n",
      "⎞\n",
      "\n",
      "i = (1 − λ)μt−1 μt\n",
      "\n",
      "i + λ tanh\n",
      "\n",
      "⎝\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Wijμt−1\n",
      "\n",
      "j + 0.5(L+\n",
      "\n",
      "i − L− i )\n",
      "\n",
      "⎠\n",
      "\n",
      "(21.52)\n",
      "\n",
      "j∈nbri\n",
      "\n",
      "for 0 < λ < 1. We can update all the nodes in parallel, or update them asychronously.\n",
      "\n",
      "Figure 21.3 shows the method in action, applied to a 2d Ising model with homogeneous attractive potentials, Wij = 1. We use parallel updates with a damping factor of λ = 0.5. (If we don’t use damping, we tend to get “checkerboard” artefacts.)\n",
      "\n",
      "21.4\n",
      "\n",
      "Structured mean ﬁeld *\n",
      "\n",
      "Assuming that all the variables are independent in the posterior is a very strong assumption that can lead to poor results. Sometimes we can exploit tractable substructure in our problem, so\n",
      "\n",
      "740\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "x1,1\n",
      "\n",
      "x1,2\n",
      "\n",
      "x1,3\n",
      "\n",
      "x1,1\n",
      "\n",
      "x1,2\n",
      "\n",
      "x1,3\n",
      "\n",
      "x1,1\n",
      "\n",
      "x1,2\n",
      "\n",
      "x1,3\n",
      "\n",
      "x2,1\n",
      "\n",
      "x2,2\n",
      "\n",
      "x2,3\n",
      "\n",
      "x3,1\n",
      "\n",
      "x3,2\n",
      "\n",
      "x3,3\n",
      "\n",
      "x2,1\n",
      "\n",
      "x2,2\n",
      "\n",
      "x2,3\n",
      "\n",
      "x2,1\n",
      "\n",
      "x2,2\n",
      "\n",
      "x2,3\n",
      "\n",
      "y1\n",
      "\n",
      "y2\n",
      "\n",
      "y3\n",
      "\n",
      "x3,1\n",
      "\n",
      "x3,2\n",
      "\n",
      "x3,3\n",
      "\n",
      "x3,1\n",
      "\n",
      "x3,2\n",
      "\n",
      "x3,3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 21.4 chains approximation. Based on Figure 2 of (Ghahramani and Jordan 1997).\n",
      "\n",
      "(a) A factorial HMM with 3 chains.\n",
      "\n",
      "(b) A fully factorized approximation.\n",
      "\n",
      "(c) A product-of-\n",
      "\n",
      "that we can efficiently handle some kinds of dependencies. This is called the structured mean ﬁeld approach (Saul and Jordan 1995). The approach is the same as before, except we group sets of variables together, and we update them simultaneously. (This follows by simply treating all the variables in the i’th group as a single “mega-variable”, and then repeating the derivation in Section 21.3.1.) As long as we can perform efficient inference in each qi, the method is tractable overall. We give an example below. See (Bouchard-Cote and Jordan 2009) for some more recent work in this area.\n",
      "\n",
      "21.4.1\n",
      "\n",
      "Example: factorial HMM\n",
      "\n",
      "Consider the factorial HMM model (Ghahramani and Jordan 1997) introduced in Section 17.6.5. Suppose there are M chains, each of length T , and suppose each hidden node has K states. The model is deﬁned as follows (cid:20)\n",
      "\n",
      "p(x, y) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "p(xtm|xt−1,m)p(yt|xtm)\n",
      "\n",
      "(21.53)\n",
      "\n",
      "m\n",
      "\n",
      "t\n",
      "\n",
      "where p(xtm = k|xt−1,m = j) =A mjk is an entry in the transition matrix for chain m, p(x1m = k|x0m) = p(x1m = k) = πmk, is the initial state distribution for chain m, and\n",
      "\n",
      "p(yt|xt) = N\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "yt|\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "Wmxtm, Σ\n",
      "\n",
      "(21.54)\n",
      "\n",
      "m=1\n",
      "\n",
      "is the observation model, where xtm is a 1-of-K encoding of xtm and Wm is a D × K matrix (assuming yt ∈ RD). Figure 21.4(a) illustrates the model for the case where M = 3. Even though each chain is a priori independent, they become coupled in the posterior due to having an observed common child, yt. The junction tree algorithm applied to this graph takes O(T M KM +1) time. Below we will derive a structured mean ﬁeld algorithm that takes O(T M K 2I) time, where I is the number of mean ﬁeld iterations (typically I ∼ 10 suffices for good performance).\n",
      "\n",
      "21.4. Structured mean ﬁeld *\n",
      "\n",
      "741\n",
      "\n",
      "We can write the exact posterior in the following form:\n",
      "\n",
      "p(x|y) =\n",
      "\n",
      "E(x, y) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "1 2\n",
      "\n",
      "−\n",
      "\n",
      "exp(−E(x, y))\n",
      "\n",
      "T(cid:4)\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "yt −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:11)T\n",
      "\n",
      "Wmxtm\n",
      "\n",
      "Σ−1\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "yt −\n",
      "\n",
      "t=1 (cid:4)\n",
      "\n",
      "m\n",
      "\n",
      "xT 1m ˜πm −\n",
      "\n",
      "T(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "xT tm ˜Amxt−1,m\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "m\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "Wmxtm\n",
      "\n",
      "(21.55)\n",
      "\n",
      "(21.56)\n",
      "\n",
      "m\n",
      "\n",
      "t=2\n",
      "\n",
      "m\n",
      "\n",
      "where ˜Am (cid:2) log Am and ˜πm (cid:2) log πm (both interpreted elementwise).\n",
      "\n",
      "We can approximate the posterior as a product of marginals, as in Figure 21.4(b), but a better approximation is to use a product of chains, as in Figure 21.4(c). Each chain can be tractably updated individually, using the forwards-backwards algorithm. More precisely, we assume\n",
      "\n",
      "q(x|y) =\n",
      "\n",
      "1 Zq\n",
      "\n",
      "M(cid:20)\n",
      "\n",
      "m=1\n",
      "\n",
      "q(x1m|ξ1m)\n",
      "\n",
      "T(cid:20)\n",
      "\n",
      "t=2\n",
      "\n",
      "q(xtm|xt−1,m, ξtm)\n",
      "\n",
      "(21.57)\n",
      "\n",
      "q(x1m|ξ1m) =\n",
      "\n",
      "q(xtm|xt−1,m, ξtm) =\n",
      "\n",
      "k=1\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "(ξ1mkπmk)x1mk ⎛\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "⎝ξtmk\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "(Amjk)xt−1,m,j\n",
      "\n",
      "⎞\n",
      "\n",
      "xtmk\n",
      "\n",
      "⎠\n",
      "\n",
      "(21.58)\n",
      "\n",
      "(21.59)\n",
      "\n",
      "k=1\n",
      "\n",
      "j=1\n",
      "\n",
      "We see that the ξtmk parameters play the role of an approximate local evidence, averaging out the effects of the other chains. This is contrast to the exact local evidence, which couples all the chains together.\n",
      "\n",
      "We can rewrite the approximate posterior as q(x) = 1 Zq\n",
      "\n",
      "exp(−Eq(x)), where\n",
      "\n",
      "Eq(x) =−\n",
      "\n",
      "T(cid:4)\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "xT tm˜ξtm −\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "xT 1m ˜πm −\n",
      "\n",
      "T(cid:4)\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "xT tm ˜Amxt−1,m\n",
      "\n",
      "(21.60)\n",
      "\n",
      "t=1\n",
      "\n",
      "m=1\n",
      "\n",
      "m=1\n",
      "\n",
      "t=2\n",
      "\n",
      "m=1\n",
      "\n",
      "where ˜ξtm = log ξtm. We see that this has the same temporal factors as the exact posterior, but the local evidence term is different. The objective function is given by\n",
      "\n",
      "KL (q||p) =E [E] − E [Eq] − log Zq + log Z\n",
      "\n",
      "(21.61)\n",
      "\n",
      "where the expectations are taken wrt q. One can show (Exercise 21.8) that the update has the form\n",
      "\n",
      "ξtm = exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "WT\n",
      "\n",
      "mΣ−1 ˜ytm − 1 2\n",
      "\n",
      "δm\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(21.62)\n",
      "\n",
      "˜ytm (cid:2) yt −\n",
      "\n",
      "δm (cid:2) diag(WT M(cid:4)\n",
      "\n",
      "mΣ−1Wm)\n",
      "\n",
      "W(cid:8)E [xt,(cid:8)]\n",
      "\n",
      "(21.63)\n",
      "\n",
      "(21.64)\n",
      "\n",
      "(cid:8)(cid:5)=m\n",
      "\n",
      "742\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "The ξtm parameter plays the role of the local evidence, averaging over the neighboring chains. Having computed this for each chain, we can perform forwards-backwards in parallel, using these approximate local evidence terms to compute q(xt,m|y1:T ) for each m and t.\n",
      "\n",
      "The update cost is O(T M K 2) for a full “sweep” over all the variational parameters, since we have to run forwards-backwards M times, for each chain independently. This is the same cost as a fully factorized approximation, but is much more accurate.\n",
      "\n",
      "21.5\n",
      "\n",
      "Variational Bayes\n",
      "\n",
      "So far we have been concentrating on inferring latent variables zi assuming the parameters θ If we of the model are known. Now suppose we want to infer the parameters themselves. k q(θk), we get a method make a fully factorized (i.e., mean ﬁeld) approximation, p(θ|D) ≈ known as variational Bayes or VB (Hinton and Camp 1993; MacKay 1995a; Attias 2000; Beal and Ghahramani 2006; Smidl and Quinn 2005).2 We give some examples of VB below, assuming that there are no latent variables. If we want to infer both latent variables and parameters, and we make an approximation of the form p(θ, z1:N |D) ≈ q(θ) i qi(zi), we get a method known as variational Bayes EM, which we described in Section 21.6.\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "21.5.1\n",
      "\n",
      "Example: VB for a univariate Gaussian\n",
      "\n",
      "Following (MacKay 2003, p429), let us consider how to apply VB to infer the posterior over the parameters for a 1d Gaussian, p(μ, λ|D), where λ = 1/σ2 is the precision. For convenience, we will use a conjugate prior of the form\n",
      "\n",
      "p(μ, λ) = N (μ|μ0, (κ0λ)−1)Ga(λ|a0, b0)\n",
      "\n",
      "(21.65)\n",
      "\n",
      "However, we will use an approximate factored posterior of the form\n",
      "\n",
      "q(μ, λ) = qμ(μ)qλ(λ)\n",
      "\n",
      "(21.66)\n",
      "\n",
      "We do not need to specify the forms for the distributions qμ and qλ; the optimal forms will “fall out” automatically during the derivation (and conveniently, they turn out to be Gaussian and Gamma respectively).\n",
      "\n",
      "You might wonder why we would want to do this, since we know how to compute the exact posterior for this model (Section 4.6.3.7). There are two reasons. First, it is a useful pedagogical exercise, since we can compare the quality of our approximation to the exact posterior. Second, it is simple to modify the method to handle a semi-conjugate prior of the form p(μ, λ) = N (μ|μ0, τ0)Ga(λ|a0, b0), for which exact inference is no longer possible.\n",
      "\n",
      "2. This method was originally called ensemble learning (MacKay 1995a), since we are using an ensemble of parameters (a distribution) instead of a point estimate. However, the term “ensemble learning” is also used to describe methods such as boosting, so we prefer the term VB.\n",
      "\n",
      "21.5. Variational Bayes\n",
      "\n",
      "743\n",
      "\n",
      "21.5.1.1\n",
      "\n",
      "Target distribution\n",
      "\n",
      "The unnormalized log posterior has the form\n",
      "\n",
      "log ˜p(μ, λ) = log p(μ, λ, D) = log p(D|μ, λ) + log p(μ|λ) + log p(λ)\n",
      "\n",
      "(21.67)\n",
      "\n",
      "=\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log λ −\n",
      "\n",
      "λ\n",
      "\n",
      "2\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "(xi − μ)2 −\n",
      "\n",
      "i=1\n",
      "\n",
      "κ0λ 2\n",
      "\n",
      "(μ − μ0)2\n",
      "\n",
      "+\n",
      "\n",
      "1 2\n",
      "\n",
      "log(κ0λ) + (a0 − 1) log λ − b0λ + const\n",
      "\n",
      "(21.68)\n",
      "\n",
      "21.5.1.2\n",
      "\n",
      "Updating qμ(μ)\n",
      "\n",
      "The optimal form for qμ(μ) is obtained by averaging over λ:\n",
      "\n",
      "log qμ(μ) =E qλ [log p(D|μ, λ) + log p(μ|λ)] + const 3\n",
      "\n",
      "= −\n",
      "\n",
      "Eqλ [λ] 2\n",
      "\n",
      "κ0(μ − μ0)2 +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(xi − μ)2\n",
      "\n",
      "4\n",
      "\n",
      "+ const\n",
      "\n",
      "(21.69)\n",
      "\n",
      "(21.70)\n",
      "\n",
      "By completing the square one can show that qμ(μ) = N (μ|μN , κ−1\n",
      "\n",
      "N ), where\n",
      "\n",
      "μN =\n",
      "\n",
      "κ0μ0 + N x κ0 + N\n",
      "\n",
      ", κN = (κ0 + N )Eqλ [λ]\n",
      "\n",
      "(21.71)\n",
      "\n",
      "At this stage we don’t know what qλ(λ) is, and hence we cannot compute E [λ], but we will derive this below.\n",
      "\n",
      "21.5.1.3\n",
      "\n",
      "Updating qλ(λ)\n",
      "\n",
      "The optimal form for qλ(λ) is given by\n",
      "\n",
      "log qλ(λ) =E qμ [log p(D|μ, λ) + log p(μ|λ) + log p(λ)] + const\n",
      "\n",
      "(21.72)\n",
      "\n",
      "= (a0 − 1) log λ − b0λ + (cid:24)\n",
      "\n",
      "−\n",
      "\n",
      "λ\n",
      "\n",
      "2\n",
      "\n",
      "Eqμ\n",
      "\n",
      "κ0(μ − μ0)2 +\n",
      "\n",
      "1 2\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "log λ +\n",
      "\n",
      "(xi − μ)2\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log λ (cid:25)\n",
      "\n",
      "+ const\n",
      "\n",
      "i=1\n",
      "\n",
      "(21.73)\n",
      "\n",
      "We recognize this as the log of a Gamma distribution, hence qλ(λ) = Ga(λ|aN , bN ), where\n",
      "\n",
      "aN = a0 +\n",
      "\n",
      "bN = b0 +\n",
      "\n",
      "N + 1 2\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "1 2\n",
      "\n",
      "Eqμ\n",
      "\n",
      "κ0(μ − μ0)2 +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "(xi − μ)2\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "i=1\n",
      "\n",
      "(21.74)\n",
      "\n",
      "(21.75)\n",
      "\n",
      "744\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "21.5.1.4\n",
      "\n",
      "Computing the expectations\n",
      "\n",
      "To implement the updates, we have to specify how to compute the various expectations. Since q(μ) = N (μ|μN , κ−1\n",
      "\n",
      "N ), we have\n",
      "\n",
      "Eq(μ)\n",
      "\n",
      "Eq(μ) [μ] =μ N (cid:14) 1 κN\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "μ2\n",
      "\n",
      "=\n",
      "\n",
      "+ μ2 N\n",
      "\n",
      "(21.76)\n",
      "\n",
      "(21.77)\n",
      "\n",
      "Since q(λ) = Ga(λ|aN , bN ), we have\n",
      "\n",
      "Eq(λ) [λ] =\n",
      "\n",
      "aN bN\n",
      "\n",
      "(21.78)\n",
      "\n",
      "We can now give explicit forms for the update equations. For q(μ) we have\n",
      "\n",
      "κ0μ0 + N x κ0 + N κN = (κ0 + N )\n",
      "\n",
      "μN =\n",
      "\n",
      "aN bN\n",
      "\n",
      "(21.79)\n",
      "\n",
      "(21.80)\n",
      "\n",
      "and for q(λ) we have\n",
      "\n",
      "aN = a0 +\n",
      "\n",
      "N + 1 2\n",
      "\n",
      "(21.81)\n",
      "\n",
      "bN = b0 + κ0(E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "μ2\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "+ μ2\n",
      "\n",
      "0 − 2E [μ] μ0) +\n",
      "\n",
      "1 2\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "x2 i + E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "μ2\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "− 2E [μ] xi\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(21.82)\n",
      "\n",
      "iteratively. do this here in order to illustrate the iterative updating scheme.)\n",
      "\n",
      "We see that μN and aN are in fact ﬁxed constants, and only κN and bN need to be updated (In fact, one can solve for the ﬁxed points of κN and bN analytically, but we don’t\n",
      "\n",
      "21.5.1.5\n",
      "\n",
      "Illustration\n",
      "\n",
      "Figure 21.5 gives an example of this method in action. The green contours represent the exact posterior, which is Gaussian-Gamma. The dotted red contours represent the variational approximation over several iterations. We see that the ﬁnal approximation is reasonably close to the exact solution. However, it is more “compact” than the true distribution. It is often the case that mean ﬁeld inference underestimates the posterior uncertainty; See Section 21.2.2 for more discussion of this point.\n",
      "\n",
      "21.5.1.6\n",
      "\n",
      "Lower bound *\n",
      "\n",
      "In VB, we are maximizing L(q), which is a lower bound on the log marginal likelihood:\n",
      "\n",
      "(cid:12) (cid:12)\n",
      "\n",
      "L(q) ≤ log p(D) = log\n",
      "\n",
      "p(D|μ, λ)p(μ, λ)dμdλ\n",
      "\n",
      "(21.83)\n",
      "\n",
      "It is very useful to compute the lower bound itself, for three reasons. First, it can be used to assess convergence of the algorithm. Second, it can be used to assess the correctness of one’s\n",
      "\n",
      "21.5. Variational Bayes\n",
      "\n",
      "745\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1.8\n",
      "\n",
      "exact vb\n",
      "\n",
      "1.8\n",
      "\n",
      "exact vb\n",
      "\n",
      "1.6\n",
      "\n",
      "1.6\n",
      "\n",
      "1.4\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "1.2\n",
      "\n",
      "λ\n",
      "\n",
      "1\n",
      "\n",
      "λ\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "μ\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0 μ\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1.8\n",
      "\n",
      "exact vb\n",
      "\n",
      "1.8\n",
      "\n",
      "exact vb\n",
      "\n",
      "1.6\n",
      "\n",
      "1.6\n",
      "\n",
      "1.4\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "1.2\n",
      "\n",
      "λ\n",
      "\n",
      "1\n",
      "\n",
      "λ\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0 μ\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0 μ\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 21.5 (a) Initial guess. (b) After updating qμ. (c) After updating qλ. (d) At convergence (after 5 iterations). Based on 10.4 of (Bishop 2006b). Figure generated by unigaussVbDemo.\n",
      "\n",
      "Factored variational approximation (red) to the Gaussian-Gamma distribution (green).\n",
      "\n",
      "code: as with EM, if the bound does not increase monotonically, there must be a bug. Third, the bound can be used as an approximation to the marginal likelihood, which can be used for Bayesian model selection.\n",
      "\n",
      "Unfortunately, computing this lower bound involves a fair amount of tedious algebra. We work out the details for this example, but for other models, we will just state the results without proof, or even omit discussion of the bound altogether, for brevity.\n",
      "\n",
      "For this model, L(q) can be computed as follows:\n",
      "\n",
      "L(q) =\n",
      "\n",
      "p(D, μ, λ) q(μ, λ) = E [log p(D|μ, λ)] + E [log p(μ|λ)] + E [log p(λ)]\n",
      "\n",
      "(cid:12) (cid:12)\n",
      "\n",
      "q(μ, λ) log\n",
      "\n",
      "dμdλ\n",
      "\n",
      "(21.84)\n",
      "\n",
      "−E [log q(μ)] − E [log q(λ)]\n",
      "\n",
      "(21.85)\n",
      "\n",
      "where all expectations are wrt q(μ, λ). We recognize the last two terms as the entropy of a Gaussian and the entropy of a Gamma distribution, which are given by\n",
      "\n",
      "H\n",
      "\n",
      "H (Ga(aN , bN )) = log Γ(aN ) − (aN − 1)ψ(aN ) − log(bN ) +a N\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "N (μN , κ−1 N )\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "= − 1 2\n",
      "\n",
      "log κN +\n",
      "\n",
      "1 2\n",
      "\n",
      "(1 + log(2π))\n",
      "\n",
      "(21.86)\n",
      "\n",
      "(21.87)\n",
      "\n",
      "746\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "where ψ() is the digamma function.\n",
      "\n",
      "To compute the other terms, we need the following facts:\n",
      "\n",
      "E [log x|x ∼ Ga(a, b)] = ψ(a) − log(b)\n",
      "\n",
      "E\n",
      "\n",
      "E (cid:13)\n",
      "\n",
      "E [x|x ∼ Ga(a, b)] = (cid:13)\n",
      "\n",
      "x|x ∼ N (μ, σ2) x2|x ∼ N (μ, σ2)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "a b = μ = μ + σ2\n",
      "\n",
      "(21.88)\n",
      "\n",
      "(21.89)\n",
      "\n",
      "(21.90)\n",
      "\n",
      "(21.91)\n",
      "\n",
      "For the expected log likelihood, one can show that\n",
      "\n",
      "Eq(μ,λ) [log p(D|μ, λ)]\n",
      "\n",
      "(21.92)\n",
      "\n",
      "= −\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log(2π) +\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "Eq(λ) [log λ] −\n",
      "\n",
      "E [λ]q(λ) 2\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "Eq(μ)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(xi − μ)2\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "= −\n",
      "\n",
      "−\n",
      "\n",
      "N\n",
      "\n",
      "2 N aN 2bN\n",
      "\n",
      "log(2π) + (cid:8)\n",
      "\n",
      "ˆσ2 + x2 − 2μN x + μ2\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "(ψ(aN ) − log bN )\n",
      "\n",
      "N +\n",
      "\n",
      "1 κN\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(21.93)\n",
      "\n",
      "(21.94)\n",
      "\n",
      "where x and ˆσ2 are the empirical mean and variance.\n",
      "\n",
      "For the expected log prior of λ, we have\n",
      "\n",
      "Eq(λ) [log p(λ)] = (a0 − 1)E [log λ] − b0E [λ] +a 0 log b0 − log Γ(a0)\n",
      "\n",
      "= (a0 − 1)(ψ(aN ) − log bN ) − b0\n",
      "\n",
      "aN bN\n",
      "\n",
      "+ a0 log b0 − log Γ(a0)\n",
      "\n",
      "(21.95)\n",
      "\n",
      "(21.96)\n",
      "\n",
      "For the expected log prior of μ, one can show that\n",
      "\n",
      "Eq(μ,λ) [log p(μ|λ)] =\n",
      "\n",
      "=\n",
      "\n",
      "1 2 1 2\n",
      "\n",
      "−\n",
      "\n",
      "log\n",
      "\n",
      "log\n",
      "\n",
      "κ0 2\n",
      "\n",
      "κ0 2π κ0 2π aN bN\n",
      "\n",
      "+ (cid:17)\n",
      "\n",
      "+\n",
      "\n",
      "1 2 1 2 1 κN\n",
      "\n",
      "E [log λ] q(λ) − 1 2 (ψ(aN ) − log bN )\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "Eq(μ,λ)\n",
      "\n",
      "+ (μN − μ0)2\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(μ − μ0)2κ0λ\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(21.97)\n",
      "\n",
      "Putting it altogether, one can show that\n",
      "\n",
      "L(q) =\n",
      "\n",
      "1 2\n",
      "\n",
      "log\n",
      "\n",
      "1 κN\n",
      "\n",
      "+ log Γ(aN ) − aN log bN + const\n",
      "\n",
      "(21.98)\n",
      "\n",
      "This quantity monotonically increases after each VB update.\n",
      "\n",
      "21.5.2\n",
      "\n",
      "Example: VB for linear regression\n",
      "\n",
      "In Section 7.6.4, we discussed an empirical Bayes approach to setting the hyper-parameters for ridge regression known as the evidence procedure. In particular, we assumed a likelihood of the form p(y|X, θ) =N (Xw, λ−1) and a prior of the form p(w) =N (w|0, α−1I). We then\n",
      "\n",
      "21.5. Variational Bayes\n",
      "\n",
      "747\n",
      "\n",
      "computed a type II estimate of α and λ. The same approach was extended in Section 13.7 to handle a prior of the form N (w|0, diag(α)−1), which allows one hyper-parameter per feature, a technique known as automatic relevancy determination.\n",
      "\n",
      "In this section, we derive a VB algorithm for this model. We follow the presentation of\n",
      "\n",
      "(Drugowitsch 2008).3 Initially we will use the following prior:\n",
      "\n",
      "p(w, λ, α) =N (w|0, (λα)−1I)Ga(λ|aλ\n",
      "\n",
      "0 , bλ\n",
      "\n",
      "0 )Ga(α|aα\n",
      "\n",
      "0 , bα 0 )\n",
      "\n",
      "(21.99)\n",
      "\n",
      "We choose to use the following factorized approximation to the posterior:\n",
      "\n",
      "q(w, α, λ) = q(w, λ)q(α)\n",
      "\n",
      "(21.100)\n",
      "\n",
      "Given these assumptions, one can show (see (Drugowitsch 2008)) that the optimal form for the posterior is\n",
      "\n",
      "q(w, α, λ) =N (w|wN , λ−1VN )Ga(λ|aλ\n",
      "\n",
      "N , bλ\n",
      "\n",
      "N )Ga(α|aα\n",
      "\n",
      "N , bα N )\n",
      "\n",
      "(21.101)\n",
      "\n",
      "where\n",
      "\n",
      "V−1 N = A + XX wN = VN XT y N\n",
      "\n",
      "N = aλ aλ\n",
      "\n",
      "N = aα aα\n",
      "\n",
      "N = bλ bλ\n",
      "\n",
      "N = bα bα\n",
      "\n",
      "A = (cid:19)α(cid:20)I =\n",
      "\n",
      "0 +\n",
      "\n",
      "0 +\n",
      "\n",
      "0 +\n",
      "\n",
      "0 +\n",
      "\n",
      "2 1 (||y − Xw||2 + wT 2 D\n",
      "\n",
      "2 1 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "aα N bα N\n",
      "\n",
      "aλ N bλ N\n",
      "\n",
      "I\n",
      "\n",
      "wT\n",
      "\n",
      "N wN + tr(VN )\n",
      "\n",
      "N AwN )\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(21.102)\n",
      "\n",
      "(21.103)\n",
      "\n",
      "(21.104)\n",
      "\n",
      "(21.105)\n",
      "\n",
      "(21.106)\n",
      "\n",
      "(21.107)\n",
      "\n",
      "(21.108)\n",
      "\n",
      "This method can be extended to the ARD case in a straightforward way, by using the following\n",
      "\n",
      "priors:\n",
      "\n",
      "p(w) =N (0, diag(α)−1) D(cid:20)\n",
      "\n",
      "p(α) =\n",
      "\n",
      "Ga(αj|aα\n",
      "\n",
      "0 , bα 0 )\n",
      "\n",
      "(21.109)\n",
      "\n",
      "(21.110)\n",
      "\n",
      "j=1\n",
      "\n",
      "The posterior for w and λ is computed as before, except we use A = diag(aα\n",
      "\n",
      "N /bα\n",
      "\n",
      "Nj ) instead of\n",
      "\n",
      "3. Note that Drugowitsch uses a0, b0 as the hyper-parameters for p(λ) and c0, d0 as the hyper-parameters for p(α), whereas (Bishop 2006b, Sec 10.3) uses a0, b0 as the hyper-parameters for p(α) and treats λ as ﬁxed. To (hopefully) avoid confusion, I use aλ\n",
      "\n",
      "0 , bλ\n",
      "\n",
      "0 as the hyper-parameters for p(λ), and aα\n",
      "\n",
      "0 , bα\n",
      "\n",
      "0 as the hyper-parameters for p(α).\n",
      "\n",
      "748\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "N /bα aα\n",
      "\n",
      "q(α) =\n",
      "\n",
      "N I. The posterior for α has the form\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Ga(αj|aα\n",
      "\n",
      "N , bα\n",
      "\n",
      "Nj )\n",
      "\n",
      "(21.111)\n",
      "\n",
      "j\n",
      "\n",
      "N = aα aα\n",
      "\n",
      "0 +\n",
      "\n",
      "Nj = bα bα\n",
      "\n",
      "0 +\n",
      "\n",
      "1 2 1 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "aλ N bλ N\n",
      "\n",
      "w2\n",
      "\n",
      "N,j + (VN )jj\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(21.112)\n",
      "\n",
      "(21.113)\n",
      "\n",
      "The algorithm alternates between updating q(w, λ) and q(α). Once w and λ have been inferred, the posterior predictive is a Student distribution, as shown in Equation 7.76. Speciﬁcally, for a single data case, we have\n",
      "\n",
      "p(y|x, D) =T\n",
      "\n",
      "(y|wT\n",
      "\n",
      "N x,\n",
      "\n",
      "bλ N aλ N\n",
      "\n",
      "(1 + xT VN x), 2aλ\n",
      "\n",
      "N )\n",
      "\n",
      "(21.114)\n",
      "\n",
      "The exact marginal likelihood, which can be used for model selection, is given by\n",
      "\n",
      "(cid:12) (cid:12) (cid:12)\n",
      "\n",
      "p(D) =\n",
      "\n",
      "p(y|X, w, λ)p(w|α)p(λ)dwdαdλ\n",
      "\n",
      "(21.115)\n",
      "\n",
      "We can compute a lower bound on log p(D) as follows:\n",
      "\n",
      "L(q) =−\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "log(2π) − 1 2\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "aλ N bλ N\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(yi − wT\n",
      "\n",
      "N xi)2 + xT\n",
      "\n",
      "i VN xi\n",
      "\n",
      "+\n",
      "\n",
      "− log Γ(aα\n",
      "\n",
      "− log Γ(aλ\n",
      "\n",
      "1 2\n",
      "\n",
      "log |VN | +\n",
      "\n",
      "2 0 log bλ 0 ) +a λ\n",
      "\n",
      "0 ) +a α\n",
      "\n",
      "D\n",
      "\n",
      "0 log bα\n",
      "\n",
      "0 − bλ 0\n",
      "\n",
      "aλ N bλ N 0 + log Γ(aα\n",
      "\n",
      "+ log Γ(aλ\n",
      "\n",
      "N ) − aα\n",
      "\n",
      "N log bα N\n",
      "\n",
      "N ) − aλ\n",
      "\n",
      "N log bλ\n",
      "\n",
      "N + aλ N\n",
      "\n",
      "(21.116)\n",
      "\n",
      "In the ARD case, the last line becomes\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "− log Γ(aα\n",
      "\n",
      "0 ) +a α\n",
      "\n",
      "0 log bα\n",
      "\n",
      "0 + log Γ(aα\n",
      "\n",
      "N ) − aα\n",
      "\n",
      "N log bα Nj\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(21.117)\n",
      "\n",
      "j=1\n",
      "\n",
      "Figure 21.6 compare VB and EB on a model selection problem for polynomial regression. We see that VB gives similar results to EB, but the precise behavior depends on the sample size. When N = 5, VB’s estimate of the posterior over models is more diffuse than EB’s, since VB models uncertainty in the hyper-parameters. When N = 30, the posterior estimate of the hyper- Indeed, if we compute E [α|D] when we have an parameters becomes more well-determined. uninformative prior, aα\n",
      "\n",
      "0 = bα\n",
      "\n",
      "0 = 0, we get\n",
      "\n",
      "α =\n",
      "\n",
      "aα N bα N\n",
      "\n",
      "=\n",
      "\n",
      "1 2 (\n",
      "\n",
      "aλ N bλ N\n",
      "\n",
      "D/2\n",
      "\n",
      "wT\n",
      "\n",
      "N wN + tr(VN ))\n",
      "\n",
      "(21.118)\n",
      "\n",
      "21.6. Variational Bayes EM\n",
      "\n",
      "749\n",
      "\n",
      "N=5, method=VB\n",
      "\n",
      "N=5, method=EB\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      ")\n",
      "\n",
      "D M P\n",
      "\n",
      "|\n",
      "\n",
      "(\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      ")\n",
      "\n",
      "D M P\n",
      "\n",
      "|\n",
      "\n",
      "(\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "M\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "M\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "N=30, method=VB\n",
      "\n",
      "N=30, method=EB\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      ")\n",
      "\n",
      "D M P\n",
      "\n",
      "|\n",
      "\n",
      "(\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      ")\n",
      "\n",
      "D M P\n",
      "\n",
      "|\n",
      "\n",
      "(\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "M\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "M\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 21.6 We plot the posterior over models (polynomials of degree 1, 2 and 3) assuming a uniform prior p(m) ∝ 1. We approximate the marginal likelihood using (a,c) VB and (b,d) EB. In (a-b), we use N = 5 data points (shown in Figure 5.7). In (c-d), we use N = 30 data points (shown in Figure 5.8). Figure generated by linregEbModelSelVsN.\n",
      "\n",
      "Compare this to Equation 13.167 for EB:\n",
      "\n",
      "ˆα =\n",
      "\n",
      "D E [wT w]\n",
      "\n",
      "=\n",
      "\n",
      "wT\n",
      "\n",
      "D N wN + tr(VN )\n",
      "\n",
      "(21.119)\n",
      "\n",
      "Modulo the aλ In hindsight this is perhaps not that surprising, since EB is trying to maximize log p(D), and VB is trying to maximize a lower bound on log p(D).\n",
      "\n",
      "N and bλ\n",
      "\n",
      "N terms, these are the same.\n",
      "\n",
      "21.6\n",
      "\n",
      "Variational Bayes EM\n",
      "\n",
      "Now consider latent variable models of the form zi → xi ← θ. This includes mixtures models, PCA, HMMs, etc. There are now two kinds of unknowns: parameters, θ, and latent variables, zi. As we saw in Section 11.4, it is common to ﬁt such models using EM, where in the E step we infer the posterior over the latent variables, p(zi|xi, θ), and in the M step, we compute a point estimate of the parameters, θ. The justiﬁcation for this is two-fold. First, it results in simple algorithms. Second, the posterior uncertainty in θ is usually less than in zi, since the θ are informed by all N data cases, whereas zi is only informed by xi; this makes a MAP estimate of\n",
      "\n",
      "750\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "θ more reasonable than a MAP estimate of zi.\n",
      "\n",
      "However, VB provides a way to be “more Bayesian”, by modeling uncertainty in the parameters θ as well in the latent variables zi, at a computational cost that is essentially the same as EM. This method is known as variational Bayes EM or VBEM. The basic idea is to use mean ﬁeld, where the approximate posterior has the form\n",
      "\n",
      "p(θ, z1:N |D) ≈ q(θ)q(z) = q(θ)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "q(zi)\n",
      "\n",
      "(21.120)\n",
      "\n",
      "i\n",
      "\n",
      "The ﬁrst factorization, between θ and z, is a crucial assumption to make the algorithm tractable. The second factorization follows from the model, since the latent variables are iid conditional on θ.\n",
      "\n",
      "In VBEM, we alternate between updating q(zi|D) (the variational E step) and updating q(θ|D) (the variational M step). We can recover standard EM from VBEM by approximating the param- eter posterior using a delta function, q(θ|D) ≈ δˆθ(θ).\n",
      "\n",
      "The variational E step is similar to a standard E step, except instead of plugging in a MAP estimate of the parameters and computing p(zi|D, ˆθ), we need to average over the parameters. Roughly speaking, this can be computed by plugging in the posterior mean of the parameters instead of the MAP estimate, and then computing p(zi|D, θ) using standard algorithms, such as forwards-backwards. Unfortunately, things are not quite this simple, but this is the basic idea. The details depend on the form of the model; we give some examples below.\n",
      "\n",
      "The variational M step is similar to a standard M step, except instead of computing a point estimate of the parameters, we update the hyper-parameters, using the expected sufficient statis- tics. This process is usually very similar to MAP estimation in regular EM. Again, the details on how to do this depend on the form of the model.\n",
      "\n",
      "The principle advantage of VBEM over regular EM is that by marginalizing out the parameters, we can compute a lower bound on the marginal likelihood, which can be used for model selection. We will see an example of this in Section 21.6.1.6. VBEM is also “egalitarian”, since it treats parameters as “ﬁrst class citizens”, just like any other unknown quantity, whereas EM makes an artiﬁcial distinction between parameters and latent variables.\n",
      "\n",
      "21.6.1\n",
      "\n",
      "Example: VBEM for mixtures of Gaussians *\n",
      "\n",
      "Let us consider how to “ﬁt” a mixture of Gaussians using VBEM. (We use scare quotes since we are not estimating the model parameters, but inferring a posterior over them.) We will follow the presentation of (Bishop 2006b, Sec 10.2). Unfortunately, the details are rather complicated. Fortunately, as with EM, one gets used to it after a bit of practice. (As usual with math, simply reading the equations won’t help much, you should really try deriving these results yourself (or try some of the exercises) if you want to learn this stuff in depth.)\n",
      "\n",
      "21.6.1.1\n",
      "\n",
      "The variational posterior\n",
      "\n",
      "The likelihood function is the usual one for Gaussian mixture models: πzik k N (xi|μk, Λ−1\n",
      "\n",
      "p(z, X|θ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "k )zik\n",
      "\n",
      "(21.121)\n",
      "\n",
      "i\n",
      "\n",
      "k\n",
      "\n",
      "where zik = 1 if data point i belongs to cluster k, and zik = 0 otherwise.\n",
      "\n",
      "21.6. Variational Bayes EM\n",
      "\n",
      "751\n",
      "\n",
      "We will assume the following factored conjugate prior\n",
      "\n",
      "p(θ) = Dir(π|α0)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "N (μk|m0, (β0Λk)−1)Wi(Λk|L0, ν0)\n",
      "\n",
      "(21.122)\n",
      "\n",
      "k\n",
      "\n",
      "where Λk is the precision matrix for cluster k. The subscript 0 means these are parameters of the prior; we assume all the prior parameters are the same for all clusters. For the mixing weights, we usually use a symmetric prior, α0 = α01.\n",
      "\n",
      "The exact posterior p(z, θ|D) is a mixture of K N distributions, corresponding to all possible labelings z. We will try to approximate the volume around one of these modes. We will use the standard VB approximation to the posterior:\n",
      "\n",
      "p(θ, z1:N |D) ≈ q(θ)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "q(zi)\n",
      "\n",
      "(21.123)\n",
      "\n",
      "i\n",
      "\n",
      "At this stage we have not speciﬁed the forms of the q functions; these will be determined by the form of the likelihood and prior. Below we will show that the optimal form is as follows:\n",
      "\n",
      "q(z, θ) =q\n",
      "\n",
      "(z|θ)q(θ) = (cid:24)\n",
      "\n",
      "Dir(π|α)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "N (μk|mk, (βkΛk)−1)Wi(Λk|Lk, νk)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "i\n",
      "\n",
      "Cat(zi|ri)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(21.124)\n",
      "\n",
      "(21.125)\n",
      "\n",
      "k\n",
      "\n",
      "(The lack of 0 subscript means these are parameters of the posterior, not the prior.) Below we will derive the update equations for these variational parameters.\n",
      "\n",
      "21.6.1.2\n",
      "\n",
      "Derivation of q(z) (variational E step)\n",
      "\n",
      "The form for q(z) can be obtained by looking at the complete data log joint, ignoring terms that do not involve z, and taking expectations of what’s left over wrt all the hidden variables except for z. We have\n",
      "\n",
      "log q(z) =E q(θ) [log p(x, z, θ)] + const (cid:4)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "zik log ρik + const\n",
      "\n",
      "(21.126)\n",
      "\n",
      "(21.127)\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "where we deﬁne\n",
      "\n",
      "log ρik (cid:2) Eq(θ) [log πk] + − 1 2\n",
      "\n",
      "Eq(θ)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(xi − μk)T Λk(xi − μk)\n",
      "\n",
      "1 2\n",
      "\n",
      "Eq(θ) [log |Λk|] − (cid:14)\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "log(2π)\n",
      "\n",
      "(21.128)\n",
      "\n",
      "Using the fact that q(π) = Dir(π), we have\n",
      "\n",
      "log ˜πk (cid:2) E [log πk] = ψ(αk) − ψ(\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "αk(cid:2) )\n",
      "\n",
      "(21.129)\n",
      "\n",
      "k(cid:2)\n",
      "\n",
      "752\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "where ψ() is the digamma function. (See Exercise 21.5 for the detailed derivation.) Next, we use the fact that\n",
      "\n",
      "q(μk, Λk) = N (μk|mk, (βkΛk)−1)Wi(Λk|Lk, νk)\n",
      "\n",
      "(21.130)\n",
      "\n",
      "to get\n",
      "\n",
      "log ˜Λk (cid:2) E [log |Λk|] =\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "j=1\n",
      "\n",
      "ψ\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "νk + 1− j 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "+ D log 2 + log |Λk|\n",
      "\n",
      "(21.131)\n",
      "\n",
      "Finally, for the expected value of the quadratic form, we get\n",
      "\n",
      "E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(xi − μk)T Λk(xi − μk)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "= Dβ−1\n",
      "\n",
      "k + νk(xi − mk)T Λk(xi − mk)\n",
      "\n",
      "(21.132)\n",
      "\n",
      "Putting it altogether, we get that the posterior responsibility of cluster k for datapoint i is\n",
      "\n",
      "rik ∝ ˜πk ˜Λ\n",
      "\n",
      "k exp\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "−\n",
      "\n",
      "D 2βk\n",
      "\n",
      "−\n",
      "\n",
      "νk 2\n",
      "\n",
      "(xi − mk)T Λk(xi − mk)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(21.133)\n",
      "\n",
      "Compare this to the expression used in regular EM:\n",
      "\n",
      "rEM ik\n",
      "\n",
      "∝ ˆπk| ˆΛ|\n",
      "\n",
      "k exp\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "(xi − ˆμk)T ˆΛk(xi − ˆμk)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(21.134)\n",
      "\n",
      "The signiﬁcance of this difference is discussed further in Section 21.6.1.7.\n",
      "\n",
      "21.6.1.3\n",
      "\n",
      "Derivation of q(θ) (variational M step)\n",
      "\n",
      "Using the mean ﬁeld recipe, we have\n",
      "\n",
      "log q(θ) = log p(π) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "log p(μk, Λk) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Eq(z) [log p(zi|π)]\n",
      "\n",
      "+\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "k\n",
      "\n",
      "i\n",
      "\n",
      "Eq(z) [zik] log N (xi|μk, Λ−1\n",
      "\n",
      "k ) + const\n",
      "\n",
      "(21.135)\n",
      "\n",
      "k\n",
      "\n",
      "i\n",
      "\n",
      "We see this factorizes into the form\n",
      "\n",
      "q(θ) =q\n",
      "\n",
      "(π)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "q(μk, Λk)\n",
      "\n",
      "(21.136)\n",
      "\n",
      "k\n",
      "\n",
      "For the π term, we have\n",
      "\n",
      "log q(π) = (α0 − 1)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "log πk +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "rik log πk + const\n",
      "\n",
      "(21.137)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "i\n",
      "\n",
      "Exponentiating, we recognize this as a Dirichlet distribution:\n",
      "\n",
      "q(π) = Dir(π|α) αk = α0 + Nk Nk =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "rik\n",
      "\n",
      "(21.138) (21.139)\n",
      "\n",
      "(21.140)\n",
      "\n",
      "i\n",
      "\n",
      "21.6. Variational Bayes EM\n",
      "\n",
      "753\n",
      "\n",
      "variational Bayes objective for GMM on old faithful data\n",
      "\n",
      "−600\n",
      "\n",
      "−650\n",
      "\n",
      "d o o h\n",
      "\n",
      "i l\n",
      "\n",
      "e k\n",
      "\n",
      "i l\n",
      "\n",
      "−700\n",
      "\n",
      "−750\n",
      "\n",
      "l\n",
      "\n",
      "a n g r a m g o\n",
      "\n",
      "i\n",
      "\n",
      "l\n",
      "\n",
      "−800\n",
      "\n",
      "−850\n",
      "\n",
      "n o d n u o b\n",
      "\n",
      "−900\n",
      "\n",
      "−950\n",
      "\n",
      "r e w o\n",
      "\n",
      "l\n",
      "\n",
      "−1000\n",
      "\n",
      "−1050\n",
      "\n",
      "−1100\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "80\n",
      "\n",
      "100\n",
      "\n",
      "iter\n",
      "\n",
      "Figure 21.7 Lower bound vs iterations for the VB algorithm in Figure 21.8. The steep parts of the curve correspond to places where the algorithm ﬁgures out that it can increase the bound by “killing off” unnecessary mixture components, as described in Section 21.6.1.6. The plateaus correspond to slowly moving the clusters around. Figure generated by mixGaussVbDemoFaithful.\n",
      "\n",
      "For the μk and Λk terms, we have q(μk, Λk) =N (μk|mk, (βkΛk)−1)Wi(Λk|Lk, νk)\n",
      "\n",
      "(21.141)\n",
      "\n",
      "βk = β0 + Nk mk = (β0m0 + Nkxk)/βk\n",
      "\n",
      "(21.142)\n",
      "\n",
      "(21.143)\n",
      "\n",
      "L−1 k\n",
      "\n",
      "= L−1\n",
      "\n",
      "0 + NkSk +\n",
      "\n",
      "β0Nk β0 + Nk\n",
      "\n",
      "(xk − m0)(xk − m0)T\n",
      "\n",
      "(21.144)\n",
      "\n",
      "xk =\n",
      "\n",
      "Sk =\n",
      "\n",
      "νk = ν0 + Nk + 1 1 Nk\n",
      "\n",
      "1 Nk\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i (cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "rikxi\n",
      "\n",
      "rik(xi − xk)(xi − xk)T\n",
      "\n",
      "(21.145)\n",
      "\n",
      "(21.146)\n",
      "\n",
      "(21.147)\n",
      "\n",
      "we are computing the parameters of the posterior over θ, rather than MAP estimates of θ.\n",
      "\n",
      "This is very similar to the M step for MAP estimation discussed in Section 11.4.2.8, except here\n",
      "\n",
      "21.6.1.4\n",
      "\n",
      "Lower bound on the marginal likelihood\n",
      "\n",
      "The algorithm is trying to maximize the following lower bound p(x, z, θ) q(z, θ)\n",
      "\n",
      "L =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "z\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "q(z, θ) log\n",
      "\n",
      "dθ ≤ log p(D)\n",
      "\n",
      "(21.148)\n",
      "\n",
      "This quantity should increase monotonically with each iteration, as shown in Figure 21.7. Un- fortunately, deriving the bound is a bit messy, because we need to compute expectations of the unnormalized log posterior as well as entropies of the q distribution. We leave the details (which are similar to Section 21.5.1.6) to Exercise 21.4.\n",
      "\n",
      "754\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "21.6.1.5\n",
      "\n",
      "Posterior predictive distribution\n",
      "\n",
      "We showed that the approximate posterior has the form\n",
      "\n",
      "q(θ) = Dir(π|α)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "N (μk|mk, (βkΛk)−1)Wi(Λk|Lk, νk)\n",
      "\n",
      "(21.149)\n",
      "\n",
      "k\n",
      "\n",
      "Consequently the posterior predictive density can be approximated as follows, using the results from Section 4.6.3.6: (cid:4)\n",
      "\n",
      "p(x|D) ≈\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(x|z, θ)p(z|θ)q(θ)dθ\n",
      "\n",
      "(21.150)\n",
      "\n",
      "=\n",
      "\n",
      "z (cid:4)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "πkN (x|μk, Λ−1\n",
      "\n",
      "k )q(θ)dθ\n",
      "\n",
      "(21.151)\n",
      "\n",
      "=\n",
      "\n",
      "Mk =\n",
      "\n",
      "k (cid:4)\n",
      "\n",
      "k(cid:2) αk(cid:2) k (νk + 1− D)βk 1 + βk\n",
      "\n",
      "αk(cid:7)\n",
      "\n",
      "T (x|mk, Mk, νk + 1− D)\n",
      "\n",
      "Lk\n",
      "\n",
      "(21.152)\n",
      "\n",
      "(21.153)\n",
      "\n",
      "This is just a weighted sum of Student distributions. If instead we used a plug-in approximation, we would get a weighted sum of Gaussian distributions.\n",
      "\n",
      "21.6.1.6 Model selection using VBEM\n",
      "\n",
      "The simplest way to select K when using VB is to ﬁt several models, and then to use the variational lower bound to the log marginal likelihood, L(K) ≤ log p(D|K), to approximate p(K|D):\n",
      "\n",
      "p(K|D) =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "eL(K) K(cid:2) eL(K(cid:2))\n",
      "\n",
      "(21.154)\n",
      "\n",
      "However, the lower bound needs to be modiﬁed somewhat to take into account the lack of identiﬁability of the parameters (Section 11.3.1). In particular, although VB will approximate the volume occupied by the parameter posterior, it will only do so around one of the local modes. With K components, there are K! equivalent modes, which differ merely by permuting the labels. Therefore we should use log p(D|K) ≈ L(K) + log(K!).\n",
      "\n",
      "21.6.1.7\n",
      "\n",
      "Automatic sparsity inducing effects of VBEM\n",
      "\n",
      "Although VB provides a reasonable approximation to the marginal likelihood (better than BIC (Beal and Ghahramani 2006)), this method still requires ﬁtting multiple models, one for each value of K being considered. A faster alternative is to ﬁt a single model, where K is set large, but where α0 is set very small, α0 (cid:22) 1. From Figure 2.14(d), we see that the resulting prior for the mixing weights π has “spikes” near the corners of the simplex, encouraging a sparse mixing weight vector.\n",
      "\n",
      "In regular EM, the MAP estimate of the mixing weights will have the form ˆπk ∝ (αk − 1), where αk = α0 + Nk. Unforuntately, this can be negative if α0 = 0 and Nk = 0 (Figueiredo\n",
      "\n",
      "21.6. Variational Bayes EM\n",
      "\n",
      "755\n",
      "\n",
      "2\n",
      "\n",
      "iter 1\n",
      "\n",
      "2\n",
      "\n",
      "iter 94\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "5\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "6\n",
      "\n",
      "0.5\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "2\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "−1.5\n",
      "\n",
      "3\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−2.5\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "−2.5\n",
      "\n",
      "−2\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1\n",
      "\n",
      "−0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 21.8 We visualize the posterior mean parameters at various stages of the VBEM algorithm applied to a mixture of Gaussians model on the Old Faithful data. Shading intensity is proportional to the mixing weight. We initialize with K-means and use α0 = 0.001 as the Dirichlet hyper-parameter. Based on Figure 10.6 of (Bishop 2006b). Figure generated by mixGaussVbDemoFaithful, based on code by Emtiyaz Khan.\n",
      "\n",
      "90\n",
      "\n",
      "iter 1\n",
      "\n",
      "180\n",
      "\n",
      "iter 94\n",
      "\n",
      "80\n",
      "\n",
      "160\n",
      "\n",
      "70\n",
      "\n",
      "140\n",
      "\n",
      "60\n",
      "\n",
      "120\n",
      "\n",
      "50\n",
      "\n",
      "100\n",
      "\n",
      "40\n",
      "\n",
      "80\n",
      "\n",
      "30\n",
      "\n",
      "60\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 21.9 We visualize the posterior values of αk for the model in Figure 21.8. We see that unnecessary components get “killed off”. Figure generated by mixGaussVbDemoFaithful.\n",
      "\n",
      "and Jain 2002). However, in VBEM, we use exp[Ψ(αk)] (cid:7)\n",
      "\n",
      "˜πk =\n",
      "\n",
      "exp[Ψ(\n",
      "\n",
      "k(cid:2) αk(cid:2) )]\n",
      "\n",
      "(21.155)\n",
      "\n",
      "Now exp(Ψ(x)) ≈ x − 0.5 for x >1. So if αk = 0, when we compute ˜πk, it’s like we substract 0.5 from the posterior counts. This will hurt small clusters more than large clusters (like a regressive tax).4 The effect is that clusters which have very few (weighted) members become more and more empty over successive iterations, whereas the popular clusters get more and more members. This is called the rich get richer phenomenon; we will encounter it again in Section 25.2, when we discuss Dirichlet process mixture models.\n",
      "\n",
      "This automatic pruning method is demonstrated in Figure 21.8. We ﬁt a mixture of 6 Gaussians to the Old Faithful dataset, but the data only really “needs” 2 clusters, so the rest get “killed off”.\n",
      "\n",
      "4. For more details, see (Liang et al. 2007).\n",
      "\n",
      "756\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "In this example, we used α0 = 0.001; if we use a larger α0, we do not get a sparsity effect. In Figure 21.9, we plot q(α|D) at various iterations; we see that the unwanted components get extinguished. This provides an efficient alternative to performing a discrete search over the number of clusters.\n",
      "\n",
      "21.7\n",
      "\n",
      "Variational message passing and VIBES\n",
      "\n",
      "We have seen that mean ﬁeld methods, at least of the fully-factorized variety, are all very similar: just compute each node’s full conditional, and average out the neighbors. This is very similar to Gibbs sampling (Section 24.2), except the derivation of the equations is usually a bit more work. Fortunately it is possible to derive a general purpose set of update equations that work for any DGM for which all CPDs are in the exponential family, and for which all parent nodes have conjugate distributions (Ghahramani and Beal 2001). (See (Wand et al. 2011) for a recent extension to handle non-conjugate priors.) One can then sweep over the graph, updating nodes one at a time, in a manner similar to Gibbs sampling. This is known as variational message passing or VMP (Winn and Bishop 2005), and has been implemented in the open-source program VIBES5. This is a VB analog to BUGS, which is a popular generic program for Gibbs sampling discussed in Section 24.2.6.\n",
      "\n",
      "VMP/ mean ﬁeld is best-suited to inference where one or more of the hidden nodes are continuous (e.g., when performing “Bayesian learning”). For models where all the hidden nodes are discrete, more accurate approximate inference algorithms can be used, as we discuss in Chapter 22.\n",
      "\n",
      "21.8\n",
      "\n",
      "Local variational bounds *\n",
      "\n",
      "So far, we have been focusing on mean ﬁeld inference, which is a form of variational inference based on minimizing KL (q||˜p), where q is the approximate posterior, assumed to be factorized, and ˜p is the exact (but unnormalized) posterior. However, there is another kind of variational inference, where we replace a speciﬁc term in the joint distribution with a simpler function, to simplify computation of the posterior. Such an approach is sometimes called a local variational approximation, since we are only modifying one piece of the model, unlike mean ﬁeld, which is a global approximation. In this section, we study several examples of this method.\n",
      "\n",
      "21.8.1 Motivating applications\n",
      "\n",
      "Before we explain how to derive local variational bounds, we give some examples of where this is useful.\n",
      "\n",
      "21.8.1.1\n",
      "\n",
      "Variational logistic regression\n",
      "\n",
      "Consider the problem of how to approximate the parameter posterior for multiclass logistic regression model under a Gaussian prior. One approach is to use a Gaussian (Laplace) approx- imation, as discussed in Section 8.4.3. However, a variational approach can produce a more\n",
      "\n",
      "5. Available at http://vibes.sourceforge.net/.\n",
      "\n",
      "21.8. Local variational bounds *\n",
      "\n",
      "757\n",
      "\n",
      "accurate approximation to the posterior, since it has tunable parameters. Another advantage is that the variational approach monotonically optimizes a lower bound on the likelihood of the data, as we will see.\n",
      "\n",
      "To see why we need a bound, note that the likelihood can be written as follows:\n",
      "\n",
      "p(y|X, w) =\n",
      "\n",
      "N(cid:20)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "yT i ηi − lse(ηi)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(21.156)\n",
      "\n",
      "i=1\n",
      "\n",
      "where ηi = Xiwi = [xT identiﬁability), and where we deﬁne the log-sum-exp or lse function as follows:\n",
      "\n",
      "lse(ηi) (cid:2) log\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "1 +\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "i w1, . . . , xT\n",
      "\n",
      "eηim\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "i wM ], where M = C − 1 (since we set wC = 0 for\n",
      "\n",
      "(21.157)\n",
      "\n",
      "m=1\n",
      "\n",
      "The main problem is that this likelihood is not conjugate to the Gaussian prior. Below we discuss how to compute “Gaussian-like” lower bounds to this likelihood, which give rise to approximate Gaussian posteriors.\n",
      "\n",
      "21.8.1.2 Multi-task learning\n",
      "\n",
      "One important application of Bayesian inference for logistic regression is where we have multiple related classiﬁers we want to ﬁt. In this case, we want to share information between the parameters for each classiﬁer; this requires that we maintain a posterior distibution over the parameters, so we have a measure of conﬁdence as well as an estimate of the value. We can embed the above variational method inside of a larger hierarchical model in order to perform such multi-task learning, as described in e.g., (Braun and McAuliffe 2010).\n",
      "\n",
      "21.8.1.3\n",
      "\n",
      "Discrete factor analysis\n",
      "\n",
      "Another situation where variational bounds are useful arises when we ﬁt a factor analysis model to discrete data. This model is just like multinomial logistic regression, except the input variables are hidden factors. We need to perform inference on the hidden variables as well as the regression weights. For simplicity, we might perform point estimation of the weights, and just integrate out the hidden variables. We can do this using variational EM, where we use the variational bound in the E step. See Section 12.4 for details.\n",
      "\n",
      "21.8.1.4\n",
      "\n",
      "Correlated topic model\n",
      "\n",
      "A topic model is a latent variable model for text documents and other forms of discrete data; see Section 27.3 for details. Often we assume the distribution over topics has a Dirichlet prior, but a more powerful model, known as the correlated topic model, uses a Gaussian prior, which can model correlations more easily (see Section 27.4.1 for details). Unfortunately, this also involves the lse function. However, we can use our variational bounds in the context of a variational EM algorithm, as we will see later.\n",
      "\n",
      "758\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "21.8.2\n",
      "\n",
      "Bohning’s quadratic bound to the log-sum-exp function\n",
      "\n",
      "All of the above examples require dealing with multiplying a Gaussian prior by a multinomial likelihood; this is difficult because of the log-sum-exp (lse) term. In this section, we derive a way to derive a “Gaussian-like” lower bound on this likelihood.\n",
      "\n",
      "Consider a Taylor series expansion of the lse function around ψi ∈ RM :\n",
      "\n",
      "lse(ηi) = lse(ψi) + (ηi − ψi)T g(ψi) + g(ψi) = exp[ψi − lse(ψi)] = S(ψi) H(ψi) = diag(g(ψi)) − g(ψi)g(ψi)T\n",
      "\n",
      "1 2\n",
      "\n",
      "(ηi − ψi)T H(ψi)(ηi − ψi)\n",
      "\n",
      "(21.158)\n",
      "\n",
      "(21.159)\n",
      "\n",
      "(21.160)\n",
      "\n",
      "where g and H are the gradient and Hessian of lse, and ψi ∈ RM is chosen such that equality holds. An upper bound to lse can be found by replacing the Hessian matrix H(ψi) with a matrix Ai such that Ai ≺ H(ψi). (Bohning 1992) showed that this can be achieved if we use the matrix Ai = 1 (Recall that M + 1 = C is the number of classes.) 2 Note that Ai is independent of ψi; however, we still write it as Ai (rather than dropping the i subscript), since other bounds that we consider below will have a data-dependent curvature term. The upper bound on lse therefore becomes\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "IM − 1\n",
      "\n",
      "M +1 1M 1T\n",
      "\n",
      "M\n",
      "\n",
      "(cid:16)\n",
      "\n",
      ".\n",
      "\n",
      "lse(ηi) ≤ 1 2 1 2\n",
      "\n",
      "Ai =\n",
      "\n",
      "i Aiηi − bT ηT (cid:17) IM − 1\n",
      "\n",
      "M + 1\n",
      "\n",
      "i ηi + ci\n",
      "\n",
      "1M 1T M\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(21.161)\n",
      "\n",
      "(21.162)\n",
      "\n",
      "bi = Aiψi − g(ψi) ψT\n",
      "\n",
      "ci =\n",
      "\n",
      "1 2\n",
      "\n",
      "i Aiψi − g(ψi)T ψi + lse(ψi)\n",
      "\n",
      "(21.163)\n",
      "\n",
      "(21.164)\n",
      "\n",
      "where ψi ∈ RM is a vector of variational parameters.\n",
      "\n",
      "We can use the above result to get the following lower bound on the softmax likelihood:\n",
      "\n",
      "log p(yi = c|xi, w) ≥\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "i Xiw − 1 yT 2\n",
      "\n",
      "wT XiAiXiw + bT\n",
      "\n",
      "i Xiw − ci\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "c\n",
      "\n",
      "(21.165)\n",
      "\n",
      "To simplify notation, deﬁne the pseudo-measurement\n",
      "\n",
      "˜yi (cid:2) A−1\n",
      "\n",
      "i\n",
      "\n",
      "(bi + yi)\n",
      "\n",
      "(21.166)\n",
      "\n",
      "Then we can get a “Gaussianized” version of the observation model:\n",
      "\n",
      "p(yi|xi, w) ≥ f (xi, ψi) N (˜yi|Xiw, A−1\n",
      "\n",
      "i\n",
      "\n",
      ")\n",
      "\n",
      "(21.167)\n",
      "\n",
      "where f (xi, ψi) is some function that does not depend on w. Given this, it is easy to compute the posterior q(w) = N (mN , VN ), using Bayes rule for Gaussians. Below we will explain how to update the variational parameters ψi.\n",
      "\n",
      "21.8. Local variational bounds *\n",
      "\n",
      "759\n",
      "\n",
      "21.8.2.1\n",
      "\n",
      "Applying Bohning’s bound to multinomial logistic regression\n",
      "\n",
      "Let us see how to apply this bound to multinomial logistic regression. From Equation 21.13, we can deﬁne the goal of variational inference as maximizing\n",
      "\n",
      "L(q) (cid:2) −KL (q(w)||p(w|D)) + Eq\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "log p(yi|xi, w)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(21.168)\n",
      "\n",
      "= −KL (q(w)||p(w|D)) + Eq\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "i=1 N(cid:4)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "yT i ηi − lse(ηi)\n",
      "\n",
      "(21.169)\n",
      "\n",
      "i=1\n",
      "\n",
      "= −KL (q(w)||p(w|D)) +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "yT i Eq [ηi] −\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "Eq [lse(ηi)]\n",
      "\n",
      "(21.170)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "where q(w) =N (w|mN , VN ) is the approximate posterior. The ﬁrst term is just the KL divergence between two Gaussians, which is given by −KL (N (m0, V0)||N (mN , VN )) = − 1 2\n",
      "\n",
      "+(mN − m0)T V−1\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "tr(VN V−1\n",
      "\n",
      "0 ) − log |VN V−1 0 |\n",
      "\n",
      "0 (mN − m0) − DM\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(21.171)\n",
      "\n",
      "where DM is the dimensionality of the Gaussian, and we assume a prior of the form p(w) = N (m0, V0), where typically μ0 = 0DM , and V0 is block diagonal. The second term is simply\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "yT i Eq [ηi] =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "yT i ˜mi\n",
      "\n",
      "(21.172)\n",
      "\n",
      "i=1\n",
      "\n",
      "i=1\n",
      "\n",
      "where ˜mi (cid:2) XimN . The ﬁnal term can be lower bounded by taking expectations of our quadratic upper bound on lse as follows:\n",
      "\n",
      "−\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "Eq [lse(ηi)] ≥ − 1 2\n",
      "\n",
      "tr(Ai ˜Vi) − 1 2\n",
      "\n",
      "˜miAi ˜mi + bT\n",
      "\n",
      "i ˜mi − ci\n",
      "\n",
      "(21.173)\n",
      "\n",
      "where ˜Vi (cid:2) XiVN XT i . Putting it altogether, we have (cid:13) LQJ (q) ≥ − 1 2 − 1 2\n",
      "\n",
      "DM +\n",
      "\n",
      "tr(VN V−1\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "0 ) − log |VN V−1\n",
      "\n",
      "i ˜mi − 1 yT 2\n",
      "\n",
      "tr(Ai ˜Vi) − 1 2\n",
      "\n",
      "0 | + (mN − m0)T V−1\n",
      "\n",
      "˜miAi ˜mi + bT\n",
      "\n",
      "0 (mN − m0)\n",
      "\n",
      "i ˜mi − ci\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(21.174)\n",
      "\n",
      "This lower bound combines Jensen’s inequality (as in mean ﬁeld inference), plus the quadratic lower bound due to the lse term, so we write it as LQJ .\n",
      "\n",
      "We will use coordinate ascent to optimize this lower bound. That is, we update the variational posterior parameters VN and mN , and then the variational likelihood parameters ψi. We leave\n",
      "\n",
      "760\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "the detailed derivation as an exercise, and just state the results. We have\n",
      "\n",
      "VN =\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "V0 +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "XT\n",
      "\n",
      "i AiXi\n",
      "\n",
      "(cid:11)−1\n",
      "\n",
      "(21.175)\n",
      "\n",
      "mN = Vn\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i=1\n",
      "\n",
      "V−1\n",
      "\n",
      "0 m0 +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "XT\n",
      "\n",
      "i (yi + bi)\n",
      "\n",
      "(21.176)\n",
      "\n",
      "i=1\n",
      "\n",
      "(21.177) We can exploit the fact that Ai is a constant matrix, plus the fact that Xi has block structure, to simplify the ﬁrst two terms as follows: (cid:11)−1\n",
      "\n",
      "VN =\n",
      "\n",
      "ψi = ˜mi = XimN\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "V0 + A ⊗\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "xixT i\n",
      "\n",
      "(21.178)\n",
      "\n",
      "mN = Vn\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i=1\n",
      "\n",
      "V−1\n",
      "\n",
      "0 m0 +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "(yi + bi) ⊗ xi\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(21.179)\n",
      "\n",
      "i=1\n",
      "\n",
      "where ⊗ denotes the kronecker product. See Algorithm 15 for some pseudocode, and http: //www.cs.ubc.ca/~emtiyaz/software/catLGM.html for some Matlab code.\n",
      "\n",
      "13 14 until converged; 15 Return mN and VN ;\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "11\n",
      "\n",
      "9\n",
      "\n",
      "8\n",
      "\n",
      "Algorithm 21.1: Variational bound 1 Input: yi ∈ {1, . . . , C}, xi ∈ RD, i = 1 : N , prior m0, V0 ; 2 Deﬁne M := C − 1; dummy encode yi ∈ {0, 1}M ; deﬁne Xi = blockdiag(xT i ) ; (cid:16) M +1 1M 1T 3 Deﬁne y := [y1; . . .; yN ], X := [X1; . . .; XN ] and A := 1 2 i AXi 4 VN := 5 Initialize mN := m0; 6 repeat 7\n",
      "\n",
      "ψ := XmN ; Ψ := reshape(m, M, N ); G := exp(Ψ − lse(Ψ)); B := AΨ − G; b := (B) ; 0 m0 + XT (y + b) mN := VN Compute the lower bound LQJ using Equation 21.174;\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "V−1\n",
      "\n",
      "0 +\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:7)n\n",
      "\n",
      "V−1\n",
      "\n",
      "i=1 XT\n",
      "\n",
      "inference for multi-class logistic regression using Bohning’s\n",
      "\n",
      "(cid:23)−1\n",
      "\n",
      ";\n",
      "\n",
      "(cid:23)\n",
      "\n",
      ";\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "IM − 1\n",
      "\n",
      "M\n",
      "\n",
      ";\n",
      "\n",
      "21.8.3\n",
      "\n",
      "Bounds for the sigmoid function\n",
      "\n",
      "In many models, we just have binary data. ηi = wT xi where w ∈ RD is a weight vector (not matrix).\n",
      "\n",
      "In this case, we have yi ∈ {0, 1}, M = 1 and In this case, the Bohning bound\n",
      "\n",
      "21.8. Local variational bounds *\n",
      "\n",
      "761\n",
      "\n",
      "1\n",
      "\n",
      "Bohning bound, χ=−2.5\n",
      "\n",
      "1\n",
      "\n",
      "JJ bound, χ=2.5\n",
      "\n",
      "0.9\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "0 −6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 21.10 Quadratic lower bounds on the sigmoid (logistic) function. In solid red, we plot sigm(x) vs x. In dotted blue, we plot the lower bound L(x, ξ) vs x for ξ = 2.5. (a) Bohning bound. This is tight at −ξ = 2.5. (b) JJ bound. This is tight at ξ = ±2.5. Figure generated by sigmoidLowerBounds.\n",
      "\n",
      "becomes\n",
      "\n",
      "log(1 + eη) ≤ 1 2 1 4\n",
      "\n",
      "a =\n",
      "\n",
      "b = Aψ − (1 + e−ψ)−1\n",
      "\n",
      "aη2 − bη + c\n",
      "\n",
      "(21.180)\n",
      "\n",
      "(21.181)\n",
      "\n",
      "(21.182)\n",
      "\n",
      "c =\n",
      "\n",
      "1 2\n",
      "\n",
      "Aψ2 − (1 + e−ψ)−1ψ + log(1 + eψ)\n",
      "\n",
      "(21.183)\n",
      "\n",
      "It is possible to derive an alternative quadratic bound for this case, as shown in (Jaakkola and Jordan 1996b, 2000). This has the following form\n",
      "\n",
      "log(1 + eη) ≤ λ(ξ)(η2 − ξ2) +\n",
      "\n",
      "λ(ξ) (cid:2) 1 4ξ\n",
      "\n",
      "tanh(ξ/2) =\n",
      "\n",
      "1 2 1 2ξ\n",
      "\n",
      "(η − ξ) + log(1 +e ξ)\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "sigm(ξ) − 1 2\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(21.184)\n",
      "\n",
      "(21.185)\n",
      "\n",
      "We shall refer to this as the JJ bound, after its inventors, (Jaakkola and Jordan 1996b, 2000).\n",
      "\n",
      "To facilitate comparison with Bohning’s bound, let us rewrite the JJ bound as a quadratic form\n",
      "\n",
      "as follows\n",
      "\n",
      "log(1 + eη) ≤ 1 2\n",
      "\n",
      "a(ξ) = 2λ(ξ) b(ξ) =− 1 2\n",
      "\n",
      "a(ξ)η2 − b(ξ)η + c(ξ)\n",
      "\n",
      "(21.186)\n",
      "\n",
      "(21.187)\n",
      "\n",
      "(21.188)\n",
      "\n",
      "ξ)ξ2 − 1 2 The JJ bound has an adaptive curvature term, since a depends on ξ. In addition, it is tight at two points, as is evident from Figure 21.10(b). By contrast, the Bohning bound is a constant curvature bound, and is only tight at one point, as is evident from Figure 21.10(a).\n",
      "\n",
      "c(ξ) =−λ(\n",
      "\n",
      "ξ + log(1 + eξ)\n",
      "\n",
      "(21.189)\n",
      "\n",
      "762\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "If we wish to use the JJ bound for binary logistic regression, we can make some small modiﬁcations to Algorithm 15. First, we use the new deﬁnitions for ai, bi and ci. The fact that ai is not constant when using the JJ bound, unlike when using the Bohning bound, means we cannot compute VN outside of the main loop, making the method a constant factor slower. Next we note that Xi = xT i , so the updates for the posterior become\n",
      "\n",
      "V−1\n",
      "\n",
      "mN = VN\n",
      "\n",
      "N = V−1 0 + 2 (cid:10)\n",
      "\n",
      "V−1\n",
      "\n",
      "0 m0 +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "λ(ξi)xixT i\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(yi − 1 2\n",
      "\n",
      "(cid:11)\n",
      "\n",
      ")xi\n",
      "\n",
      "(21.190)\n",
      "\n",
      "(21.191)\n",
      "\n",
      "Finally, to compute the update for ξi, we isolate the terms in LQJ that depend on ξi to get\n",
      "\n",
      "L(ξ) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "(\n",
      "\n",
      "ln sigm(ξi) − ξi/2 − λ(ξi)(xT\n",
      "\n",
      "i Eq\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "wwT\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "xi − ξ2 i )\n",
      "\n",
      ")\n",
      "\n",
      "+ const\n",
      "\n",
      "(21.192)\n",
      "\n",
      "i=1\n",
      "\n",
      "Optimizing this wrt ξi gives the equation (cid:13)\n",
      "\n",
      "0 =λ (cid:2)(ξi)(xT\n",
      "\n",
      "i Eq\n",
      "\n",
      "wwT\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "xi − ξ2 i )\n",
      "\n",
      "(21.193)\n",
      "\n",
      "Now λ(cid:2)(ξi) is monotonic for ξi ≥ 0, and we do not need to consider negative values of ξi by symmetry of the bound around ξi = 0 (see Figure 21.10). Hence the only way to make the above expression 0 is if we have (xT\n",
      "\n",
      "i E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "wwT\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "xi − ξ2\n",
      "\n",
      "i ) = 0. Hence the update becomes\n",
      "\n",
      "(ξnew i\n",
      "\n",
      ")2 = xT\n",
      "\n",
      "i (VN + mN mT\n",
      "\n",
      "N )xi\n",
      "\n",
      "(21.194)\n",
      "\n",
      "Although the JJ bound is tighter than the Bohning bound, sometimes it is not tight enough in order to estimate the posterior covariance accurately. A more accurate approach, which uses a piecewise quadratic upper bound to lse, is described in (Marlin et al. 2011). By increasing the number of pieces, the bound can be made arbitrarily tight.\n",
      "\n",
      "21.8.4\n",
      "\n",
      "Other bounds and approximations to the log-sum-exp function *\n",
      "\n",
      "There are several other bounds and approximations to the multiclass lse function which we can use, which we brieﬂy summarize below. Note, however, that all of these require numerical optimization methods to compute mN and VN , making them more complicated to implement.\n",
      "\n",
      "21.8.4.1\n",
      "\n",
      "Product of sigmoids\n",
      "\n",
      "The approach in (Bouchard 2007) exploits the fact that\n",
      "\n",
      "log\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "eηk\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "≤ α +\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "log(1 + eηk−α)\n",
      "\n",
      "(21.195)\n",
      "\n",
      "k=1\n",
      "\n",
      "k=1\n",
      "\n",
      "It then applies the JJ bound to the term on the right.\n",
      "\n",
      "21.8. Local variational bounds *\n",
      "\n",
      "763\n",
      "\n",
      "21.8.4.2\n",
      "\n",
      "Jensen’s inequality\n",
      "\n",
      "The approach in (Blei and Lafferty 2006a, 2007) uses Jensen’s inequality as follows:\n",
      "\n",
      "Eq [lse(ηi)] = Eq\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "log\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "1 +\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "exp(xT\n",
      "\n",
      "i wc)\n",
      "\n",
      "(cid:11)(cid:25)\n",
      "\n",
      "(21.196)\n",
      "\n",
      "≤ log\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "1 +\n",
      "\n",
      "M(cid:4)\n",
      "\n",
      "Eq\n",
      "\n",
      "c=1\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "exp(xT\n",
      "\n",
      "i wc)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(21.197)\n",
      "\n",
      "≤ log\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "1 +\n",
      "\n",
      "c=1 M(cid:4)\n",
      "\n",
      "c=1\n",
      "\n",
      "exp(xT\n",
      "\n",
      "i mN,c +\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "xT i VN,ccxi)\n",
      "\n",
      "(21.198)\n",
      "\n",
      "where the last term follows from the mean of a log-normal distribution, which is eμ+σ2/2.\n",
      "\n",
      "21.8.4.3 Multivariate delta method\n",
      "\n",
      "The approach in (Ahmed and Xing 2007; Braun and McAuliffe 2010) uses the multivariate delta method, which is a way to approximate moments of a function using a Taylor series expansion. In more detail, let f (w) be the function of interest. Using a second-order approximation around m we have\n",
      "\n",
      "f (w) ≈ f (m) + (w − m)T g(w − m) +\n",
      "\n",
      "1 2\n",
      "\n",
      "(w − m)T H(w − m)\n",
      "\n",
      "(21.199)\n",
      "\n",
      "where g and H are the gradient and Hessian evaluated at m. If q(w) = N (w|m, V), we have\n",
      "\n",
      "Eq [f (w)] ≈ f (m) +\n",
      "\n",
      "1 2\n",
      "\n",
      "tr[HV]\n",
      "\n",
      "(21.200)\n",
      "\n",
      "If we use f (w) = lse(Xiw), we get\n",
      "\n",
      "Eq [lse(Xiw)] ≈ lse(Xim) +\n",
      "\n",
      "1 2\n",
      "\n",
      "tr[XiHXT\n",
      "\n",
      "i V]\n",
      "\n",
      "(21.201)\n",
      "\n",
      "where g and H for the lse function are deﬁned in Equations 21.159 and 21.160.\n",
      "\n",
      "21.8.5\n",
      "\n",
      "Variational inference based on upper bounds\n",
      "\n",
      "So far, we have been concentrating on lower bounds. However, sometimes we need to use an upper bound. For example, (Saul et al. 1996) derives a mean ﬁeld algorithm for sigmoid belief nets, which are DGMs in which each CPD is a logistic regression function (Neal 1992). Unlike the case of Ising models, the resulting MRF is not pairwise, but contains higher order interactions. This makes the standard mean ﬁeld updates intractable. In particular, they turn out to involve computing an expression which requires evaluating\n",
      "\n",
      "E\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "log(1 + e−\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "j∈pai\n",
      "\n",
      "wij xj )\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "= E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "− log sigm(wT\n",
      "\n",
      "i xpa(i))\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(21.202)\n",
      "\n",
      "(Notice the minus sign in front.) (Saul et al. 1996) show how to derive an upper bound on the sigmoid function so as to make this update tractable, resulting in a monotonically convergent inference procedure.\n",
      "\n",
      "764\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 21.1 Laplace approximation to p(μ, log σ|D) for a univariate Gaussian Compute a Laplace approximation of p(μ, log σ|D) for a Gaussian, using an uninformative prior p(μ, log σ) ∝ 1.\n",
      "\n",
      "Exercise 21.2 Laplace approximation to normal-gamma Consider estimating μ and (cid:7) = log σ for a Gaussian using an uniformative normal-Gamma prior. The log posterior is\n",
      "\n",
      "log p(μ, (cid:7)|D) = −n log σ − 1\n",
      "\n",
      "2σ2 [ns2 + n(y − μ)2]\n",
      "\n",
      "(21.203)\n",
      "\n",
      "a. Show that the ﬁrst derivatives are\n",
      "\n",
      "∂ ∂μ ∂ ∂(cid:7)\n",
      "\n",
      "log p(μ, (cid:7)|D) =\n",
      "\n",
      "n(y − μ) σ2\n",
      "\n",
      "log p(μ, (cid:7)|D) =−n +\n",
      "\n",
      "ns2 + n(y − μ)2 σ2\n",
      "\n",
      "(21.204)\n",
      "\n",
      "(21.205)\n",
      "\n",
      "b. Show that the Hessian matrix is given by\n",
      "\n",
      "H =\n",
      "\n",
      "=\n",
      "\n",
      "%\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "∂2 ∂μ2 log p(μ, (cid:7)|D) ∂2 ∂(cid:12)2 log p(μ, (cid:7)|D) − n σ2 −2n y−μ σ2\n",
      "\n",
      "− 2\n",
      "\n",
      "−2n y−μ σ2 σ2 (ns2 + n(y − μ)2)\n",
      "\n",
      "∂2 ∂μ∂(cid:12) log p(μ, (cid:7)|D) ∂2 ∂(cid:12)2 log p(μ, (cid:7)|D) (cid:4)\n",
      "\n",
      "&\n",
      "\n",
      "(21.206)\n",
      "\n",
      "(21.207)\n",
      "\n",
      "c. Use this to derive a Laplace approximation to the posterior p(μ, (cid:7)|D).\n",
      "\n",
      "Exercise 21.3 Variational lower bound for VB for univariate Gaussian Fill in the details of the derivation in Section 21.5.1.6.\n",
      "\n",
      "Exercise 21.4 Variational lower bound for VB for GMMs Consider VBEM for GMMs as in Section 21.6.1.4. Show that the lower bound has the following form\n",
      "\n",
      "L = E [ln p(x|z, μ, Λ)] + E [ln p(z|π)] + E [ln p(π)] + E [ln p(μ, Λ)]\n",
      "\n",
      "−E [ln q(z)] − E [ln q(π)] − E [ln q(μ, Λ)]\n",
      "\n",
      "(21.208)\n",
      "\n",
      "21.8. Local variational bounds *\n",
      "\n",
      "765\n",
      "\n",
      "where\n",
      "\n",
      "E [ln p(x|z, μ, Λ)] =\n",
      "\n",
      "E [ln p(z|π)] =\n",
      "\n",
      "−νk(xk − mk)T Lk(xk − mk) − D ln(2π) (cid:12)\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "k\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "Nk\n",
      "\n",
      "rik ln ˜πk\n",
      "\n",
      "(\n",
      "\n",
      "ln ˜Λk − Dβ−1\n",
      "\n",
      "k − νktr(SkLk)\n",
      "\n",
      ")\n",
      "\n",
      "(21.209)\n",
      "\n",
      "(21.210)\n",
      "\n",
      "i\n",
      "\n",
      "k\n",
      "\n",
      "E [ln p(π)] = ln Cdir(α0) + (α0 − 1)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "ln ˜πk\n",
      "\n",
      "(21.211)\n",
      "\n",
      "E [ln p(μ, Λ)] =\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "D ln(β0/2π) + ln ˜Λk −\n",
      "\n",
      "Dβ0 βk\n",
      "\n",
      "E [ln q(z)] =\n",
      "\n",
      "−β0νk(mk − m0)T Lk(mk − m0)\n",
      "\n",
      "+ ln CW i(L0, ν0) + (cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "rik ln rik\n",
      "\n",
      "ν0 − D − 1 2\n",
      "\n",
      "ln ˜Λk − 1 2\n",
      "\n",
      "\n",
      "\n",
      "νktr(L−1\n",
      "\n",
      "0 Lk)\n",
      "\n",
      "(21.212)\n",
      "\n",
      "(21.213)\n",
      "\n",
      "E [ln q(π)] =\n",
      "\n",
      "i (cid:12)\n",
      "\n",
      "(αk − 1) ln ˜πk + ln Cdir(α)\n",
      "\n",
      "k\n",
      "\n",
      "(21.214)\n",
      "\n",
      "E [ln q(μ, Λ)] =\n",
      "\n",
      "k (cid:12)\n",
      "\n",
      "k\n",
      "\n",
      "1 2\n",
      "\n",
      "ln ˜Λk +\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "ln\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "βk 2π\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "−\n",
      "\n",
      "D\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "− H (q(Λk))\n",
      "\n",
      "(21.215)\n",
      "\n",
      "where the normalization constant for the Dirichlet and Wishart is given by\n",
      "\n",
      "CW i(L, ν) (cid:2) |L|−ν/2\n",
      "\n",
      "Cdir(α) (cid:2)\n",
      "\n",
      "Γ( (cid:14)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "k αk) k Γ(αk) (cid:16)\n",
      "\n",
      "2νD/2ΓD(ν/2)\n",
      "\n",
      "(cid:17)−1\n",
      "\n",
      "(21.216)\n",
      "\n",
      "(21.217)\n",
      "\n",
      "ΓD(α) (cid:2) πD(D−1)/4\n",
      "\n",
      "D(cid:13)\n",
      "\n",
      "Γ (α + (1 − j)/2)\n",
      "\n",
      "(21.218)\n",
      "\n",
      "j=1\n",
      "\n",
      "where ΓD(ν) is the multivariate Gamma function. Finally, the entropy of the Wishart is given by\n",
      "\n",
      "H (Wi(L, ν)) = − ln CW i(L, ν) −\n",
      "\n",
      "ν − D − 1 2\n",
      "\n",
      "E [ln |Λ|] +\n",
      "\n",
      "νD\n",
      "\n",
      "2\n",
      "\n",
      "(21.219)\n",
      "\n",
      "where E [ln |Λ|] is given in Equation 21.131.\n",
      "\n",
      "Exercise 21.5 Derivation of E [log πk] under a Dirichlet distribution Show that\n",
      "\n",
      "exp(E [log πk]) =\n",
      "\n",
      "exp(Ψ(\n",
      "\n",
      "exp(Ψ(αk)) (cid:2)\n",
      "\n",
      "k(cid:2) αk(cid:2) ))\n",
      "\n",
      "(21.220)\n",
      "\n",
      "where π ∼ Dir(α).\n",
      "\n",
      "Exercise 21.6 Alternative derivation of the mean ﬁeld updates for the Ising model Derive Equation 21.50 by directly optimizing the variational free energy one term at a time.\n",
      "\n",
      "766\n",
      "\n",
      "Chapter 21. Variational inference\n",
      "\n",
      "Exercise 21.7 Forwards vs reverse KL divergence (Source: Exercise 33.7 of (MacKay 2003).) Consider a factored approximation q(x, y) = q(x)q(y) to a joint distribution p(x, y). Show that to minimize the forwards KL KL (p||q) we should set q(x) = p(x) and q(y) = p(y), i.e., the optimal approximation is a product of marginals Now consider the following joint distribution, where the rows represent y and the columns x.\n",
      "\n",
      "1 2 3 4\n",
      "\n",
      "1 1/8 1/8 0 0\n",
      "\n",
      "x 2 1/8 1/8 0 0\n",
      "\n",
      "3 0 0 1/4 0\n",
      "\n",
      "4 0 0 0 1/4\n",
      "\n",
      "Show that the reverse KL KL (q||p) for this p has three distinct minima. evaluate KL (q||p) at each of them. What is the value of KL (q||p) if we set q(x, y) = p(x)p(y)?\n",
      "\n",
      "Identify those minima and\n",
      "\n",
      "Exercise 21.8 Derivation of the structured mean ﬁeld updates for FHMM Derive the updates in Section 21.4.1.\n",
      "\n",
      "Exercise 21.9 Variational EM for binary FA with sigmoid link Consider the binary FA model:\n",
      "\n",
      "p(xi|zi, θ) =\n",
      "\n",
      "D(cid:13)\n",
      "\n",
      "Ber(xij|sigm(wT\n",
      "\n",
      "j zi + βj)) =\n",
      "\n",
      "D(cid:13)\n",
      "\n",
      "Ber(xij|sigm(ηij))\n",
      "\n",
      "(21.221)\n",
      "\n",
      "j=1\n",
      "\n",
      "j=1\n",
      "\n",
      "ηi = ˜W˜zi ˜zi (cid:2) (zi; 1) ˜W (cid:2) (W, β) p(zi) =N (0, I)\n",
      "\n",
      "(21.222)\n",
      "\n",
      "(21.223)\n",
      "\n",
      "(21.224) (21.225)\n",
      "\n",
      "Derive an EM algorithm to ﬁt this model, using the Jaakkola-Jordan bound. Hint: the answer is in (Tipping 1998), but the exercise asks you to derive these equations.\n",
      "\n",
      "Exercise 21.10 VB for binary FA with probit link In Section 11.4.6, we showed how to use EM to ﬁt probit regression, using a model of the form p(yi = 1|zi) =I (zi > 0), where zi ∼ N (wT xi, 1) is latent. Now consider the case where the inputs xi are also unknown, as in binary factor analysis. Show how to ﬁt this model using variational Bayes, making an l=1 q(wl). Hint: q(xi) and i=1 q(xi)q(zi) approximation to the posterior of the form q(x, z, W) = q(wi) will be Gaussian, and q(zi) will be a truncated univariate Gaussian.\n",
      "\n",
      "(cid:14)N\n",
      "\n",
      "(cid:14)L\n",
      "\n",
      "22 More variational inference\n",
      "\n",
      "22.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In Chapter 21, we discussed mean ﬁeld inference, which approximates the posterior by a product of marginal distributions. This allows us to use different parametric forms for each variable, which is particularly useful when performing Bayesian inference for the parameters of statistical models (such as the mean and variance of a Gaussian or GMM, or the regression weights in a GLM), as we saw when we discussed variational Bayes and VB-EM.\n",
      "\n",
      "In this chapter, we discuss a slightly different kind of variational inference. The basic idea is to minimize J(q) = KL (q||˜p), where ˜p is the exact but unnormalized posterior as before, but where we no longer require q to be factorized. In fact, we do not even require q to be a globally Instead, we only require that q is locally consistent, meaning that the valid joint distribution. joint distribution of two adjacent nodes agrees with the corresponding marginals (we will deﬁne this more precisely below).\n",
      "\n",
      "In addition to this new kind of inference, we will discuss approximate methods for MAP state estimation in discrete graphical models. It turns out that algorithms for solving the MAP problem are very similar to some approximate methods for computing marginals, as we will see.\n",
      "\n",
      "22.2\n",
      "\n",
      "Loopy belief propagation: algorithmic issues\n",
      "\n",
      "There is a very simple approximate inference algorithm for discrete (or Gaussian) graphical models known as loopy belief propagation or LBP. The basic idea is extremely simple: we apply the belief propagation algorithm of Section 20.2 to the graph, even if it has loops (i.e., even if it is not a tree). This method is simple and efficient, and often works well in practice, outperforming mean ﬁeld (Weiss 2001). In this section, we discuss the algorithm in more detail. In the next section, we analyse this algorithm in terms of variational inference.\n",
      "\n",
      "22.2.1\n",
      "\n",
      "A brief history\n",
      "\n",
      "When applied to loopy graphs, BP is not guaranteed to give correct results, and may not even converge. Indeed, Judea Pearl, who invented belief propagation for trees, wrote the following about loopy BP in 1988:\n",
      "\n",
      "When loops are present, the network is no longer singly connected and local propagation\n",
      "\n",
      "768\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "schemes will invariably run into trouble . . . If we ignore the existence of loops and permit the nodes to continue communicating with each other as if the network were singly connected, messages may circulate indeﬁnitely around the loops and the process may not converge to a stable equilibrium . . . Such oscillations do not normally occur in probabilistic networks . . . which tend to bring all messages to some stable equilibrium as time goes on. However, this asymptotic equilibrium is not coherent, in the sense that it does not represent the posterior probabilities of all nodes of the network — (Pearl 1988, p.195)\n",
      "\n",
      "Despite these reservations, Pearl advocated the use of belief propagation in loopy networks as an approximation scheme (J. Pearl, personal communication) and exercise 4.7 in (Pearl 1988) investigates the quality of the approximation when it is applied to a particular loopy belief network.\n",
      "\n",
      "However, the main impetus behind the interest in BP arose when McEliece et al. (1998) showed that a popular algorithm for error correcting codes known as turbo codes (Berrou et al. 1993) could be viewed as an instance of BP applied to a certain kind of graph. This was an important observation since turbo codes have gotten very close to the theoretical lower bound on coding efficiency proved by Shannon. (Another approach, known as low density parity check or LDPC codes, has achieved comparable performance; it also uses LBP for decoding — see Figure 22.1 In (Murphy et al. 1999), LBP was experimentally shown to also work well for for an example.) inference in other kinds of graphical models beyond the error-correcting code context, and since then, the method has been widely used in many different applications.\n",
      "\n",
      "22.2.2\n",
      "\n",
      "LBP on pairwise models\n",
      "\n",
      "We now discuss how to apply LBP to an undirected graphical model with pairwise factors (we discuss the directed case, which can involve higher order factors, in the next section). The See method is simple: Algorithm 8 for the pseudocode, and beliefPropagation for some Matlab code. We will discuss issues such as convergence and accuracy of this method shortly.\n",
      "\n",
      "just continually apply Equations 20.11 and 20.10 until convergence.\n",
      "\n",
      "Algorithm 22.1: Loopy belief propagation for a pairwise MRF 1 Input: node potentials ψs(xs), edge potentials ψst(xs, xt); 2 Initialize messages ms→t(xt) = 1 for all edges s − t; 3 Initialize beliefs bels(xs) = 1 for all nodes s; 4 repeat 5\n",
      "\n",
      "6 7 until beliefs don’t change signiﬁcantly; 8 Return marginal beliefs bels(xs);\n",
      "\n",
      "Send message on each edge (cid:7) ms→t(xt) = Update belief of each node bels(xs) ∝ ψs(xs)\n",
      "\n",
      "xs\n",
      "\n",
      "!\n",
      "\n",
      "ψs(xs)ψst(xs, xt)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "u∈nbrs\\t mu→s(xs)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "t∈nbrs\n",
      "\n",
      "; mt→s(xs);\n",
      "\n",
      "#\n",
      "\n",
      "22.2. Loopy belief propagation: algorithmic issues\n",
      "\n",
      "769\n",
      "\n",
      "Figure 22.1 (a) A simple factor graph representation of a (2,3) low-density parity check code (factor graphs are deﬁned in Section 22.2.3.1). Each message bit (hollow round circle) is connected to two parity factors (solid black squares), and each parity factor is connected to three bits. Each parity factor has the form ψstu(xs, xt, xu) =I (xs ⊗ xt ⊗ xu = 1), where ⊗ is the xor operator. The local evidence factors for each hidden node are not shown. (b) A larger example of a random LDPC code. We see that this graph is “locally tree-like”, meaning there are no short cycles; rather, each cycle has length ∼ log m, where m is the number of nodes. This gives us a hint as to why loopy BP works so well on such graphs. (Note, however, that some error correcting code graphs have short loops, so this is not the full explanation.) Source: Figure 2.9 from (Wainwright and Jordan 2008b). Used with kind permission of Martin Wainwright.\n",
      "\n",
      "22.2.3\n",
      "\n",
      "LBP on a factor graph\n",
      "\n",
      "To handle models with higher-order clique potentials (which includes directed models where some nodes have more than one parent), it is useful to use a representation known as a factor graph. We explain this representation below, and then describe how to apply LBP to such models.\n",
      "\n",
      "22.2.3.1\n",
      "\n",
      "Factor graphs\n",
      "\n",
      "A factor graph (Kschischang et al. 2001; Frey 2003) is a graphical representation that uniﬁes directed and undirected models, and which simpliﬁes certain message passing algorithms. More precisely, a factor graph is an undirected bipartite graph with two kinds of nodes. Round nodes represent variables, square nodes represent factors, and there is an edge from each variable to If we assume every factor that mentions it. For example, consider the MRF in Figure 22.2(a). one potential per maximal clique, we get the factor graph in Figure 22.2(b), which represents the function\n",
      "\n",
      "f (x1, x2, x3, x4) = f124(x1, x2, x4)f234(x2, x3, x4)\n",
      "\n",
      "(22.1)\n",
      "\n",
      "If we assume one potential per edge. we get the factor graph in Figure 22.2(c), which represents the function\n",
      "\n",
      "f (x1, x2, x3, x4) = f14(x1, x4)f12(x1, x2)f34(x3, x4)f23(x2, x3)f24(x2, x4)\n",
      "\n",
      "(22.2)\n",
      "\n",
      "770\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 22.2 (a) A simple UGM. (b) A factor graph representation assuming one potential per maximal clique. (c) A factor graph representation assuming one potential per edge.\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "p(x1)\n",
      "\n",
      "p(x2)\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "p(x3|x1, x2)\n",
      "\n",
      "x3\n",
      "\n",
      "x4\n",
      "\n",
      "x5\n",
      "\n",
      "x4\n",
      "\n",
      "p(x4|x3)\n",
      "\n",
      "p(x5|x3)\n",
      "\n",
      "x5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 22.3 (a) A simple DGM. (b) Its corresponding factor graph. Based on Figure 5 of (Yedidia et al. 2001)..\n",
      "\n",
      "We can also convert a DGM to a factor graph: just create one factor per CPD, and connect that factor to all the variables that use that CPD. For example, Figure 22.3 represents the following factorization:\n",
      "\n",
      "f (x1, x2, x3, x4, x5) = f1(x1)f2(x2)f123(x1, x2, x3)f34(x3, x4)f35(x3, x5)\n",
      "\n",
      "(22.3)\n",
      "\n",
      "where we deﬁne f123(x1, x2, x3) = p(x3|x1, x2), etc. If each node has at most one parent (and hence the graph is a chain or simple tree), then there will be one factor per edge (root nodes can have their prior CPDs absorvbed into their children’s factors). Such models are equivalent to pairwise MRFs.\n",
      "\n",
      "22.2. Loopy belief propagation: algorithmic issues\n",
      "\n",
      "771\n",
      "\n",
      "Figure 22.4 Message passing on a bipartite factor graph. Square nodes represent factors, and circles represent variables. Source: Figure 6 of (Kschischang et al. 2001). Used with kind permission of Brendan Frey.\n",
      "\n",
      "22.2.3.2\n",
      "\n",
      "BP on a factor graph\n",
      "\n",
      "We now derive a version of BP that sends messages on a factor graph, as proposed in (Kschis- chang et al. 2001). Speciﬁcally, we now have two kinds of messages: variables to factors\n",
      "\n",
      "mx→f (x) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "mh→x(x)\n",
      "\n",
      "(22.4)\n",
      "\n",
      "h∈nbr(x)\\{f }\n",
      "\n",
      "and factors to variables:\n",
      "\n",
      "mf→x(x) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "f (x, y)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "my→f (y)\n",
      "\n",
      "(22.5)\n",
      "\n",
      "y\n",
      "\n",
      "y∈nbr(f )\\{x}\n",
      "\n",
      "Here nbr(x) are all the factors that are connected to variable x, and nbr(f ) are all the variables that are connected to factor f . These messages are illustrated in Figure 22.4. At convergence, we can compute the ﬁnal beliefs as a product of incoming messages:\n",
      "\n",
      "bel(x) ∝\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "mf→x(x)\n",
      "\n",
      "(22.6)\n",
      "\n",
      "f ∈nbr(x)\n",
      "\n",
      "In the following sections, we will focus on LBP for pairwise models, rather than for factor\n",
      "\n",
      "graphs, but this is just for notational simplicity.\n",
      "\n",
      "22.2.4\n",
      "\n",
      "Convergence\n",
      "\n",
      "LBP does not always converge, and even when it does, it may converge to the wrong answers. This raises several questions: how can we predict when convergence will occur? what can we do to increase the probability of convergence? what can we do to increase the rate of convergence? We brieﬂy discuss these issues below. We then discuss the issue of accuracy of the results at convergence.\n",
      "\n",
      "772\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "d e g r e v n o c\n",
      "\n",
      "s e g a s s e m\n",
      "\n",
      "f o %\n",
      "\n",
      "1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0\n",
      "\n",
      "0\n",
      "\n",
      "10 20 30 40 50 60 70 80 90 100\n",
      "\n",
      ") 0 =\n",
      "\n",
      "5 1 1 X\n",
      "\n",
      "( P\n",
      "\n",
      "1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      ") 0 =\n",
      "\n",
      "0 1 X\n",
      "\n",
      "( P\n",
      "\n",
      "1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "Time (seconds)\n",
      "\n",
      "Time (seconds)\n",
      "\n",
      "Time (seconds)\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      ") 0 =\n",
      "\n",
      "1 6 X\n",
      "\n",
      "( P\n",
      "\n",
      "1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      ") 0 =\n",
      "\n",
      "7 X\n",
      "\n",
      "( P\n",
      "\n",
      "1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      ") 0 =\n",
      "\n",
      "7 1 X\n",
      "\n",
      "( P\n",
      "\n",
      "1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0\n",
      "\n",
      "0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "Time (seconds)\n",
      "\n",
      "Time (seconds)\n",
      "\n",
      "Time (seconds)\n",
      "\n",
      "(d)\n",
      "\n",
      "(e)\n",
      "\n",
      "(f)\n",
      "\n",
      "Synchronous\n",
      "\n",
      "Asynchronous\n",
      "\n",
      "No smoothing\n",
      "\n",
      "True\n",
      "\n",
      "Illustration of the behavior of loopy belief propagation on an 11 × 11 Ising grid with Figure 22.5 random potentials, wij ∼ Unif(−C, C), where C = 11. For larger C, inference becomes harder. (a) Percentage of messasges that have converged vs time for 3 different update schedules: Dotted = damped sychronous (few nodes converge), dashed = undamped asychnronous (half the nodes converge), solid = (b-f) Marginal beliefs of certain nodes vs time. Solid straight damped asychnronous (all nodes converge). Source: Figure 11.C.1 of (Koller and line = truth, dashed = sychronous, solid = damped asychronous. Friedman 2009). Used with kind permission of Daphne Koller.\n",
      "\n",
      "22.2.4.1 When will LBP converge?\n",
      "\n",
      "The details of the analysis of when LBP will converge are beyond the scope of this chapter, but we brieﬂy sketch the basic idea. The key analysis tool is the computation tree, which visualizes the messages that are passed as the algorithm proceeds. Figure 22.6 gives a simple example. In the second iteration, it In the ﬁrst iteration, node 1 receives messages from nodes 2 and 3. receives one message from node 3 (via node 2), one from node 2 (via node 3), and two messages from node 4 (via nodes 2 and 3). And so on.\n",
      "\n",
      "The key insight is that T iterations of LBP is equivalent to exact computation in a computation tree of height T + 1. If the strengths of the connections on the edges is sufficiently weak, then the inﬂuence of the leaves on the root will diminish over time, and convergence will occur. See (Wainwright and Jordan 2008b) and references therein for more information.\n",
      "\n",
      "22.2. Loopy belief propagation: algorithmic issues\n",
      "\n",
      "773\n",
      "\n",
      "Figure 22.6 (a) A simple loopy graph. (b) The computation tree, rooted at node 1, after 4 rounds of message passing. Nodes 2 and 3 occur more often in the tree because they have higher degree than nodes 1 and 2. Source: Figure 8.2 of (Wainwright and Jordan 2008b). Used with kind permission of Martin Wainwright.\n",
      "\n",
      "22.2.4.2 Making LBP converge\n",
      "\n",
      "Although the theoretical convergence analysis is very interesting, in practice, when faced with a model where LBP is not converging, what should we do?\n",
      "\n",
      "sending the message M k\n",
      "\n",
      "One simple way to reduce the chance of oscillation is to use damping. That is, instead of\n",
      "\n",
      "ts, we send a damped message of the form\n",
      "\n",
      "˜M k\n",
      "\n",
      "ts(xs) = λMts(xs) + (1− λ) ˜M k−1\n",
      "\n",
      "ts\n",
      "\n",
      "(xs)\n",
      "\n",
      "(22.7)\n",
      "\n",
      "where 0 ≤ λ ≤ 1 is the damping factor Clearly if λ = 1 this reduces to the standard scheme, but for λ < 1, this partial updating scheme can help improve convergence. Using a value such as λ ∼ 0.5 is standard practice. The beneﬁts of this approach are shown in Figure 22.5, where we see that damped updating results in convergence much more often than undamped updating. It is possible to devise methods, known as double loop algorithms, which are guaranteed to converge to a local minimum of the same objective that LBP is minimizing (Yuille 2001; Welling and Teh 2001). Unfortunately, these methods are rather slow and complicated, and the accuracy of the resulting marginals is usually not much greater than with standard LBP. (Indeed, oscillating marginals is sometimes a sign that the LBP approximation itself is a poor one.) Consequently, these techniques are not very widely used. In Section 22.4.2, we will see a different convergent version of BP that is widely used.\n",
      "\n",
      "22.2.4.3\n",
      "\n",
      "Increasing the convergence rate: message scheduling\n",
      "\n",
      "Even if LBP converges, it may take a long time. The standard approach when implementing LBP is to perform synchronous updates, where all nodes absorb messages in parallel, and then send out messages in parallel. That is, the new messages at iteration k + 1 are computed in parallel using\n",
      "\n",
      "mk+1 = (f1(mk), . . . , fE(mk))\n",
      "\n",
      "(22.8)\n",
      "\n",
      "where E is the number of edges, and fst(m) is the function that computes the message for edge s → t given all the old messages. This is analogous to the Jacobi method for solving linear\n",
      "\n",
      "774\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "systems of equations. It is well known (Bertsekas 1997) that the Gauss-Seidel method, which performs asynchronous updates in a ﬁxed round-robin fashion, converges faster when solving linear systems of equations. We can apply the same idea to LBP, using updates of the form\n",
      "\n",
      "mk+1\n",
      "\n",
      "i = fi\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "{mk+1 j\n",
      "\n",
      ": j < i}, {mk\n",
      "\n",
      "j : j > i}\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(22.9)\n",
      "\n",
      "where the message for edge i is computed using new messages (iteration k + 1) from edges earlier in the ordering, and using old messages (iteration k) from edges later in the ordering.\n",
      "\n",
      "This raises the question of what order to update the messages in. One simple idea is to use a ﬁxed or random order. The beneﬁts of this approach are shown in Figure 22.5, where we see that (damped) asynchronous updating results in convergence much more often than synchronous updating.\n",
      "\n",
      "A smarter approach is to pick a set of spanning trees, and then to perform an up-down sweep on one tree at a time, keeping all the other messages ﬁxed. This is known as tree reparameterization (TRP) (Wainwright et al. 2001), which should not be confused with the more sophisticated tree-reweighted BP (often abbreviated to TRW) to be discussed in Section 22.4.2.1. However, we can do even better by using an adaptive ordering. The intuition is that we should focus our computational efforts on those variables that are most uncertain. (Elidan et al. 2006) proposed a technique known as residual belief propagation, in which messages are scheduled to be sent according to the norm of the difference from their previous value. That is, we deﬁne the residual of new message mst at iteration k to be\n",
      "\n",
      "r(s, t, k) = || log mst − log mk\n",
      "\n",
      "st||∞ = max\n",
      "\n",
      "i\n",
      "\n",
      "| log\n",
      "\n",
      "mst(i) mk st(i)\n",
      "\n",
      "|\n",
      "\n",
      "(22.10)\n",
      "\n",
      "We can store messages in a priority queue, and always send the one with highest residual. When a message is sent from s to t, all of the other messages that depend on mst (i.e., messages of the form mtu where u ∈ nbr(t) \\ s) need to be recomputed; their residual is recomputed, and they are added back to the queue. In (Elidan et al. 2006), they showed (experimentally) that this method converges more often, and much faster, than using sychronous updating, asynchronous updating with a ﬁxed order, and the TRP approach.\n",
      "\n",
      "A reﬁnement of residual BP was presented in (Sutton and McCallum 2007). In this paper, they use an upper bound on the residual of a message instead of the actual residual. This means that messages are only computed if they are going to be sent; they are not just computed for the purposes of evaluating the residual. This was observed to be about ﬁve times faster than residual BP, although the quality of the ﬁnal results is similar.\n",
      "\n",
      "22.2.5\n",
      "\n",
      "Accuracy of LBP\n",
      "\n",
      "For a graph with a single loop, one can show that the max-product version of LBP will ﬁnd the correct MAP estimate, if it converges (Weiss 2000). For more general graphs, one can bound the error in the approximate marginals computed by LBP, as shown in (Wainwright et al. 2003; Vinyals et al. 2010). Much stronger results are available in the case of Gaussian models (Weiss and Freeman 2001a; Johnson et al. 2006; Bickson 2009). In particular, in the Gaussian case, if the method converges, the means are exact, although the variances are not (typically the beliefs are over conﬁdent).\n",
      "\n",
      "22.2. Loopy belief propagation: algorithmic issues\n",
      "\n",
      "775\n",
      "\n",
      "22.2.6\n",
      "\n",
      "Other speedup tricks for LBP *\n",
      "\n",
      "There are several tricks one can use to make BP run faster. We discuss some of them below.\n",
      "\n",
      "22.2.6.1\n",
      "\n",
      "Fast message computation for large state spaces The cost of computing each message in BP (whether in a tree or a loopy graph) is O(K f ), where K is the number of states, and f is the size of the largest factor (f = 2 for pairwise In many vision problems (e.g., image denoising), K is quite large (say 256), because UGMs). it represents the discretization of some underlying continuous space, so O(K 2) per message is too expensive. Fortunately, for certain kinds of pairwise potential functions of the form ψst(xs, xt) =ψ (xs − xt), one can compute the sum-product messages in O(K log K) time using the fast Fourier transform or FFT, as explained in (Felzenszwalb and Huttenlocher 2006). The key insight is that message computation is just convolution:\n",
      "\n",
      "M k\n",
      "\n",
      "st(xt) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ψ(xs − xt)h(xs)\n",
      "\n",
      "(22.11)\n",
      "\n",
      "xs\n",
      "\n",
      "where h(xs) = ψs(xs) (xs). If the potential function ψ(z) is a Gaussian-like potential, we can compute the convolution in O(K) time by sequentially convolving with a small number of box ﬁlters (Felzenszwalb and Huttenlocher 2006).\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "v∈nbr(s)\\t M k−1\n",
      "\n",
      "vs\n",
      "\n",
      "For the max-product case, a technique called the distance transform can be used to compute messages in O(K) time. However, this only works if ψ(z) = exp(−E(z)) and where E(z) has one the following forms: quadratic, E(z) = z2; truncated linear, E(z) = min(c1|z|, c2); or Potts model, E(z) = c I(z (cid:8)= 0). See (Felzenszwalb and Huttenlocher 2006) for details.\n",
      "\n",
      "22.2.6.2 Multi-scale methods\n",
      "\n",
      "A method which is speciﬁc to 2d lattice structures, which commonly arise in computer vision, is based on multi-grid techniques. Such methods are widely used in numerical linear algebra, where one of the core problems is the fast solution of linear systems of equations; this is equivalent to MAP estimation in a Gaussian MRF. In the computer vision context, (Felzenszwalb and Huttenlocher 2006) suggested using the following heuristic to signiﬁcantly speedup BP: construct a coarse-to-ﬁne grid, compute messages at the coarse level, and use this to initialize messages at the level below; when we reach the bottom level, just a few iterations of standard BP are required, since long-range communication has already been achieved via the initialization process.\n",
      "\n",
      "The beliefs at the coarse level are computed over a small number of large blocks. The local evidence is computed from the average log-probability each possible block label assigns to all the pixels in the block. The pairwise potential is based on the discrepancy between labels of neighboring blocks, taking into account their size. We can then run LBP at the coarse level, and then use this to initialize the messages one level down. Note that the model is still a ﬂat grid; however, the initialization process exploits the multi-scale nature of the problem. See (Felzenszwalb and Huttenlocher 2006) for details.\n",
      "\n",
      "776\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "22.2.6.3\n",
      "\n",
      "Cascades\n",
      "\n",
      "Another trick for handling high-dimensional state-spaces, that can also be used with exact inference (e.g., for chain-structured CRFs), is to prune out improbable states based on a com- In fact, one can create a hierarchy of models which tradeoff putationally cheap ﬁltering step. speed and accuracy. This is called a computational cascade. In the case of chains, one can guarantee that the cascade will never ﬁlter out the true MAP solution (Weiss et al. 2010).\n",
      "\n",
      "22.3\n",
      "\n",
      "Loopy belief propagation: theoretical issues *\n",
      "\n",
      "We now attempt to understand the LBP algorithm from a variational point of view. Our presen- tation is closely based on an excellent 300-page review article (Wainwright and Jordan 2008a). This paper is sometimes called “the monster” (by its own authors!) in view of its length and technical difficulty. This section just sketches some of the main results.\n",
      "\n",
      "To simplify the presentation, we focus on the special case of pairwise UGMs with discrete variables and tabular potentials. Many of the results generalize to UGMs with higher-order clique potentials (which includes DGMs), but this makes the notation more complex (see (Koller and Friedman 2009) for details of the general case).\n",
      "\n",
      "22.3.1\n",
      "\n",
      "UGMs represented in exponential family form\n",
      "\n",
      "We assume the distribution has the following form:\n",
      "\n",
      "p(x|θ, G) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "exp\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "s∈V\n",
      "\n",
      "θs(xs) +\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "⎫ ⎬\n",
      "\n",
      "θst(xs, xt)\n",
      "\n",
      "⎭\n",
      "\n",
      "(22.12)\n",
      "\n",
      "where graph G has nodes V and edges E. (Henceforth we will drop the explicit conditioning on θ and G for brevity, since we assume both are known and ﬁxed.) We can rewrite this in exponential family form as follows:\n",
      "\n",
      "p(x|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "exp(−E(x))\n",
      "\n",
      "(22.13)\n",
      "\n",
      "E(x) (cid:2) −θT φ(x)\n",
      "\n",
      "(22.14)\n",
      "\n",
      "where θ = ({θs;j}, {θs,t;j,k}) are all the node and edge parameters (the canonical parameters), and φ(x) = ({I(xs = j)}, {I(xs = j, xt = k)}) are all the node and edge indicator functions (the sufficient statistics). Note: we use s, t ∈ V to index nodes and j, k ∈ X to index states.\n",
      "\n",
      "The mean of the sufficient statistics are known as the mean parameters of the model, and are\n",
      "\n",
      "given by\n",
      "\n",
      "(22.15) This is a vector of length d = |X ||V | + |X |2|E|, containing the node and edge marginals. It completely characterizes the distribution p(x|θ), so we sometimes treat μ as a distribution itself.\n",
      "\n",
      "μ = E [φ(x)] = ({p(xs = j)}s, {p(xs = j, xt = k)}s(cid:5)=t) = ({μs;j}s, {μst;jk}s(cid:5)=t)\n",
      "\n",
      "It is called “overcom- plete” because it ignores the sum-to-one constraints. In some cases, it is convenient to remove\n",
      "\n",
      "Equation 22.12 is called the standard overcomplete representation.\n",
      "\n",
      "22.3. Loopy belief propagation: theoretical issues *\n",
      "\n",
      "777\n",
      "\n",
      "this redundancy. For example, consider an Ising model where Xs ∈ {0, 1}. The model can be written as\n",
      "\n",
      "p(x) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "exp\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "s∈V\n",
      "\n",
      "θsxs +\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θstxsxt\n",
      "\n",
      "⎫ ⎬\n",
      "\n",
      "⎭\n",
      "\n",
      "(22.16)\n",
      "\n",
      "Hence we can use the following minimal parameterization\n",
      "\n",
      "φ(x) = (xs, s ∈ V ; xsxt, (s, t) ∈ E) ∈ Rd\n",
      "\n",
      "(22.17)\n",
      "\n",
      "where d = |V | + |E|. The corresponding mean parameters are μs = p(xs = 1) and μst = p(xs = 1, xt = 1).\n",
      "\n",
      "22.3.2\n",
      "\n",
      "The marginal polytope\n",
      "\n",
      "The space of allowable μ vectors is called the marginal polytope, and is denoted M(G), where G is the structure of the graph deﬁning the UGM. This is deﬁned to be the set of all mean parameters for the given model that can be generated from a valid probability distribution:\n",
      "\n",
      "M(G) (cid:2) {μ ∈ Rd : ∃p\n",
      "\n",
      "s.t. μ =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "φ(x)p(x) for some p(x) ≥ 0,\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(x) = 1}(22.18)\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "If we have just two nodes connected as X1 − X2, one can show that we have the following minimal set of constraints: 0 ≤ μ12, 0 ≤ μ12 ≤ μ1, 0 ≤ μ12 ≤ μ2, and 1 + μ12 − μ1 − μ2 ≥ 0. We can write these in matrix-vector form as\n",
      "\n",
      "For example, consider an Ising model.\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "0 1 0 −1 −1\n",
      "\n",
      "0 1 0 −1 1 −1 1\n",
      "\n",
      "⎟ ⎟ ⎠\n",
      "\n",
      "⎞\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "μ1 μ2 μ12\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠ ≥\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "0 0 0 −1\n",
      "\n",
      "⎟ ⎟ ⎠\n",
      "\n",
      "⎞\n",
      "\n",
      "(22.19)\n",
      "\n",
      "These four constraints deﬁne a series of half-planes, whose intersection deﬁnes a polytope,\n",
      "\n",
      "as shown in Figure 22.7(a).\n",
      "\n",
      "Since M(G) is obtained by taking a convex combination of the φ(x) vectors, it can also be\n",
      "\n",
      "written as the convex hull of the feature set:\n",
      "\n",
      "M(G) = conv{φ1(x), . . . , φd(x)}\n",
      "\n",
      "(22.20)\n",
      "\n",
      "For example, for a 2 node MRF X1 − X2 with binary states, we have\n",
      "\n",
      "M(G) = conv{(0, 0, 0), (1, 0, 0), (0, 1, 0), (1, 1, 1)}\n",
      "\n",
      "(22.21)\n",
      "\n",
      "These are the four black dots in Figure 22.7(a). We see that the convex hull deﬁnes the same volume as the intersection of half-spaces.\n",
      "\n",
      "The marginal polytope will play a crucial role in the approximate inference algorithms we\n",
      "\n",
      "discuss in the rest of this chapter.\n",
      "\n",
      "778\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 22.7 (b) Cartoon (a) Illustration of the marginal polytope for an Ising model with two variables. illustration of the set MF (G), which is a nonconvex inner bound on the marginal polytope M(G). MF (G) is used by mean ﬁeld. (c) Cartoon illustration of the relationship between M(G) and L(G), which is used by loopy BP. The set L(G) is always an outer bound on M(G), and the inclusion M(G) ⊂ L(G) is strict whenever G has loops. Both sets are polytopes, which can be deﬁned as an intersection of half-planes (deﬁned by facets), or as the convex hull of the vertices. L(G) actually has fewer facets than M(G), despite the picture. In fact, L(G) has O(|X ||V | + |X |2|E|) facets, where |X | is the number of states per variable, |V | is the number of variables, and |E| is the number of edges. By contrast, M(G) has O(|X ||V |) facets. On the other hand, L(G) has more vertices than M(G), despite the picture, since L(G) contains all the binary vector extreme points μ ∈ M(G), plus additional fractional extreme points. Source: Figures 3.6, 5.4 and 4.2 of (Wainwright and Jordan 2008a). Used with kind permission of Martin Wainwright.\n",
      "\n",
      "22.3.3\n",
      "\n",
      "Exact inference as a variational optimization problem\n",
      "\n",
      "Recall from Section 21.2 that the goal of variational inference is to ﬁnd the distribution q that maximizes the energy functional\n",
      "\n",
      "where ˜p(x) =Zp (x) is the unnormalized posterior. let q = p, then the exact energy functional becomes\n",
      "\n",
      "L(q) = −KL (q||p) + log Z = Eq [log ˜p(x)] + H (q) ≤ log Z\n",
      "\n",
      "(22.22) If we write log ˜p(x) =θ T φ(x), and we\n",
      "\n",
      "max μ∈M(G)\n",
      "\n",
      "θT μ + H (μ)\n",
      "\n",
      "(22.23)\n",
      "\n",
      "where μ = Ep [φ(x)] is a joint distribution over all state conﬁgurations x (so it is valid to write H (μ)). Since the KL divergence is zero when p = q, we know that\n",
      "\n",
      "max μ∈M(G)\n",
      "\n",
      "θT μ + H (μ) = log Z(θ)\n",
      "\n",
      "(22.24)\n",
      "\n",
      "This is a way to cast exact inference as a variational optimization problem.\n",
      "\n",
      "Equation 22.24 seems easy to optimize: the objective is concave, since it is the sum of a linear function and a concave function (see Figure 2.21 to see why entropy is concave); furthermore, we are maximizing this over a convex set. However, the marginal polytope M(G) has exponentially many facets. In some cases, there is structure to this polytope that can be exploited by dynamic programming (as we saw in Chapter 20), but in general, exact inference takes exponential time. Most of the existing deterministic approximate inference schemes that have been proposed in the literature can be seen as different approximations to the marginal polytope, as we explain below.\n",
      "\n",
      "22.3. Loopy belief propagation: theoretical issues *\n",
      "\n",
      "779\n",
      "\n",
      "22.3.4 Mean ﬁeld as a variational optimization problem\n",
      "\n",
      "We discussed mean ﬁeld at length in Chapter 21. Let us re-interpret mean ﬁeld inference in our new more abstract framework. This will help us compare it to other approximate methods which we discuss below.\n",
      "\n",
      "First, let F be an edge subgraph of the original graph G, and let I(F ) ⊆ I be the subset of sufficient statistics associated with the cliques of F . Let Ω be the set of canonical parameters for the full model, and deﬁne the canonical parameter space for the submodel as follows:\n",
      "\n",
      "Ω(F ) (cid:2) {θ ∈ Ω :θ α = 0 ∀α ∈ I \\ I(F )}\n",
      "\n",
      "(22.25)\n",
      "\n",
      "In other words, we require that the natural parameters associated with the sufficient statistics α outside of our chosen class to be zero. in the case of a fully factorized approximation, F0, we remove all edges from the graph, giving\n",
      "\n",
      "For example,\n",
      "\n",
      "Ω(F0) (cid:2) {θ ∈ Ω :θ st = 0 ∀(s, t) ∈ E}\n",
      "\n",
      "(22.26)\n",
      "\n",
      "In the case of structured mean ﬁeld (Section 21.4), we set θst = 0 for edges which are not in our tractable subgraph.\n",
      "\n",
      "Next, we deﬁne the mean parameter space of the restricted model as follows: MF (G) (cid:2) {μ ∈ Rd : μ = Eθ [φ(x)]\n",
      "\n",
      "for some θ ∈ Ω(F )}\n",
      "\n",
      "(22.27)\n",
      "\n",
      "This is called an inner approximation to the marginal polytope, since MF (G) ⊆ M(G). See Figure 22.7(b) for a sketch. Note that MF (G) is a non-convex polytope, which results in multiple local optima. By contrast, some of the approximations we will consider later will be convex.\n",
      "\n",
      "We deﬁne the entropy of our approximation H (μ(F )) as the entropy of the distribution μ deﬁned on submodel F . Then we deﬁne the mean ﬁeld energy functional optimization problem as follows:\n",
      "\n",
      "max μ∈MF (G)\n",
      "\n",
      "θT μ + H (μ) ≤ log Z(θ)\n",
      "\n",
      "(22.28)\n",
      "\n",
      "In the case of the fully factorized mean ﬁeld approximation for pairwise UGMs, we can write this objective as follows: (cid:4)\n",
      "\n",
      "max μ∈P d\n",
      "\n",
      "s∈V\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "xs\n",
      "\n",
      "θs(xs)μs(xs) +\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "xs,xt\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θst(xs, xt)μs(xs)μt(xt) +\n",
      "\n",
      "s∈V\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "H (μs)\n",
      "\n",
      "(22.29)\n",
      "\n",
      "where μs ∈ P, and P is the probability simplex over X .\n",
      "\n",
      "Mean ﬁeld involves a concave objective being maximized over a non-convex set. It is typically optimized using coordinate ascent, since it is easy to optimize a scalar concave function over P for each μs. For example, for a pairwise UGM we get\n",
      "\n",
      "⎛\n",
      "\n",
      "⎞\n",
      "\n",
      "μs(xs) ∝ exp(θs(xs)) exp\n",
      "\n",
      "⎝\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "μt(xt)θst(xs, xt)\n",
      "\n",
      "⎠\n",
      "\n",
      "(22.30)\n",
      "\n",
      "t∈nbr(s)\n",
      "\n",
      "xt\n",
      "\n",
      "22.3.5\n",
      "\n",
      "LBP as a variational optimization problem\n",
      "\n",
      "In this section, we explain how LBP can be viewed as a variational inference problem.\n",
      "\n",
      "780\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "Figure 22.8 (a) Illustration of pairwise UGM on binary nodes, together with a set of pseudo marginals (b) A slice of the marginal polytope illustrating the set of feasible edge that are not globally consistent. marginals, assuming the node marginals are clamped at μ1 = μ2 = μ3 = 0.5. Source: Figure 4.1 of (Wainwright and Jordan 2008a). Used with kind permission of Martin Wainwright.\n",
      "\n",
      "22.3.5.1\n",
      "\n",
      "An outer approximation to the marginal polytope\n",
      "\n",
      "If we want to consider all possible probability distributions which are Markov wrt our model, we need to consider all vectors μ ∈ M(G). Since the set M(G) is exponentially large, it is usually infeasible to optimize over. A standard strategy in combinatorial optimization is to relax the constraints. In this case, instead of requiring probability vector μ to live in M(G), we consider a vector τ that only satisﬁes the following local consistency constraints:\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "τs(xs) = 1\n",
      "\n",
      "(22.31)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "xs τst(xs, xt) = τs(xs)\n",
      "\n",
      "(22.32)\n",
      "\n",
      "xt\n",
      "\n",
      "The ﬁrst constraint is called the normalization constraint, and the second is called the marginal- ization constraint. We then deﬁne the set\n",
      "\n",
      "L(G) (cid:2) {τ ≥ 0 : (22.31) holds ∀s ∈ V and (22.32) holds ∀(s, t) ∈ E}\n",
      "\n",
      "(22.33)\n",
      "\n",
      "The set L(G) is also a polytope, but it only has O(|V | + |E|) constraints. It is a convex outer approximation on M(G), as shown in Figure 22.7(c).\n",
      "\n",
      "We call the terms τs, τst ∈ L(G) pseudo marginals, since they may not correspond to marginals of any valid probability distribution. As an example of this, consider Figure 22.8(a). The picture shows a set of pseudo node and edge marginals, which satisfy the local consistency requirements. However, they are not globally consistent. To see why, note that τ12 implies p(X1 = X2) = 0.8, τ23 implies p(X2 = X3) = 0.8, but τ13 implies p(X1 = X3) = 0.2, which is not possible (see (Wainwright and Jordan 2008b, p81) for a formal proof). Indeed, Figure 22.8(b) shows that L(G) contains points that are not in M(G).\n",
      "\n",
      "We claim that M(G) ⊆ L(G), with equality iff G is a tree. To see this, ﬁrst consider\n",
      "\n",
      "22.3. Loopy belief propagation: theoretical issues *\n",
      "\n",
      "781\n",
      "\n",
      "an element μ ∈ M(G). Any such vector must satisfy the normalization and marginalization constraints, hence M(G) ⊆ L(G).\n",
      "\n",
      "Now consider the converse. Suppose T is a tree, and let μ ∈ L(T ). By deﬁnition, this satisﬁes the normalization and marginalization constraints. However, any tree can be represented in the form\n",
      "\n",
      "pμ(x) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "s∈V\n",
      "\n",
      "μs(xs)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "μst(xs, xt) μs(xs)μt(xt)\n",
      "\n",
      "(22.34)\n",
      "\n",
      "Hence satsifying normalization and local consistency is enough to deﬁne a valid distribution for any tree. Hence μ ∈ M(T ) as well.\n",
      "\n",
      "In contrast, if the graph has loops, we have that M(G) (cid:8)= L(G). See Figure 22.8(b) for an\n",
      "\n",
      "example of this fact.\n",
      "\n",
      "22.3.5.2\n",
      "\n",
      "The entropy approximation\n",
      "\n",
      "From Equation 22.34, we can write the exact entropy of any tree structured distribution μ ∈ M(T ) as follows:\n",
      "\n",
      "H (μ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Hs(μs) −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Ist(μst)\n",
      "\n",
      "(22.35)\n",
      "\n",
      "Hs(μs) =−\n",
      "\n",
      "s∈V\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "μs(xs) log μs(xs)\n",
      "\n",
      "(22.36)\n",
      "\n",
      "Ist(μst) =\n",
      "\n",
      "xs∈Xs (cid:4)\n",
      "\n",
      "(xs,xt)∈Xs×Xt\n",
      "\n",
      "μst(xs, xt) log\n",
      "\n",
      "μst(xs, xt) μs(xs)μt(xt)\n",
      "\n",
      "(22.37)\n",
      "\n",
      "Note that we can rewrite the mutual information term in the form Ist(μst) = Hs(μs)+Ht(μt)− Hst(μst), and hence we get the following alternative but equivalent expression:\n",
      "\n",
      "H (μ) =−\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(ds − 1)Hs(μs) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Hst(μst)\n",
      "\n",
      "(22.38)\n",
      "\n",
      "s∈V\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "where ds is the degree (number of neighbors) for node s.\n",
      "\n",
      "The Bethe1 approximation to the entropy is simply the use of Equation 22.35 even when we\n",
      "\n",
      "don’t have a tree:\n",
      "\n",
      "HBethe(τ ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Hs(τs) −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Ist(τst)\n",
      "\n",
      "(22.39)\n",
      "\n",
      "s∈V\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "We deﬁne the Bethe free energy as\n",
      "\n",
      "FBethe(τ ) (cid:2) −\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "θT τ + HBethe(τ )\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(22.40)\n",
      "\n",
      "We deﬁne the Bethe energy functional as the negative of the Bethe free energy.\n",
      "\n",
      "1. Hans Bethe was a German-American physicist, 1906–2005.\n",
      "\n",
      "782\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "22.3.5.3\n",
      "\n",
      "The LBP objective\n",
      "\n",
      "Combining the outer approximation L(G) with the Bethe approximation to the entropy, we get the following Bethe variational problem (BVP):\n",
      "\n",
      "min τ ∈L(G)\n",
      "\n",
      "FBethe(τ ) = max τ ∈L(G)\n",
      "\n",
      "θT τ + HBethe(τ )\n",
      "\n",
      "(22.41)\n",
      "\n",
      "The space we are optimizing over is a convex set, but the objective itself is not concave (since HBethe is not concave). Thus there can be multiple local optima of the BVP.\n",
      "\n",
      "In the case of trees, the approximation is exact, and in the case of models with attractive potentials, the approximation turns out to be an upper bound (Sudderth et al. 2008).\n",
      "\n",
      "The value obtained by the BVP is an approximation to log Z(θ).\n",
      "\n",
      "22.3.5.4 Message passing and Lagrange multipliers\n",
      "\n",
      "In this subsection, we will show that any ﬁxed point of the LBP algorithm deﬁnes a stationary point of the above constrained objective. Let us deﬁne the normalization constraint at Css(τ ) (cid:2) 1 − τst(xs, xt) for each edge t → s. We can now write the Lagrangian as\n",
      "\n",
      "L(τ , λ; θ) (cid:2) θT τ + HBethe(τ ) +\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "xs\n",
      "\n",
      "τs(xs), and the marginalization constraint as Cts(xs; τ ) (cid:2) τs(xs) −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "λssCss(τ )\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "xt\n",
      "\n",
      "+\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "s\n",
      "\n",
      "λts(xs)Cts(xs; τ ) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "λst(xt)Cst(xt; τ )\n",
      "\n",
      "(22.42)\n",
      "\n",
      "s,t\n",
      "\n",
      "xs\n",
      "\n",
      "xt\n",
      "\n",
      "(The constraint that τ ≥ 0 is not explicitly enforced, but one can show that it will hold at the optimum since θ > 0.) Some simple algebra then shows that ∇τ L = 0 yields\n",
      "\n",
      "log τs(xs) =λ ss + θs(xs) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "λts(xs)\n",
      "\n",
      "(22.43)\n",
      "\n",
      "t∈nbr(s)\n",
      "\n",
      "where we have deﬁned ˜τs(xs) (cid:2) xt straint implies ˜τs(xs) = τs(xs), we get\n",
      "\n",
      "log\n",
      "\n",
      "τst(xs, xt) ˜τs(xs)˜τt(xt)\n",
      "\n",
      "= θst(xs, xt) − λts(xs) − λst(xt)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "τ (xs, xt). Using the fact that the marginalization con-\n",
      "\n",
      "(22.44)\n",
      "\n",
      "log τst(xs, xt) =λ ss + λtt + θst(xs, xt) +θ s(xs) +θ t(xt) λut(xt)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "λus(xs) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(22.45)\n",
      "\n",
      "u∈nbr(s)\\t\n",
      "\n",
      "u∈nbr(t)\\s\n",
      "\n",
      "To make the connection to message passing, deﬁne Mts(xs) = exp(λts(xs)). With this\n",
      "\n",
      "notation, we can rewrite the above equations (after taking exponents of both sides) as follows:\n",
      "\n",
      "τs(xs) ∝ exp(θs(xs))\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Mts(xs)\n",
      "\n",
      "(22.46)\n",
      "\n",
      "t∈nbr(s)\n",
      "\n",
      "τst(xs, xt) ∝ exp (θst(xs, xt) +θ s(xs) +θ t(xt))\n",
      "\n",
      "×\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Mus(xs)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Mut(xt)\n",
      "\n",
      "(22.47)\n",
      "\n",
      "u∈nbr(s)\\t\n",
      "\n",
      "u∈nbr(t)\\s\n",
      "\n",
      "22.4. Extensions of belief propagation *\n",
      "\n",
      "783\n",
      "\n",
      "where the λ terms are absorbed into the constant of proportionality. We see that this is equivalent to the usual expression for the node and edge marginals in LBP.\n",
      "\n",
      "λts), we enforce the marginalization condition that\n",
      "\n",
      "To derive an equation for the messages in terms of other messages (rather than in terms of (cid:7) τst(xs, xt) =τ s(xs). Then one can show\n",
      "\n",
      "⎡\n",
      "\n",
      "xt\n",
      "\n",
      "⎤\n",
      "\n",
      "Mts(xs) ∝\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "⎣exp {θst(xs, xt) +θ t(xt)}\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Mut(xt)\n",
      "\n",
      "⎦\n",
      "\n",
      "(22.48)\n",
      "\n",
      "xt\n",
      "\n",
      "u∈nbr(t)\\s\n",
      "\n",
      "We see that this is equivalent to the usual expression for the messages in LBP.\n",
      "\n",
      "22.3.6\n",
      "\n",
      "Loopy BP vs mean ﬁeld\n",
      "\n",
      "It is interesting to compare the naive mean ﬁeld (MF) and LBP approximations. There are several obvious differences. First, LBP is exact for trees whereas MF is not, suggesting LBP will in general be more accurate (see (Wainwright et al. 2003) for an analysis). Second, LBP optimizes over node and edge marginals, whereas MF only optimizes over node marginals, again suggesting LBP will be more accurate. Third, in the case that the true edge marginals factorize, so μst = μsμt, the free energy approximations will be the same in both cases.\n",
      "\n",
      "What is less obvious, but which nevertheless seems to be true, is that the MF objective has many more local optima than the LBP objective, so optimizing the MF objective seems to be harder. In particular, (Weiss 2001), shows empirically that optimizing MF starting from uniform or random initial conditions often leads to poor results, whereas optimizing BP from uniform initial messages often leads to good results. Furthermore, initializing MF with the BP marginals also leads to good results (although MF tends to be more overconﬁdent than BP), indicating that the problem is caused not by the inaccuracy of the MF approximation, but rather by the severe non-convexity of the MF objective, and by the weakness of the standard coordinate descent optimization method used by MF.2 However, the advantage of MF is that it gives a lower bound on the partition function, unlike BP, which is useful when using it as a subroutine inside a learning algorithm. Also, MF is easier to extend to other distributions besides discrete and Gaussian, as we saw in Chapter 21. Intuitively, this is because MF only works with marginal distributions, which have a single type, rather than needing to deﬁne pairwise distributions, which may need to have two different types.\n",
      "\n",
      "22.4\n",
      "\n",
      "Extensions of belief propagation *\n",
      "\n",
      "In this section, we discuss various extensions of LBP.\n",
      "\n",
      "22.4.1\n",
      "\n",
      "Generalized belief propagation\n",
      "\n",
      "We can improve the accuracy of loopy BP by clustering together nodes that form a tight loop. This is known as the cluster variational method. The result is a hyper-graph, which is a graph\n",
      "\n",
      "2. (Honkela et al. 2003) discusses the use of the pattern search algorithm to speedup mean ﬁeld inference in the case of continuous random variables. It is possible that similar ideas could be adapted to the discrete case, although there may be no reason to do this, given that LBP already works well in the discrete case.\n",
      "\n",
      "784\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "1 2 4 5\n",
      "\n",
      "2 3 5 6\n",
      "\n",
      "52\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "4 5\n",
      "\n",
      "5\n",
      "\n",
      "5 6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "5 8\n",
      "\n",
      "54\n",
      "\n",
      "7 8\n",
      "\n",
      "5 6 8 9\n",
      "\n",
      "Figure 22.9 (a) Kikuchi clusters superimposed on a 3 × 3 lattice graph. Source: Figure 4.5 of (Wainwright and Jordan 2008b). Used with kind permission of Martin Wainwright.\n",
      "\n",
      "(b) Corresponding hyper-graph.\n",
      "\n",
      "where there are hyper-edges between sets of vertices instead of between single vertices. Note that a junction tree (Section 20.4.1) is a kind of hyper-graph. We can represent hyper-graph using a poset (partially ordered set) diagram, where each node represents a hyper-edge, and there is an arrow e1 → e2 if e2 ⊂ e1. See Figure 22.9 for an example.\n",
      "\n",
      "Let t be the size of the largest hyper-edge in the hyper-graph. If we allow t to be as large as the treewidth of the graph, then we can represent the hyper-graph as a tree, and the method will be exact, just as LBP is exact on regular trees (with treewidth 1). In this way, we can deﬁne a continuum of approximations, from LBP all the way to exact inference.\n",
      "\n",
      "Deﬁne Lt(G) to be the set of all pseudo-marginals such that normalization and marginaliza- tion constraints hold on a hyper-graph whose largest hyper-edge is of size t + 1. For example, in Figure 22.9, we impose constraints of the form (cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "τ1245(x1, x2, x4, x5) = τ45(x4, x5),\n",
      "\n",
      "τ56(x5, x6) = τ5(x5), . . .\n",
      "\n",
      "(22.49)\n",
      "\n",
      "x1,x2\n",
      "\n",
      "x6\n",
      "\n",
      "Furthermore, we approximate the entropy as follows:\n",
      "\n",
      "HKikuchi(τ ) (cid:2)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "c(g)Hg(τg)\n",
      "\n",
      "(22.50)\n",
      "\n",
      "g∈E\n",
      "\n",
      "where Hg(τg) is the entropy of the joint (pseudo) distribution on the vertices in set g, and c(g) is called the overcounting number of set g. These are related to Mobious numbers in set theory. Rather than giving a precise deﬁnition, we just give a simple example. For the graph in Figure 22.9, we have\n",
      "\n",
      "HKikuchi(τ ) = [H1245 + H2356 + H4578 + H5689] −[H25 + H45 + H56 + H58] +H 5\n",
      "\n",
      "(22.51)\n",
      "\n",
      "Putting these two approximations together, we can deﬁne the Kikuchi free energy3 as follows:\n",
      "\n",
      "FKikuchi(τ ) (cid:2) −\n",
      "\n",
      "(cid:15)\n",
      "\n",
      "θT τ + HKikuchi(τ )\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(22.52)\n",
      "\n",
      "3. Ryoichi Kikuchi is a Japanese physicist.\n",
      "\n",
      "22.4. Extensions of belief propagation *\n",
      "\n",
      "785\n",
      "\n",
      "Our variational problem becomes FKikuchi(τ ) = max\n",
      "\n",
      "min τ ∈Lt(G)\n",
      "\n",
      "τ ∈Lt(G)\n",
      "\n",
      "θT τ + HKikuchi(τ )\n",
      "\n",
      "(22.53)\n",
      "\n",
      "Just as with the Bethe free energy, this is not a concave objective. There are several possible algorithms for ﬁnding a local optimum of this objective, including a message passing algorithm known as generalized belief propagation. However, the details are beyond the scope of this chapter. See e.g., (Wainwright and Jordan 2008b, Sec 4.2) or (Koller and Friedman 2009, Sec 11.3.2) for more information. Suffice it to say that the method gives more accurate results than LBP, but at increased computational cost (because of the need to handle clusters of nodes). This cost, plus the complexity of the approach, have precluded it from widespread use.\n",
      "\n",
      "22.4.2\n",
      "\n",
      "Convex belief propagation\n",
      "\n",
      "The mean ﬁeld energy functional is concave, but it is maximized over a non-convex inner approximation to the marginal polytope. The Bethe and Kikuchi energy functionals are not concave, but they are maximized over a convex outer approximation to the marginal polytope. Consequently, for both MF and LBP, the optimization problem has multiple optima, so the methods are sensitive to the initial conditions. Given that the exact formulation (Equation 22.24) it is natural to try to come up with an a concave objective maximized over a convex set, appproximation which involves a concave objective being maximized over a convex set.\n",
      "\n",
      "We now describe one method, known as convex belief propagation. This involves working with a set of tractable submodels, F, such as trees or planar graphs. For each model F ⊂ G, the entropy is higher, H (μ(F )) ≥ H (μ(G)), since F has fewer constraints. Consequently, any convex combination of such subgraphs will have higher entropy, too:\n",
      "\n",
      "H (μ(G)) ≤\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ρ(F )H (μ(F )) (cid:2) H(μ, ρ)\n",
      "\n",
      "(22.54)\n",
      "\n",
      "F ∈F where ρ(F ) ≥ 0 and deﬁne the convex free energy as\n",
      "\n",
      "FConvex(μ, ρ) (cid:2) −\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "F ρ(F ) = 1. Furthermore, H(μ, ρ) is a concave function of μ. We now\n",
      "\n",
      "μT θ + H(μ, ρ)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(22.55)\n",
      "\n",
      "We deﬁne the concave energy functional as the negative of the convex free energy. We discuss how to optimize ρ below.\n",
      "\n",
      "Having deﬁned an upper bound on the entropy, we now consider a convex outerbound on the marginal polytope of mean parameters. We want to ensure we can evaluate the entropy of any vector τ in this set, so we restrict it so that the projection of τ onto the subgraph G lives in the projection of M onto F :\n",
      "\n",
      "(22.56) This is a convex set since each M(F ) is a projection of a convex set. Hence we deﬁne our problem as\n",
      "\n",
      "L(G; F) (cid:2) {τ ∈ Rd : τ (F ) ∈ M(F ) ∀F ∈ F}\n",
      "\n",
      "min τ ∈L(G;F )\n",
      "\n",
      "FConvex(τ , ρ) = max\n",
      "\n",
      "τ ∈L(G;F )\n",
      "\n",
      "τ T θ + H(τ , ρ)\n",
      "\n",
      "(22.57)\n",
      "\n",
      "This is a concave objective being maximized over a convex set, and hence has a unique maxi- mum. We give a speciﬁc example below.\n",
      "\n",
      "786\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "f\n",
      "\n",
      "f\n",
      "\n",
      "f\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "\n",
      "b\n",
      "\n",
      "b\n",
      "\n",
      "b\n",
      "\n",
      "e\n",
      "\n",
      "e\n",
      "\n",
      "e\n",
      "\n",
      "e\n",
      "\n",
      "Figure 22.10 (a) A graph. (b-d) Some of its spanning trees. Source: Figure 7.1 of (Wainwright and Jordan 2008b). Used with kind permission of Martin Wainwright.\n",
      "\n",
      "22.4.2.1\n",
      "\n",
      "Tree-reweighted belief propagation\n",
      "\n",
      "Consider the speciﬁc case where F is all spanning trees of a graph. For any given tree, the entropy is given by Equation 22.35. To compute the upper bound, obtained by averaging over F ρ(F )H(μ(F )s) for single nodes will just be Hs, since node s all trees, note that the terms F ρ(F ) = 1. But the mutual information term Ist receives weight appears in every tree, and ρst = Eρ [I((s, t) ∈ E(T ))], known as the edge appearance probability. Hence we have the following upper bound on the entropy:\n",
      "\n",
      "H (μ) ≤\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Hs(μs) −\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ρstIst(μst)\n",
      "\n",
      "(22.58)\n",
      "\n",
      "s∈V\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "The edge appearance probabilities live in a space called the spanning tree polytope. This is because they are constrained to arise from a distribution over trees. Figure 22.10 gives an example of a graph and three of its spanning trees. Suppose each tree has equal weight under ρ. The edge f occurs in 1 of the 3 trees, so ρf = 1/3. The edge e occurs in 2 of the 3 trees, so ρe = 2/3. The edge b appears in all of the trees, so ρb = 1. And so on. Ideally we can ﬁnd a distribution ρ, or equivalently edge probabilities in the spanning tree polytope, that make the above bound as tight as possible. An algorithm to do this is described in (Wainwright et al. (A simpler approach is to generate spanning trees of G at random until all edges are 2005). covered, or use all single edges with weight ρe = 1/E.)\n",
      "\n",
      "What about the set we are optimizing over? We require μ(T ) ∈ M(T ) for each tree T , which means enforcing normalization and local consistency. Since we have to do this for every tree, we are enforcing normalization and local consistency on every edge. Hence L(G; F) = L(G). So our ﬁnal optimization problem is as follows:\n",
      "\n",
      "max τ ∈L(G)\n",
      "\n",
      "⎧ ⎨\n",
      "\n",
      "⎩\n",
      "\n",
      "τ T θ +\n",
      "\n",
      "s∈V\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Hs(τs) −\n",
      "\n",
      "(s,t)∈E(G)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ρstIst(τst)\n",
      "\n",
      "⎫ ⎬\n",
      "\n",
      "⎭\n",
      "\n",
      "(22.59)\n",
      "\n",
      "which is the same as the LBP objective except for the crucial ρst weights. So long as ρst > 0 for all edges (s, t), this problem is strictly concave with a unique maximum.\n",
      "\n",
      "How can we ﬁnd this global optimum? As for LBP, there are several algorithms, but perhaps the simplest is a modiﬁcation of belief propagation known as tree reweighted belief propagation,\n",
      "\n",
      "22.5. Expectation propagation\n",
      "\n",
      "787\n",
      "\n",
      "also called TRW or TRBP for short. The message from t to s is now a function of all messages sent from other neighbors v to t, as before, but now it is also a function of the message sent from s to t. Speciﬁcally (cid:4)\n",
      "\n",
      "Mts(xs) ∝\n",
      "\n",
      "xt\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 ρst\n",
      "\n",
      "θst(xs, xt) +θ t(xt)\n",
      "\n",
      "(cid:9) (cid:26)\n",
      "\n",
      "v∈nbr(t)\\s[Mvt(xt)]ρvt [Mst(xt)]1−ρts\n",
      "\n",
      "(22.60)\n",
      "\n",
      "At convergence, the node and edge pseudo marginals are given by\n",
      "\n",
      "τs(xs) ∝ exp(θs(xs))\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "[Mvs(xs)]ρvs\n",
      "\n",
      "(22.61)\n",
      "\n",
      "ϕst(xs, xt) (cid:2) exp\n",
      "\n",
      "τst(xs, xt) ∝ ϕst(xs, xt)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 ρst\n",
      "\n",
      "θst(xs, xt) +θ s(xs) +θ t(xt)\n",
      "\n",
      "v∈nbr(s) (cid:26) v∈nbr(s)\\t[Mvs(xs)]ρvs [Mts(xs)]1−ρst\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "v∈nbr(t)\\s[Mvt(xt)]ρvt [Mst(xt)]1−ρts\n",
      "\n",
      "(22.62)\n",
      "\n",
      "(22.63)\n",
      "\n",
      "This algorithm can be derived using a method similar to that described in Section 22.3.5.4.\n",
      "\n",
      "If ρst = 1 for all edges (s, t) ∈ E, the algorithm reduces to the standard LBP algorithm. However, the condition ρst = 1 implies every edge is present in every spanning tree with probability 1, which is only possible if the original graph is a tree. Hence the method is only equivalent to standard LBP on trees, when the method is of course exact.\n",
      "\n",
      "In general, this message passing scheme is not guaranteed to converge to the unique global optimum. One can devise double-loop methods that are guaranteed to converge (Hazan and Shashua 2008), but in practice, using damped updates as in Equation 22.7 is often sufficient to ensure convergence.\n",
      "\n",
      "It is also possible to produce a convex version of the Kikuchi free energy, which one can optimize with a modiﬁed version of generalized belief propagation. See (Wainwright and Jordan 2008b, Sec 7.2.2) for details.\n",
      "\n",
      "From Equation 22.59, and using the fact that the TRBP entropy approximation is an upper bound on the true entropy, wee see that the TRBP objective is an upper bound on log Z. Using the fact that Ist = Hs + Ht − Hst, we can rewrite the upper bound as follows:\n",
      "\n",
      "log ˆZ(θ) (cid:2) τ T θ +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ρstHst(τst) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "csHs(τs) ≤ log Z(θ)\n",
      "\n",
      "(22.64)\n",
      "\n",
      "st\n",
      "\n",
      "s\n",
      "\n",
      "where cs (cid:2) 1 −\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "t ρst.\n",
      "\n",
      "22.5\n",
      "\n",
      "Expectation propagation\n",
      "\n",
      "Expectation propagation (EP) (Minka 2001c) is a form of belief propagation where the mes- sages are approximated. It is a generalization of the assumed density ﬁltering (ADF) algorithm, discussed in Section 18.5.3. In that method, we approximated the posterior at each step using an assumed functional form, such as a Gaussian. This posterior can be computed using mo- ment matching, which locally optimizes KL (p||q) for a single term. From this, we derived the message to send to the next time step.\n",
      "\n",
      "788\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "ADF works well for sequential Bayesian updating, but the answer it gives depends on the order in which the data is seen. EP essentially corrects this ﬂaw by making multiple passes over the data (thus EP is an offline or batch inference algorithm).\n",
      "\n",
      "22.5.1\n",
      "\n",
      "EP as a variational inference problem\n",
      "\n",
      "We now explain how to view EP in terms of variational inference. We follow the presentation of (Wainwright and Jordan 2008b, Sec 4.3), which should be consulted for further details. Suppose the joint distribution can be written in exponential family form as follows:\n",
      "\n",
      "p(x|θ, ˜θ) ∝ f0(x) exp(θT φ(x))\n",
      "\n",
      "dI(cid:20)\n",
      "\n",
      "exp(˜θ\n",
      "\n",
      "T i Φi(x))\n",
      "\n",
      "(22.65)\n",
      "\n",
      "i=1\n",
      "\n",
      "where we have partitioned the parameters and the sufficient statistics into a tractable term θ of size dT and dI intractable terms ˜θi, each of size b.\n",
      "\n",
      "For example, consider the problem of inferring an unknown vector x, when the observation model is a mixture of two Gaussians, one centered at x and one centered at 0. (This can be used to represent outliers, for example.) Minka (who invented EP) calls this the clutter problem. More formally, we assume an observation model of the form\n",
      "\n",
      "p(y|x) = (1 − w)N (y|x, I) +w N (y|0, aI)\n",
      "\n",
      "(22.66)\n",
      "\n",
      "where 0 < w <1 is the known mixing weight (fraction of outliers), and a >0 is the variance of the background distribution. Assuming a ﬁxed prior of the form p(x) =N (x|0, Σ), we can write our model in the required form as follows:\n",
      "\n",
      "p(x|y1:N ) ∝ N (x|0, Σ)\n",
      "\n",
      "N(cid:20)\n",
      "\n",
      "p(yi|x)\n",
      "\n",
      "(22.67)\n",
      "\n",
      "This matches our canonical form where f0(x) exp(θT φ(x)) corresponds to exp using φ(x) = (x, xxT ), and we set Φi(x) = log p(yi|x), ˜θi = 1, and dI = N .\n",
      "\n",
      "= exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "xT Σ−1x\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "i=1\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "log p(yi|x)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "− 1\n",
      "\n",
      "2 xT Σ−1x\n",
      "\n",
      "(22.68)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      ",\n",
      "\n",
      "The exact inference problem corresponds to\n",
      "\n",
      "max (τ ,˜τ )∈M(φ,Φ)\n",
      "\n",
      "τ T θ + ˜τ T ˜θ + H ((τ , ˜τ ))\n",
      "\n",
      "(22.69)\n",
      "\n",
      "where M(φ, Φ) is the set of mean parameters realizable by any probability distribution as seen through the eyes of the sufficient statistics:\n",
      "\n",
      "M(φ, Φ) ={( μ, ˜μ) ∈ RdT × RdI b : (μ, ˜μ) = E [(φ(X), Φ1(X), . . . , ΦdI (X))]}\n",
      "\n",
      "(22.70)\n",
      "\n",
      "As it stands, it is intractable to perform inference in this distribution. For example, in our clutter example, the posterior contains 2N modes. But suppose we incorporate just one of the intractable terms, say the i’th one; we will call this the Φi-augmented distribution:\n",
      "\n",
      "p(x|θ, ˜θi) ∝ f0(x) exp(θT φ(x)) exp(˜θ\n",
      "\n",
      "T i Φi(x))\n",
      "\n",
      "(22.71)\n",
      "\n",
      "22.5. Expectation propagation\n",
      "\n",
      "789\n",
      "\n",
      "In our clutter example, this becomes\n",
      "\n",
      "p(x|θ, ˜θi) = exp\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "− 1 2\n",
      "\n",
      "xT Σ−1x\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "[wN (yi|0, aI) + (1− w)N (yi|x, I)]\n",
      "\n",
      "(22.72)\n",
      "\n",
      "This is tractable to compute, since it is just a mixture of 2 Gaussians.\n",
      "\n",
      "fashion. First, we approximate the convex set M(φ, Φ) with another, larger convex set:\n",
      "\n",
      "The key idea behind EP is to work with these the Φi-augmented distributions in an iterative\n",
      "\n",
      "L(φ, Φ) (cid:2) {(τ , ˜τ ) : τ ∈ M(φ), (τ , ˜τ i) ∈ M(φ, Φi)}\n",
      "\n",
      "(22.73)\n",
      "\n",
      "where M(φ) = {μ ∈ RdT : μ = E [φ(X)]} and M(φ, Φi) = {(μ, ˜μi) ∈ RdT × Rb : (μ, ˜μi) =E [(φ(X), Φi(X))]. Next we approximate the entropy by the following term-by-term approximation:\n",
      "\n",
      "Hep(τ , ˜τ ) (cid:2) H (τ ) +\n",
      "\n",
      "dI(cid:4)\n",
      "\n",
      "[H (τ , ˜τ i) − H (τ )]\n",
      "\n",
      "(22.74)\n",
      "\n",
      "i=1\n",
      "\n",
      "Then the EP problem becomes\n",
      "\n",
      "max (τ ,˜τ )∈L(φ,Φ)\n",
      "\n",
      "τ T θ + ˜τ T ˜θ + Hep(τ , ˜τ )\n",
      "\n",
      "(22.75)\n",
      "\n",
      "22.5.2\n",
      "\n",
      "Optimizing the EP objective using moment matching\n",
      "\n",
      "We now discuss how to maximize the EP objective in Equation 22.75. Let us duplicate τ dI times to yield ηi = τ . The augmented set of parameters we need to optimize is now\n",
      "\n",
      "(τ , (ηi, ˜τ i)dI\n",
      "\n",
      "i=1) ∈ RdT × (RdT × Rb)dI\n",
      "\n",
      "(22.76)\n",
      "\n",
      "subject to the constraints that ηi = τ and (ηi, ˜τ i) ∈ M(φ; Φi). Let us associate a vector of Lagrange multipliers λi ∈ RdT with the ﬁrst set of constraints. Then the partial Lagrangian becomes\n",
      "\n",
      "L(τ ; λ) =τ\n",
      "\n",
      "T θ + H (τ ) +\n",
      "\n",
      "di(cid:4)\n",
      "\n",
      "(cid:15) i ˜θi + H ((ηi, ˜τ i)) − H (ηi) +λ T ˜τ T\n",
      "\n",
      "i (τ − ηi)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(22.77)\n",
      "\n",
      "i=1\n",
      "\n",
      "By solving ∇τ L(τ ; λ) = 0, we can show that the corresponding distribution in M(φ) has\n",
      "\n",
      "the form\n",
      "\n",
      "q(x|θ, λ) ∝ f0(x) exp{(θ +\n",
      "\n",
      "dI(cid:4)\n",
      "\n",
      "λi)T φ(x)}\n",
      "\n",
      "(22.78)\n",
      "\n",
      "i=1\n",
      "\n",
      "The λT i φ(x) terms represents an approximation to the i’th intractable term using the sufficient statistics from the base distribution, as we will see below. Similarly, by solving ∇(ηi,˜τ i)L(τ ; λ) = 0, we ﬁnd that the corresponding distribution in M(φ, Φi) has the form\n",
      "\n",
      "qi(x|θ, ˜θi, λ) ∝ f0(x) exp{(θ +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "λj)T φ(x) + ˜θ\n",
      "\n",
      "T i Φi(x)}\n",
      "\n",
      "(22.79)\n",
      "\n",
      "j(cid:5)=i\n",
      "\n",
      "790\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "This corresponds to removing the approximation to the i’th term, λi, from the base distribution, and adding in the correct i’th term, Φi. Finally, ∇λL(τ ; λ) =0 just enforces the constraints that τ = Eq [φ(X)] and ηi = Eqi [φ(X)] are equal. In other words, we get the following moment matching constraints:\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "q(x|θ, λ)φ(x)dx =\n",
      "\n",
      "qi(x|θ, ˜θi, λ)φ(x)dx\n",
      "\n",
      "(22.80)\n",
      "\n",
      "Thus the overall algorithm is as follows. First we initialize the λi. Then we iterate the following to convergence: pick a term i; compute qi (corresponding to removing the old approximation to Φi and adding in the new one); then update the λi term in q by solving the moment matching equation Eqi [φ(X)] = Eq [φ(X)]. (Note that this particular optimization scheme is not guaranteed to converge to a ﬁxed point.)\n",
      "\n",
      "An equivalent way of stating the algorithm is as follows. Let us assume the true distribution\n",
      "\n",
      "is given by\n",
      "\n",
      "p(x|D) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "i\n",
      "\n",
      "fi(x)\n",
      "\n",
      "(22.81)\n",
      "\n",
      "We approximate each fi by ˜fi and set\n",
      "\n",
      "q(x) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "i\n",
      "\n",
      "˜fi(x)\n",
      "\n",
      "(22.82)\n",
      "\n",
      "Now we repeat the following until convergence: 1. Choose a factor ˜fi to reﬁne. 2. Remove ˜fi from the posterior by dividing it out:\n",
      "\n",
      "q−i(x) =\n",
      "\n",
      "q(x) ˜fi(x)\n",
      "\n",
      "(22.83)\n",
      "\n",
      "This can be implemented by substracting off the natural parameters of ˜fi from q.\n",
      "\n",
      "3. Compute the new posterior qnew(x) by solving\n",
      "\n",
      "min qnew(x)\n",
      "\n",
      "KL\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "1 Zi\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "fi(x)q−i(x)||qnew(x)\n",
      "\n",
      "(22.84)\n",
      "\n",
      "This can be done by equating the moments of qnew(x) with those of qi(x) ∝ q−i(x)fi(x). The corresponding normalization constant has the form\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "Zi =\n",
      "\n",
      "q−i(x)fi(x)dx\n",
      "\n",
      "(22.85)\n",
      "\n",
      "4. Compute the new factor (message) that was implicitly used (so it can be later removed):\n",
      "\n",
      "˜fi(x) = Zi\n",
      "\n",
      "qnew(x) q−i(x)\n",
      "\n",
      "(22.86)\n",
      "\n",
      "22.5. Expectation propagation\n",
      "\n",
      "791\n",
      "\n",
      "After convergence, we can approximate the marginal likelihood using\n",
      "\n",
      "p(D) ≈\n",
      "\n",
      "(cid:12) (cid:20)\n",
      "\n",
      "˜fi(x)dx\n",
      "\n",
      "(22.87)\n",
      "\n",
      "i\n",
      "\n",
      "We will give some examples of this below which will make things clearer.\n",
      "\n",
      "22.5.3\n",
      "\n",
      "EP for the clutter problem\n",
      "\n",
      "Let us return to considering the clutter problem. Our presentation is based on (Bishop 2006b).4 For simplicity, we will assume that the prior is a spherical Gaussian, p(x) = N (0, bI). Also, we choose to approximate the posterior by a spherical Gaussian, q(x) = N (m, vI). We set f0(x) to be the prior; this can be held ﬁxed. The factor approximations will be “Gaussian like” terms of the form\n",
      "\n",
      "˜fi(x) = siN (x|mi, viI)\n",
      "\n",
      "(22.88)\n",
      "\n",
      "Note, however, that in the EP updates, the variances may be negative! Thus these terms should be interpreted as functions, but not necessarily probability distributions. (If the variance is negative, it means the that ˜fi curves upwards instead of downwards.)\n",
      "\n",
      "First we remove ˜fi(x) from q(x) by division, which yields q−i(x) = N (m−i, v−iI), where v−1 −i = v−1 − v−1 m−i = m + v−iv−1\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "(m − mi)\n",
      "\n",
      "(22.89)\n",
      "\n",
      "(22.90)\n",
      "\n",
      "The normalization constant is given by\n",
      "\n",
      "Zi = (1 − w)N (yi|m−i, (v−i + 1)I) +w N (yi|0, aI)\n",
      "\n",
      "(22.91)\n",
      "\n",
      "Next we compute qnew(x) by computing the mean and variance of q−i(x)fi(x) as follows:\n",
      "\n",
      "m = m−i + ρi\n",
      "\n",
      "ρi = 1 −\n",
      "\n",
      "v = v−i − ρi\n",
      "\n",
      "w Zi\n",
      "\n",
      "v−i v−i + 1 v2 −i v−i + 1 N (yi|0, aI)\n",
      "\n",
      "+ ρi(1 − ρi)\n",
      "\n",
      "(yi − m−i)\n",
      "\n",
      "−i||yi − mi||2 v2 D(v−i + 1)2\n",
      "\n",
      "(22.92)\n",
      "\n",
      "(22.93)\n",
      "\n",
      "(22.94)\n",
      "\n",
      "where D is the dimensionality of x and ρi can be interpreted as the probability that yi is not clutter.\n",
      "\n",
      "Finally, we compute the new factor ˜fi whose parameters are given by v−1 i mi = m−i + (vi + v−i)v−1\n",
      "\n",
      "= v−1 − v−1 −i\n",
      "\n",
      "−i (m − m−i)\n",
      "\n",
      "(22.96)\n",
      "\n",
      "(22.95)\n",
      "\n",
      "si =\n",
      "\n",
      "Zi (2πvi)D/2N (mi|m−i, (vi + v−i)I)\n",
      "\n",
      "(22.97)\n",
      "\n",
      "4. For a handy “crib sheet”, containing many of the standard equations needed for deriving Gaussian EP algorithms, see http://research.microsoft.com/en-us/um/people/minka/papers/ep/minka-ep-quickref.pdf.\n",
      "\n",
      "792\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "At convergence, we can approximate the marginal likelihood as follows:\n",
      "\n",
      "p(D) ≈ (2πv)D/2 exp(c/2)\n",
      "\n",
      "N(cid:20)\n",
      "\n",
      "si(2πvi)−D/2\n",
      "\n",
      "(22.98)\n",
      "\n",
      "i=1\n",
      "\n",
      "c (cid:2)\n",
      "\n",
      "mT m v\n",
      "\n",
      "−\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "mT i mi vi\n",
      "\n",
      "(22.99)\n",
      "\n",
      "In (Minka 2001d), it is shown that, at least on this example, EP gives better accuracy per unit\n",
      "\n",
      "of CPU time than VB and MCMC.\n",
      "\n",
      "22.5.4\n",
      "\n",
      "LBP is a special case of EP\n",
      "\n",
      "We now show that loopy belief propagation is a special case of EP, where the base distribution contains the node marginals and the “intractable” terms correspond to the edge potentials. We If there are m nodes, the assume the model has the pairwise form shown in Equation 22.12. base distribution takes the form\n",
      "\n",
      "p(x|θ1, . . . , θm, 0) ∝\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "exp(θs(xs))\n",
      "\n",
      "(22.100)\n",
      "\n",
      "s∈V\n",
      "\n",
      "The entropy of this distribution is simply\n",
      "\n",
      "H (τ 1:m) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "H (τ s)\n",
      "\n",
      "(22.101)\n",
      "\n",
      "s\n",
      "\n",
      "If we add in the u − v edge, the Φuv augmented distribution has the form\n",
      "\n",
      "p(x|θ1:m, θuv) ∝\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "exp(θs(xs))\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "exp(θuv(xu, xv))\n",
      "\n",
      "(22.102)\n",
      "\n",
      "s∈V\n",
      "\n",
      "Since this graph is a tree, the exact entropy of this distribution is given by\n",
      "\n",
      "H (τ 1:m, ˜τ uv) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "H (τ s) − I(˜τ uv)\n",
      "\n",
      "(22.103)\n",
      "\n",
      "s\n",
      "\n",
      "where I(τ uv) =H (τ u) +H (τ v) − H (τ uv) is the mutual information. Thus the EP approxi- mation to the entropy of the full distribution is given by\n",
      "\n",
      "Hep(τ , ˜τ ) =H (τ ) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "[H (τ 1:m, ˜τ uv) − H (τ )]\n",
      "\n",
      "(22.104)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(u,v)∈E\n",
      "\n",
      "H (τ s) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "H (τ s) − I(˜τ uv) −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "H (τ s)\n",
      "\n",
      "(22.105)\n",
      "\n",
      "=\n",
      "\n",
      "s (cid:4)\n",
      "\n",
      "H (τ s) −\n",
      "\n",
      "(u,v)∈E (cid:4)\n",
      "\n",
      "s\n",
      "\n",
      "I(˜τ uv)\n",
      "\n",
      "s\n",
      "\n",
      "(22.106)\n",
      "\n",
      "s\n",
      "\n",
      "(u,v)∈E\n",
      "\n",
      "which is precisely the Bethe approximation to the entropy.\n",
      "\n",
      "22.5. Expectation propagation\n",
      "\n",
      "793\n",
      "\n",
      "We now show that the convex set that EP is optimizing over, L(φ, Φ) given by Equation 22.73, is the same as the one that LBP is optimizing over, L(G) given in Equation 22.33. First, let us consider the set M(φ). This consists of all marginal distributions (τ s, s ∈ V ), realizable by a factored distribution. This is therefore equivalent to the set of all distributions which satisfy non-negativity τs(xs) ≥ 0 and the local normalization constraint τ (xs) = 1. Now consider the set M(φ, Φuv) for a single u−v edge. This is equivalent to the marginal polytope M(Guv), where Guv is the graph with the single u − v edge added. Since this graph corresponds to a tree, this set also satisﬁes the marginalization conditions\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "τuv(xu, xv) = τu(xu),\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "τuv(xu, xv) = τv(xv)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "xs\n",
      "\n",
      "(22.107)\n",
      "\n",
      "xv\n",
      "\n",
      "xu\n",
      "\n",
      "Since L(φ, Φ) is the union of such sets, as we sweep over all edges in the graph, we recover the same set as L(G).\n",
      "\n",
      "We have shown that the Bethe approximation is equivalent to the EP approximation. We now show how the EP algorithm reduces to LBP. Associated with each intractable term i = (u, v) will be a pair of Lagrange multipliers, (λuv(xv), λvu(xu)). Recalling that θT φ(x) = [θs(xs)]s, the base distribution in Equation 22.78 has the form\n",
      "\n",
      "q(x|θ, λ) ∝\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "exp(θs(xs))\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "exp(λuv(xv) +λ vu(xu))\n",
      "\n",
      "(22.108)\n",
      "\n",
      "s\n",
      "\n",
      "⎛\n",
      "\n",
      "(u,v)∈E\n",
      "\n",
      "⎞\n",
      "\n",
      "=\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "exp\n",
      "\n",
      "⎝θs(xs) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "λts(xs)\n",
      "\n",
      "⎠\n",
      "\n",
      "(22.109)\n",
      "\n",
      "s\n",
      "\n",
      "t∈N (s)\n",
      "\n",
      "Similarly, the augmented distribution in Equation 22.79 has the form\n",
      "\n",
      "quv(x|θ, λ) ∝ q(x|θ, λ) exp (θuv(xu, xv) − λuv(xv) − λvu(xu))\n",
      "\n",
      "(22.110)\n",
      "\n",
      "We now need to update τu(xu) and τv(xv) to enforce the moment matching constraints:\n",
      "\n",
      "(Eq [xs] , Eq [xt]) = (Equv [xs] , Equv [xt])\n",
      "\n",
      "(22.111)\n",
      "\n",
      "It can be shown that this can be done by performing the usual sum-product message passing step along the u − v edge (in both directions), where the messages are given by Muv(xv) = exp(λuv(xv)), and Mvu(xu) = exp(λvu(xu)). Once we have updated q, we can derive the corresponding messages λuv and λvu.\n",
      "\n",
      "The above analysis suggests a natural extension, where we make the base distribution be a tree structure instead of a fully factored distribution. We then add in one edge at a time, absorb its effect, and approximate the resulting distribution by a new tree. This is known as tree EP (Minka and Qi 2003), and is more accurate than LBP, and sometimes faster. By considering other kinds of structured base distributions, we can derive algorothms that outperform generalization belief propagation (Welling et al. 2005).\n",
      "\n",
      "22.5.5\n",
      "\n",
      "Ranking players using TrueSkill\n",
      "\n",
      "We now present an interesting application of EP to the problem of ranking players who compete in games. Microsoft uses this method — known as TrueSkill (Herbrich et al. 2007) — to rank\n",
      "\n",
      "794\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "(cid:54)(cid:20)\n",
      "\n",
      "(cid:54)(cid:21)\n",
      "\n",
      "(cid:54)(cid:22)\n",
      "\n",
      "(cid:54)(cid:23)\n",
      "\n",
      "(cid:51)(cid:20)\n",
      "\n",
      "(cid:51)(cid:21)\n",
      "\n",
      "(cid:51)(cid:22)\n",
      "\n",
      "(cid:51)(cid:23)\n",
      "\n",
      "(cid:87)(cid:20)\n",
      "\n",
      "(cid:87)(cid:21)\n",
      "\n",
      "(cid:87)(cid:22)\n",
      "\n",
      "(cid:71)(cid:20)\n",
      "\n",
      "(cid:71)(cid:21)\n",
      "\n",
      "(cid:92)(cid:20)\n",
      "\n",
      "(cid:92)(cid:21)\n",
      "\n",
      "(a)\n",
      "\n",
      "(cid:41)(cid:20)\n",
      "\n",
      "(cid:41)(cid:21)\n",
      "\n",
      "(cid:41)(cid:22)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:54)(cid:20)\n",
      "\n",
      "(cid:54)(cid:21)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:54)(cid:22)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:75)(cid:20)\n",
      "\n",
      "(cid:75)(cid:21)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:71)(cid:20)\n",
      "\n",
      "(cid:71)(cid:21)\n",
      "\n",
      "(cid:78)(cid:20)\n",
      "\n",
      "(cid:78)(cid:21)\n",
      "\n",
      "(cid:92)(cid:20)\n",
      "\n",
      "(cid:92)(cid:21)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 22.11 (a) A DGM representing the TrueSkill model for 4 players and 3 teams, where team 1 is player 1, team 2 is players 2 and 3, and team 3 is player 4. We assume there are two games, team 1 vs team 2, and team 2 vs team 3. Nodes with double circles are deterministic. (b) A factor graph representation of the model where we assume there are 3 players (and no teams). There are 2 games, player 1 vs player 2, and player 2 vs player 3. The numbers inside circles represent steps in the message passing algorithm.\n",
      "\n",
      "22.5. Expectation propagation\n",
      "\n",
      "795\n",
      "\n",
      "players who use the Xbox 360 Live online gaming system; this system process over 105 games per day, making this one of the largest application of Bayesian statistics to date.5 The same method can also be applied to other games, such as tennis or chess.6\n",
      "\n",
      "The basic idea is shown in Figure 22.11(a). We assume each player i has a latent or true underlying skill level si ∈ R. These skill levels can evolve over time according to a simple dynamical model, p(st , γ2). In any given game, we deﬁne the performance of player i to be pi, which has the conditional distribution p(pi|si) =N (pi|si, β2). We then deﬁne the performance of a team to be the sum of the performance of its constituent players. For example, in Figure 22.11(a), we assume team 2 is composed of players 2 and 3, so we deﬁne t2 = p2 + p3. Finally, we assume that the outcome of a game depends on the difference in performance levels of the two teams. For example, in Figure 22.11(a), we assume y1 = sign(d1), where d1 = t1 − t2, and where y1 = +1 means team 1 won, and y1 = −1 means team 2 won. Thus the prior probability that team 1 wins is\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "i|st−1 i\n",
      "\n",
      ") = N (st\n",
      "\n",
      "i|st−1 i\n",
      "\n",
      "p(y1 = +1|s) =\n",
      "\n",
      "p(d1 > 0|t1, t2)p(t1|s1)p(t2|s2)dt1dt2\n",
      "\n",
      "(22.112)\n",
      "\n",
      "where t1 ∼ N (s1, β2) and t2 ∼ N (s2 + s3, β2).7\n",
      "\n",
      "To simplify the presentation of the algorithm, we will ignore the dynamical model and assume a common static factored Gaussian prior, N (μ0, σ2 0), on the skills. Also, we will assume that each team consists of 1 player, so ti = pi, and that there can be no ties. Finally, we will integrate out the performance variables pi, and assume β2 = 1, leading to a ﬁnal model of the form\n",
      "\n",
      "p(s) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "N (si|μ0, σ2)\n",
      "\n",
      "(22.113)\n",
      "\n",
      "i\n",
      "\n",
      "p(dg|s) =N (dg|sig p(yg|dg) =I\n",
      "\n",
      "(yg = sign(dg))\n",
      "\n",
      "− sjg\n",
      "\n",
      ", 1)\n",
      "\n",
      "(22.114)\n",
      "\n",
      "(22.115)\n",
      "\n",
      "where ig is the ﬁrst player of game g, and jg is the second player. This is represented in factor graph form in in Figure 22.11(b). We have 3 kinds of factors: the prior factor, fi(si) = N (si|μ0, σ2 0), the game factor, hg(sig , sjg , dg) =N (dg|sig − sjg , 1), and the outcome factor, kg(dg, yg) = I(yg = sign(dg)).\n",
      "\n",
      "Since the likelihood term (yg|dg) is not conjugate to the Gaussian priors, we will have to perform approximate inference. Thus even when the graph is a tree, we will need to iterate. (If there were an additional game, say between player 1 and player 3, then the graph would no longer be a tree.) We will represent all messages and marginal beliefs by 1d Gaussians. We will use the notation μ and v for the mean and variance (the moment parameters), and λ = 1/v and η = λμ for the precision and precision-adjusted mean (the natural parameters).\n",
      "\n",
      "5. Naive Bayes classiﬁers, which are widely used in spam ﬁlters, are often described as the most common application of Bayesian methods. However, the parameters of such models are usually ﬁt using non-Bayesian methods, such as penalized maximum likelihood. 6. Our presentation of this algorithm is based in part on lecture notes by Carl Rasmussen Joaquin Quinonero-Candela, available at http://mlg.eng.cam.ac.uk/teaching/4f13/1112/lect13.pdf. 7. Note that this is very similar to probit regression, discussed in Section 9.4, except the inputs are (the differences of) latent 1 dimensional factors. If we assume a logistic noise model instead of a Gaussian noise model, we recover the Bradley Terry model of ranking.\n",
      "\n",
      "796\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "variables si are uniform, i.e.,\n",
      "\n",
      "We initialize by assuming that at iteration 0, the initial upward messages from factors hg to\n",
      "\n",
      "m0\n",
      "\n",
      "hg→sig\n",
      "\n",
      "(sig ) = 1, λ0\n",
      "\n",
      "hg→sig\n",
      "\n",
      "= 0, η0\n",
      "\n",
      "hg→sig\n",
      "\n",
      "= 0\n",
      "\n",
      "(22.116)\n",
      "\n",
      "and similarly m0 as illustrated in Figure 22.11(b). We give the details of these steps below.\n",
      "\n",
      "hg→sjg\n",
      "\n",
      "(sjg ) = 1. The messages passing algorithm consists of 6 steps per game,\n",
      "\n",
      "1. Compute the posterior over the skills variables:\n",
      "\n",
      "qt(si) =f\n",
      "\n",
      "(si)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "λt i = λ0 +\n",
      "\n",
      "g (cid:4)\n",
      "\n",
      "g\n",
      "\n",
      "mt−1\n",
      "\n",
      "λt−1\n",
      "\n",
      "hg→si\n",
      "\n",
      "hg→si (si) = Nc(si|ηt (cid:4)\n",
      "\n",
      ", ηt\n",
      "\n",
      "i = η0 +\n",
      "\n",
      "g\n",
      "\n",
      "i , λt i)\n",
      "\n",
      "ηt−1 hg→si\n",
      "\n",
      "(22.117)\n",
      "\n",
      "(22.118)\n",
      "\n",
      "2. Compute the message from the skills variables down to the game factor hg:\n",
      "\n",
      "mt\n",
      "\n",
      "sig →hg (sig ) =\n",
      "\n",
      "qt(sig )\n",
      "\n",
      "mt\n",
      "\n",
      "hg→sig\n",
      "\n",
      "(sig )\n",
      "\n",
      ", mt\n",
      "\n",
      "sjg →hg (sjg ) =\n",
      "\n",
      "qt(sjg )\n",
      "\n",
      "mt\n",
      "\n",
      "hg→sjg\n",
      "\n",
      "(sjg )\n",
      "\n",
      "(22.119)\n",
      "\n",
      "where the division is implemented by subtracting the natural parameters as follows:\n",
      "\n",
      "sig →hg = λt λt sig\n",
      "\n",
      "− λt\n",
      "\n",
      "hg→sig\n",
      "\n",
      ", ηt\n",
      "\n",
      "sig →hg = ηt sig\n",
      "\n",
      "− ηt\n",
      "\n",
      "hg→sig\n",
      "\n",
      "(22.120)\n",
      "\n",
      "and similarly for sjg .\n",
      "\n",
      "3. Compute the message from the game factor hg down to the difference variable dg:\n",
      "\n",
      "(cid:12) (cid:12)\n",
      "\n",
      "mt\n",
      "\n",
      "hg→dg (dg) =\n",
      "\n",
      "(cid:12) (cid:12)\n",
      "\n",
      "hg(dg, sig\n",
      "\n",
      ", sjg )mt\n",
      "\n",
      "sig →hg (sig )mt\n",
      "\n",
      "sjg →hg (sjg )dsig\n",
      "\n",
      "dsjg\n",
      "\n",
      "(22.121)\n",
      "\n",
      "=\n",
      "\n",
      "N (dg|sig − sjg , 1)N (sig |μt\n",
      "\n",
      "sig →hg\n",
      "\n",
      ", vt\n",
      "\n",
      "sig →hg )\n",
      "\n",
      "(22.122)\n",
      "\n",
      "hg→dg = 1 + vt vt hg→dg = μt μt\n",
      "\n",
      "N (sjg |μt = N (dg|μt\n",
      "\n",
      "sig →hg\n",
      "\n",
      ", vt hg→dg sig →hg + vt − μt\n",
      "\n",
      "sjg →hg\n",
      "\n",
      "sjg →hg\n",
      "\n",
      ", vt\n",
      "\n",
      "sjg →hg )dsig dsjg hg→dg )\n",
      "\n",
      "sjg →hg\n",
      "\n",
      "(22.123)\n",
      "\n",
      "(22.124)\n",
      "\n",
      "(22.125)\n",
      "\n",
      "(22.126)\n",
      "\n",
      "4. Compute the posterior over the difference variables:\n",
      "\n",
      "qt(dg) ∝ mt\n",
      "\n",
      "hg→dg (dg)mkg→dg (dg)\n",
      "\n",
      "(22.127)\n",
      "\n",
      "= N (dg|μt ≈ N (dg|μt\n",
      "\n",
      "hg→dg g, vt g)\n",
      "\n",
      ", vt\n",
      "\n",
      "hg→dg )I(yg = sign(dg))\n",
      "\n",
      "(22.128)\n",
      "\n",
      "(22.129)\n",
      "\n",
      "22.5. Expectation propagation\n",
      "\n",
      "797\n",
      "\n",
      "Ψ function\n",
      "\n",
      "Λ function\n",
      "\n",
      "7\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "6\n",
      "\n",
      "0.8\n",
      "\n",
      "5\n",
      "\n",
      "0.7\n",
      "\n",
      "4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "3\n",
      "\n",
      "0.4\n",
      "\n",
      "2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "1\n",
      "\n",
      "0.1\n",
      "\n",
      "0 −6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "0 −6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 22.12 (a) Ψ function. (b) Λ function. Based on Figure 2 of (Herbrich et al. 2007). Figure generated by trueskillPlot.\n",
      "\n",
      "(Note that the upward message from the kg factor is constant.) We can ﬁnd these parameters by moment matching as follows:\n",
      "\n",
      "g = ygμt μt\n",
      "\n",
      "g = vt vt\n",
      "\n",
      "hg→dg\n",
      "\n",
      "hg→dg + σt (cid:24)\n",
      "\n",
      "1 − Λ\n",
      "\n",
      "hg→dg Ψ (cid:10)\n",
      "\n",
      "ygμt σt\n",
      "\n",
      "hg→dg\n",
      "\n",
      "hg→dg\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "ygμt σt\n",
      "\n",
      "hg→dg (cid:11)(cid:25)\n",
      "\n",
      "hg→dg\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(22.130)\n",
      "\n",
      "(22.131)\n",
      "\n",
      "Ψ(x) (cid:2)\n",
      "\n",
      "N (x|0, 1) Φ(x)\n",
      "\n",
      "(22.132)\n",
      "\n",
      "Λ(x) (cid:2) Ψ(x)(Ψ(x) +x)\n",
      "\n",
      "(22.133)\n",
      "\n",
      "(The derivation of these equations is left as a modiﬁcation to Exercise 11.15.) These functions are plotted in Figure 22.12. Let us try to understand these equations. Suppose μt hg→dg is a large positive number. That means we expect, based on the current estimate of the skills, that dg will be large and positive. Consequently, if we observe yg = +1, we will not be surprised that ig is the winner, which is reﬂected in the fact that the update factor for the mean is small, Ψ(ygμt hg→dg ) ≈ 0. Similarly, the update factor for the variance is small, Λ(ygμt hg→dg ) ≈ 0. However, if we observe yg = −1, then the update factor for the mean and variance becomes quite large.\n",
      "\n",
      "5. Compute the upward message from the difference variable to the game factor hg:\n",
      "\n",
      "mt\n",
      "\n",
      "dg→hg (dg) =\n",
      "\n",
      "qt(dg) mt dg→hg (dg) g − λt dg→hh = λt λt\n",
      "\n",
      "hg→dg\n",
      "\n",
      ", ηt\n",
      "\n",
      "dg→hh = ηt\n",
      "\n",
      "g − ηt\n",
      "\n",
      "hg→dg\n",
      "\n",
      "(22.134)\n",
      "\n",
      "(22.135)\n",
      "\n",
      "6. Compute the upward messages from the game factor to the skill variables. Let us assume\n",
      "\n",
      "798\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1.5\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "4\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "−1.5\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 22.13 (a) A DAG representing a partial ordering of players. (b) Posterior mean plus/minus 1 standard deviation for the latent skills of each player based on 26 games. Figure generated by trueskillDemo.\n",
      "\n",
      "that ig is the winner, and jg is the loser. Then we have (cid:12) (cid:12)\n",
      "\n",
      "mt\n",
      "\n",
      "hg→sig\n",
      "\n",
      "(sig ) =\n",
      "\n",
      "hg(dg, sig\n",
      "\n",
      ", sjg )mt\n",
      "\n",
      "dg→hg (dg)mt\n",
      "\n",
      "sjg →hg (sjg )ddgdsjg\n",
      "\n",
      "(22.136)\n",
      "\n",
      "vt hg→sig\n",
      "\n",
      "μt\n",
      "\n",
      "hg→sig\n",
      "\n",
      "= N (sig = 1 + vt = μt\n",
      "\n",
      "dg→hg + μt\n",
      "\n",
      "|μt hg→sig dg→hg + vt\n",
      "\n",
      "sjg →hg\n",
      "\n",
      ", vt\n",
      "\n",
      "sjg →hg\n",
      "\n",
      "hg→sig\n",
      "\n",
      ")\n",
      "\n",
      "(22.137)\n",
      "\n",
      "(22.138)\n",
      "\n",
      "(22.139)\n",
      "\n",
      "And similarly\n",
      "\n",
      "(cid:12) (cid:12)\n",
      "\n",
      "mt\n",
      "\n",
      "hg→sjg\n",
      "\n",
      "(sjg ) =\n",
      "\n",
      "hg(dg, sig\n",
      "\n",
      ", sjg )mt\n",
      "\n",
      "dg→hg (dg)mt\n",
      "\n",
      "sig →hg (sig )ddgdsig\n",
      "\n",
      "(22.140)\n",
      "\n",
      "vt hg→sjg\n",
      "\n",
      "μt\n",
      "\n",
      "hg→sjg\n",
      "\n",
      "= N (sjg = 1 + vt = μt\n",
      "\n",
      "dg→hg\n",
      "\n",
      "|μt hg→sjg dg→hg + vt − μt\n",
      "\n",
      "sig →hg\n",
      "\n",
      ", vt\n",
      "\n",
      "sig →hg\n",
      "\n",
      "hg→sjg\n",
      "\n",
      ")\n",
      "\n",
      "(22.141)\n",
      "\n",
      "(22.142)\n",
      "\n",
      "(22.143)\n",
      "\n",
      "When we compute qt+1(sig ) at the next iteration, by combining mt (sig ) with the prior factor, we will see that the posterior mean of sig goes up. Similarly, the posterior mean of sjg goes down.\n",
      "\n",
      "hg→sig\n",
      "\n",
      "It is straightforward to combine EP with ADF to perform online inference, which is necessary\n",
      "\n",
      "for most practical applications.\n",
      "\n",
      "Let us consider a simple example of this method. We create a partial ordering of 5 players as shown in Figure 22.13(a). We then sample some game outcomes from this graph, where a\n",
      "\n",
      "22.6. MAP state estimation\n",
      "\n",
      "799\n",
      "\n",
      "parent always beats a child. We pass this data into (5 iterations of) the EP algorithm and infer the posterior mean and variance for each player’s skill level. The results are shown in Figure 22.13(b). We see that the method has correctly inferred the rank ordering of the players.\n",
      "\n",
      "22.5.6\n",
      "\n",
      "Other applications of EP\n",
      "\n",
      "The TrueSkill model was developed by researchers at Microsoft. They and others have extended the model to a variety of other interesting applications, including personalized ad recommenda- tion (Stern et al. 2009), predicting click-through-rate on ads in the Bing search engine (Graepel et al. 2010), etc. They have also developed a general purpose Bayesian inference toolbox based on EP called infer.net (Minka et al. 2010).\n",
      "\n",
      "EP has also been used for a variety of other models, such as Gaussian process classiﬁcation (Nickisch and Rasmussen 2008). See http://research.microsoft.com/en-us/um/people/ minka/papers/ep/roadmap.html for a list of other EP applications.\n",
      "\n",
      "22.6 MAP state estimation\n",
      "\n",
      "In this section, we consider the problem of ﬁnding the most probable conﬁguration of variables in a discrete-state graphical model, i.e., our goal is to ﬁnd a MAP assignment of the following form:\n",
      "\n",
      "x∗ = arg max x∈X m\n",
      "\n",
      "p(x|θ) = arg max x∈X m\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i∈V\n",
      "\n",
      "θi(xi) +\n",
      "\n",
      "f ∈F\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θf (xf ) = arg max x∈X m\n",
      "\n",
      "θT φ(x)\n",
      "\n",
      "(22.144)\n",
      "\n",
      "where θi are the singleton node potentials, and θf are the factor potentials. (In this section, we follow the notation of (Sontag et al. 2011), which considers the case of general potentials, not just pairwise ones.) Note that the partition function Z(θ) plays no role in MAP estimation.\n",
      "\n",
      "If the treewidth is low, we can solve this problem with the junction tree algorithm (Sec- tion 20.4), but in general this problem is intractable. In this section, we discuss various approxi- mations, building on the material from Section 22.3.\n",
      "\n",
      "22.6.1\n",
      "\n",
      "Linear programming relaxation\n",
      "\n",
      "We can rewrite the objective in terms of the variational parameters as follows:\n",
      "\n",
      "arg max x∈X m\n",
      "\n",
      "θT φ(x) = arg max μ∈M(G)\n",
      "\n",
      "θT μ\n",
      "\n",
      "(22.145)\n",
      "\n",
      "where φ(x) = [{I(xs = j)}, {I(xf = k)}) and μ is a probability vector in the marginal polytope. To see why this equation is true, note that we can just set μ to be a degenerate distribution with μ(xs) = I(xs = x∗ s is the optimal assigment of node s. So instead of optimizing over discrete assignments, we now optimize over probability distributions μ.\n",
      "\n",
      "s), where x∗\n",
      "\n",
      "It seems like we have an easy problem to solve, since the objective in Equation 22.145 is linear in μ, and the constraint set M(G) is convex. The trouble is, M(G) in general has a number of facets that is exponential in the number of nodes.\n",
      "\n",
      "In this case, instead of requiring probability vector μ to live in the marginal polytope M(G), we allow it to\n",
      "\n",
      "A standard strategy in combinatorial optimization is to relax the constraints.\n",
      "\n",
      "800\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "live inside a convex outer bound L(G). Having deﬁned this relaxed constraint set, we have\n",
      "\n",
      "max x∈X m\n",
      "\n",
      "θT φ(x) = max μ∈M(G)\n",
      "\n",
      "θT μ ≤ max τ ∈L(G)\n",
      "\n",
      "θT τ\n",
      "\n",
      "(22.146)\n",
      "\n",
      "If the solution is integral, it is exact; if it is fractional, it is an approximation. This is called a (ﬁrst order) linear programming relaxtion. The reason it is called ﬁrst-order is that the constraints that are enforced are those that correspond to consistency on a tree, which is a graph of treewidth 1. It is possible to enforce higher-order consistency, using graphs with larger treewidth (see (Wainwright and Jordan 2008b, sec 8.5) for details).\n",
      "\n",
      "How should we actually perform the optimization? We can use a generic linear programming package, but this is often very slow. Fortunately, in the case of graphical models, it is possible to devise specialised distributed message passing algorithms for solving this optimization problem, as we explain below.\n",
      "\n",
      "22.6.2 Max-product belief propagation\n",
      "\n",
      "The MAP objective in Equation 22.145, maxμ∈M(G) θT μ, is almost identical to the inference objective in Equation 22.23, maxμ∈M(G) θT μ + H (μ), apart from the entropy term. One heuristic way to proceed would be to consider the zero temperature limit of the probability distribution μ, where the probability distribution has all its mass centered on its mode (see Section 4.2.2). In such a setting, the entropy term becomes zero. We can then modify the message passing methods used to solve the inference problem so that they solve the MAP estimation problem instead. in the zero temperature limit, the sum operator becomes the max operator, which results in a method called max-product belief propagation.\n",
      "\n",
      "In particular,\n",
      "\n",
      "In more detail, let\n",
      "\n",
      "A(θ) (cid:2) max μ∈M(G)\n",
      "\n",
      "θT μ + H (μ)\n",
      "\n",
      "(22.147)\n",
      "\n",
      "Now consider an inverse temperature β going to inﬁnity. We have\n",
      "\n",
      "lim β→+∞\n",
      "\n",
      "A(βθ) β\n",
      "\n",
      "=\n",
      "\n",
      "= max\n",
      "\n",
      "lim β→+∞\n",
      "\n",
      "μ∈M(G)\n",
      "\n",
      "1 β (cid:19)\n",
      "\n",
      "max μ∈M(G)\n",
      "\n",
      "θT μ + lim β→+∞\n",
      "\n",
      "(\n",
      "\n",
      "(βθ)T μ + H (μ)\n",
      "\n",
      "1 β\n",
      "\n",
      "H (μ)\n",
      "\n",
      "%\n",
      "\n",
      ")\n",
      "\n",
      "(22.148)\n",
      "\n",
      "(22.149)\n",
      "\n",
      "= max\n",
      "\n",
      "μ∈M(G)\n",
      "\n",
      "θT μ\n",
      "\n",
      "(22.150)\n",
      "\n",
      "It is the concavity of the objective function that allows us to interchange the lim and max operators (see (Wainwright and Jordan 2008b, p274) for details).\n",
      "\n",
      "Now consider the Bethe approximation, which has the form maxτ ∈L(G) θT τ + HBethe(τ ). We showed that loopy BP ﬁnds a local optimum of this objective. In the zero temperature limit, this objective is equivalent to the LP relaxation of the MAP problem. Unfortunately, max-product loopy BP does not solve this LP relaxation unless the graph is a tree (Wainwright and Jordan 2008b, p211). The reason is that Bethe energy functional is not concave (except on trees), so we are not licensed to swap the limit and max operators in the above zero-temperature derivation. However, if we use tree-reweighted BP, or TRBP/ TRW, we have a concave objective. In this case,\n",
      "\n",
      "22.6. MAP state estimation\n",
      "\n",
      "801\n",
      "\n",
      "one can show (Kolmogorov and Wainwright 2005) that the max-product version of TRBP does solve the above LP relaxation.\n",
      "\n",
      "A certain scheduling of this algorithm, known as sequential TRBP, TRBP-S, orTRW-S, can be shown to always converge (Kolmogorov 2006), and furthermore, it typically does so faster than the standard parallel updates. The idea is to pick an arbitrary node ordering X1, . . . , XN . We then consider a set of trees which is a subsequence of this ordering. At each iteration, we perform max-product BP from X1 towards XN and back along one of these trees. It can be shown that this monotonically minimizes a lower bound on the energy, and thus is guaranteed to converge to the global optimum of the LP relaxation.\n",
      "\n",
      "22.6.3\n",
      "\n",
      "Graphcuts\n",
      "\n",
      "In this section, we show how to ﬁnd MAP state estimates, or equivalently, minimum energy conﬁgurations, by using the max ﬂow/min cut algorithm for graphs.8 This class of methods is known as graphcuts and is very widely used, especially in computer vision applications.\n",
      "\n",
      "We will start by considering the case of MRFs with binary nodes and a restricted class of potentials; in this case, graphcuts will ﬁnd the exact global optimum. We then consider the case of multiple states per node, which are assumed to have some underlying ordering; we can approximately solve this case by solving a series of binary subproblems, as we will see.\n",
      "\n",
      "22.6.3.1\n",
      "\n",
      "Graphcuts for the generalized Ising model\n",
      "\n",
      "Let us start by considering a binary MRF where the edge energies have the following form:\n",
      "\n",
      "Euv(xu, xv) =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "0 λst\n",
      "\n",
      "if xu = xv if xu (cid:8)= xv\n",
      "\n",
      "(22.151)\n",
      "\n",
      "where λst ≥ 0 is the edge cost. This encourages neighboring nodes to have the same value (since we are trying to minimize energy). Since we are free to add any constant we like to the overall energy without affecting the MAP state estimate, let us rescale the local energy terms such that either Eu(1) = 0 or Eu(0) = 0.\n",
      "\n",
      "Now let us construct a graph which has the same set of nodes as the MRF, plus two distin- guished nodes: the source s and the sinkt. If Eu(1) = 0, we add the edge xu → t with cost (This ensures that if u is not in partition Xt, meaning u is assigned to state 0, we will Eu(0). pay a cost of Eu(0) in the cut.) Similarly, If Eu(0) = 0, we add the edge xu → s with cost Eu(1). Finally, for every pair of variables that are connected in the MRF, we add edges xu → xv and xv → xu, both with cost λu,v ≥ 0. Figure 22.14 illustrates this construction for an MRF with 4 nodes, and with the following non-zero energy values:\n",
      "\n",
      "E1(0) = 7, E2(1) = 2, E3(1) = 1, E4(1) = 6\n",
      "\n",
      "(22.152)\n",
      "\n",
      "λ1,2 = 6, λ2,3 = 6, λ3,4 = 2, λ1,4 = 1\n",
      "\n",
      "(22.153)\n",
      "\n",
      "Having constructed the graph, we compute a minimal s − t cut. This is a partition of the nodes into two sets, Xs, which are nodes connected to s, and Xt, which are nodes connected to t. We\n",
      "\n",
      "8. There are a variety of ways to implement this algorithm, see e.g., O(EV log V ) or O(V 3) time, where E is the number of edges and V is the number of nodes.\n",
      "\n",
      "(Sedgewick and Wayne 2011). The best take\n",
      "\n",
      "802\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "t\n",
      "\n",
      "7\n",
      "\n",
      "z1\n",
      "\n",
      "6\n",
      "\n",
      "z2\n",
      "\n",
      "1\n",
      "\n",
      "6\n",
      "\n",
      "2\n",
      "\n",
      "z4\n",
      "\n",
      "2\n",
      "\n",
      "z3\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      "s\n",
      "\n",
      "Figure 22.14 Illustration of graphcuts applied to an MRF with 4 nodes. Dashed lines are ones which contribute to the cost of the cut (for bidirected edges, we only count one of the costs). Here the min cut has cost 6. Source: Figure 13.5 from (Koller and Friedman 2009). Used with kind permission of Daphne Koller.\n",
      "\n",
      "pick the partition which minimizes the sum of the cost of the edges between nodes on different sides of the partition:\n",
      "\n",
      "cost(Xs, Xt) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "cost(xu, sv)\n",
      "\n",
      "(22.154)\n",
      "\n",
      "xu∈Xs,xv∈Xt\n",
      "\n",
      "In Figure 22.14, we see that the min-cut has cost 6.\n",
      "\n",
      "Minimizing the cost in this graph is equivalent to minimizing the energy in the MRF. Hence nodes that are assigned to s have an optimal state of 0, and the nodes that are assigned to t have an optimal state of 1. In Figure 22.14, we see that the optimal MAP estimate is (1, 1, 1, 0).\n",
      "\n",
      "22.6.3.2\n",
      "\n",
      "Graphcuts for binary MRFs with submodular potentials\n",
      "\n",
      "We now discuss how to extend the graphcuts construction to binary MRFs with more general kinds of potential functions. In particular, suppose each pairwise energy satisﬁes the following condition:\n",
      "\n",
      "Euv(1, 1) + Euv(0, 0) ≤ Euv(1, 0) + Euv(0, 1)\n",
      "\n",
      "(22.155)\n",
      "\n",
      "In other words, the sum of the diagonal energies is less than the sum of the off-diagonal energies. In this case, we say the energies are submodular (Kolmogorov and Zabin 2004).9 An example of a submodular energy is an Ising model where λuv > 0. This is also known as an attractive MRF or associative MRF, since the model “wants” neighboring states to be the same.\n",
      "\n",
      "9. Submodularity is the discrete analog of convexity. Intuitively, it corresponds to the “law of diminishing returns”, that is, the extra value of adding one more element to a set is reduced if the set is already large. More formally, we say that f : 2S → R is submodular if for any A ⊂ B ⊂ S and x ∈ S, we havef (A ∪ {x}) − f (A) ≥ f (B ∪ {x}) − f (B). If −f is submodular, then f is supermodular.\n",
      "\n",
      "22.6. MAP state estimation\n",
      "\n",
      "803\n",
      "\n",
      "To apply graphcuts to a binary MRF with submodular potentials, we construct the pairwise\n",
      "\n",
      "edge weights as follows:\n",
      "\n",
      "E(cid:2)\n",
      "\n",
      "u,v(0, 1) = Eu,v(1, 0) + Eu,v(0, 1) − Eu,v(0, 0) − Eu,v(1, 1)\n",
      "\n",
      "(22.156)\n",
      "\n",
      "This is guaranteed to be non-negative by virtue of the submodularity assumption. In addition, we construct new local edge weights as follows: ﬁrst we initialize E(cid:2)(u) = E(u), and then for each edge pair (u, v), we update these values as follows:\n",
      "\n",
      "E(cid:2) E(cid:2)\n",
      "\n",
      "u(1) = E(cid:2) v(1) = E(cid:2)\n",
      "\n",
      "u(1) + (Eu,v(1, 0) − Eu,v(0, 0)) u(1) + (Eu,v(1, 1) − Eu,v(1, 0))\n",
      "\n",
      "(22.157)\n",
      "\n",
      "(22.158)\n",
      "\n",
      "add the edge u → s with cost E(cid:2) E(cid:2) xu − xv with cost E(cid:2)\n",
      "\n",
      "u(0) − E(cid:2)\n",
      "\n",
      "We now construct a graph in a similar way to before. Speciﬁcally, if E(cid:2) u(1) − E(cid:2)\n",
      "\n",
      "u(1). Finally for every MRF edge for which E(cid:2)\n",
      "\n",
      "u,v(0, 1). (We don’t need to add the edge in both directions.)\n",
      "\n",
      "u(0), we u(0), otherwise we add the edge u → t with cost u,v(0, 1) > 0, we add a graphcuts edge\n",
      "\n",
      "u(1) > E(cid:2)\n",
      "\n",
      "One can show (Exercise 22.1) that the min cut in this graph is the same as the minimum energy conﬁguration. Thus we can use max ﬂow/min cut to ﬁnd the globally optimal MAP estimate (Greig et al. 1989).\n",
      "\n",
      "22.6.3.3\n",
      "\n",
      "Graphcuts for nonbinary metric MRFs\n",
      "\n",
      "We now discuss how to use graphcuts for approximate MAP estimation in MRFs where each node can have multiple states (Boykov et al. 2001). However, we require that the pairwise energies form a metric. We call such a model a metric MRF. For example, suppose the states have a natural ordering, as commonly arises if they are a discretization of an underlying continuous In this case, we can deﬁne a metric of the form E(xs, xt) = min(δ, ||xs − xt||) or a space. semi-metric of the form E(xs, xt) = min(δ, (xs − xt)2), for some constant δ > 0. This energy encourages neighbors to have similar labels, but never “punishes” them by more than δ. This δ term prevents over-smoothing, which we illustrate in Figure 19.20.\n",
      "\n",
      "One version of graphcuts is the alpha expansion. At each step, it picks one of the available labels or states and calls it α; then it solves a binary subproblem where each variable can choose to remain in its current state, or to become state α (see Figure 22.15(d) for an illustration). More precisely, we deﬁne a new MRF on binary nodes, and we deﬁne the energies of this new model, relative to the current assignment x, as follows:\n",
      "\n",
      "E(cid:2)\n",
      "\n",
      "E(cid:2) u,v(0, 1) = Eu,v(xu, α), E(cid:2)\n",
      "\n",
      "u(0) = Eu(xu), E(cid:2)\n",
      "\n",
      "u,v(1, 0) = Eu,v(α, xv), E(cid:2)\n",
      "\n",
      "u(1) = Eu(α), E(cid:2)\n",
      "\n",
      "u,v(0, 0) = Eu,v(xu, xv) u,v(1, 1) = Eu,v(α, α)\n",
      "\n",
      "(22.159)\n",
      "\n",
      "(22.160)\n",
      "\n",
      "To optimize E(cid:2) using graph cuts (and thus ﬁgure out the optimal alpha expansion move), we require that the energies be submodular. Plugging in the deﬁnition we get the following constraint:\n",
      "\n",
      "Eu,v(xu, xv) +E u,v(α, α) ≤ Eu,v(xu, α) +E u,v(α, xv)\n",
      "\n",
      "(22.161)\n",
      "\n",
      "For any distance function, Eu,v(α, α) = 0, and the remaining inequality follows from the triangle inequality. Thus we can apply the alpha expansion move to any metric MRF.\n",
      "\n",
      "804\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "(a) initial labeling\n",
      "\n",
      "(b) standard move\n",
      "\n",
      "(c) α-β-swap\n",
      "\n",
      "(d) α-expansion\n",
      "\n",
      "Figure 22.15 just ﬂips the label of one pixel. be relabeled as β if this decreases the energy. labeled as α to be relabeled as α if this decreases the energy. Used with kind permission of Ramin Zabih.\n",
      "\n",
      "(a) An image with 3 labels.\n",
      "\n",
      "(b) A standard local move (e.g., by iterative conditional modes) (c) An α − β swap allows all nodes that are currently labeled as α to (d) An α expansion allows all nodes that are not currently Source: Figure 2 of (Boykov et al. 2001).\n",
      "\n",
      "At each step of alpha expansion, we ﬁnd the optimal move from amongst an exponentially large set; thus we reach a strong local optimum, of much lower energy than the local optima found by standard greedy label ﬂipping methods such as iterative conditional modes. In fact, one can show that, once the algorithm has converged, the energy of the resulting solution is at most 2c times the optimal energy, where\n",
      "\n",
      "c = max (u,v)∈E\n",
      "\n",
      "maxα(cid:5)=β Euv(α, β) minα(cid:5)=β Euv(α, β)\n",
      "\n",
      "(22.162)\n",
      "\n",
      "See Exercise 22.3 for the proof. approximation.\n",
      "\n",
      "In the case of the Potts model, c = 1, so we have a 2-\n",
      "\n",
      "Another version of graphcuts is the alpha-beta swap. At each step, two labels are chosen, call them α and β. All the nodes currently labeled α can change to β (and vice versa) if this reduces the energy (see Figure 22.15(c) for an illustration). The resulting binary subproblem can be solved exactly, even if the energies are only semi-metric (that is, the triangle inequality need not hold; see Exercise 22.2). Although the α − β swap version can be applied to a broader class of models than the α-expansion version, it is theoretically not as powerful. Indeed, in various low-level vision problems, (Szeliski et al. 2008) show empirically that the expansion version is usually better than the swap version (see Section 22.6.4).\n",
      "\n",
      "22.6.4\n",
      "\n",
      "Experimental comparison of graphcuts and BP\n",
      "\n",
      "In Section 19.6.2.7, we described lattice-structured CRFs for various low-level vision problems. (Szeliski et al. 2008) performed an extensive comparison of different approximate optimization techniques for this class of problems. Some of the results, for the problem of stereo depth estimation, are shown in Figure 22.16. We see that the graphcut and tree-reweighted max- product BP (TRW) give the best results, with regular max-product BP being much worse. In terms of speed, graphcuts is the fastest, with TRW a close second. Other algorithms, such as ICM, simulated annealing or a standard domain-speciﬁc heuristic known as normalize correlation, are\n",
      "\n",
      "22.6. MAP state estimation\n",
      "\n",
      "805\n",
      "\n",
      "2\n",
      "\n",
      "× 106\n",
      "\n",
      "4.2\n",
      "\n",
      "× 105\n",
      "\n",
      "y g r e n E\n",
      "\n",
      "1.9\n",
      "\n",
      "1.8\n",
      "\n",
      "1.7\n",
      "\n",
      "1.6\n",
      "\n",
      "Max-Product BP a-Expansion a-b Swap TRW\n",
      "\n",
      "y g r e n E\n",
      "\n",
      "4.1\n",
      "\n",
      "4\n",
      "\n",
      "3.9\n",
      "\n",
      "Max-Product BP a-Expansion a-b Swap TRW\n",
      "\n",
      "1.5\n",
      "\n",
      "3.8\n",
      "\n",
      "1.4\n",
      "\n",
      "3.7\n",
      "\n",
      "1.3\n",
      "\n",
      "100\n",
      "\n",
      "101 Running Time (s)\n",
      "\n",
      "102\n",
      "\n",
      "103\n",
      "\n",
      "3.6\n",
      "\n",
      "100\n",
      "\n",
      "101 Running Time (s)\n",
      "\n",
      "102\n",
      "\n",
      "Figure 22.16 Energy minimization on a CRF for stereo depth estimation. Top row: two input images along with the ground truth depth values. Bottom row: energy vs time for 4 different optimization algorithms. Bottom left: results are for the Teddy image (shown in top row). Bottom right: results are for the Tsukuba image (shown in Figure 22.17(a)). Source: Figure 13.B.1 of (Koller and Friedman 2009). Used with kind permission of Daphne Koller.\n",
      "\n",
      "even worse, as shown qualitatively in Figure 22.17.\n",
      "\n",
      "Since TRW is optimizing the dual of the relaxed LP problem, we can use its value at conver- gence to evaluate the optimal energy. It turns out that for many of the images in the stereo benchmark dataset, the ground truth has higher energy (lower probability) than the globally op- timal estimate (Meltzer et al. 2005). This indicates that we are optimizing the wrong model. This is not surprising, since the pairwise CRF ignores known long-range constraints. Unfortunately, if we add these constraints to the model, the graph either becomes too dense (making BP slow), and/or the potentials become non-submodular (making graphcuts inapplicable).\n",
      "\n",
      "One way around this is to generate a diverse set of local modes, using repeated applications of graph cuts, as described in (Yadollahpour et al. 2011). We can then apply a more sophisticated model, which uses global features, to rerank the solutions.\n",
      "\n",
      "806\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "(a) Left image: 384x288, 15 labels\n",
      "\n",
      "(b) Ground truth\n",
      "\n",
      "(c) Swap algorithm\n",
      "\n",
      "(d) Expansion algorithm\n",
      "\n",
      "(e) Normalized correlation\n",
      "\n",
      "(f) Simulated annealing\n",
      "\n",
      "Figure 22.17 An example of stereo depth estimation using an MRF. (a) Left image, of size 384 × 288 pixels, from the University of Tsukuba. (b) (c-f): MAP estimates using different methods: (c) α − β Ground truth depth map, quantized to 15 levels. swap, (d) α expansion, (e) normalized cross correlation, (f) simulated annealing. Source: Figure 10 of (Boykov et al. 2001). Used with kind permission of Ramin Zabih.\n",
      "\n",
      "(The corresponding right image is similar, but not shown.)\n",
      "\n",
      "22.6.5\n",
      "\n",
      "Dual decomposition\n",
      "\n",
      "We are interested in computing (cid:4)\n",
      "\n",
      "p∗ = max x∈X m\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i∈V\n",
      "\n",
      "θi(xi) +\n",
      "\n",
      "f ∈F\n",
      "\n",
      "θf (xf )\n",
      "\n",
      "(22.163)\n",
      "\n",
      "where F represents a set of factors. We will assume that we can tractably optimize each local factor, but the combination of all of these factors makes the problem intractable. One way to proceed is to optimize each term independently, but then to introduce constraints that force all the local estimates of the variables’ values to agree with each other. We explain this in more detail below, following the presentation of (Sontag et al. 2011).\n",
      "\n",
      "22.6. MAP state estimation\n",
      "\n",
      "807\n",
      "\n",
      "x1\n",
      "\n",
      "θf(x1, x2)\n",
      "\n",
      "x2\n",
      "\n",
      "θg(x1, x3)\n",
      "\n",
      "θh(x2, x4)\n",
      "\n",
      "x3\n",
      "\n",
      "θk(x3, x4)\n",
      "\n",
      "x4\n",
      "\n",
      "xg 1\n",
      "\n",
      "θg(xg\n",
      "\n",
      "1, xg 3) xg 3\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "xf 1\n",
      "\n",
      "θf(xf\n",
      "\n",
      "1, xf 2)\n",
      "\n",
      "xf 2\n",
      "\n",
      "=\n",
      "\n",
      "x1\n",
      "\n",
      "= x2\n",
      "\n",
      "x3\n",
      "\n",
      "x4\n",
      "\n",
      "= xk 3\n",
      "\n",
      "θk(xk\n",
      "\n",
      "= xk 4 3, xk 4)\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "xh 2 θh(xh\n",
      "\n",
      "2, xh 4)\n",
      "\n",
      "xh 4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 22.18 (a) A pairwise MRF with 4 different edge factors. (b) We have 4 separate variables, plus a copy of each variable for each factor it participates in. Source: Figure 1.2-1.3 of (Sontag et al. 2011). Used with kind permission of David Sontag.\n",
      "\n",
      "22.6.5.1\n",
      "\n",
      "Basic idea\n",
      "\n",
      "Let us duplicate the variables xi, once for each factor, and then force them to be equal. Speciﬁcally, let xf i }i∈f be the set of variables used by factor f . This construction is illustrated in Figure 22.18. We can reformulate the objective as follows:\n",
      "\n",
      "p∗ = max x,xf\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i∈V\n",
      "\n",
      "f = {xf\n",
      "\n",
      "θi(xi) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "f ∈F\n",
      "\n",
      "θf (xf f )\n",
      "\n",
      "s.t. xf\n",
      "\n",
      "i = xi ∀f, i ∈ f\n",
      "\n",
      "(22.164)\n",
      "\n",
      "Let us now introduce Lagrange multipliers, or dual variables, δf i(k), to enforce these constraints. The Lagrangian becomes (cid:4)\n",
      "\n",
      "L(δ, x, xf ) =\n",
      "\n",
      "θi(xi) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θf (xf f )\n",
      "\n",
      "(22.165)\n",
      "\n",
      "i∈V\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "f ∈F\n",
      "\n",
      "δf i(ˆxi)\n",
      "\n",
      "!\n",
      "\n",
      "I(xi = ˆxi) − I(xf\n",
      "\n",
      "i = ˆxi)\n",
      "\n",
      "#\n",
      "\n",
      "(22.166)\n",
      "\n",
      "f ∈F\n",
      "\n",
      "i∈f\n",
      "\n",
      "ˆxi\n",
      "\n",
      "This is equivalent to our original problem in the following sense: for any value of δ, we have\n",
      "\n",
      "p∗ = max x,xf\n",
      "\n",
      "L(δ, x, xf )\n",
      "\n",
      "s.t. xf\n",
      "\n",
      "i = xi ∀f, i ∈ f\n",
      "\n",
      "(22.167)\n",
      "\n",
      "since if the constraints hold, the last term is zero. We can get an upper bound by dropping the consistency constraints, and just optimizing the following upper bound:\n",
      "\n",
      "L(δ) (cid:2) max x,xf\n",
      "\n",
      "L(δ, x, xf ) ⎛\n",
      "\n",
      "⎞\n",
      "\n",
      "⎛\n",
      "\n",
      "(22.168)\n",
      "\n",
      "⎞\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "max xi\n",
      "\n",
      "⎝θi(xi) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "f :i∈f\n",
      "\n",
      "δf i(xi)\n",
      "\n",
      "⎠ +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "f\n",
      "\n",
      "max xf\n",
      "\n",
      "⎝θf (xf ) −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i∈f\n",
      "\n",
      "δf i(xi)\n",
      "\n",
      "⎠(22.169)\n",
      "\n",
      "See Figure 22.19 for an illustration.\n",
      "\n",
      "808\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "θf(x1, x2) x1\n",
      "\n",
      "−\n",
      "\n",
      "δf 1(x1)\n",
      "\n",
      "− x2\n",
      "\n",
      "δf 2(x2)\n",
      "\n",
      "θg(x1, x3) − δg3(x3) − δg1(x1)\n",
      "\n",
      "x1\n",
      "\n",
      "x3\n",
      "\n",
      "δf 1(x1) + δg1(x1)\n",
      "\n",
      "δg3(x3) + δk3(x3)\n",
      "\n",
      "x1\n",
      "\n",
      "x3\n",
      "\n",
      "x2\n",
      "\n",
      "x4\n",
      "\n",
      "δf 2(x2) + δh2(x2)\n",
      "\n",
      "δk4(x4) + δh4(x4)\n",
      "\n",
      "x2\n",
      "\n",
      "x4\n",
      "\n",
      "θh(x2, x4) − δh2(x2) − δh4(x4)\n",
      "\n",
      "x3 θk(x3, x4)\n",
      "\n",
      "−\n",
      "\n",
      "δk3(x3)\n",
      "\n",
      "x4 − δk4(x4)\n",
      "\n",
      "Figure 22.19 Illustration of dual decomposition. kind permission of David Sontag.\n",
      "\n",
      "Source: Figure 1.2 of (Sontag et al. 2011). Used with\n",
      "\n",
      "This objective is tractable to optimize, since each xf term is decoupled. Furthermore, we see that L(δ) ≥ p∗, since by relaxing the consistency constraints, we are optimizing over a larger space. Furthermore, we have the property that\n",
      "\n",
      "min δ\n",
      "\n",
      "L(δ) = p∗\n",
      "\n",
      "(22.170)\n",
      "\n",
      "so the upper bound is tight at the optimal value of δ, which enforces the original constraints.\n",
      "\n",
      "Minimizing this upper bound is known as dual decomposition or Lagrangian relaxation (Komodakis et al. 2011; Sontag et al. 2011; Rush and Collins 2012). Furthemore, it can be shown that L(δ) is the dual to the same LP relaxation we saw before. We will discuss several possible optimization algorithms below.\n",
      "\n",
      "The main advantage of dual decomposition from a practical point of view is that it allows one to mix and match different kinds of optimization algorithms in a convenient way. For example, we can combine a grid structured graph with local submodular factors to perform image segmentation, together with a tree structured model to perform pose estimation (see Exercise 22.4). Analogous methods can be used in natural language processing, where we often have a mix of local and global constraints (see e.g., (Koo et al. 2010; Rush and Collins 2012)).\n",
      "\n",
      "22.6.5.2\n",
      "\n",
      "Theoretical guarantees\n",
      "\n",
      "What can we say about the quality of the solutions obtained in this way? To understand this, let us ﬁrst introduce some more notation:\n",
      "\n",
      "θδ i (xi) (cid:2) θi(xi) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "δf i(xi)\n",
      "\n",
      "(22.171)\n",
      "\n",
      "θδ f (xf ) (cid:2) θf (xf ) −\n",
      "\n",
      "f :i∈f (cid:4)\n",
      "\n",
      "δf i(xi)\n",
      "\n",
      "(22.172)\n",
      "\n",
      "i∈f\n",
      "\n",
      "22.6. MAP state estimation\n",
      "\n",
      "809\n",
      "\n",
      "This represents a reparameterization of the original problem, in the sense that\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θi(xi) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θf (xf ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θδ i (xi) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θδ f (xf )\n",
      "\n",
      "(22.173)\n",
      "\n",
      "i\n",
      "\n",
      "f\n",
      "\n",
      "i\n",
      "\n",
      "f\n",
      "\n",
      "and hence\n",
      "\n",
      "L(δ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "max xi\n",
      "\n",
      "θδ i (xi) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "f\n",
      "\n",
      "max xf\n",
      "\n",
      "θδ f (xf )\n",
      "\n",
      "(22.174)\n",
      "\n",
      "and an assignment x∗ such that the maxi- mizing assignments to the singleton terms agrees with the assignments to the factor terms, i.e., so that x∗\n",
      "\n",
      "Now suppose there is a set of dual variables δ∗\n",
      "\n",
      "L(δ∗) =\n",
      "\n",
      "i ∈ argmaxxi (cid:4)\n",
      "\n",
      "θδ∗ i (x∗\n",
      "\n",
      "i ) +\n",
      "\n",
      "θδ∗ i (xi) and x∗ θδ∗ f (x∗\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "f ) =\n",
      "\n",
      "f ∈ argmaxxf (cid:4)\n",
      "\n",
      "θi(x∗\n",
      "\n",
      "i ) +\n",
      "\n",
      "θδ∗ f (xf ). In this case, we have\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θf (x∗ f )\n",
      "\n",
      "(22.175)\n",
      "\n",
      "i\n",
      "\n",
      "f\n",
      "\n",
      "i\n",
      "\n",
      "f\n",
      "\n",
      "Now since (cid:4)\n",
      "\n",
      "θi(x∗\n",
      "\n",
      "i ) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "θf (x∗\n",
      "\n",
      "f ) ≤ p∗ ≤ L(δ∗)\n",
      "\n",
      "(22.176)\n",
      "\n",
      "i\n",
      "\n",
      "f\n",
      "\n",
      "we conclude that L(δ∗) = p∗, sox ∗ is the MAP assignment.\n",
      "\n",
      "So if we can ﬁnd a solution where all the subproblems agree, we can be assured that it is the\n",
      "\n",
      "global optimum. This happens surprisingly often in practical problems.\n",
      "\n",
      "22.6.5.3\n",
      "\n",
      "Subgradient descent L(δ) is a convex and continuous objective, but it is non-differentiable at points δ where θδ or θδ the elements of δ at the same time, as follows: f i(xi) − αtgt\n",
      "\n",
      "δt+1 f i (xi) = δt\n",
      "\n",
      "i (xi) f (xf ) have multiple optima. One approach is to use subgradient descent. This updates all\n",
      "\n",
      "f i(xi)\n",
      "\n",
      "(22.177)\n",
      "\n",
      "where gt the subgradient of L(δ) at δt tion 8.5.2.1), this method is guaranteed to converge to a global optimum of the dual. (Komodakis et al. 2011) for details.)\n",
      "\n",
      "argmaxxi if xf i (cid:8)= xs +1 and gf i(xf i ), bringing them closer to agreement. Similarly, the subgradient update will decrease the value of θδt f (xf f (xs To compute the gradient, we need to be able to solve subproblems of the following form:\n",
      "\n",
      "One can show that the gradient is given by the following sparse vector. First let xs\n",
      "\n",
      "i , xf \\i) and increasing the value of θδt\n",
      "\n",
      "θδt i (xi) and xf i (so factor f disagrees with the local term on how to set variable i), we set gf i(xs i ) and increasing θδt\n",
      "\n",
      "i ) =−1. This has the effect of decreasing θδt\n",
      "\n",
      "f ∈ argmaxxf\n",
      "\n",
      "⎡\n",
      "\n",
      "i ∈ θδt f (xf ). Next let gf i(xi) = 0 for all elements. Finally, i ) =\n",
      "\n",
      ".\n",
      "\n",
      "If the step sizes αt are set appropriately (see Sec- (See\n",
      "\n",
      "i , xf \\i).\n",
      "\n",
      "⎤\n",
      "\n",
      "i (xs\n",
      "\n",
      "i (xf\n",
      "\n",
      "argmax xf\n",
      "\n",
      "θδt f (xf ) = argmax\n",
      "\n",
      "xf\n",
      "\n",
      "⎣θf (xf ) −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i∈f\n",
      "\n",
      "δt f i(xi)\n",
      "\n",
      "⎦\n",
      "\n",
      "(22.178)\n",
      "\n",
      "810\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "(In (Komodakis et al. 2011), these subproblems are called slaves, whereas L(δ) is called the master.) Obviously if the scope of factor f is small, this is simple. For example, if each factor is pairwise, and each variable has K states, the cost is just K 2. However, there are some kinds of global factors that also support exact and efficient maximization, including the following:\n",
      "\n",
      "Graphical models with low tree width.\n",
      "\n",
      "Factors that correspond to bipartite graph matchings (see e.g.,\n",
      "\n",
      "(Duchi et al. 2007)). This is useful for data association problems, where we must match up a sensor reading with an unknown source. We can ﬁnd the maximal matching using the so-called Hungarian algorithm in O(|f |3) time (see e.g., (Padadimitriou and Steiglitz 1982)).\n",
      "\n",
      "Supermodular functions. We discuss this case in more detail in Section 22.6.3.2.\n",
      "\n",
      "Cardinality constraints. For example, we might have a factor over a large set of binary variables that enforces that a certain number of bits are turned on; this can be useful in problems such as image segmentation. In particular, suppose θf (xf ) = 0 if i∈f xi = L and θf (xf ) = −∞ otherwise. We can ﬁnd the maximizing assignment in O(|f | log |f |) time as follows: ﬁrst deﬁne ei = δf i(1) − δf i(0); now sort the ei; ﬁnally set xi = 1 for the ﬁrst L values, and xi = 0 for the rest (Tarlow et al. 2010).\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Factors which are constant for all but a small set S of distinguished values of xf . Then we\n",
      "\n",
      "can optimize over the factor in O(|S|) time (Rother et al. 2009).\n",
      "\n",
      "22.6.5.4\n",
      "\n",
      "Coordinate descent\n",
      "\n",
      "An alternative to updating the entire δ vector at once (albeit sparsely) is to update it using block coordinate descent. By choosing the size of the blocks, we can trade off convergence speed with ease of the local optimization problem.\n",
      "\n",
      "One approach, which optimizes δf i(xi) for all i ∈ f and all xi at the same time (for a ﬁxed factor f ), is known as max product linear programming (Globerson and Jaakkola 2008). Algorithmically, this is similar to belief propagation on a factor graph. In particular, we deﬁne δf→i as messages sent from factor f to variable i, and we deﬁne δi→f as messages sent from variable i to factor f . These messages can be computed as follows (see (Globerson and Jaakkola 2008) for the derivation):10\n",
      "\n",
      "δi→f (xi) =θ\n",
      "\n",
      "i(xi) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "δg→i(xi)\n",
      "\n",
      "(22.179)\n",
      "\n",
      "g(cid:5)=f\n",
      "\n",
      "⎡\n",
      "\n",
      "⎤\n",
      "\n",
      "δf→i(xi) =−δ\n",
      "\n",
      "i→f (xi) +\n",
      "\n",
      "1 |f |\n",
      "\n",
      "max xf \\i\n",
      "\n",
      "⎣θf (xf ) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "j∈f\n",
      "\n",
      "δj→f (xj)\n",
      "\n",
      "⎦\n",
      "\n",
      "(22.180)\n",
      "\n",
      "We then set the dual variables δf i(xi) to be the messages δf→i(xi).\n",
      "\n",
      "For example, consider a 2 × 2 grid MRF, with the following pairwise factors: θf (x1, x2), θg(x1, x3), θh(x2, x4), and θk(x3, x4). The outgoing message from factor f to variable 2 is a\n",
      "\n",
      "10. Note that we denote their δ−f\n",
      "\n",
      "i\n",
      "\n",
      "(xi) by δi→f (xi).\n",
      "\n",
      "22.6. MAP state estimation\n",
      "\n",
      "811\n",
      "\n",
      "function of all messages coming into f , and f ’s local factor:\n",
      "\n",
      "δf→2(x2) = −δ2→f (x2) +\n",
      "\n",
      "1 2\n",
      "\n",
      "max x1\n",
      "\n",
      "[θf (x1, x2) +δ 1→f (x1) +δ 2→f (x2)]\n",
      "\n",
      "(22.181)\n",
      "\n",
      "Similarly, the outgoing message from variable 2 to factor f is a function of all the messages sent into variable 2 from other connected factors (in this example, just factor h) and the local potential:\n",
      "\n",
      "δ2→f (x2) = θ2(2) +δ h2(x2)\n",
      "\n",
      "(22.182)\n",
      "\n",
      "The key computational bottleneck is computing the max marginals of each factor, where we max out all the variables from xf except for xi, i.e., we need to be able to compute the following max marginals efficiently:\n",
      "\n",
      "max xf \\i\n",
      "\n",
      "h(xf \\i, xi), h(xf \\i, xi) (cid:2) θf (xf ) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "j∈f\n",
      "\n",
      "δjf (xj)\n",
      "\n",
      "(22.183)\n",
      "\n",
      "The difference from Equation 22.178 is that we are maxing over all but one of the variables. We can solve this efficiently for low treewidth graphical models using message passing; we can also solve this efficiently for factors corresponding to bipartite matchings (Duchi et al. 2007) or to cardinality constraints (Tarlow et al. 2010). However, there are cases where maximizing over all the variables in a factor’s scope is computationally easier than maximizing over all-but-one (see (Sontag et al. 2011, Sec 1.5.4) for an example); in such cases, we may prefer to use a subgradient method.\n",
      "\n",
      "Coordinate descent is a simple algorithm that is often much faster at minimizing the dual than gradient descent, especially in the early iterations. It also reduces the objective monotonically, and does not need any step size parameters. Unfortunately, it is not guaranteed to converge to the global optimum, since L(δ) is convex but not strictly convex (which implies there may be more than one globally optimizing value). One way to ensure convergence is to replace the max function in the deﬁnition of L(δ) with the soft-max function, which makes the objective strictly convex (see e.g., (Hazan and Shashua 2010) for details).\n",
      "\n",
      "22.6.5.5\n",
      "\n",
      "Recovering the MAP assignment So far, we have been focussing on ﬁnding the optimal value of δ∗ . But what we really want is the optimal value of x∗. In general, computing x∗ from δ∗ is NP-hard, even if the LP relaxation is tight and the MAP assignment is unique (Sontag et al. 2011, Theorem 1.4). (The troublesome cases arise when there are fractional assignments with the same optimal value as the MAP estimate.)\n",
      "\n",
      "i has a unique maximum, x∗ is locally decodable to x∗. One can show than in this case, the LP relaxation is unique and its solution is indeed x∗. If many, but not all, of the nodes are uniquely decodable, we can “clamp” the uniquely decodable ones to their MAP value, and then use exact inference algorithms to ﬁgure out the optimal assignment to the remaining variables. Using this method, (Meltzer et al. 2005) was able to optimally solve various stereo vision CRF estimation problems, and (Yanover et al. 2007) was able to optimally solve various protein side-chain structure predicition problems. Another approach is to use the upper bound provided by the dual in a branch and bound\n",
      "\n",
      "However, suppose that each θδ∗\n",
      "\n",
      "i ; in this case, we say that δ∗\n",
      "\n",
      "search procedure (Geoffrion 1974).\n",
      "\n",
      "812\n",
      "\n",
      "Chapter 22. More variational inference\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 22.1 Graphcuts for MAP estimation in binary submodular MRFs (Source: Ex. 13.14 of (Koller and Friedman 2009).). Show that using the graph construction described in Section 22.6.3.2, the cost of the cut is equal to the energy of the corresponding assignment, up to an irrelevant constant. (Warning: this exercise involves a lot of algebraic book-keeping.)\n",
      "\n",
      "Exercise 22.2 Graphcuts for alpha-beta swap (Source: Ex. 13.15 of (Koller and Friedman 2009).). Show how the optimal alpha-beta swap can be found by running min-cut on an appropriately constructed graph. More precisely, a. Deﬁne a set of binary variables t1, . . . , tn such that ti = 0 means x(cid:2)\n",
      "\n",
      "x(cid:2) i = xi is unchanged f xi (cid:13)= α and xi (cid:13)= β.\n",
      "\n",
      "i = α, ti = 1 if x(cid:2)\n",
      "\n",
      "i = β, and\n",
      "\n",
      "b. Deﬁne an energy function over the new variables such that E(cid:2)(t) = E(x) + const. c. Show that E(cid:2) is submodular if E is a semimetric.\n",
      "\n",
      "Exercise 22.3 Constant factor optimality for alpha-expansion (Source: Daphne Koller.). Let X be a pairwise metric Markov random ﬁeld over a graph G = (V, E). Suppose that the variables are nonbinary and that the node potentials are nonnegative. Let A denote the set of labels for each X ∈ X . Though it is not possible to (tractably) ﬁnd the globally optimal assignment x(cid:15) in general, the α-expansion algorithm provides a method for ﬁnding assignments ˆx that are locally optimal with respect to a large set of transformations, i.e., the possible α-expansion moves. Despite the fact that α-expansion only produces a locally optimal MAP assignment, it is possible to prove that the energy of this assignment is within a known factor of the energy of the globally optimal solution x(cid:15). In fact, this is a special case of a more general principle that applies to a wide variety of algorithms, including max-product belief propagation and more general move-making algorithms: If one can prove that the solutions obtained by the algorithm are ‘strong local minima’, i.e., local minima with respect to a large set of potential moves, then it is possible to derive bounds on the (global) suboptimality of these solutions, and the quality of the bounds will depend on the nature of the moves considered. (There is a precise deﬁnition of ‘large set of moves’.) Consider the following approach to proving the suboptimality bound for α-expansion.\n",
      "\n",
      "a. Let ˆx be a local minimum with respect to expansion moves. For each α ∈ A, let V α = {s ∈ V | x(cid:15)\n",
      "\n",
      "b. Building on the previous part, show that E(ˆx) ≤ 2cE(x(cid:15)), where c = max(s,t)∈E\n",
      "\n",
      "s = α}, i.e., the set of nodes labelled α in the global minimum. Let x(cid:2) be an assignment that is equal to x(cid:15) on V α and equal to ˆx elsewhere; this is an α-expansion of ˆx. Verify that E(x(cid:15)) ≤ E(ˆx) ≤ E(x(cid:2)). (cid:17)\n",
      "\n",
      "and E denotes the energy of an assignment. Hint. Think about where x(cid:2) agrees with ˆx and where it agrees with x(cid:15).\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "maxα(cid:5)=β εst(α,β) minα(cid:5)=β εst(α,β)\n",
      "\n",
      "Exercise 22.4 Dual decomposition for pose segmentation (Source: Daphne Koller.). Two important problems in computer vision are that of parsing articulated objects (e.g., the human body), called pose estimation, and segmenting the foreground and the background, called segmentation. Intuitively, these two problems are linked, in that solving either one would be easier if the solution to the other were available. We consider solving these problems simultaneously using a joint model over human poses and foreground/background labels and then using dual decomposition for MAP inference in this model. We construct a two-level model, where the high level handles pose estimation and the low level handles pixel-level background segmentation. Let G = (V, E) be an undirected grid over the pixels. Each node i ∈ V represents a pixel. Suppose we have one binary variable xi for each pixel, where xi = 1 means that pixel i is in the foreground. Denote the full set of these variables by x = (xi).\n",
      "\n",
      "22.6. MAP state estimation\n",
      "\n",
      "813\n",
      "\n",
      "In addition, suppose we have an undirected tree structure T = (V (cid:2), E (cid:2)) on the parts. For each body part, we have a discrete set of candidate poses that the part can be in, where each pose is characterized (These candidates are generated by a procedure by parameters specifying its position and orientation. external to the algorithm described here.) Deﬁne yjk to be a binary variable indicating whether body part j ∈ V (cid:2) is in conﬁguration k. Then the full set of part variables is given by y = (yjk), with j ∈ V (cid:2) and k = 1, . . . , K, where J is the total number of body parts and K is the number of candidate poses for each part. Note that in order to describe a valid conﬁguration, y must satisfy the constraint that (cid:2)K\n",
      "\n",
      "k=1 yjk = 1 for each j.\n",
      "\n",
      "Suppose we have the following energy function on pixels:\n",
      "\n",
      "E1(x) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "1[xi = 1] · θi +\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "1[xi (cid:13)= xj] · θij.\n",
      "\n",
      "i∈V\n",
      "\n",
      "(i,j)∈E\n",
      "\n",
      "Assume that the θij arises from a metric (e.g., based on differences in pixel intensities), so this can be viewed as the energy for a pairwise metric MRF with respect to G. We then have the following energy function for parts:\n",
      "\n",
      "E2(y) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "θp(yp) +\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "θpq(yp, yq).\n",
      "\n",
      "p∈V (cid:2)\n",
      "\n",
      "(p,q)∈E(cid:2)\n",
      "\n",
      "Since each part candidate yjk is assumed to come with a position and orientation, we can compute a binary mask in the image plane. The mask assigns a value to each pixel, denoted by {wi jk}i∈V , where wi jk = 1 if pixel i lies on the skeleton and decreases as we move away. We can use this to deﬁne an energy function relating the parts and the pixels:\n",
      "\n",
      "E3(x, y) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "K(cid:12)\n",
      "\n",
      "1[xi = 0, yjk = 1] · wi\n",
      "\n",
      "jk.\n",
      "\n",
      "i∈V\n",
      "\n",
      "j∈V (cid:2)\n",
      "\n",
      "k=1\n",
      "\n",
      "In other words, this energy term only penalizes the case where a part candidate is active but the pixel underneath is labeled as background. Formulate the minimization of E1 + E2 + E3 as an integer program and show how you can use dual decomposition to solve the dual of this integer program. Your solution should describe the decomposition into slaves, the method for solving each one, and the update rules for the overall algorithm. Brieﬂy justify your design choices, particularly your choice of inference algorithms for the slaves.\n",
      "\n",
      "23 Monte Carlo inference\n",
      "\n",
      "23.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "So far, we discussed various deterministic algorithms for posterior inference. These meth- ods enjoy many of the beneﬁts of the Bayesian approach, while still being about as fast as optimization-based point-estimation methods. The trouble with these methods is that they can be rather complicated to derive, and they are somewhat limited in their domain of applicabil- ity (e.g., they usually assume conjugate priors and exponential family likelihoods, although see (Wand et al. 2011) for some recent extensions of mean ﬁeld to more complex distributions). Fur- thermore, although they are fast, their accuracy is often limited by the form of the approximation which we choose.\n",
      "\n",
      "In this chapter, we discuss an alternative class of algorithms based on the idea of Monte Carlo approximation, which we ﬁrst introduced in Section 2.7. The idea is very simple: generate some (unweighted) samples from the posterior, xs ∼ p(x|D), and then use these to compute any quantity of interest, such as a posterior marginal, p(x1|D), or the posterior of the difference of two quantities, p(x1 − x2|D), or the posterior predictive, p(y|D), etc. All of these quantities can be approximated by E [f |D] ≈ 1 S\n",
      "\n",
      "(cid:7)S\n",
      "\n",
      "s=1 f (xs) for some suitable function f .\n",
      "\n",
      "By generating enough samples, we can achieve any desired level of accuracy we like. The main issue is: how do we efficiently generate samples from a probability distribution, particularly in high dimensions? In this chapter, we discuss non-iterative methods for generating independent In the next chapter, we discuss an iterative method known as Markov Chain Monte samples. Carlo, or MCMC for short, which produces dependent samples but which works well in high dimensions. Note that sampling is a large topic. The reader should consult other books, such as (Liu 2001; Robert and Casella 2004), for more information.\n",
      "\n",
      "23.2\n",
      "\n",
      "Sampling from standard distributions\n",
      "\n",
      "We brieﬂy discuss some ways to sample from 1 or 2 dimensional distributions of standard form. These methods are often used as subroutines by more complex methods.\n",
      "\n",
      "23.2.1\n",
      "\n",
      "Using the cdf\n",
      "\n",
      "The simplest method for sampling from a univariate distribution is based on the inverse prob- ability transform. Let F be a cdf of some distribution we want to sample from, and let F −1\n",
      "\n",
      "816\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "1\n",
      "\n",
      "F\n",
      "\n",
      "u\n",
      "\n",
      "0 0\n",
      "\n",
      "x\n",
      "\n",
      "Figure 23.1\n",
      "\n",
      "Sampling using an inverse CDF. Figure generated by sampleCdf.\n",
      "\n",
      "be its inverse. Then we have the following result.\n",
      "\n",
      "Theorem 23.2.1. If U ∼ U (0, 1) is a uniform rv, then F −1(U ) ∼ F .\n",
      "\n",
      "Proof.\n",
      "\n",
      "Pr(F −1(U ) ≤ x) = Pr(U ≤ F (x))\n",
      "\n",
      "(applying F to both sides)\n",
      "\n",
      "(23.1)\n",
      "\n",
      "= F (x)\n",
      "\n",
      "(because Pr(U ≤ y) = y\n",
      "\n",
      "(23.2)\n",
      "\n",
      "where the ﬁrst line follows since F is a monotonic function, and the second line follows since U is uniform on the unit interval.\n",
      "\n",
      "Hence we can sample from any univariate distribution, for which we can evaluate its inverse cdf, as follows: generate a random number u ∼ U (0, 1) using a pseudo random number generator (see e.g., (Press et al. 1988) for details). Let u represent the height up the y axis. Then “slide along” the x axis until you intersect the F curve, and then “drop down” and return the corresponding x value. This corresponds to computing x = F −1(u). See Figure 23.1 for an illustration.\n",
      "\n",
      "For example, consider the exponential distribution\n",
      "\n",
      "Expon(x|λ) (cid:2) λe−λx I(x ≥ 0)\n",
      "\n",
      "(23.3)\n",
      "\n",
      "The cdf is\n",
      "\n",
      "F (x) = 1 − e−λx I(x ≥ 0)\n",
      "\n",
      "(23.4)\n",
      "\n",
      "whose inverse is the quantile function\n",
      "\n",
      "F −1(p) = − ln(1 − p)\n",
      "\n",
      "λ\n",
      "\n",
      "(23.5)\n",
      "\n",
      "By the above theorem, if U ∼ Unif(0, 1), we know that F −1(U ) ∼ Expon(λ). Furthermore, since 1 − U ∼ Unif(0, 1) as well, we can sample from the exponential distribution by ﬁrst sampling from the uniform and then transforming the results using − ln(u)/λ.\n",
      "\n",
      "23.3. Rejection sampling\n",
      "\n",
      "817\n",
      "\n",
      "23.2.2\n",
      "\n",
      "Sampling from a Gaussian (Box-Muller method)\n",
      "\n",
      "We now describe a method to sample from a Gaussian. The idea is we sample uniformly from a unit radius circle, and then use the change of variables formula to derive samples from a spherical 2d Gaussian. This can be thought of as two samples from a 1d Gaussian.\n",
      "\n",
      "In more detail, sample z1, z2 ∈ (−1, 1) uniformly, and then discard pairs that do not satisfy 1 + z2 z2 2 ≤ 1. The result will be points uniformly distributed inside the unit circle, so p(z) = 1 π I(z inside circle). Now deﬁne\n",
      "\n",
      "xi = zi\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "−2 lnr 2 r2\n",
      "\n",
      "(cid:9) 1\n",
      "\n",
      "2\n",
      "\n",
      "(23.6)\n",
      "\n",
      "for i = 1 : 2, wherer 2 = z2\n",
      "\n",
      "p(x1, x2) = p(z1, z2)|\n",
      "\n",
      "∂(z1, z2) ∂(x1, x2)\n",
      "\n",
      "1 + z2\n",
      "\n",
      "2. Using the multivariate change of variables formula, we have\n",
      "\n",
      "| =\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "1√ 2π\n",
      "\n",
      "exp(− 1 2\n",
      "\n",
      "x2 1)\n",
      "\n",
      "(cid:18) (cid:17)\n",
      "\n",
      "1√ 2π\n",
      "\n",
      "exp(− 1 2\n",
      "\n",
      "x2 2)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(23.7)\n",
      "\n",
      "Hence x1 and x2 are two independent samples from a univariate Gaussian. This is known as the Box-Muller method.\n",
      "\n",
      "To sample from a multivariate Gaussian, we ﬁrst compute the Cholesky decomposition of its covariance matrix, Σ = LLT , where L is lower triangular. Next we sample x ∼ N (0, I) using the Box-Muller method. Finally we set y = Lx + μ. This is valid since\n",
      "\n",
      "cov [y] = Lcov [x] LT = L I LT = Σ\n",
      "\n",
      "(23.8)\n",
      "\n",
      "23.3\n",
      "\n",
      "Rejection sampling\n",
      "\n",
      "When the inverse cdf method cannot be used, one simple alternative is to use rejection sam- pling, which we now explain.\n",
      "\n",
      "23.3.1\n",
      "\n",
      "Basic idea\n",
      "\n",
      "In rejection sampling, we create a proposal distribution q(x) which satisifes M q(x) ≥ ˜p(x), for some constant M , where ˜p(x) is an unnormalized version of p(x) (i.e., p(x) = ˜p(x)/Zp for some possibly unknown constant Zp). The function M q(x) provides an upper envelope for ˜p. We then sample x ∼ q(x), which corresponds to picking a random x location, and then we sample u ∼ U (0, 1), which corresponds to picking a random height (y location) under the envelope. If u > ˜p(x) M q(x) , we reject the sample, otherwise we accept it. See Figure 23.2(a). where the acceptance region is shown shaded, and the rejection region is the white region between the shaded zone and the upper envelope.\n",
      "\n",
      "We now prove that this procedure is correct. Let\n",
      "\n",
      "S = {(x, u) :u ≤ ˜p(x)/M q(x)}, S0 = {(x, u) :x ≤ x0, u ≤ ˜p(x)/M q(x)}\n",
      "\n",
      "(23.9)\n",
      "\n",
      "818\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "1.4\n",
      "\n",
      "(i) Mq(x )\n",
      "\n",
      "1.2\n",
      "\n",
      "target p(x) comparison function Mq(x)\n",
      "\n",
      "(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2) (cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0) (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)\n",
      "\n",
      "Accept region\n",
      "\n",
      "(i) p(x )\n",
      "\n",
      "(i) uMq(x )\n",
      "\n",
      "Reject region\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "(i) x ~ q(x)\n",
      "\n",
      "x\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 23.2 (a) Schematic illustration of rejection sampling. Source: Figure 2 of (Andrieu et al. 2003). (b) Rejection sampling from a Ga(α = 5.7, λ = 2) Used with kind permission of Nando de Freitas. distribution (solid blue) using a proposal of the form M Ga(k, λ − 1) (dotted red), where k = (cid:15)5.7(cid:16) = 5. The curves touch at α − k = 0.7. Figure generated by rejectionSamplingDemo.\n",
      "\n",
      "Then the cdf of the accepted points is given by\n",
      "\n",
      "P (x ≤ x0|x accepted) =\n",
      "\n",
      "=\n",
      "\n",
      "P (x ≤ x0, x accepted) P (x accepted) I((x, u) ∈ S0)q(x)dudx I((x, u) ∈ S)q(x)dudx\n",
      "\n",
      "(cid:21) (cid:21) (cid:21) (cid:21)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:21) x0 −∞ ˜p(x)dx (cid:21) ∞ −∞ ˜p(x)dx\n",
      "\n",
      "(23.10)\n",
      "\n",
      "(23.11)\n",
      "\n",
      "which is the cdf of p(x), as desired. How efficient is this method?\n",
      "\n",
      "probability\n",
      "\n",
      "˜p(x) M q(x) , the probability of acceptance is\n",
      "\n",
      "Since we generate with probability q(x) and accept with\n",
      "\n",
      "p(accept) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "˜p(x) M q(x)\n",
      "\n",
      "q(x)dx =\n",
      "\n",
      "1 M\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "˜p(x)dx\n",
      "\n",
      "(23.12)\n",
      "\n",
      "Hence we want to choose M as small as possible while still satisfying M q(x) ≥ ˜p(x).\n",
      "\n",
      "23.3.2\n",
      "\n",
      "Example\n",
      "\n",
      "For example, suppose we want to sample from a Gamma distribution:1\n",
      "\n",
      "Ga(x|α, λ) =\n",
      "\n",
      "1 Γ(α)\n",
      "\n",
      "xα−1λα exp(−λx)\n",
      "\n",
      "(23.13)\n",
      "\n",
      "iid∼ Expon(λ), and Y = X1 + · · · + Xk, then Y ∼ Ga(k, λ). For One can show that if Xi non-integer shape parameters, we cannot use this trick. However, we can use rejection sampling\n",
      "\n",
      "1. This section is based on notes by Ioana A. Cosma, available at http://users.aims.ac.za/~ioana/cp2.pdf.\n",
      "\n",
      "23.3. Rejection sampling\n",
      "\n",
      "819\n",
      "\n",
      "f(x) half−gaussian\n",
      "\n",
      "samples from f(x) (by ARS)\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "1000\n",
      "\n",
      "0.8\n",
      "\n",
      "900\n",
      "\n",
      "800\n",
      "\n",
      "0.7\n",
      "\n",
      "700\n",
      "\n",
      "0.6\n",
      "\n",
      "600\n",
      "\n",
      "0.5\n",
      "\n",
      "500\n",
      "\n",
      "0.4\n",
      "\n",
      "400\n",
      "\n",
      "0.3\n",
      "\n",
      "300\n",
      "\n",
      "0.2\n",
      "\n",
      "200\n",
      "\n",
      "0.1\n",
      "\n",
      "100\n",
      "\n",
      "0\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "0\n",
      "\n",
      "−8\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 23.3 (a) Idea behind adaptive rejection sampling. We place piecewise linear upper (and lower) bounds on the log-concave density. Based on Figure 1 of (Gilks and Wild 1992). Figure generated by arsEnvelope. (b-c) Using ARS to sample from a half-Gaussian. Figure generated by arsDemo, written by Daniel Eaton.\n",
      "\n",
      "using a Ga(k, λ − 1) distribution as a proposal, where k = (cid:28)α(cid:29). The ratio has the form\n",
      "\n",
      "p(x) q(x)\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "Ga(x|α, λ) Ga(x|k, λ − 1) Γ(k)λα Γ(α)(λ − 1)k\n",
      "\n",
      "xα−k exp(−x)\n",
      "\n",
      "=\n",
      "\n",
      "xα−1λα exp(−λx)/Γ(α) xk−1(λ − 1)k exp(−(λ − 1)x)/Γ(k)\n",
      "\n",
      "(23.14)\n",
      "\n",
      "(23.15)\n",
      "\n",
      "This ratio attains its maximum when x = α − k. Hence\n",
      "\n",
      "M =\n",
      "\n",
      "Ga(α − k|α, λ) Ga(α − k|k, λ − 1)\n",
      "\n",
      "(23.16)\n",
      "\n",
      "See Figure 23.2(b) for a plot. based on the Cauchy distribution.)\n",
      "\n",
      "(Exercise 23.2 asks you to devise a better proposal distribution\n",
      "\n",
      "23.3.3\n",
      "\n",
      "Application to Bayesian statistics\n",
      "\n",
      "Suppose we want to draw (unweighted) samples from the posterior, p(θ|D) = p(D|θ)p(θ)/p(D). We can use rejection sampling with ˜p(θ) = p(D|θ)p(θ) as the target distribution, q(θ) = p(θ) as our proposal, and M = p(D|ˆθ), where ˆθ = arg max p(D|θ) is the MLE; this was ﬁrst suggested in (Smith and Gelfand 1992). We accept points with probability\n",
      "\n",
      "˜p(θ) M q(θ)\n",
      "\n",
      "=\n",
      "\n",
      "p(D|θ) p(D|ˆθ)\n",
      "\n",
      "(23.17)\n",
      "\n",
      "Thus samples from the prior that have high likelihood are more likely to be retained in the posterior. Of course, if there is a big mismatch between prior and posterior (which will be the case if the prior is vague and the likelihood is informative), this procedure is very inefficient. We discuss better algorithms later.\n",
      "\n",
      "23.3.4\n",
      "\n",
      "Adaptive rejection sampling\n",
      "\n",
      "We now describe a method that can automatically come up with a tight upper envelope q(x) to any log concave density p(x). The idea is to upper bound the log density with a piecewise\n",
      "\n",
      "820\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "linear function, as illustrated in Figure 23.3(a). We choose the initial locations for the pieces based on a ﬁxed grid over the support of the distribution. We then evaluate the gradient of the log density at these locations, and make the lines be tangent at these points.\n",
      "\n",
      "Since the log of the envelope is piecewise linear, the envelope itself is piecewise exponential:\n",
      "\n",
      "q(x) =M iλi exp(−λi(x − xi−1)), xi−1 < x ≤ xi\n",
      "\n",
      "(23.18)\n",
      "\n",
      "where xi are the grid points. It is relatively straightforward to sample from this distribution. If the sample x is rejected, we create a new grid point at x, and thereby reﬁne the envelope. As the number of grid points is increased, the tightness of the envelope improves, and the rejection rate goes down. This is known as adaptive rejection sampling (ARS) (Gilks and Wild 1992). Figure 23.3(b-c) gives an example of the method in action. As with standard rejection sampling, it can be applied to unnormalized distributions.\n",
      "\n",
      "23.3.5\n",
      "\n",
      "Rejection sampling in high dimensions\n",
      "\n",
      "It is clear that we want to make our proposal q(x) as close as possible to the target distribution p(x), while still being an upper bound. But this is quite hard to achieve, especially in high dimensions. To see this, consider sampling from p(x) =N (0, σ2 pI) using as a proposal q(x) = N (0, σ2 In D dimensions, the optimum value is given by M = (σq/σp)D. The acceptance rate is 1/M (since both p and q are normalized), which decreases exponentially fast with dimension. For example, if σq exceeds σp by just 1%, then in 1000 dimensions the acceptance ratio will be about 1/20,000. This is a fundamental weakness of rejection sampling.\n",
      "\n",
      "q I). Obviously we must have σ2\n",
      "\n",
      "q ≥ σ2\n",
      "\n",
      "p in order to be an upper bound.\n",
      "\n",
      "In Chapter 24, we will describe MCMC sampling, which is a more efficient way to sample from high dimensional distributions. Sometimes this uses (adaptive) rejection sampling as a subroutine, which is known as adaptive rejection Metropolis sampling (Gilks et al. 1995).\n",
      "\n",
      "23.4\n",
      "\n",
      "Importance sampling\n",
      "\n",
      "We now describe a Monte Carlo method known as importance sampling for approximating integrals of the form\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "I = E [f ] =\n",
      "\n",
      "f (x)p(x)dx\n",
      "\n",
      "(23.19)\n",
      "\n",
      "23.4.1\n",
      "\n",
      "Basic idea\n",
      "\n",
      "The idea is to draw samples x in regions which have high probability, p(x), but also where |f (x)| is large. The result can be super efficient, meaning it needs less samples than if we were to sample from the exact distribution p(x). The reason is that the samples are focussed on the important parts of space. For example, suppose we want to estimate the probability of a rare event. Deﬁne f (x) =I (x ∈ E), for some set E. Then it is better to sample from a proposal of the form q(x) ∝ f (x)p(x) than to sample from p(x) itself.\n",
      "\n",
      "Importance sampling samples from any proposal, q(x). It then uses these samples to estimate\n",
      "\n",
      "23.4.\n",
      "\n",
      "Importance sampling\n",
      "\n",
      "821\n",
      "\n",
      "the integral as follows: (cid:12)\n",
      "\n",
      "E [f ] =\n",
      "\n",
      "f (x)\n",
      "\n",
      "p(x) q(x)\n",
      "\n",
      "q(x)dx ≈ 1 S\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "wsf (xs) = ˆI\n",
      "\n",
      "(23.20)\n",
      "\n",
      "where ws (cid:2) p(xs) the samples.\n",
      "\n",
      "q(xs) are the importance weights. Note that, unlike rejection sampling, we use all\n",
      "\n",
      "estimate ˆI =\n",
      "\n",
      "How should we choose the proposal? A natural criterion is to minimize the variance of the\n",
      "\n",
      "varq(x) [f (x)w(x)] = Eq(x)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "s wsf (xs). Now (cid:13)\n",
      "\n",
      "f 2(x)w2(x)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "− I 2\n",
      "\n",
      "(23.21)\n",
      "\n",
      "Since the last term is independent of q, we can ignore it. By Jensen’s inequality, we have the following lower bound:\n",
      "\n",
      "Eq(x)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "f 2(x)w2(x)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "≥ (Eq(x) [|f (x)w(x)|])2 =\n",
      "\n",
      "(cid:8)(cid:12)\n",
      "\n",
      "(cid:9)2\n",
      "\n",
      "|f (x)|p(x)dx\n",
      "\n",
      "(23.22)\n",
      "\n",
      "The lower bound is obtained when we use the optimal importance distribution:\n",
      "\n",
      "q∗(x) =\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "|f (x)|p(x) |f (x(cid:2))|p(x(cid:2))dx(cid:2)\n",
      "\n",
      "(23.23)\n",
      "\n",
      "When we don’t have a particular target function f (x) in mind, we often just try to make q(x) as close as possible to p(x). In general, this is difficult, especially in high dimensions, but it is possible to adapt the proposal distribution to improve the approximation. This is known as adaptive importance sampling (Oh and Berger 1992).\n",
      "\n",
      "23.4.2\n",
      "\n",
      "Handling unnormalized distributions\n",
      "\n",
      "It is frequently the case that we can evaluate the unnormalized target distribution, ˜p(x), but not its normalization constant, Zp. We may also want to use an unnormalized proposal, ˜q(x), with possibly unknown normlization constant Zq. We can do this as follows. First we evaluate\n",
      "\n",
      "E [f ] =\n",
      "\n",
      "Zq Zp\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "f (x)\n",
      "\n",
      "˜p(x) ˜q(x)\n",
      "\n",
      "q(x)dx ≈\n",
      "\n",
      "Zq Zp\n",
      "\n",
      "1 S\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "˜wsf (xs)\n",
      "\n",
      "(23.24)\n",
      "\n",
      "where ˜ws (cid:2) ˜p(xs) to evaluate the ratio Zp/Zq as follows:\n",
      "\n",
      "˜q(xs) is the unnormalized importance weight. We can use the same set of samples\n",
      "\n",
      "Zp Zq\n",
      "\n",
      "=\n",
      "\n",
      "1 Zq\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "˜p(x)dx =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "˜p(x) ˜q(x)\n",
      "\n",
      "q(x)dx ≈ 1 S\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "˜ws\n",
      "\n",
      "(23.25)\n",
      "\n",
      "Hence\n",
      "\n",
      "ˆI =\n",
      "\n",
      "1 S\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "1 S\n",
      "\n",
      "s ˜wsf (xs) (cid:7) s ˜ws\n",
      "\n",
      "=\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "wsf (xs)\n",
      "\n",
      "(23.26)\n",
      "\n",
      "822\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "where\n",
      "\n",
      "ws (cid:2)\n",
      "\n",
      "˜ws(cid:7)\n",
      "\n",
      "s(cid:2) ˜ws(cid:2)\n",
      "\n",
      "(23.27)\n",
      "\n",
      "are the normalized importance weights. The resulting estimate is a ratio of two estimates, and hence is biased. However, as S → ∞, we have that ˆI → I, under weak assumptions (see e.g., (Robert and Casella 2004) for details).\n",
      "\n",
      "23.4.3\n",
      "\n",
      "Importance sampling for a DGM: likelihood weighting\n",
      "\n",
      "We now describe a way to use importance sampling to generate samples from a distribution which can be represented as a directed graphical model (Chapter 10).\n",
      "\n",
      "If we have no evidence, we can sample from the unconditional joint distribution of a DGM p(x) as follows: ﬁrst sample the root nodes, then sample their children, then sample their children, etc. This is known as ancestral sampling. It works because, in a DAG, we can always topologically order the nodes so that parents preceed children. (Note that there is no equivalent easy method for sampling from an unconditional undirected graphical model.)\n",
      "\n",
      "Now suppose we have some evidence, so some nodes are “clamped” to observed values, and we want to sample from the posterior p(x|D). If all the variables are discrete, we can use the following simple procedure: perform ancestral sampling, but as soon as we sample a value that is inconsistent with an observed value, reject the whole sample and start again. This is known as logic sampling (Henrion 1988).\n",
      "\n",
      "Needless to say, logic sampling is very inefficient, and it cannot be applied when we have real-valued evidence. However, it can be modiﬁed as follows. Sample unobserved variables as before, conditional on their parents. But don’t sample observed variables; instead we just use their observed values. This is equivalent to using a proposal of the form\n",
      "\n",
      "q(x) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "p(xt|xpa(t))\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "δx∗\n",
      "\n",
      "t (xt)\n",
      "\n",
      "(23.28)\n",
      "\n",
      "t(cid:5)∈E\n",
      "\n",
      "t∈E\n",
      "\n",
      "where E is the set of observed nodes, and x∗ therefore give the overall sample an importance weight as follows:\n",
      "\n",
      "t is the observed value for node t. We should\n",
      "\n",
      "w(x) =\n",
      "\n",
      "p(x) q(x)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "t(cid:5)∈E\n",
      "\n",
      "p(xt|xpa(t)) p(xt|xpa(t))\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "t∈E\n",
      "\n",
      "p(xt|xpa(t)) 1\n",
      "\n",
      "=\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "t∈E\n",
      "\n",
      "p(xt|xpa(t))\n",
      "\n",
      "(23.29)\n",
      "\n",
      "This technique is known as likelihood weighting (Fung and Chang 1989; Shachter and Peot 1989).\n",
      "\n",
      "23.4.4\n",
      "\n",
      "Sampling importance resampling (SIR)\n",
      "\n",
      "We can draw unweighted samples from p(x) by ﬁrst using importance sampling (with proposal q) to generate a distribution of the form\n",
      "\n",
      "p(x) ≈\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "wsδxs (x)\n",
      "\n",
      "(23.30)\n",
      "\n",
      "s\n",
      "\n",
      "23.5. Particle ﬁltering\n",
      "\n",
      "823\n",
      "\n",
      "where ws are the normalized importance weights. We then sample with replacement from Equation 23.30, where the probability that we pick xs is ws. Let this procedure induce a distribution denoted by ˆp. To see that this is valid, note that\n",
      "\n",
      "ˆp(x ≤ x0) =\n",
      "\n",
      "→\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "s (cid:21)\n",
      "\n",
      "q(x) q(x)dx I(x ≤ x0)˜p(x)dx ˜p(x)dx\n",
      "\n",
      "I(x ≤ x0) (cid:21) ˜p(x)\n",
      "\n",
      "I(xs ≤ x0)ws =\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "˜p(x) q(x) q(x)dx\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "s I(xs ≤ x0)˜p(xs)/q(xs) s ˜p(xs)/q(xs)\n",
      "\n",
      "I(x ≤ x0)p(x)dx = p(x ≤ x0)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(23.32)\n",
      "\n",
      "(23.33)\n",
      "\n",
      "(23.31)\n",
      "\n",
      "This is known as sampling importance resampling (SIR) (Rubin 1998). The result is an un- weighted approximation of the form\n",
      "\n",
      "p(x) ≈ 1 S(cid:2)\n",
      "\n",
      "S(cid:2)(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "δxs (x)\n",
      "\n",
      "(23.34)\n",
      "\n",
      "Note that we typically take S(cid:2) (cid:22) S.\n",
      "\n",
      "This algorithm can be used to perform Bayesian inference in low-dimensional settings (Smith and Gelfand 1992). That is, suppose we want to draw (unweighted) samples from the posterior, p(θ|D) = p(D|θ)p(θ)/p(D). We can use importance sampling with ˜p(θ) = p(D|θ)p(θ) as the unnormalized posterior, and q(θ) = p(θ) as our proposal. The normalized weights have the form\n",
      "\n",
      "p(D|θs) s(cid:2) p(D|θs(cid:2) ) We can then use SIR to sample from p(θ|D).\n",
      "\n",
      "ws =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "˜p(θs)/q(θs) s(cid:2) ˜p(θs(cid:2) )/q(θs(cid:2) )\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(23.35)\n",
      "\n",
      "Of course, if there is a big discrepancy between our proposal (the prior) and the target (the posterior), we will need a huge number of importance samples for this technique to work reliably, since otherwise the variance of the importance weights will be very large, implying that most (This issue will come up again in Section 23.5, when we samples carry no useful information. discuss particle ﬁltering.)\n",
      "\n",
      "23.5\n",
      "\n",
      "Particle ﬁltering\n",
      "\n",
      "Particle ﬁltering (PF) is a Monte Carlo, or simulation based, algorithm for recursive Bayesian inference. That is, it approximates the predict-update cycle described in Section 18.3.1. It is very widely used in many areas, including tracking, time-series forecasting, online parameter learning, etc. We explain the basic algorithm below. For a book-length treatment, see (Doucet et al. 2001); for a good tutorial, see (Arulampalam et al. 2002), or just read on.\n",
      "\n",
      "824\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "23.5.1\n",
      "\n",
      "Sequential importance sampling\n",
      "\n",
      "The basic idea is to appproximate the belief state (of the entire state trajectory) using a weighted set of particles:\n",
      "\n",
      "p(z1:t|y1:t) ≈\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "ˆws\n",
      "\n",
      "t δzs\n",
      "\n",
      "1:t (z1:t)\n",
      "\n",
      "(23.36)\n",
      "\n",
      "s=1\n",
      "\n",
      "where ˆws t is the normalized weight of sample s at time t. From this representation, we can easily compute the marginal distribution over the most recent state, p(zt|y1:t), by simply ignoring the previous parts of the trajectory, z1:t−1. (The fact that PF samples in the space of entire trajectories has various implications which we will discuss later.)\n",
      "\n",
      "q(zs\n",
      "\n",
      "We update this belief state using importance sampling. 1:t|y1:t), then the importance weights are given by\n",
      "\n",
      "If the proposal has the form\n",
      "\n",
      "ws\n",
      "\n",
      "t ∝\n",
      "\n",
      "p(zs q(zs\n",
      "\n",
      "1:t|y1:t) 1:t|y1:t)\n",
      "\n",
      "(23.37)\n",
      "\n",
      "which can be normalized as follows:\n",
      "\n",
      "ˆws\n",
      "\n",
      "t =\n",
      "\n",
      "ws t(cid:7) s(cid:2) ws(cid:2) t\n",
      "\n",
      "(23.38)\n",
      "\n",
      "We can rewrite the numerator recursively as follows: p(yt|z1:t, y1:t−1)p(z1:t|y1:t−1) p(yt|y1:t−1)\n",
      "\n",
      "p(z1:t|y1:t) =\n",
      "\n",
      "(23.39)\n",
      "\n",
      "=\n",
      "\n",
      "p(yt|zt)p(zt|z1:t−1, y1:t−1)p(z1:t−1|y1:t−1) p(yt|y1:t−1)\n",
      "\n",
      "(23.40)\n",
      "\n",
      "∝ p(yt|zt)p(zt|zt−1)p(z1:t−1|y1:t−1)\n",
      "\n",
      "(23.41)\n",
      "\n",
      "where we have made the usual Markov assumptions. We will restrict attention to proposal densities of the following form:\n",
      "\n",
      "q(z1:t|y1:t) = q(zt|z1:t−1, y1:t)q(z1:t−1|y1:t−1)\n",
      "\n",
      "(23.42)\n",
      "\n",
      "so that we can “grow” the trajectory by adding the new state zt to the end. importance weights simplify to t−1)p(zs t |zs t )p(zs 1:t−1, y1:t)q(zs t )p(zs t |zs t−1) 1:t−1, y1:t)\n",
      "\n",
      "ws\n",
      "\n",
      "t ∝\n",
      "\n",
      "= ws\n",
      "\n",
      "p(yt|zs t |zs q(zs p(yt|zs t |zs q(zs\n",
      "\n",
      "t−1\n",
      "\n",
      "1:t−1|y1:t−1) 1:t−1|y1:t−1)\n",
      "\n",
      "In this case, the\n",
      "\n",
      "(23.43)\n",
      "\n",
      "(23.44)\n",
      "\n",
      "If we further assume that q(zt|z1:t−1, y1:t) =q (zt|zt−1, yt), then we only need to keep the most recent part of the trajectory and observation sequence, rather than the whole history, in order to compute the new sample. In this case, the weight becomes\n",
      "\n",
      "ws\n",
      "\n",
      "t ∝ ws\n",
      "\n",
      "t−1\n",
      "\n",
      "p(yt|zs q(zs\n",
      "\n",
      "t )p(zs t |zs\n",
      "\n",
      "t |zs t−1, yt)\n",
      "\n",
      "t−1)\n",
      "\n",
      "(23.45)\n",
      "\n",
      "23.5. Particle ﬁltering\n",
      "\n",
      "825\n",
      "\n",
      "Hence we can approximate the posterior ﬁltered density using\n",
      "\n",
      "p(zt|y1:t) ≈\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "ˆws\n",
      "\n",
      "t δzs\n",
      "\n",
      "t (zt)\n",
      "\n",
      "(23.46)\n",
      "\n",
      "s=1\n",
      "\n",
      "As S → ∞, one can show that this approaches the true posterior (Crisan et al. 1999).\n",
      "\n",
      "The basic algorithm is now very simple: for each old sample s, propose an extension using zs t ∼ q(zt|zs t using Equation 23.45. Unfortunately, this basic algorithm does not work very well, as we discuss below.\n",
      "\n",
      "t−1, yt), and give this new particle weight ws\n",
      "\n",
      "23.5.2\n",
      "\n",
      "The degeneracy problem\n",
      "\n",
      "The basic sequential importance sampling algorithm fails after a few steps because most of the particles will have negligible weight. This is called the degeneracy problem, and occurs because we are sampling in a high-dimensional space (in fact, the space is growing in size over time), using a myopic proposal distribution.\n",
      "\n",
      "We can quantify the degree of degeneracy using the effective sample size, deﬁned by\n",
      "\n",
      "where w∗s t−1, yt) is the “true weight” of particle s. This quantity cannot be computed exactly, since we don’t know the true posterior, but we can approximate it using\n",
      "\n",
      "S\n",
      "\n",
      "eff\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "t = p(zs\n",
      "\n",
      "S 1 + var [w∗s t ] t |y1:t)/q(zs\n",
      "\n",
      "t |zs\n",
      "\n",
      "(23.47)\n",
      "\n",
      "ˆS eff =\n",
      "\n",
      "(cid:7)S\n",
      "\n",
      "1 s=1(ws\n",
      "\n",
      "t )2\n",
      "\n",
      "(23.48)\n",
      "\n",
      "If the variance of the weights is large, then we are wasting our resources updating particles with low weight, which do not contribute much to our posterior estimate.\n",
      "\n",
      "There are two main solutions to the degeneracy problem: adding a resampling step, and using\n",
      "\n",
      "a good proposal distribution. We discuss both of these in turn.\n",
      "\n",
      "23.5.3\n",
      "\n",
      "The resampling step\n",
      "\n",
      "The main improvement to the basic SIS algorithm is to monitor the effective sampling size, and whenever it drops below a threshold, to eliminate particles with low weight, and then (Hence PF is sometimes called survival of the to create replicates of the surviving particles. t }S ﬁttest (Kanazawa et al. 1995).) In particular, we generate a new set {zs∗ s=1 by sampling with replacement S times from the weighted distribution\n",
      "\n",
      "p(zt|y1:t) ≈\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "ˆws\n",
      "\n",
      "t δzs\n",
      "\n",
      "t (zt)\n",
      "\n",
      "(23.49)\n",
      "\n",
      "s=1\n",
      "\n",
      "where the probability of choosing particle j for replication is wj (This is sometimes called t . rejuvenation.) The result is an iid unweighted sample from the discrete density Equation 23.49, so we set the new weights to ws\n",
      "\n",
      "t = 1/S. This scheme is illustrated in Figure 23.4.\n",
      "\n",
      "826\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "(cid:51)(cid:11)(cid:93)(cid:11)(cid:87)(cid:16)(cid:20)(cid:12)(cid:3)(cid:95)(cid:3)(cid:92)(cid:11)(cid:20)(cid:29)(cid:87)(cid:16)(cid:20)(cid:12)(cid:12)\n",
      "\n",
      "(cid:83)(cid:85)(cid:82)(cid:83)(cid:82)(cid:86)(cid:68)(cid:79)\n",
      "\n",
      "(cid:51)(cid:11)(cid:93)(cid:11)(cid:87)(cid:12)(cid:3)(cid:95)(cid:3)(cid:92)(cid:11)(cid:20)(cid:29)(cid:87)(cid:16)(cid:20)(cid:12)(cid:12)\n",
      "\n",
      "(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:76)(cid:81)(cid:74)\n",
      "\n",
      "(cid:51)(cid:11)(cid:92)(cid:11)(cid:87)(cid:12)(cid:3)(cid:95)(cid:3)(cid:93)(cid:11)(cid:87)(cid:12)(cid:12)\n",
      "\n",
      "(cid:85)(cid:72)(cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)\n",
      "\n",
      "(cid:51)(cid:11)(cid:93)(cid:11)(cid:87)(cid:12)(cid:3)(cid:95)(cid:3)(cid:92)(cid:11)(cid:20)(cid:29)(cid:87)(cid:12)(cid:12)\n",
      "\n",
      "Figure 23.4\n",
      "\n",
      "Illustration of particle ﬁltering.\n",
      "\n",
      "There are a variety of algorithms for peforming the resampling step. The simplest is multi-\n",
      "\n",
      "nomial resampling, which computes\n",
      "\n",
      "(K1, . . . , KS) ∼ Mu(S, (w1\n",
      "\n",
      "t , . . . , wS\n",
      "\n",
      "t ))\n",
      "\n",
      "(23.50)\n",
      "\n",
      "We then make Ks copies of zs t . Various improvements exist, such as systematic resampling residual resampling, and stratiﬁed sampling, which can reduce the variance of the weights. All these methods take O(S) time. See (Doucet et al. 2001) for details.\n",
      "\n",
      "The overall particle ﬁltering algorithm is summarized in Algorithm 6. (Note that if an estimate of the state is required, it should be computed before the resampling step, since this will result in lower variance.)\n",
      "\n",
      "Algorithm 23.1: One step of a generic particle ﬁlter 1 for s = 1 : S do Draw zs t ∼ q(zt|zs 2 Compute weight ws\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "3\n",
      "\n",
      "4 Normalize weights: ws 1(cid:2)S 5 Compute ˆS 6 if ˆS < S\n",
      "\n",
      "9\n",
      "\n",
      "eff Resample S indices π ∼ wt; t = zπ z: t ; ws t = 1/S ;\n",
      "\n",
      "eff = min then\n",
      "\n",
      "s=1(ws\n",
      "\n",
      "t =\n",
      "\n",
      "t−1, yt) ; t ∝ ws ws t(cid:2) s(cid:2) ws(cid:2)\n",
      "\n",
      "t )2 ;\n",
      "\n",
      "t−1\n",
      "\n",
      "t\n",
      "\n",
      "p(yt|zs q(zs\n",
      "\n",
      ";\n",
      "\n",
      "t )p(zs t |zs\n",
      "\n",
      "t |zs t−1,yt)\n",
      "\n",
      "t−1)\n",
      "\n",
      ";\n",
      "\n",
      "Although the resampling step helps with the degeneracy problem, it introduces problems of its own. In particular, since the particles with high weight will be selected many times, there is a loss of diversity amongst the population. This is known as sample impoverishment. In the\n",
      "\n",
      "23.5. Particle ﬁltering\n",
      "\n",
      "827\n",
      "\n",
      "extreme case of no process noise (e.g., if we have static but unknown parameters as part of the state space), then all the particles will collapse to a single point within a few iterations.\n",
      "\n",
      "(1) Only resample when (The original bootstrap ﬁlter (Gordon 1993) resampled at necessary, not at every time step. every step, but this is suboptimal.) (2) After replicating old particles, sample new values using an MCMC step which leaves the posterior distribution invariant (see e.g., the resample-move (3) Create a kernel density estimate on top of the algorithm in (Gilks and Berzuini 2001)). particles,\n",
      "\n",
      "To mitigate this problem, several solutions have been proposed.\n",
      "\n",
      "p(zt|y1:t) ≈\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "ws\n",
      "\n",
      "t κ(zt − zs t )\n",
      "\n",
      "(23.51)\n",
      "\n",
      "s=1\n",
      "\n",
      "where κ is some smoothing kernel. We then sample from this smoothed distribution. This is known as a regularized particle ﬁlter (Musso et al. 2001). (4) When performing inference on static parameters, add some artiﬁcial process noise. (If this is undesirable, other algorithms must be used for online parameter estimation, e.g., (Andrieu et al. 2005)).\n",
      "\n",
      "23.5.4\n",
      "\n",
      "The proposal distribution\n",
      "\n",
      "The simplest and most widely used proposal distribution is to sample from the prior:\n",
      "\n",
      "q(zt|zs\n",
      "\n",
      "t−1, yt) = p(zt|zs\n",
      "\n",
      "t−1)\n",
      "\n",
      "(23.52)\n",
      "\n",
      "In this case, the weight update simpliﬁes to\n",
      "\n",
      "ws\n",
      "\n",
      "t ∝ ws\n",
      "\n",
      "t−1p(yt|zs t )\n",
      "\n",
      "(23.53)\n",
      "\n",
      "This can be thought of a “generate and test” approach: we sample values from the dynamic model, and then evaluate how good they are after we see the data (see Figure 23.4). This is the approach used in the condensation algorithm (which stands for “conditional density propagation”) used for visual tracking (Isard and Blake 1998). However, if the likelihood is narrower than the dynamical prior (meaning the sensor is more informative than the motion model, which is often the case), this is a very inefficient approach, since most particles will be assigned very low weight.\n",
      "\n",
      "It is much better to actually look at the data yt when generating a proposal.\n",
      "\n",
      "In fact, the\n",
      "\n",
      "optimal proposal distribution has the following form: p(yt|zt)p(zt|zs p(yt|zs t−1)\n",
      "\n",
      "q(zt|zs\n",
      "\n",
      "t−1, yt) = p(zt|zs\n",
      "\n",
      "t−1, yt) =\n",
      "\n",
      "t−1)\n",
      "\n",
      "(23.54)\n",
      "\n",
      "If we use this proposal, the new weight is given by\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "ws\n",
      "\n",
      "t ∝ ws\n",
      "\n",
      "t−1p(yt|zs\n",
      "\n",
      "t−1) = ws\n",
      "\n",
      "t−1\n",
      "\n",
      "p(yt|z(cid:2)\n",
      "\n",
      "t)p(z(cid:2)\n",
      "\n",
      "t|zs\n",
      "\n",
      "t−1)dz(cid:2) t\n",
      "\n",
      "(23.55)\n",
      "\n",
      "This proposal is optimal since, for any given zs regardless of the value drawn for zs true weights var [w∗s\n",
      "\n",
      "t ], is zero.\n",
      "\n",
      "t . Hence, conditional on the old values z.\n",
      "\n",
      "t−1, the new weight ws\n",
      "\n",
      "t takes the same value t−1, the variance of\n",
      "\n",
      "828\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "t−1, yt) and to evaluate the integral needed to compute the predictive density p(yt|zs t−1). However, there are two cases when the optimal proposal distribution can be used. The ﬁrst setting is when zt is discrete, so the integral becomes a sum. Of course, if the entire state space is discrete, we can use an HMM ﬁlter instead, but in some cases, some parts of the state are discrete, and some continuous. The second setting is when p(zt|zs t−1, yt) is Gaussian. This occurs when the dynamics are nonlinear but the observations are linear. See Exercise 23.3 for the details.\n",
      "\n",
      "In general, it is intractable to sample from p(zt|zs\n",
      "\n",
      "In cases where the model is not linear-Gaussian, we may still compute a Gaussian approxima- tion to p(zt|zs t−1, yt) using the unscented transform (Section 18.5.2) and use this as a proposal. This is known as the unscented particle ﬁlter (van der Merwe et al. 2000). In more general settings, we can use other kinds of data-driven proposals, perhaps based on discriminative models. Unlike MCMC, we do not need to worry about the proposals being reversible.\n",
      "\n",
      "23.5.5\n",
      "\n",
      "Application: robot localization\n",
      "\n",
      "Consider a mobile robot wandering around an office environment. We will assume that it already has a map of the world, represented in the form of an occupancy grid, which just speciﬁes whether each grid cell is empty space or occupied by an something solid like a wall. The goal is for the robot to estimate its location. This can be solved optimally using an HMM ﬁlter, since we are assuming the state space is discrete. However, since the number of states, K, is often very large, the O(K 2) time complexity per update is prohibitive. We can use a particle ﬁlter as a sparse approximation to the belief state. This is known as Monte Carlo localization, and is described in detail in (Thrun et al. 2006).\n",
      "\n",
      "Figure 23.5 gives an example of the method in action. The robot uses a sonar range ﬁnder, so it can only sense distance to obstacles. It starts out with a uniform prior, reﬂecting the fact that the owner of the robot may have turned it on in an arbitrary location. (Figuring out where you are, starting from a uniform prior, is called global localization.) After the ﬁrst scan, which indicates two walls on either side, the belief state is shown in (b). The posterior is still fairly broad, since the robot could be in any location where the walls are fairly close by, such as a corridor or any of the narrow rooms. After moving to location 2, the robot is pretty sure it must be in the corridor, as shown in (c). After moving to location 3, the sensor is able to detect the end of the corridor. However, due to symmetry, it is not sure if it is in location I (the true location) or location II. (This is an example of perceptual aliasing, which refers to the fact that different things may look the same.) After moving to locations 4 and 5, it is ﬁnally able to ﬁgure out precisely where it is. The whole process is analogous to someone getting lost in an office building, and wandering the corridors until they see a sign they recognize.\n",
      "\n",
      "In Section 23.6.3, we discuss how to estimate location and the map at the same time.\n",
      "\n",
      "23.5.6\n",
      "\n",
      "Application: visual object tracking\n",
      "\n",
      "Our next example is concerned with tracking an object (in this case, a remote-controlled heli- copter) in a video sequence. The method uses a simple linear motion model for the centroid of the object, and a color histogram for the likelihood model, using Bhattacharya distance to compare histograms. The proposal distribution is obtained by sampling from the likelihood. See (Nummiaro et al. 2003) for further details.\n",
      "\n",
      "23.5. Particle ﬁltering\n",
      "\n",
      "829\n",
      "\n",
      "(a) Path and reference poses\n",
      "\n",
      "(b) Belief at reference pose 1\n",
      "\n",
      "Room A\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "Start\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "Room C\n",
      "\n",
      "Room B\n",
      "\n",
      "(c) Belief at reference pose 2\n",
      "\n",
      "(d) Belief at reference pose 3\n",
      "\n",
      "II\n",
      "\n",
      "I\n",
      "\n",
      "(e) Belief at reference pose 4\n",
      "\n",
      "(f) Belief at reference pose 5\n",
      "\n",
      "I\n",
      "\n",
      "II\n",
      "\n",
      "Figure 23.5 with kind permission of Sebastian Thrun.\n",
      "\n",
      "Illustration of Monte Carlo localization.\n",
      "\n",
      "Source: Figure 8.7 of (Thrun et al. 2006). Used\n",
      "\n",
      "Figure 23.6 shows some example frames. The system uses S = 250 particles, with an effective sample size of ˆS eff = 134. (a) shows the belief state at frame 1. The system has had to resample 5 times to keep the effective sample size above the threshold of 150; (b) shows the belief state at frame 251; the red lines show the estimated location of the center of the object over the last 250 frames. (c) shows that the system can handle visual clutter, as long as it does not have the same color as the target object. (d) shows that the system is confused between the grey of the helicopter and the grey of the building. The posterior is bimodal. The green ellipse, representing the posterior mean and covariance, is in between the two modes. (e) shows that the probability (f) shows the particles spread mass has shifted to the wrong mode: the system has lost track. out over the gray building; recovery of the object is very unlikely from this state using this\n",
      "\n",
      "830\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(e)\n",
      "\n",
      "(f)\n",
      "\n",
      "Figure 23.6 Example of particle ﬁltering applied to visual object tracking, based on color histograms. (a-c) succesful tracking: green ellipse is on top of the helicopter. (d-f): tracker gets distracted by gray clutter in the background. See text for details. Figure generated by pfColorTrackerDemo, written by Sebastien Paris.\n",
      "\n",
      "proposal.\n",
      "\n",
      "We see that the method is able to keep track for a fairly long time, despite the presence of clutter. However, eventually it loses track of the object. Note that since the algorithm is stochastic, simply re-running the demo may ﬁx the problem. But in the real world, this is not an option. The simplest way to improve performance is to use more particles. An alternative is to perform tracking by detection, by running an object detector over the image every few frames. See (Forsyth and Ponce 2002; Szeliski 2010; Prince 2012) for details.\n",
      "\n",
      "23.6. Rao-Blackwellised particle ﬁltering (RBPF)\n",
      "\n",
      "831\n",
      "\n",
      "23.5.7\n",
      "\n",
      "Application: time series forecasting\n",
      "\n",
      "In Section 18.2.4, we discussed how to use the Kalman ﬁlter to perform time series forecasting. This assumes that the model is a linear-Gaussian state-space model. There are many models which are either non-linear and/or non-Gaussian. For example, stochastic volatility models, which are widely used in ﬁnance, assume that the variance of the system and/or observation noise changes over time. Particle ﬁltering is widely used in such settings. See e.g., (Doucet et al. 2001) and references therein for details.\n",
      "\n",
      "23.6\n",
      "\n",
      "Rao-Blackwellised particle ﬁltering (RBPF)\n",
      "\n",
      "In some models, we can partition the hidden variables into two kinds, qt and zt, such that we can analytically integrate out zt provided we know the values of q1:t. This means we only have sample q1:t, and can represent p(zt|q1:t) parametrically. Thus each particle s represents a value for qs 1:t). These hybrid particles are are sometimes called distributional particles or collapsed particles (Koller and Friedman 2009, Sec 12.4).\n",
      "\n",
      "1:t and a distribution of the form p(zt|y1:t, qs\n",
      "\n",
      "The advantage of this approach is that we reduce the dimensionality of the space in which we are sampling, which reduces the variance of our estimate. Hence this technique is known as Rao-Blackwellised particle ﬁltering or RBPF for short, named after Theorem 24.20. The method is best explained using a speciﬁc example.\n",
      "\n",
      "23.6.1\n",
      "\n",
      "RBPF for switching LG-SSMs\n",
      "\n",
      "A canonical example for which RBPF can be applied is the switching linear dynamical system (SLDS) model discussed in Section 18.6 (Chen and Liu 2000; Doucet et al. 2001). We can represent p(zt|y1:t, qs 1:t) using a mean and covariance matrix for each particle s, where qt ∈ {1, . . . , K}.\n",
      "\n",
      "If we propose from the prior, q(qt = k|qs ws\n",
      "\n",
      "t ∝ ws\n",
      "\n",
      "t−1p(yt|qt = k, qs\n",
      "\n",
      "1:t−1, y1:t−1) = ws\n",
      "\n",
      "t−1), the weight update becomes\n",
      "\n",
      "t−1Ls t,k\n",
      "\n",
      "(23.56)\n",
      "\n",
      "where\n",
      "\n",
      "Ls\n",
      "\n",
      "tk =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(yt|qt = k, zt,(cid:3)(cid:3)(cid:3)y1:t−1,\n",
      "\n",
      "(cid:2)(cid:2)qs 1:t−1)p(zt|qt = k, y1:t−1qs (cid:2)\n",
      "\n",
      "1:t−1, )dzt\n",
      "\n",
      "(23.57)\n",
      "\n",
      "The quantity Ls the history qs constant of the Kalman ﬁlter, Equation 18.41.\n",
      "\n",
      "tk is the predictive density for the new observation yt conditioned on qt = k and 1:t−1. In the case of SLDS models, this can be computed using the normalization\n",
      "\n",
      "We give some pseudo-code in Algorithm 8. (The step marked “KFupdate” refers to the Kalman\n",
      "\n",
      "ﬁlter update equations in Section 18.3.1.) This is known as a mixture of Kalman ﬁlters.\n",
      "\n",
      "If K is small, we can compute the optimal proposal distribution, which is p(qt = k|y1:t, qs\n",
      "\n",
      "1:t−1) = ˆps t−1(qt = k|yt) t−1(yt|qt = k)ˆps ˆps ˆps t−1(yt) tkp(qt = k|qs\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Ls k(cid:2) Ls\n",
      "\n",
      "t−1) tk(cid:2) p(qt = k(cid:2)|qs\n",
      "\n",
      "t−1(qt = k)\n",
      "\n",
      "t−1)\n",
      "\n",
      "(23.58)\n",
      "\n",
      "(23.59)\n",
      "\n",
      "(23.60)\n",
      "\n",
      "832\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "10\n",
      "\n",
      "11\n",
      "\n",
      "9\n",
      "\n",
      "4\n",
      "\n",
      "Algorithm 23.2: One step of RBPF for SLDS using prior as proposal 1 for s = 1 : S do k ∼ p(qt|qs 2 qs t := k; t , Σs (μs t = ws ws 5 6 Normalize weights: ws 1(cid:2)S 7 Compute ˆS 8 if ˆS < S\n",
      "\n",
      "3\n",
      "\n",
      "eff Resample S indices π ∼ wt; t = Σπ t = qπ q: t = μπ t , μ: ws t = 1/S ;\n",
      "\n",
      "t , Ls t−1Lk ts;\n",
      "\n",
      "eff = min then\n",
      "\n",
      "t−1) ;\n",
      "\n",
      "tk) = KFupdate(μs\n",
      "\n",
      "s=1(ws\n",
      "\n",
      "t =\n",
      "\n",
      "t , Σ:\n",
      "\n",
      "t )2 ;\n",
      "\n",
      "ws t(cid:2) s(cid:2) ws(cid:2)\n",
      "\n",
      "t\n",
      "\n",
      "t−1, Σs\n",
      "\n",
      "t , ;\n",
      "\n",
      ";\n",
      "\n",
      "t−1, yt, θk);\n",
      "\n",
      "where we use the following shorthand:\n",
      "\n",
      "t−1(·) = p(·|y1:t−1, qs ˆps 1:t−1) We then sample from p(qt|qs\n",
      "\n",
      "ws\n",
      "\n",
      "t ∝ ws\n",
      "\n",
      "t−1p(yt|qs\n",
      "\n",
      "1:t−1, y1:t−1) = ws\n",
      "\n",
      "1:t−1, y1:t) and give the resulting particle weight\n",
      "\n",
      "t−1\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "Ls\n",
      "\n",
      "tkp(qt = k|qs\n",
      "\n",
      "t−1)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(23.61)\n",
      "\n",
      "(23.62)\n",
      "\n",
      "k\n",
      "\n",
      "Since the weights of the particles in Equation 23.62 are independent of the new value that is actually sampled for qt, we can compute these weights ﬁrst, and use them to decide which particles to propagate. That is, we choose the ﬁttest particles at time t − 1 using information from time t. This is called look-ahead RBPF (de Freitas et al. 2004).\n",
      "\n",
      "In more detail, the idea is this. We pass each sample in the prior through all K models to get K posteriors, one per sample. The normalization constants of this process allow us to compute the optimal weights in Equation 23.62. We then resample S indices. Finally, for each old particle s that is chosen, we sample one new state qs t = k, and use the corresponding posterior from the K possible alternative that we have already computed. The pseudo-code is shown in Algorithm 7. This method needs O(KS) storage, but has the advantage that each particle is chosen using the latest information, yt.\n",
      "\n",
      "A further improvement can be obtained by exploiting the fact that the state space is discrete. Hence we can use the resampling method of (Fearnhead 2004) which avoids duplicating particles.\n",
      "\n",
      "23.6.2\n",
      "\n",
      "Application: tracking a maneuvering target\n",
      "\n",
      "One application of SLDS is to track moving objects that have piecewise linear dynamics. For example, suppose we want to track an airplane or missile; qt can specify if the object is ﬂying normally or is taking evasive action. This is called maneuvering target tracking.\n",
      "\n",
      "Figure 23.7 gives an example of an object moving in 2d. The setup is essentially the same as in Section 18.2.1, except that we add a three-state discrete Markov chain which controls the\n",
      "\n",
      "23.6. Rao-Blackwellised particle ﬁltering (RBPF)\n",
      "\n",
      "833\n",
      "\n",
      "10\n",
      "\n",
      "11\n",
      "\n",
      "9\n",
      "\n",
      "8\n",
      "\n",
      "3\n",
      "\n",
      "Algorithm 23.3: One step of look-ahead RBPF for SLDS using optimal proposal 1 for s = 1 : S do 2\n",
      "\n",
      "for k = 1 : K do tk, Σs tk, Lk (μs (cid:7) k Lk t = ws t−1[ 4 5 Normalize weights: ws 6 Resample S indices π ∼ wt; 7 for s ∈ π do\n",
      "\n",
      "ws\n",
      "\n",
      "Compute optimal proposal p(k|qs Sample k ∼ p(k|qs t = k, μs qs ws t = 1/S;\n",
      "\n",
      "t = μs\n",
      "\n",
      "tk, Σs\n",
      "\n",
      "ts) = KFupdate(μs tsp(qt = k|qs t−1)]; ws t(cid:2) s(cid:2) ws(cid:2)\n",
      "\n",
      "t =\n",
      "\n",
      "1:t−1, y1:t); t = Σs tk;\n",
      "\n",
      "t\n",
      "\n",
      ";\n",
      "\n",
      "1:t−1, y1:t) =\n",
      "\n",
      "t−1, Σs\n",
      "\n",
      "t−1, yt, θk);\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "Ls k(cid:2) Ls\n",
      "\n",
      "tkp(qt=k|qs\n",
      "\n",
      "t−1) tkp(qt=k|qs\n",
      "\n",
      "t−1) ;\n",
      "\n",
      "Method misclassiﬁcation rate MSE 21.051 PF 18.168 RBPF\n",
      "\n",
      "0.440 0.340\n",
      "\n",
      "Time (seconds) 6.086 10.986\n",
      "\n",
      "Table 23.1 Comparison of PF an RBPF on the maneuvering target problem in Figure 23.7.\n",
      "\n",
      "input to the system. We deﬁne ut = 1 and set\n",
      "\n",
      "B1 = (0, 0, 0, 0)T , B2 = (−1.225, −0.35, 1.225, 0.35)T , B3 = (1.225, 0.35, −1.225, −0.35)T\n",
      "\n",
      "so the system will turn in different directions depending on the discrete state.\n",
      "\n",
      "Figure 23.7(a) shows the true state of the system from a sample run, starting at (0, 0): the colored symbols denote the discrete state, and the location of the symbol denotes the (x, y) location. The small dots represent noisy observations. Figure 23.7(b) shows the estimate of the state computed using particle ﬁltering with 500 particles, where the proposal is to sample from the prior. The colored symbols denote the MAP estimate of the state, and the location of the symbol denotes the MMSE (minimum mean square error) estimate of the location, which is given by the posterior mean. Figure 23.7(c) shows the estimate computing using RBPF with 500 particles, using the optimal proposal distribution. A more quantitative comparison is shown in Table 23.1. We see that RBPF has slightly better performance, although it is also slightly slower. Figure 23.8 visualizes the belief state of the system. In (a) we show the distribution over the discrete states. We see that the particle ﬁlter estimate of the belief state (second column) is not as accurate as the RBPF estimate (third column) in the beginning, although after the ﬁrst few observations performance is similar for both methods. In (b), we plot the posterior over the x locations. For simplicity, we use the PF estimate, which is a set of weighted samples, but we could also have used the RBPF estimate, which is a set of weighted Gaussians.\n",
      "\n",
      "834\n",
      "\n",
      "Chapter 23. Monte Carlo inference\n",
      "\n",
      "data\n",
      "\n",
      "pf, mse 21.051\n",
      "\n",
      "50\n",
      "\n",
      "50\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−50\n",
      "\n",
      "−50\n",
      "\n",
      "−100\n",
      "\n",
      "−100\n",
      "\n",
      "−150\n",
      "\n",
      "−150\n",
      "\n",
      "−200\n",
      "\n",
      "−200\n",
      "\n",
      "−250\n",
      "\n",
      "−90\n",
      "\n",
      "−80\n",
      "\n",
      "−70\n",
      "\n",
      "−60\n",
      "\n",
      "−50\n",
      "\n",
      "−40\n",
      "\n",
      "−30\n",
      "\n",
      "−20\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "−250\n",
      "\n",
      "−80\n",
      "\n",
      "−70\n",
      "\n",
      "−60\n",
      "\n",
      "−50\n",
      "\n",
      "−40\n",
      "\n",
      "−30\n",
      "\n",
      "−20\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "50\n",
      "\n",
      "rbpf, mse 18.168\n",
      "\n",
      "0\n",
      "\n",
      "−50\n",
      "\n",
      "−100\n",
      "\n",
      "−150\n",
      "\n",
      "−200\n",
      "\n",
      "−250\n",
      "\n",
      "−80\n",
      "\n",
      "−70\n",
      "\n",
      "−60\n",
      "\n",
      "−50\n",
      "\n",
      "−40\n",
      "\n",
      "−30\n",
      "\n",
      "−20\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 23.7 Particle ﬁlter estimate. Nando de Freitas.\n",
      "\n",
      "(a) A maneuvering target. The colored symbols represent the hidden discrete state.\n",
      "\n",
      "(b) (c) RBPF estimate. Figure generated by rbpfManeuverDemo, based on code by\n",
      "\n",
      "23.6.3\n",
      "\n",
      "Application: Fast SLAM\n",
      "\n",
      "In Section 18.2.2, we introduced the problem of simultaneous localization and mapping or SLAM for mobile robotics. The main problem with the Kalman ﬁlter implementation is that it is cubic in the number of landmarks. However, by looking at the DGM in Figure 18.2, we see that, conditional on knowing the robot’s path, q1:t, where qt ∈ R2, the landmark locations z ∈ R2L (We assume the landmarks don’t move, so we drop the t subscript). That is, are independent. (cid:26)L p(z|q1:t, y1:t) = l=1 p(zl|q1:t, y1:t). Consequently we can use RBPF, where we sample the robot’s trajectory, q1:t, and we run L independent 2d Kalman ﬁlters inside each particle. This takes O(L) time per particle. Fortunately, the number of particles needed for good performance is quite small (this partly depends on the control / exploration policy), so the algorithm is essentially linear in the number of particles. This technique has the additional advantage that\n",
      "\n",
      "23.6. Rao-Blackwellised particle ﬁltering (RBPF)\n",
      "\n",
      "835\n",
      "\n",
      "truth\n",
      "\n",
      "pf, error rate 0.440\n",
      "\n",
      "rbpf, error rate 0.340\n",
      "\n",
      "PF\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      ")\n",
      "\n",
      "y |\n",
      "\n",
      "x ( p\n",
      "\n",
      "t : 1\n",
      "\n",
      "t , 1\n",
      "\n",
      "100\n",
      "\n",
      "1 0.5 0\n",
      "\n",
      "80\n",
      "\n",
      "60\n",
      "\n",
      "60\n",
      "\n",
      "60\n",
      "\n",
      "70\n",
      "\n",
      "70\n",
      "\n",
      "70\n",
      "\n",
      "60\n",
      "\n",
      "0\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "80\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "40\n",
      "\n",
      "t\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "−80\n",
      "\n",
      "−60\n",
      "\n",
      "−40\n",
      "\n",
      "x 1,t\n",
      "\n",
      "−20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 23.8 Belief states corresponding to Figure 23.7. (a) Discrete state. The system starts in state 2 (red x in Figure 23.7), then moves to state 3 (black * in Figure 23.7), returns brieﬂy to state 2, then switches to state 1 (blue circle in Figure 23.7), etc. (b) Horizontal location (PF estimate). Figure generated by rbpfManeuverDemo, based on code by Nando de Freitas.\n",
      "\n",
      "it is easy to use sampling to handle the data association ambiguity, and that it allows for other representations of the map, such as occupancy grids. This idea was ﬁrst suggested in (Murphy 2000), and was subsequently extended and made practical in (Thrun et al. 2004), who christened the technique FastSLAM. See rbpfSlamDemo for a simple demo in a discrete grid world.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 23.1 Sampling from a Cauchy Show how to use inverse probability transform to sample from a standard Cauchy, T (x|0, 1, 1).\n",
      "\n",
      "Exercise 23.2 Rejection sampling from a Gamma using a Cauchy proposal Show how to use a Cauchy proposal to perform rejection sampling from a Gamma distribution. Derive the optimal constant M , and plot the density and its upper envelope.\n",
      "\n",
      "Exercise 23.3 Optimal proposal for particle ﬁltering with linear-Gaussian measurement model Consider a state-space model of the following form:\n",
      "\n",
      "zt = ft(zt−1) + N (0, Qt−1) yt = Htzt + N (0, Rt)\n",
      "\n",
      "(23.63) (23.64)\n",
      "\n",
      "Derive expressions for p(zt|zt−1, yt) and p(yt|zt−1), which are needed to compute the optimal (minimum variance) proposal distribution. Hint: use Bayes rule for Gaussians.\n",
      "\n",
      "24 Markov chain Monte Carlo (MCMC)\n",
      "\n",
      "inference\n",
      "\n",
      "24.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In Chapter 23, we introduced some simple Monte Carlo methods, including rejection sampling and importance sampling. The trouble with these methods is that they do not work well in high dimensional spaces. The most popular method for sampling from high-dimensional distributions is Markov chain Monte Carlo or MCMC. In a survey bySIAM News 1, MCMC was placed in the top 10 most important algorithms of the 20th century.\n",
      "\n",
      "The basic idea behind MCMC is to construct a Markov chain (Section 17.2) on the state space X whose stationary distribution is the target density p∗(x) of interest (this may be a prior or a posterior). That is, we perform a random walk on the state space, in such a way that the fraction of time we spend in each state x is proportional to p∗(x). By drawing (correlated!) samples x0, x1, x2, . . . , from the chain, we can perform Monte Carlo integration wrt p∗. We give the details below.\n",
      "\n",
      "It was discovered by physicists working on the atomic bomb at Los Alamos during World War II, and was ﬁrst published in the open literature in (Metropolis et al. 1953) in a chemistry journal. An extension was published in the statistics literature in (Hastings 1970), but was largely unnoticed. A special case (Gibbs sampling, Section 24.2) was independently invented in 1984 in the context of Ising models and was published in (Geman and Geman 1984). But it was not until (Gelfand and Smith 1990) that the algorithm became well-known to the wider statistical community. Since then it has become wildly popular in Bayesian statistics, and is becoming increasingly popular in machine learning. It is worth brieﬂy comparing MCMC to variational inference (Chapter 21). The advantages of variational it is deterministic; (3) is it easy to determine when to stop; (4) it often provides a lower bound on the log likelihood. The advantages of sampling are: (1) it is often easier to implement; (2) it is applicable to a broader range of models, such as models whose size or structure changes depending on the values of certain variables (e.g., as happens in matching problems), or models without nice conjugate priors; (3) sampling can be faster than variational methods when applied to really huge models or datasets.2\n",
      "\n",
      "The MCMC algorithm has an interesting history.\n",
      "\n",
      "inference are (1) for small to medium problems,\n",
      "\n",
      "it is usually faster;\n",
      "\n",
      "(2)\n",
      "\n",
      "1. Source: http://www.siam.org/pdf/news/637.pdf. 2. The reason is that sampling passes speciﬁc values of variables (or sets of variables), whereas in variational inference, we pass around distributions. Thus sampling passes sparse messages, whereas variational inference passes dense messages For comparisons of the two approaches, see e.g., (Yoshida and West 2010) and articles in (Bekkerman et al.\n",
      "\n",
      "838\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "24.2\n",
      "\n",
      "Gibbs sampling\n",
      "\n",
      "In this section, we present one of the most popular MCMC algorithms, known as Gibbs sam- pling.3 (In physics, this method is known as Glauber dynamics or the heat bath method.) This is the MCMC analog of coordinate descent.\n",
      "\n",
      "24.2.1\n",
      "\n",
      "Basic idea\n",
      "\n",
      "The idea behind Gibbs sampling is that we sample each variable in turn, conditioned on the values of all the other variables in the distribution. That is, given a joint sample xs of all the variables, we generate a new sample xs+1 by sampling each component in turn, based on the most recent values of the other variables. For example, if we have D = 3 variables, we use • xs+1 • xs+1 • xs+1\n",
      "\n",
      "1 ∼ p(x1|xs 2, xs 3) 2 ∼ p(x2|xs+1 3 ∼ p(x3|xs+1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      ", xs 3)\n",
      "\n",
      ", xs+1 2\n",
      "\n",
      ")\n",
      "\n",
      "This readily generalizes to D variables. If xi is a visible variable, we do not sample it, since its value is already known.\n",
      "\n",
      "The expression p(xi|x−i) is called the full conditional for variable i. In general, xi may only depend on some of the other variables. If we represent p(x) as a graphical model, we can infer the dependencies by looking at i’s Markov blanket, which are its neighbors in the graph. Thus to sample xi, we only need to know the values of i’s neighbors. In this sense, Gibbs sampling is a distributed algorithm. However, it is not a parallel algorithm, since the samples must be generated sequentially.\n",
      "\n",
      "it is necessary to discard some of the initial samples until the Markov chain has burned in, or entered its stationary distribution. We discuss how to estimate when burnin has occured in Section 24.4.1. In the examples below, we just discard the initial 25% of the samples, for simplicity.\n",
      "\n",
      "For reasons that we will explain in Section 24.4.1,\n",
      "\n",
      "24.2.2\n",
      "\n",
      "Example: Gibbs sampling for the Ising model\n",
      "\n",
      "In Section 21.3.2, we applied mean ﬁeld to an Ising model. Here we apply Gibbs sampling.\n",
      "\n",
      "Gibbs sampling in pairwise MRF/CRF takes the form\n",
      "\n",
      "p(xt|x−t, θ) ∝\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "ψst(xs, xt)\n",
      "\n",
      "(24.1)\n",
      "\n",
      "s∈nbr(t)\n",
      "\n",
      "In the case of an Ising model with edge potentials ψ(xs, xt) = exp(Jxsxt), where xt ∈\n",
      "\n",
      "2011) 3. Josiah Willard Gibbs, 1839–1903, was an American physicist.\n",
      "\n",
      "24.2. Gibbs sampling\n",
      "\n",
      "839\n",
      "\n",
      "sample 1, Gibbs\n",
      "\n",
      "1\n",
      "\n",
      "sample 5, Gibbs\n",
      "\n",
      "1\n",
      "\n",
      "mean after 15 sweeps of Gibbs\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−0.5\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 24.1 Example of image denoising. We use an Ising prior with Wij = J = 1 and a Gaussian noise model with σ = 2. We use Gibbs sampling (Section 24.2) to perform approximate inference. (a) Sample from the posterior after one sweep over the image. (b) Sample after 5 sweeps. (c) Posterior mean, computed by averaging over 15 sweeps. Compare to Figure 21.3 which shows the results of using mean ﬁeld inference. Figure generated by isingImageDenoiseDemo.\n",
      "\n",
      "{−1, +1}, the full conditional becomes\n",
      "\n",
      "p(xt = +1|x−t, θ) =\n",
      "\n",
      "=\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "exp[J\n",
      "\n",
      "s∈nbr(t) ψ(st = +1, xs) + exp[J\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "s∈nbr(t) xs] + exp[−J\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "s∈nbr(t) ψst(xt = +1, xs) (cid:26)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "s∈nbr(t) xs] (cid:7)\n",
      "\n",
      "s∈nbr(t) ψ(xt = −1, xs)\n",
      "\n",
      "s∈nbr(t) xs]\n",
      "\n",
      "(24.2)\n",
      "\n",
      "(24.3)\n",
      "\n",
      "s∈nbr(t) xt and sigm(u) = 1/(1 + e−u) is the sigmoid where J is the coupling strength, ηt (cid:2) function. It is easy to see that ηt = xt(at − dt), where at is the number of neighbors that agree with (have the same sign as) t, and dt is the number of neighbors who disagree. If this number is equal, the “forces” on xt cancel out, so the full conditional is uniform.\n",
      "\n",
      "observation model, we have ψt(xt) = N (yt|xt, σ2). The full conditional becomes\n",
      "\n",
      "We can combine an Ising prior with a local evidence term ψt. For example, with a Gaussian\n",
      "\n",
      "=\n",
      "\n",
      "exp[Jηt] exp[Jηt] + exp[−Jηt] (cid:7)\n",
      "\n",
      "= sigm(2Jηt)\n",
      "\n",
      "(24.4)\n",
      "\n",
      "p(xt = +1|x−t, y, θ) =\n",
      "\n",
      "= sigm\n",
      "\n",
      "exp[Jηt]ψt(+1) exp[Jηt]ψt(+1) + exp[−Jηt]ψt(−1)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "2Jηt − log\n",
      "\n",
      "ψt(+1) ψt(−1)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(24.5)\n",
      "\n",
      "(24.6)\n",
      "\n",
      "Now the probability of xt entering each state is determined both by compatibility with its neighbors (the Ising prior) and compatibility with the data (the local likelihood term).\n",
      "\n",
      "See Figure 24.1 for an example of this algorithm applied to a simple image denoising problem. The results are similar to mean ﬁeld (Figure 21.3) except that the ﬁnal estimate (based on averaging the samples) is somewhat “blurrier”, due to the fact that mean ﬁeld tends to be over-conﬁdent.\n",
      "\n",
      "840\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "24.2.3\n",
      "\n",
      "Example: Gibbs sampling for inferring the parameters of a GMM\n",
      "\n",
      "It is straightforward to derive a Gibbs sampling algorithm to “ﬁt” a mixture model, especially if we use conjugate priors. We will focus on the case of mixture of Gaussians, although the results are easily extended to other kinds of mixture models. (The derivation, which follows from the results of Section 4.6, is much easier than the corresponding variational Bayes algorithm in Section 21.6.1.)\n",
      "\n",
      "Suppose we use a semi-conjugate prior. Then the full joint distribution is given by\n",
      "\n",
      "p(x, z, μ, Σ, π) =p( x|z, μ, Σ)p(z|π)p(π)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "N(cid:20)\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "(πkN (xi|μk, Σk))\n",
      "\n",
      "k=1\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "p(μk)p(Σk) (cid:11)\n",
      "\n",
      "I(zi=k)\n",
      "\n",
      "×\n",
      "\n",
      "(24.7)\n",
      "\n",
      "(24.8)\n",
      "\n",
      "i=1\n",
      "\n",
      "k=1\n",
      "\n",
      "Dir(π|α)\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "N (μk|m0, V0)IW(Σk|S0, ν0)\n",
      "\n",
      "(24.9)\n",
      "\n",
      "k=1\n",
      "\n",
      "We use the same prior for each mixture component. The full conditionals are as follows. For the discrete indicators, we have\n",
      "\n",
      "p(zi = k|xi, μ, Σ, π) ∝ πkN (xi|μk, Σk)\n",
      "\n",
      "(24.10)\n",
      "\n",
      "For the mixing weights, we have (using results from Section 3.4)\n",
      "\n",
      "p(π|z) = Dir({αk +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "I(zi = k)}K\n",
      "\n",
      "k=1)\n",
      "\n",
      "(24.11)\n",
      "\n",
      "i=1\n",
      "\n",
      "For the means, we have (using results from Section 4.6.1)\n",
      "\n",
      "p(μk|Σk, z, x) =N (μk|mk, Vk) 0 + NkΣ−1\n",
      "\n",
      "V−1 = V−1 k mk = Vk(Σ−1\n",
      "\n",
      "k Nkxk + V−1\n",
      "\n",
      "k\n",
      "\n",
      "0 m0)\n",
      "\n",
      "(24.12)\n",
      "\n",
      "(24.13)\n",
      "\n",
      "(24.14)\n",
      "\n",
      "Nk (cid:2)\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "I(zi = k)\n",
      "\n",
      "(24.15)\n",
      "\n",
      "xk (cid:2)\n",
      "\n",
      "i=1 (cid:7)N\n",
      "\n",
      "i=1 I(zi = k)xi Nk\n",
      "\n",
      "(24.16)\n",
      "\n",
      "For the covariances, we have (using results from Section 4.6.2)\n",
      "\n",
      "p(Σk|μk, z, x) = IW(Σk|Sk, νk)\n",
      "\n",
      "(24.17)\n",
      "\n",
      "Sk = S0 +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "I(zi = k)(xi − μk)(xi − μk)T\n",
      "\n",
      "(24.18)\n",
      "\n",
      "i=1 νk = ν0 + Nk\n",
      "\n",
      "(24.19)\n",
      "\n",
      "values for x, if necessary.)\n",
      "\n",
      "See gaussMissingFitGibbs for some Matlab code.\n",
      "\n",
      "(This code can also sample missing\n",
      "\n",
      "24.2. Gibbs sampling\n",
      "\n",
      "841\n",
      "\n",
      "24.2.3.1\n",
      "\n",
      "Label switching\n",
      "\n",
      "Although it is simple to implement, Gibbs sampling for mixture models has a fundamental weakness. The problem is that the parameters of the model θ, and the indicator functions z, are unidentiﬁable, since we can arbitrarily permute the hidden labels without affecting the likelihood (see Section 11.3.1). Consequently, we cannot just take a Monte Carlo average of the samples to compute posterior means, since what one sample considers the parameters for cluster 1 may be what another sample considers the parameters for cluster 2. Indeed, if we could average over all modes, we would ﬁnd E [μk|D] is the same for all k (assuming a symmetric prior). This is called the label switching problem.\n",
      "\n",
      "This problem does not arise in EM or VBEM, which just “lock on” to a single mode. However, it arises in any method that visits multiple modes. In 1d problems, one can try to prevent this problem by introducing constraints on the parameters to ensure identiﬁability, e.g., μ1 < μ2 < μ3 (Richardson and Green 1997). However, this does not always work, since the likelihood might overwhelm the prior and cause label switching anyway. Furthermore, this technique does not scale to higher dimensions. Another approach is to post-process the samples by searching for a global label permutation to apply to each sample that minimizes some loss function (Stephens 2000); however, this can be slow.\n",
      "\n",
      "Perhaps the best solution is simply to “not ask” questions that cannot be uniquely identiﬁed. For example, instead of asking for the probability that data point i belongs to cluster k, ask for the probability that data points i and j belong to the same cluster. The latter question is invariant to the labeling. Furthermore, it only refers to observable quantities (are i and j grouped together or not), rather than referring to unobservable quantities, such as latent clusters. This approach has the further advantage that it extends to inﬁnite mixture models, discussed in Section 25.2, where K is unbounded; in such models, the notion of a hidden cluster is not well deﬁned, but the notion of a partitioning of the data is well deﬁned\n",
      "\n",
      "24.2.4\n",
      "\n",
      "Collapsed Gibbs sampling *\n",
      "\n",
      "In some cases, we can analytically integrate out some of the unknown quantities, and just sample the rest. This is called a collapsed Gibbs sampler, and it tends to be much more efficient, since it is sampling in a lower dimensional space.\n",
      "\n",
      "More precisely, suppose we sample z and integrate out θ. Thus the θ parameters do not participate in the Markov chain; consequently we can draw conditionally independent samples θs ∼ p(θ|zs, D), which will have much lower variance than samples drawn from the joint state space (Liu et al. 1994). This process is called Rao-Blackwellisation, named after the following theorem:\n",
      "\n",
      "Theorem 24.2.1 (Rao-Blackwell). Let z and θ be dependent random variables, and f (z, θ) be some scalar function. Then\n",
      "\n",
      "varz,θ [f (z, θ)] ≥ varz [Eθ [f (z, θ)|z]]\n",
      "\n",
      "(24.20)\n",
      "\n",
      "This theorem guarantees that the variance of the estimate created by analytically integrating out θ will always be lower (or rather, will never be higher) than the variance of a direct MC In collapsed Gibbs, we sample z with θ integrated out; the above Rao-Blackwell estimate. theorem still applies in this case (Liu et al. 1994).\n",
      "\n",
      "842\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "α\n",
      "\n",
      "α\n",
      "\n",
      "π\n",
      "\n",
      "z1\n",
      "\n",
      "zi\n",
      "\n",
      "zN\n",
      "\n",
      "z1\n",
      "\n",
      "zi\n",
      "\n",
      "zN\n",
      "\n",
      "x1\n",
      "\n",
      "xi\n",
      "\n",
      "xN\n",
      "\n",
      "x1\n",
      "\n",
      "xi\n",
      "\n",
      "xN\n",
      "\n",
      "θK\n",
      "\n",
      "β\n",
      "\n",
      "β\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 24.2 (a) A mixture model. (b) After integrating out the parameters.\n",
      "\n",
      "We will encounter Rao-Blackwellisation again in Section 23.6. Although it can reduce statistical variance, it is only worth doing if the integrating out can be done quickly, otherwise we will not be able to produce as many samples per second as the naive method. We give an example of this below.\n",
      "\n",
      "24.2.4.1\n",
      "\n",
      "Example: collapsed Gibbs for ﬁtting a GMM\n",
      "\n",
      "Consider a GMM with a fully conjugate prior. In this case we can analytically integrate out the model parameters μk, Σk and π, and just sample the indicators z. Once we integrate out π, all the zi nodes become inter-dependent. Similarly, once we integrate out θk, all the xi nodes become inter-dependent, as shown in Figure 24.2(b). Nevertheless, we can easily compute the full conditionals as follows:\n",
      "\n",
      "p(zi = k|z−i, x, α, β) ∝ p(zi = k|z−i, α, (cid:14)(cid:14)β)p(x|zi = k, z−i,(cid:4)α, β) ∝ p(zi = k|z−i, α)p(xi|x−i, zi = k, z−i, β)\n",
      "\n",
      "p(x−i|(cid:3)(cid:3)(cid:3)zi = k, z−i, β)\n",
      "\n",
      "(24.21)\n",
      "\n",
      "(24.22)\n",
      "\n",
      "∝ p(zi = k|z−i, α)p(xi|x−i, zi = k, z−i, β)\n",
      "\n",
      "(24.23)\n",
      "\n",
      "where β = (m0, V0, S0, ν0) are the hyper-parameters for the class-conditional densities. The ﬁrst term can be obtained by integrating out π. Suppose we use a symmetric prior of the form π ∼ Dir(α), whereα k = α/K. From Equation 5.26 we have\n",
      "\n",
      "p(z1, . . . , zN |α) =\n",
      "\n",
      "Γ(α) Γ(N + α)\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "k=1\n",
      "\n",
      "Γ(Nk + α/K) Γ(α/K)\n",
      "\n",
      "(24.24)\n",
      "\n",
      "24.2. Gibbs sampling\n",
      "\n",
      "843\n",
      "\n",
      "Hence\n",
      "\n",
      "where Nk,−i (cid:2) xΓ(x).\n",
      "\n",
      "p(zi = k|z−i, α) =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "n(cid:5)=i I(zn = k) =N k − 1, and where we exploited the fact that Γ(x + 1) =\n",
      "\n",
      "=\n",
      "\n",
      "p(z1:N |α) p(z−i|α) Γ(N + α − 1) Γ(N + α)\n",
      "\n",
      "=\n",
      "\n",
      "1 Γ(N +α) 1 Γ(N +α−1) Γ(Nk,−i + 1 + α/K) Γ(Nk,−i + α/K)\n",
      "\n",
      "× Γ(Nk + α/K) Γ(Nk,−i + α/K)\n",
      "\n",
      "=\n",
      "\n",
      "Nk,−i + α/K N + α − 1\n",
      "\n",
      "(24.25)\n",
      "\n",
      "(24.26)\n",
      "\n",
      "xi given all the other data and all the assignments, we use the fact that\n",
      "\n",
      "To obtain the second term in Equation 24.23, which is the posterior predictive distribution for\n",
      "\n",
      "p(xi|x−i, z−i, zi = k, β) = p(xi|D−i,k)\n",
      "\n",
      "(24.27)\n",
      "\n",
      "where D−i,k = {xj : zj = k, j (cid:8)= i} is all the data assigned to cluster k except for xi. If we use a conjugate prior for θk, we can compute p(xi|D−i,k) in closed form. Furthermore, we can efficiently update these predictive likelihoods by caching the sufficient statistics for each cluster. To compute the above expression, we remove xi’s statistics from its current cluster (namely zi), and then evaluate xi under each cluster’s posterior predictive. Once we have picked a new cluster, we add xi’s statistics to this new cluster.\n",
      "\n",
      "Some pseudo-code for one step of the algorithm is shown in Algorithm 1, based on (Sud- derth 2006, p94). (We update the nodes in random order to improve the mixing time, as suggested in (Roberts and Sahu 1997).) We can initialize the sample by sequentially sampling from p(zi|z1:i−1, x1:i). (See fmGibbs for some Matlab code, by Yee-Whye Teh.) In the case of GMMs, both the naive sampler and collapsed sampler take O(N KD) time per step.\n",
      "\n",
      "Algorithm 24.1: Collapsed Gibbs sampler for a mixture model 1 for each i = 1 :N in random order do 2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "Remove xi’s sufficient statistics from old cluster zi ; for each k = 1 :K do\n",
      "\n",
      "Compute pk(xi) (cid:2) p(xi|{xj : zj = k, j (cid:8)= i}) ; Compute p(zi = k|z−i, D) ∝ (Nk,−i + α/K)pk(xi); Sample zi ∼ p(zi|·) ; Add xi’s sufficient statistics to new cluster zi\n",
      "\n",
      "A comparison of this method with the standard Gibbs sampler is shown in Figure 24.3. The\n",
      "\n",
      "vertical axis is the data log probability at each iteration, computed using\n",
      "\n",
      "log p(D|z, θ) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "log [πzi\n",
      "\n",
      "p(xi|θzi )]\n",
      "\n",
      "(24.28)\n",
      "\n",
      "i=1\n",
      "\n",
      "To compute this quantity using the collapsed sampler, we have to sample θ = (π, θ1:K) given the data and the current assignment z.\n",
      "\n",
      "In Figure 24.3 we see that the collapsed sampler does indeed generally work better than the vanilla sampler. Occasionally, however, both methods can get stuck in poor local modes. (Note\n",
      "\n",
      "844\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "−350\n",
      "\n",
      "−350\n",
      "\n",
      "−400\n",
      "\n",
      "−400\n",
      "\n",
      ") θ\n",
      "\n",
      ", π\n",
      "\n",
      "−450\n",
      "\n",
      ") θ\n",
      "\n",
      ", π\n",
      "\n",
      "−450\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "x ( p\n",
      "\n",
      "x ( p\n",
      "\n",
      "g o\n",
      "\n",
      "l\n",
      "\n",
      "−500\n",
      "\n",
      "g o\n",
      "\n",
      "l\n",
      "\n",
      "−500\n",
      "\n",
      "−550\n",
      "\n",
      "−550\n",
      "\n",
      "−600\n",
      "\n",
      "0 10\n",
      "\n",
      "1 10\n",
      "\n",
      "Iteration\n",
      "\n",
      "Standard Gibbs Sampler Rao−Blackwellized Sampler\n",
      "\n",
      "2 10\n",
      "\n",
      "3 10\n",
      "\n",
      "−600\n",
      "\n",
      "0 10\n",
      "\n",
      "1 10\n",
      "\n",
      "Iteration\n",
      "\n",
      "Standard Gibbs Sampler Rao−Blackwellized Sampler\n",
      "\n",
      "2 10\n",
      "\n",
      "3 10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 24.3 Comparison of collapsed (red) and vanilla (blue) Gibbs sampling for a mixture of K = 4 two- dimensional Gaussians applied to N = 300 data points (shown in Figure 25.7). We plot log probability of the data vs iteration. (a) 20 different random initializations. (b) logprob averaged over 100 different random initializations. Solid line is the median, thick dashed in the 0.25 and 0.75 quantiles, and thin dashed are the 0.05 and 0.95 quintiles. Source: Figure 2.20 of (Sudderth 2006). Used with kind permission of Erik Sudderth.\n",
      "\n",
      "80\n",
      "\n",
      "10\n",
      "\n",
      "80\n",
      "\n",
      "70\n",
      "\n",
      "70\n",
      "\n",
      "e r o c S h t a M\n",
      "\n",
      "60\n",
      "\n",
      "50\n",
      "\n",
      "e p o S\n",
      "\n",
      "l\n",
      "\n",
      "5\n",
      "\n",
      "0\n",
      "\n",
      "e r o c S h t a M\n",
      "\n",
      "60\n",
      "\n",
      "50\n",
      "\n",
      "40\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "−5\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0 SES\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "20 15 Sample Size\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0 SES\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 24.4 (a) Least squares regression lines for math scores vs socio-economic status for 100 schools. (b) Plot of ˆw2j (the slope) vs Nj (sample size) for the 100 Population mean (pooled estimate) is in bold. schools. The extreme slopes tend to correspond to schools with smaller sample sizes. (c) Predictions from the hierarchical model. Population mean is in bold. Based on Figure 11.1 of (Hoff 2009). Figure generated by multilevelLinregDemo, written by Emtiyaz Khan.\n",
      "\n",
      "that the error bars in Figure 24.3(b) are averaged over starting values, whereas the theorem refers to MC samples in a single run.)\n",
      "\n",
      "24.2.5\n",
      "\n",
      "Gibbs sampling for hierarchical GLMs\n",
      "\n",
      "Often we have data from multiple related sources. If some sources are more reliable and/or data-rich than others, it makes sense to model all the data simultaneously, so as to enable the borrowing of statistical strength. One of the most natural way to solve such problems is to use hierarchical Bayesian modeling, also called multi-level modeling. In Section 9.6, we discussed a way to perform approximate inference in such models using variational methods. Here we discuss how to use Gibbs sampling.\n",
      "\n",
      "To explain the method, consider the following example. Suppose we have data on students\n",
      "\n",
      "24.2. Gibbs sampling\n",
      "\n",
      "845\n",
      "\n",
      "μw\n",
      "\n",
      "Σw\n",
      "\n",
      "wj\n",
      "\n",
      "yij\n",
      "\n",
      "σ2\n",
      "\n",
      "xij\n",
      "\n",
      "Nj\n",
      "\n",
      "J\n",
      "\n",
      "Figure 24.5 Multi-level model for linear regression.\n",
      "\n",
      "in different schools. Such data is naturally modeled in a two-level hierarchy: we let yij be the response variable we want to predict for student i in school j. This prediction can be based on school and student speciﬁc covariates, xij. Since the quality of schools varies, we want to use a separate parameter for each school. So our model becomes\n",
      "\n",
      "yij = xT\n",
      "\n",
      "ijwj + (cid:16)ij\n",
      "\n",
      "(24.29)\n",
      "\n",
      "We will illustrate this model below, using a dataset from (Hoff 2009, p197), where xij is the socio-economic status (SES) of student i in school y, and yij is their math score.\n",
      "\n",
      "We could ﬁt each wj separately, but this can give poor results if the sample size of a given school is small. This is illustrated in Figure 24.4(a), which plots the least squares regression line estimated separately for each of the J = 100 schools. We see that most of the slopes are positive, but there are a few “errant” cases where the slope is negative. It turns out that the lines with extreme slopes tend to be in schools with small sample size, as shown in Figure 24.4(b). Thus we may not necessarily trust these ﬁts.\n",
      "\n",
      "We can get better results if we construct a hierarchical Bayesian model, in which the wj are assumed to come from a common prior: wj ∼ N (μw, Σw). This is illustrated in Figure 24.5. In this model, the schools with small sample size borrow statistical strength from the schools with larger sample size, because the wj’s are correlated via the latent common parents (μw, Σw). (It is crucial that these hyper-parameters be inferrred from data; if they were ﬁxed constants, the wj would be conditionally independent, and there would be no information sharing between them.)\n",
      "\n",
      "To complete the model speciﬁcation, we must specify priors for the shared parameters. Fol-\n",
      "\n",
      "lowing (Hoff 2009, p198), we will use the following semi-conjugate forms, for convenience:\n",
      "\n",
      "μw ∼ N (μ0, V0) Σw ∼ IW(η0, S−1 0 ) σ2 ∼ IG(ν0/2, ν0σ2\n",
      "\n",
      "0/2)\n",
      "\n",
      "(24.30)\n",
      "\n",
      "(24.31)\n",
      "\n",
      "(24.32)\n",
      "\n",
      "Given this, it is simple to show that the full conditionals needed for Gibbs sampling have the\n",
      "\n",
      "846\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "following forms. For the group-speciﬁc weights:\n",
      "\n",
      "p(wj|Dj, θ) =N (wj|μj, Σj)\n",
      "\n",
      "(24.33)\n",
      "\n",
      "Σ−1 = Σ−1 + XT j Xj/σ2 j μj = Σj(Σ−1μ + XT\n",
      "\n",
      "j yj/σ2)\n",
      "\n",
      "(24.34)\n",
      "\n",
      "(24.35)\n",
      "\n",
      "For the overall mean:\n",
      "\n",
      "where w = 1 J\n",
      "\n",
      "p(μw|w1:J , Σw) =N (μ|μN , ΣN ) 0 + JΣ−1\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Σ−1 N = V−1 μN = ΣN (V−1\n",
      "\n",
      "j wj. For the overall covariance:\n",
      "\n",
      "0 μ0 + JΣ−1w)\n",
      "\n",
      "(24.36)\n",
      "\n",
      "(24.37)\n",
      "\n",
      "(24.38)\n",
      "\n",
      "p(Σw|μw, w1:J ) = IW((S0 + Sμ)−1, η0 + J) (cid:4) (wj − μw)(wj − μw)T\n",
      "\n",
      "Sμ =\n",
      "\n",
      "(24.39)\n",
      "\n",
      "(24.40)\n",
      "\n",
      "j\n",
      "\n",
      "For the noise variance:\n",
      "\n",
      "p(σ2|D, w1:J ) = IG([ν0 + N ]/2, [ν0σ2 J(cid:4)\n",
      "\n",
      "SSR(w1:J ) =\n",
      "\n",
      "Nj(cid:4)\n",
      "\n",
      "(yij − wT\n",
      "\n",
      "j xij)2\n",
      "\n",
      "0 + SSR(w1:J )]/2)\n",
      "\n",
      "(24.41)\n",
      "\n",
      "(24.42)\n",
      "\n",
      "j=1\n",
      "\n",
      "i=1\n",
      "\n",
      "Applying Gibbs sampling to our hierarchical model, we get the results shown in Figure 24.4(c).\n",
      "\n",
      "The light gray lines plot the mean of the posterior predictive distribution for each school:\n",
      "\n",
      "E [yj|xij] = xT\n",
      "\n",
      "ij ˆwj\n",
      "\n",
      "(24.43)\n",
      "\n",
      "where\n",
      "\n",
      "ˆwj = E [wj|D] ≈ 1 S\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "w(s) j\n",
      "\n",
      "(24.44)\n",
      "\n",
      "The dark gray line in the middle plots the prediction using the overall mean parameters, xT ij ˆμw. We see that the method has regularized the ﬁts quite nicely, without enforcing too much (The amount of shrinkage is controlled by Σw, which in turns depends on the uniformity. hyper-parameters; in this example, we used vague values.)\n",
      "\n",
      "24.2.6\n",
      "\n",
      "BUGS and JAGS\n",
      "\n",
      "One reason Gibbs sampling is so popular is that it is possible to design general purpose software that will work for almost any model. This software just needs a model speciﬁcation, usually in the form a directed graphical model (speciﬁed in a ﬁle, or created with a graphical user interface), and a library of methods for sampling from different kinds of full conditionals. (This can often be done using adaptive rejection sampling, described in Section 23.3.4.) An example\n",
      "\n",
      "24.2. Gibbs sampling\n",
      "\n",
      "847\n",
      "\n",
      "of such a package is BUGS (Lunn et al. 2000), which stands for “Bayesian updating using Gibbs Sampling”. BUGS is very widely used in biostatistics and social science. Another more recent, but very similar, package is JAGS (Plummer 2003), which stands for “Just Another Gibbs Sampler”. This uses a similar model speciﬁcation language to BUGS.\n",
      "\n",
      "For example, we can describe the model in Figure 24.5 as follows:\n",
      "\n",
      "model {\n",
      "\n",
      "for (i in 1:N) {\n",
      "\n",
      "for (j in 1:J) {\n",
      "\n",
      "y[i,j] ~ dnorm(y.hat[i,j], tau.y) y.hat[i,j] <- inprod(W[j, ], X[i, j, ])\n",
      "\n",
      "}\n",
      "\n",
      "} tau.y <- pow(sigma.y, -2) sigma.y ~ dunif(0,100)\n",
      "\n",
      "for (j in 1:J) {\n",
      "\n",
      "W[j,] ~ dmnorm(mu, SigmaInv)\n",
      "\n",
      "} SigmaInv ~ dwish(S0[,], eta0) mu ~ dmnorm(mu0, V0inv) }\n",
      "\n",
      "We can then just pass this model to BUGS or JAGS, which will generate samples for us. See\n",
      "\n",
      "the webpages for details.\n",
      "\n",
      "Although this approach is appealing, unfortunately it can be much slower than using hand- written code, especially for complex models. There has been some work on automatically deriving model-speciﬁc optimized inference code (Fischer and Schumann 2003), but fast code still typically requires human expertise.\n",
      "\n",
      "24.2.7\n",
      "\n",
      "The Imputation Posterior (IP) algorithm\n",
      "\n",
      "The Imputation Posterior or IP algorithm (Tanner and Wong 1987) is a special case of Gibbs sampling in which we group the variables into two classes: hidden variables z and parameters θ. This should sound familiar: it is basically an MCMC version of EM, where the E step gets replaced by the I step, and the M step gets replaced the P step. This is an example of a more general strategy called data augmentation, whereby we introduce auxiliary variables in order to simplify the posterior computations (here the computation of p(θ|D)). See (Tanner 1996; van Dyk and Meng 2001) for more information.\n",
      "\n",
      "24.2.8\n",
      "\n",
      "Blocking Gibbs sampling\n",
      "\n",
      "Gibbs sampling can be quite slow, since it only updates one variable at a time (so-called single site updating). If the variables are highly correlated, it will take a long time to move away from the current state. This is illustrated in Figure 24.6, where we illustrate sampling from a 2d Gaussian (see Exercise 24.1 for the details). If the variables are highly correlated, the algorithm\n",
      "\n",
      "848\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(cid:2)(cid:5)\n",
      "\n",
      "(cid:2)(cid:4)\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:4)\n",
      "\n",
      "(cid:2)(cid:5)\n",
      "\n",
      "(cid:6)\n",
      "\n",
      "(cid:5)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "Figure 24.6 Illustration of potentially slow sampling when using Gibbs sampling for a skewed 2D Gaus- sian. Based on Figure 11.11 of (Bishop 2006b). Figure generated by gibbsGaussDemo.\n",
      "\n",
      "will move very slowly through the state space. In particular, the size of the moves is controlled by the variance of the conditional distributions. If this is (cid:2) in the x1 direction, and the support of the distribution is L along this dimension, then we need O((L/(cid:2))2) steps to obtain an independent sample.\n",
      "\n",
      "In some cases we can efficiently sample groups of variables at a time. This is called blocking Gibbs sampling or blocked Gibbs sampling (Jensen et al. 1995; Wilkinson and Yeung 2002), and can make much bigger moves through the state space.\n",
      "\n",
      "24.3 Metropolis Hastings algorithm\n",
      "\n",
      "Although Gibbs sampling is simple, it is somewhat restricted in the set of models to which it can be applied. For example, it is not much help in computing p(w|D) for a logistic regression model, since the corresponding graphical model has no useful Markov structure. In addition, Gibbs sampling can be quite slow, as we mentioned above.\n",
      "\n",
      "Fortunately, there is a more general algorithm that can be used, known as the Metropolis\n",
      "\n",
      "Hastings or MH algorithm, which we describe below.\n",
      "\n",
      "24.3.1\n",
      "\n",
      "Basic idea\n",
      "\n",
      "The basic idea in MH is that at each step, we propose to move from the current state x to a new state x(cid:2) with probability q(x(cid:2)|x), where q is called the proposal distribution (also called the kernel). The user is free to use any kind of proposal they want, subject to some conditions which we explain below. This makes MH quite a ﬂexible method. A commonly used proposal is a symmetric Gaussian distribution centered on the current state, q(x(cid:2)|x) =N (x(cid:2)|x, Σ); this is called a random walk Metropolis algorithm. We discuss how to choose Σ in Section 24.3.3. If we use a proposal of the form q(x(cid:2)|x) =q (x(cid:2)), where the new state is independent of the old state, we get a method known as the independence sampler, which is similar to importance sampling (Section 23.4).\n",
      "\n",
      "Having proposed a move to x(cid:2), we then decide whether to accept this proposal or not according to some formula, which ensures that the fraction of time spent in each state is proportional to p∗(x). If the proposal is accepted, the new state is x(cid:2), otherwise the new state\n",
      "\n",
      "24.3. Metropolis Hastings algorithm\n",
      "\n",
      "849\n",
      "\n",
      "is the same as the current state, x (i.e., we repeat the sample).\n",
      "\n",
      "If the proposal is symmetric, so q(x(cid:2)|x) = q(x|x(cid:2)), the acceptance probability is given by the\n",
      "\n",
      "following formula:\n",
      "\n",
      "r = min(1,\n",
      "\n",
      "p∗(x(cid:2)) p∗(x)\n",
      "\n",
      ")\n",
      "\n",
      "(24.45)\n",
      "\n",
      "p∗(x(cid:2)) We see that if x(cid:2) is more probable than x, we deﬁnitely move there (since p∗(x) > 1), but if x(cid:2) is less probable, we may still move there anyway, depending on the relative probabilities. So instead of greedily moving to only more probable states, we occasionally allow “downhill” moves to less probable states. In Section 24.3.6, we prove that this procedure ensures that the fraction of time we spend in each state x is proportional to p∗(x).\n",
      "\n",
      "If the proposal is asymmetric, so q(x(cid:2)|x) (cid:8)= q(x|x(cid:2)), we need the Hastings correction, given\n",
      "\n",
      "by the following:\n",
      "\n",
      "r = min(1, α)\n",
      "\n",
      "(24.46)\n",
      "\n",
      "α =\n",
      "\n",
      "p∗(x(cid:2))q(x|x(cid:2)) p∗(x)q(x(cid:2)|x)\n",
      "\n",
      "=\n",
      "\n",
      "p∗(x(cid:2))/q(x(cid:2)|x) p∗(x)/q(x|x(cid:2))\n",
      "\n",
      "(24.47)\n",
      "\n",
      "This correction is needed to compensate for the fact that the proposal distribution itself (rather than just the target distribution) might favor certain states.\n",
      "\n",
      "know the target density up to a normalization constant. In particular, suppose p∗(x) = 1 where ˜p(x) is an unnormalized distribution and Z is the normalization constant. Then\n",
      "\n",
      "An important reason why MH is a useful algorithm is that, when evaluating α, we only need to Z ˜p(x),\n",
      "\n",
      "α =\n",
      "\n",
      "(˜p(x(cid:2))/Z) q(x|x(cid:2)) (˜p(x)/Z) q(x(cid:2)|x)\n",
      "\n",
      "(24.48)\n",
      "\n",
      "so the Z’s cancel. Hence we can sample from p∗ even if Z is unknown. have to do is evaluate ˜p pointwise, where ˜p(x) = p∗(x)Z. The overall algorithm is summarized in Algorithm 2.\n",
      "\n",
      "In particular, all we\n",
      "\n",
      "24.3.2\n",
      "\n",
      "Gibbs sampling is a special case of MH\n",
      "\n",
      "It turns out that Gibbs sampling, which we discussed in Section 24.2, is a special case of MH. In particular, it is equivalent to using MH with a sequence of proposals of the form\n",
      "\n",
      "q(x(cid:2)|x) = p(x(cid:2)\n",
      "\n",
      "i|x−i)I(x(cid:2)\n",
      "\n",
      "−i = x−i)\n",
      "\n",
      "(24.49)\n",
      "\n",
      "That is, we move to a new state where xi is sampled from its full conditional, but x−i is left unchanged.\n",
      "\n",
      "We now prove that the acceptance rate of each such proposal is 1, so the overall algorithm\n",
      "\n",
      "also has an acceptance rate of 100%. We have\n",
      "\n",
      "α =\n",
      "\n",
      "=\n",
      "\n",
      "p(x(cid:2))q(x|x(cid:2)) p(x)q(x(cid:2)|x) p(x(cid:2) p(xi|x−i)p(x−i)p(x(cid:2)\n",
      "\n",
      "i|x−i)p(x−i)p(xi|x−i) i|x−i)\n",
      "\n",
      "=\n",
      "\n",
      "p(x(cid:2) −i)p(x(cid:2) p(xi|x−i)p(x−i)p(x(cid:2)\n",
      "\n",
      "i|x(cid:2)\n",
      "\n",
      "= 1\n",
      "\n",
      "−i)p(xi|x(cid:2)\n",
      "\n",
      "−i) i|x−i)\n",
      "\n",
      "(24.50)\n",
      "\n",
      "(24.51)\n",
      "\n",
      "850\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "Algorithm 24.2: Metropolis Hastings algorithm 1 Initialize x0 ; 2 for s = 0, 1, 2, . . . do Deﬁne x = xs; 3 Sample x(cid:2) ∼ q(x(cid:2)|x); Compute acceptance probability\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "α =\n",
      "\n",
      "˜p(x(cid:2))q(x|x(cid:2)) ˜p(x)q(x(cid:2)|x)\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "Compute r = min(1, α); Sample u ∼ U (0, 1) ; Set new sample to\n",
      "\n",
      "xs+1 =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "x(cid:2) xs\n",
      "\n",
      "if u < r if u ≥ r\n",
      "\n",
      "where we exploited the fact that x(cid:2)\n",
      "\n",
      "−i = x−i, and that q(x(cid:2)|x) = p(x(cid:2)\n",
      "\n",
      "i|x−i).\n",
      "\n",
      "The fact that the acceptance rate is 100% does not necessarily mean that Gibbs will converge rapidly, since it only updates one coordinate at a time (see Section 24.2.8). Fortunately, there are many other kinds of proposals we can use, as we discuss below.\n",
      "\n",
      "24.3.3\n",
      "\n",
      "Proposal distributions For a given target distribution p∗, a proposal distribution q is valid or admissible if it gives a non-zero probability of moving to the states that have non-zero probability in the target. Formally, we can write this as supp(p∗) ⊆ ∪xsupp(q(·|x))\n",
      "\n",
      "(24.52)\n",
      "\n",
      "For example, a Gaussian random walk proposal has non-zero probability density on the entire state space, and hence is a valid proposal for any continuous state space.\n",
      "\n",
      "Of course, in practice, it is important that the proposal spread its probability mass in just the right way. Figure 24.7 shows an example where we use MH to sample from a mixture of two 1D Gaussians using a random walk proposal, q(x(cid:2)|x) =N (x(cid:2)|x, v). This is a somewhat tricky target distribution, since it consists of two well separated modes. It is very important to set the variance of the proposal v correctly: If the variance is too low, the chain will only explore one of the modes, as shown in Figure 24.7(a), but if the variance is too large, most of the moves will be rejected, and the chain will be very sticky, i.e., it will stay in the same state for a long time. This is evident from the long stretches of repeated values in Figure 24.7(b). If we set the proposal’s variance just right, we get the trace in Figure 24.7(c), where the samples clearly explore the support of the target distribution. We discuss how to tune the proposal below.\n",
      "\n",
      "One big advantage of Gibbs sampling is that one does not need to choose the proposal\n",
      "\n",
      "24.3. Metropolis Hastings algorithm\n",
      "\n",
      "851\n",
      "\n",
      "MH with N(0,1.0002) proposal\n",
      "\n",
      "MH with N(0,500.0002) proposal\n",
      "\n",
      "0.2\n",
      "\n",
      "0.06\n",
      "\n",
      "0.04\n",
      "\n",
      "0.1\n",
      "\n",
      "0.02\n",
      "\n",
      "0 0\n",
      "\n",
      "0 0\n",
      "\n",
      "200\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "100\n",
      "\n",
      "400\n",
      "\n",
      "100\n",
      "\n",
      "600\n",
      "\n",
      "50\n",
      "\n",
      "600\n",
      "\n",
      "50\n",
      "\n",
      "800\n",
      "\n",
      "−50\n",
      "\n",
      "0\n",
      "\n",
      "800\n",
      "\n",
      "−50\n",
      "\n",
      "0\n",
      "\n",
      "Iterations\n",
      "\n",
      "1000\n",
      "\n",
      "−100\n",
      "\n",
      "Samples\n",
      "\n",
      "Iterations\n",
      "\n",
      "1000\n",
      "\n",
      "−100\n",
      "\n",
      "Samples\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "MH with N(0,8.0002) proposal\n",
      "\n",
      "0.03\n",
      "\n",
      "0.02\n",
      "\n",
      "0.01\n",
      "\n",
      "0 0\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "100\n",
      "\n",
      "600\n",
      "\n",
      "50\n",
      "\n",
      "800\n",
      "\n",
      "−50\n",
      "\n",
      "0\n",
      "\n",
      "Iterations\n",
      "\n",
      "1000\n",
      "\n",
      "−100\n",
      "\n",
      "Samples\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 24.7 An example of the Metropolis Hastings algorithm for sampling from a mixture of two 1D Gaussians (μ = (−20, 20), π = (0.3, 0.7), σ = (100, 100)), using a Gaussian proposal with variances of (a) When v = 1, the chain gets trapped near the starting state and fails to sample from v ∈ {1, 500, 8}. the mode at μ = −20. (b) When v = 500, the chain is very “sticky”, so its effective sample size is low (as reﬂected by the rough histogram approximation at the end). (c) Using a variance of v = 8 is just right and leads to a good approximation of the true distribution (shown in red). Figure generated by mcmcGmmDemo. Based on code by Christophe Andrieu and Nando de Freitas.\n",
      "\n",
      "distribution, and furthermore, the acceptance rate is 100%. Of course, a 100% acceptance can trivially be achieved by using a proposal with variance 0 (assuming we start at a mode), but this is obviously not exploring the posterior. So having a high acceptance is not the ultimate goal. We can increase the amount of exploration by increasing the variance of the Gaussian kernel. Often one experiments with different parameters until the acceptance rate is between 25% and 40%, which theory suggests is optimal, at least for Gaussian target distributions. These short initial runs, used to tune the proposal, are called pilot runs.\n",
      "\n",
      "852\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "0.2\n",
      "\n",
      "0.18\n",
      "\n",
      "1500\n",
      "\n",
      "w0 intercept\n",
      "\n",
      "1500\n",
      "\n",
      "w1 slope\n",
      "\n",
      "0.16\n",
      "\n",
      "0.14\n",
      "\n",
      "0.12\n",
      "\n",
      "1000\n",
      "\n",
      "1000\n",
      "\n",
      "1 w\n",
      "\n",
      "0.1\n",
      "\n",
      "0.08\n",
      "\n",
      "0.06\n",
      "\n",
      "500\n",
      "\n",
      "500\n",
      "\n",
      "0.04\n",
      "\n",
      "0.02\n",
      "\n",
      "0 −120\n",
      "\n",
      "−100\n",
      "\n",
      "−80\n",
      "\n",
      "−60 w0\n",
      "\n",
      "−40\n",
      "\n",
      "−20\n",
      "\n",
      "0\n",
      "\n",
      "0 −120\n",
      "\n",
      "−100\n",
      "\n",
      "−80\n",
      "\n",
      "−60\n",
      "\n",
      "−40\n",
      "\n",
      "−20\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.05\n",
      "\n",
      "0.1\n",
      "\n",
      "0.15\n",
      "\n",
      "0.2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 24.8 (a) Joint posterior of the parameters for 1d logistic regression when applied to some SAT data. (b) Marginal for the offset w0. (c) Marginal for the slope w1. We see that the marginals do not capture the fact that the parameters are highly correlated. Figure generated by logregSatMhDemo.\n",
      "\n",
      "24.3.3.1\n",
      "\n",
      "Gaussian proposals\n",
      "\n",
      "If we have a continuous state space, the Hessian H at a local mode ˆw can be used to deﬁne the covariance of a Gaussian proposal distribution. This approach has the advantage that the Hessian models the local curvature and length scales of each dimension; this approach therefore avoids some of the slow mixing behavior of Gibbs sampling shown in Figure 24.6.\n",
      "\n",
      "There are two obvious approaches: (1) an independence proposal, q(w(cid:2)|w) = N (w(cid:2)| ˆw, H−1) or (2), a random walk proposal, q(w(cid:2)|w) = N (w(cid:2)|w, s2H−1), where s2 is a scale factor chosen to facilitate rapid mixing. (Roberts and Rosenthal 2001) prove that, if the posterior is Gaussian, the asymptotically optimal value is to use s2 = 2.382/D, where D is the dimensionality of w; this results in an acceptance rate of 0.234.\n",
      "\n",
      "For example, consider MH for binary logistic regression. From Equation 8.7, we have that the Hessian of the log-likelihood is Hl = XT DX, where D = diag(μi(1 − μi)) and μi = sigm( ˆwT xi). If we assume a Gaussian prior, p(w) = N (0, V0), we have H = V−1 0 + Hl, so the asymptotically optimal Gaussian proposal has the form\n",
      "\n",
      "q(w(cid:2)|w) = N\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "w, 2.382 D\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "V−1\n",
      "\n",
      "0 + XT DX\n",
      "\n",
      "(cid:23)−1\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(24.53)\n",
      "\n",
      "See (Gamerman 1997; Rossi et al. 2006; Fruhwirth-Schnatter and Fruhwirth 2010) for further details. The approach is illustrated in Figure 24.8, where we sample parameters from a 1d logistic regression model ﬁt to some SAT data. We initialize the chain at the mode, computed using IRLS, and then use the above random walk Metropolis sampler.\n",
      "\n",
      "If you cannot afford to compute the mode or its Hessian XDX, an alternative approach,\n",
      "\n",
      "suggested in (Scott 2009), is to approximate the above proposal as follows:\n",
      "\n",
      "q(w(cid:2)|w) = N\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "w,\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "V−1\n",
      "\n",
      "0 +\n",
      "\n",
      "6 π2\n",
      "\n",
      "XT X\n",
      "\n",
      "(cid:9)−1\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(24.54)\n",
      "\n",
      "24.3. Metropolis Hastings algorithm\n",
      "\n",
      "853\n",
      "\n",
      "24.3.3.2 Mixture proposals\n",
      "\n",
      "If one doesn’t know what kind of proposal to use, one can try a mixture proposal, which is a convex combination of base proposals:\n",
      "\n",
      "q(x(cid:2)|x) =\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "wkqk(x(cid:2)|x)\n",
      "\n",
      "(24.55)\n",
      "\n",
      "k=1\n",
      "\n",
      "where wk are the mixing weights. As long as each qk is individually valid, the overall proposal will also be valid.\n",
      "\n",
      "24.3.3.3\n",
      "\n",
      "Data-driven MCMC\n",
      "\n",
      "The most efficient proposals depend not just on the previous hidden state, but also the visible data, i.e., they have the form q(x(cid:2)|x, D). This is called data-driven MCMC (see e.g., (Tu and Zhu 2002)). To create such proposals, one can sample (x, D) pairs from the forwards model and then train a discriminative classiﬁer to predict p(x|f (D)), where f (D) are some features extracted from the visible data.\n",
      "\n",
      "Typically x is a high-dimensional vector (e.g., position and orientation of all the limbs of a person in a visual object detector), so it is hard to predict the entire state vector, p(x|f (D)). Instead we might train a discriminative detector to predict parts of the state-space, p(xk|fk(D)), such as the location of just the face of a person. We can then use a proposal of the form\n",
      "\n",
      "q(x(cid:2)|x, D) = π0q0(x(cid:2)|x) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "πkqk(x(cid:2)\n",
      "\n",
      "k|fk(D))\n",
      "\n",
      "(24.56)\n",
      "\n",
      "k\n",
      "\n",
      "where q0 is a standard data-independent proposal (e.g., random walk), and qk updates the k’th component of the state space. For added efficiency, the discriminative proposals should suggest joint changes to multiple variables, but this is often hard to do.\n",
      "\n",
      "The overall procedure is a form of generate and test: the discriminative proposals q(x(cid:2)|x) p(x(cid:2)|D) p(x|D) , to generate new hypotheses, which are then “tested” by computing the posterior ratio see if the new hypothesis is better or worse. By adding an annealing step, one can modify the algorithm to ﬁnd posterior modes; this is called simulated annealing, and is described in Section 24.6.1. One advantage of using the mode-seeking version of the algorithm is that we do not need to ensure the proposal distribution is reversible.\n",
      "\n",
      "24.3.4\n",
      "\n",
      "Adaptive MCMC\n",
      "\n",
      "One can change the parameters of the proposal as the algorithm is running to increase efficiency. This is called adaptive MCMC. This allows one to start with a broad covariance (say), allowing large moves through the space until a mode is found, followed by a narrowing of the covariance to ensure careful exploration of the region around the mode.\n",
      "\n",
      "However, one must be careful not to violate the Markov property; thus the parameters of the proposal should not depend on the entire history of the chain. It turns out that a sufficient condition to ensure this is that the adaption is “faded out” gradually over time. See e.g., (Andrieu and Thoms 2008) for details.\n",
      "\n",
      "854\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "24.3.5\n",
      "\n",
      "Initialization and mode hopping\n",
      "\n",
      "It is necessary to start MCMC in an initial state that has non-zero probability. If the model has deterministic constraints, ﬁnding such a legal conﬁguration may be a hard problem in itself. It is therefore common to initialize MCMC methods at a local mode, found using an optimizer.\n",
      "\n",
      "In some domains (especially with discrete state spaces), it is a more effective use of computa- tion time to perform multiple restarts of an optimizer, and to average over these modes, rather than exploring similar points around a local mode. However, in continuous state spaces, the mode contains negligible volume (Section 5.2.1.3), so it is necessary to locally explore around each mode, in order to visit enough posterior probability mass.\n",
      "\n",
      "24.3.6 Why MH works *\n",
      "\n",
      "To prove that the MH procedure generates samples from p∗, we have to use a bit of Markov chain theory, so be sure to read Section 17.2.3 ﬁrst.\n",
      "\n",
      "The MH algorithm deﬁnes a Markov chain with the following transition matrix:\n",
      "\n",
      "p(x(cid:2)|x) =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "q(x(cid:2)|x)r(x(cid:2)|x) q(x|x) +\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "x(cid:2)(cid:5)=x q(x(cid:2)|x)(1 − r(x(cid:2)|x))\n",
      "\n",
      "if x(cid:2) (cid:8)= x otherwise\n",
      "\n",
      "(24.57)\n",
      "\n",
      "This follows from a case analysis: if you move to x(cid:2) from x, you must have proposed it (with probability q(x(cid:2)|x)) and it must have been accepted (with probability r(x(cid:2)|x)); otherwise you stay in state x, either because that is what you proposed (with probability q(x|x)), or because you proposed something else (with probability q(x(cid:2)|x)) but it was rejected (with probability 1 − r(x(cid:2)|x)).\n",
      "\n",
      "Let us analyse this Markov chain. Recall from Section 17.2.3.4 that a chain satisﬁes detailed\n",
      "\n",
      "balance if\n",
      "\n",
      "(24.58) We also showed that if a chain satisﬁes detailed balance, then p∗ is its stationary distribution. Our goal is to show that the MH algorithm deﬁnes a transition function that satisﬁes detailed balance and hence that p∗ is its stationary distribution. (If Equation 24.58 holds, we say that p∗ is an invariant distribution wrt the Markov transition kernel q.)\n",
      "\n",
      "p(x(cid:2)|x)p∗(x) = p(x|x(cid:2))p∗(x(cid:2))\n",
      "\n",
      "Theorem 24.3.1. If the transition matrix deﬁned by the MH algorithm (given by Equation 24.57) is ergodic and irreducible, then p∗ is its unique limiting distribution. Proof. Consider two states x and x(cid:2). Either\n",
      "\n",
      "p∗(x)q(x(cid:2)|x) < p∗(x(cid:2))q(x|x(cid:2))\n",
      "\n",
      "(24.59)\n",
      "\n",
      "or\n",
      "\n",
      "p∗(x)q(x(cid:2)|x) > p∗(x(cid:2))q(x|x(cid:2))\n",
      "\n",
      "(24.60)\n",
      "\n",
      "We will ignore ties (which occur with probability zero for continuous distributions). Without loss of generality, assume that p∗(x)q(x(cid:2)|x) > p∗(x(cid:2))q(x|x(cid:2)). Hence\n",
      "\n",
      "α(x(cid:2)|x) =\n",
      "\n",
      "p∗(x(cid:2))q(x|x(cid:2)) p∗(x)q(x(cid:2)|x)\n",
      "\n",
      "< 1\n",
      "\n",
      "(24.61)\n",
      "\n",
      "24.3. Metropolis Hastings algorithm\n",
      "\n",
      "855\n",
      "\n",
      "Hence we have r(x(cid:2)|x) = α(x(cid:2)|x) and r(x|x(cid:2)) = 1.\n",
      "\n",
      "Now to move fromx to x(cid:2) we must ﬁrst propose x(cid:2) and then accept it. Hence\n",
      "\n",
      "p(x(cid:2)|x) = q(x(cid:2)|x)r(x(cid:2)|x) = q(x(cid:2)|x)\n",
      "\n",
      "p∗(x(cid:2))q(x|x(cid:2)) p∗(x)q(x(cid:2)|x)\n",
      "\n",
      "=\n",
      "\n",
      "p∗(x(cid:2)) p∗(x)\n",
      "\n",
      "q(x|x(cid:2))\n",
      "\n",
      "(24.62)\n",
      "\n",
      "Hence\n",
      "\n",
      "p∗(x)p(x(cid:2)|x) = p∗(x(cid:2))q(x|x(cid:2))\n",
      "\n",
      "(24.63)\n",
      "\n",
      "The backwards probability is\n",
      "\n",
      "p(x|x(cid:2)) = q(x|x(cid:2))r(x|x(cid:2)) = q(x|x(cid:2))\n",
      "\n",
      "(24.64)\n",
      "\n",
      "since r(x|x(cid:2)) = 1. Inserting this into Equation 24.63 we get\n",
      "\n",
      "(24.65) so detailed balance holds wrt p∗. Hence, from Theorem 17.2.3, p∗ is a stationary distribution. Furthermore, from Theorem 17.2.2, this distribution is unique, since the chain is ergodic and irreducible.\n",
      "\n",
      "p∗(x)p(x(cid:2)|x) = p∗(x(cid:2))p(x|x(cid:2))\n",
      "\n",
      "24.3.7\n",
      "\n",
      "Reversible jump (trans-dimensional) MCMC *\n",
      "\n",
      "Suppose we have a set of models with different numbers of parameters, e.g., mixture models in which the number of mixture components is unknown. Let the model be denoted by m, and let its unknowns (e.g., parameters) be denoted by xm ∈ Xm (e.g., Xm = Rnm, where nm is the dimensionality of model m). Sampling in spaces of differing dimensionality is called trans- dimensional MCMC (Green 2003). We could sample the model indicator m ∈ {1, . . . , M } and m=1 Xm, but this is very inefficient. It is sample all the parameters from the product space more parsimonious to sample in the union space X = ∪M m=1{m} × Xm, where we only worry about parameters for the currently active model.\n",
      "\n",
      "(cid:26)M\n",
      "\n",
      "The difficulty with this approach arises when we move between models of different dimen- sionality. The trouble is that when we compute the MH acceptance ratio, we are comparing densities deﬁned in different dimensionality spaces, which is meaningless. It is like trying to compare a sphere with a circle. The solution, proposed by (Green 1998) and known as reversible jump MCMC or RJMCMC, is to augment the low dimensional space with extra random variables so that the two spaces have a common measure.\n",
      "\n",
      "Unfortunately, we do not have space to go into details here. Suffice it to say that the method can be made to work in theory, although it is a bit tricky in practice. If, however, the continuous parameters can be integrated out (resulting in a method called collapsed RJMCMC), much of the difficulty goes away, since we are just left with a discrete state space, where there is no need to worry about change of measure. For example, (Denison et al. 2002) includes many examples of applications of collapsed RJMCMC applied to Bayesian inference fro adaptive basis-function models. They sample basis functions from a ﬁxed set of candidates (e.g., centered on the data points), and integrate out the other parameters analytically. This provides a Bayesian alternative to using RVMs or SVMs.\n",
      "\n",
      "856\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "Initial Condition X = 10 0\n",
      "\n",
      "Initial Condition X = 17 0\n",
      "\n",
      "p(0)(x)\n",
      "\n",
      "p(0)(x)\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "p(1)(x)\n",
      "\n",
      "p(1)(x)\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "p(2)(x)\n",
      "\n",
      "p(2)(x)\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "p(3)(x)\n",
      "\n",
      "p(3)(x)\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "p(10)(x)\n",
      "\n",
      "p(10)(x)\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "p(100)(x)\n",
      "\n",
      "p(100)(x)\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "p(200)(x)\n",
      "\n",
      "p(200)(x)\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "p(400)(x)\n",
      "\n",
      "p(400)(x)\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 24.9 Illustration of convergence to the uniform distribution over {0, 1, . . . ,20} using a symmetric random walk starting from (left) state 10, and (right) state 17. Based on Figures 29.14 and 29.15 of (MacKay 2003). Figure generated by randomWalk0to20Demo.\n",
      "\n",
      "24.4\n",
      "\n",
      "Speed and accuracy of MCMC\n",
      "\n",
      "In this section, we discuss a number of important theoretical and practical issues to do with MCMC.\n",
      "\n",
      "24.4.1\n",
      "\n",
      "The burn-in phase\n",
      "\n",
      "We start MCMC from an arbitrary initial state. As we explained in Section 17.2.3, only when the chain has “forgotten” where it started from will the samples be coming from the chain’s stationary distribution. Samples collected before the chain has reached its stationary distribution do not come from p∗, and are usually thrown away. The initial period, whose samples will be ignored, is called the burn-in phase.\n",
      "\n",
      "For example, consider a uniform distribution on the integers {0, 1, . . . , 20}. Suppose we sample from this using a symmetric random walk. In Figure 24.9, we show two runs of the algorithm. On the left, we start in state 10; on the right, we start in state 17. Even in this small problem it takes over 100 steps until the chain has “forgotten” where it started from.\n",
      "\n",
      "It is difficult to diagnose when the chain has burned in, an issue we discuss in more detail below. (This is one of the fundamental weaknesses of MCMC.) As an interesting example of what can happen if you start collecting samples too early, consider the Potts model. Figure 24.10(a), likes shows a sample after 500 iterations of Gibbs sampling. This suggests that the model\n",
      "\n",
      "24.4. Speed and accuracy of MCMC\n",
      "\n",
      "857\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 24.10 Illustration of problems caused by poor mixing. (a) One sample from a 5-state Potts model on a 128 × 128 grid with 8 nearest neighbor connectivity and J = 2/3 (as in (Geman and Geman 1984)), after 200 iterations. (b) One sample from the same model after 10,000 iterations. Used with kind permission of Erik Sudderth.\n",
      "\n",
      "medium-sized regions where the label is the same, implying the model would make a good prior for image segmentation. Indeed, this was suggested in the original Gibbs sampling paper (Geman and Geman 1984).\n",
      "\n",
      "However, it turns out that if you run the chain long enough, you get isolated speckles, as in Figure 24.10(b). The results depend on the coupling strength, but in general, it is very hard to ﬁnd a setting which produces nice medium-sized blobs: most parameters result in a few super-clusters, or lots of small fragments. In fact, there is a rapid phase transition between these two regimes. This led to a paper called “The Ising/Potts model is not well suited to segmentation tasks” (Morris et al. 1996). It is possible to create priors more suited to image segmentation (e.g., (Sudderth and Jordan 2008)), but the main point here is that sampling before reaching convergence can lead to erroneous conclusions.\n",
      "\n",
      "24.4.2 Mixing rates of Markov chains *\n",
      "\n",
      "The amount of time it takes for a Markov chain to converge to the stationary distribution, and forget its initial state, is called the mixing time. More formally, we say that the mixing time from state x0 is the minimal time such that, for any constant (cid:16) > 0, we have that\n",
      "\n",
      "τ(cid:16)(x0) (cid:2) min{t : ||δx0 (x)T t − p∗||1 ≤ (cid:16)}\n",
      "\n",
      "(24.66)\n",
      "\n",
      "where δx0 (x) is a distribution with all its mass in state x0, T is the transition matrix of the chain (which depends on the target p∗ and the proposal q), and δx0 (x)T t is the distribution after t steps. The mixing time of the chain is deﬁned as\n",
      "\n",
      "τ(cid:16) (cid:2) max x0\n",
      "\n",
      "τ(cid:16)(x0)\n",
      "\n",
      "(24.67)\n",
      "\n",
      "The mixing time is determined by the eigengap γ = λ1 − λ2, which is the difference of the\n",
      "\n",
      "858\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "x5\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x4\n",
      "\n",
      "x6\n",
      "\n",
      "x3\n",
      "\n",
      "x7\n",
      "\n",
      "Figure 24.11 A Markov chain with low conductance. The dotted arcs represent transitions with very low Source: Figure 12.6 of (Koller and Friedman 2009). Used with kind permission of Daphne probability. Koller.\n",
      "\n",
      "ﬁrst and second eigenvalues of the transition matrix. In particular, one can show that\n",
      "\n",
      "τ(cid:16) ≤ O(\n",
      "\n",
      "1 γ\n",
      "\n",
      "log\n",
      "\n",
      "n (cid:16)\n",
      "\n",
      ")\n",
      "\n",
      "(24.68)\n",
      "\n",
      "where n is the number of states. Since computing the transition matrix can be hard to do, especially for high dimensional and/or continuous state spaces, it is useful to ﬁnd other ways to estimate the mixing time.\n",
      "\n",
      "An alternative approach is to examine the geometry of the state space. For example, consider the chain in Figure 24.11. We see that the state space consists of two “islands”, each of which is connected via a narrow “bottleneck”. (If they were completely disconnected, the chain would not be ergodic, and there would no longer be a unique stationary distribution.) We deﬁne the conductance φ of a chain as the minimum probability, over all subsets of states, of transitioning from that set to its complement:\n",
      "\n",
      "φ (cid:2)\n",
      "\n",
      "min S:0≤p∗(S)≤0.5\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "x∈S,x(cid:2)∈Sc T (x → x(cid:2)) p∗(S)\n",
      "\n",
      ",\n",
      "\n",
      "(24.69)\n",
      "\n",
      "One can show that\n",
      "\n",
      "τ(cid:16) ≤ O(\n",
      "\n",
      "1 φ2 log\n",
      "\n",
      "n (cid:16)\n",
      "\n",
      ")\n",
      "\n",
      "(24.70)\n",
      "\n",
      "Hence chains with low conductance have high mixing time. For example, distributions with well-separated modes usually have high mixing time. Simple MCMC methods often do not work well in such cases, and more advanced algorithms, such as parallel tempering, are necessary (see e.g., (Liu 2001)).\n",
      "\n",
      "24.4.3\n",
      "\n",
      "Practical convergence diagnostics\n",
      "\n",
      "Computing the mixing time of a chain is in general quite difficult, since the transition matrix is usually very hard to compute. In practice various heuristics have been proposed to diagnose\n",
      "\n",
      "24.4. Speed and accuracy of MCMC\n",
      "\n",
      "859\n",
      "\n",
      "convergence — see (Geyer 1992; Cowles and Carlin 1996; Brooks and Roberts 1998) for a review. Strictly speaking, these methods do not diagnose convergence, but rather non-convergence. That is, the method may claim the chain has converged when in fact it has not. This is a ﬂaw common to all convergence diagnostics, since diagnosing convergence is computationally intractable in general (Bhatnagar et al. 2010).\n",
      "\n",
      "One of the simplest approaches to assessing when the method has converged is to run multiple chains from very different overdispersed starting points, and to plot the samples of some variables of interest. This is called a trace plot. If the chain has mixed, it should have “forgotten” where it started from, so the trace plots should converge to the same distribution, and thus overlap with each other.\n",
      "\n",
      "Figure 24.12 gives an example. We show the traceplot for x which was sampled from a mixture of two 1D Gaussians using four different methods: MH with a symmetric Gaussian proposal of variance σ2 ∈ {1, 8, 500}, and Gibbs sampling. We see that σ2 = 1 has not mixed, which is also evident from Figure 24.7(a), which shows that a single chain never leaves the area where it started. The results for the other methods indicate that the chains rapidly converge to (The sticky nature of the σ2 = 500 the stationary distribution, no matter where they started. proposal is very evident. This reduces the computational efficiency, as we discuss below, but not the statistical validity.)\n",
      "\n",
      "24.4.3.1\n",
      "\n",
      "Estimated potential scale reduction (EPSR)\n",
      "\n",
      "We can assess convergence more quantitatively as follows. The basic idea is to compare the variance of a quantity within each chain to its variance across chains. More precisely, suppose we collect S samples (after burn-in) from each of C chains of D variables, xisc, i = 1 : D, s = 1 :S , c = 1 :C . Let ysc be a scalar quantity of interest derived from x1:D,s,c (e.g., ysc = xisc for some chosen i). Deﬁne the within-sequence mean and overall mean as\n",
      "\n",
      "y·c (cid:2) 1 S\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "ysc, y·· (cid:2) 1 C\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "c=1\n",
      "\n",
      "y·c\n",
      "\n",
      "(24.71)\n",
      "\n",
      "Deﬁne the between-sequence and within-sequence variance as\n",
      "\n",
      "B (cid:2)\n",
      "\n",
      "S C − 1\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "c=1\n",
      "\n",
      "(y·c − y··)2, W (cid:2) 1 C\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "c=1\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "1 S − 1\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "(ysc − y·c)2\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(24.72)\n",
      "\n",
      "We can now construct two estimates of the variance of y. The ﬁrst estimate is W : this should underestimate var [y] if the chains have not ranged over the full posterior. The second estimate is\n",
      "\n",
      "ˆV =\n",
      "\n",
      "S − 1 S\n",
      "\n",
      "W +\n",
      "\n",
      "1 S\n",
      "\n",
      "B\n",
      "\n",
      "(24.73)\n",
      "\n",
      "This is an estimate of var [y] that is unbiased under stationarity, but is an overestimate if the starting points were overdispersed (Gelman and Rubin 1992). From this, we can deﬁne the following convergence diagnostic statistic, known as the estimated potential scale reduction or EPSR:\n",
      "\n",
      "8\n",
      "\n",
      "ˆR (cid:2)\n",
      "\n",
      "ˆV W\n",
      "\n",
      "(24.74)\n",
      "\n",
      "860\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "MH N(0,1.0002), Rhat = 1.493\n",
      "\n",
      "MH N(0,8.0002), Rhat = 1.039\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "40\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "−20\n",
      "\n",
      "0\n",
      "\n",
      "−40\n",
      "\n",
      "−10\n",
      "\n",
      "0\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "600\n",
      "\n",
      "800\n",
      "\n",
      "1000\n",
      "\n",
      "−60\n",
      "\n",
      "0\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "600\n",
      "\n",
      "800\n",
      "\n",
      "1000\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "50\n",
      "\n",
      "MH N(0,500.0002), Rhat = 1.005\n",
      "\n",
      "60\n",
      "\n",
      "gibbs, Rhat = 1.007\n",
      "\n",
      "40\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "20\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−10\n",
      "\n",
      "−20\n",
      "\n",
      "−20\n",
      "\n",
      "−30\n",
      "\n",
      "−40\n",
      "\n",
      "−40\n",
      "\n",
      "−50\n",
      "\n",
      "0\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "600\n",
      "\n",
      "800\n",
      "\n",
      "1000\n",
      "\n",
      "−60\n",
      "\n",
      "0\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "600\n",
      "\n",
      "800\n",
      "\n",
      "1000\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 24.12 Traceplots for MCMC samplers. Each color represents the samples from a different starting (a-c) MH with proposal N (x(cid:2)|x, σ2) for σ2 ∈ {1, 8, 500}, corresponding to Figure 24.7. point. (d) Gibbs sampling. Figure generated by mcmcGmmDemo.\n",
      "\n",
      "This quantity, which was ﬁrst proposed in (Gelman and Rubin 1992), measures the degree to which the posterior variance would decrease if we were to continue sampling in the S → If ˆR ≈ 1 for any given quantity, then that estimate is reliable (or at least is not ∞ limit. unreliable). The ˆR values for the four samplers in Figure 24.12 are 1.493, 1.039, 1.005 and 1.007. So this diagnostic has correctly identiﬁed that the sampler using the ﬁrst (σ2 = 1) proposal is untrustworthy.\n",
      "\n",
      "24.4.4\n",
      "\n",
      "Accuracy of MCMC\n",
      "\n",
      "The samples produced by MCMC are auto-correlated, and this reduces their information content relative to independent or “perfect” samples. We can quantify this as follows.4 Suppose we want\n",
      "\n",
      "4. This Section is based on (Hoff 2009, Sec 6.6).\n",
      "\n",
      "24.4. Speed and accuracy of MCMC\n",
      "\n",
      "861\n",
      "\n",
      "MH N(0,1.0002)\n",
      "\n",
      "MH N(0,8.0002)\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.9\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "1.2\n",
      "\n",
      "MH N(0,500.0002)\n",
      "\n",
      "1.2\n",
      "\n",
      "gibbs\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "−0.2\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "25\n",
      "\n",
      "30\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 24.13 Autocorrelation functions corresponding to Figure 24.12. Figure generated by mcmcGmmDemo.\n",
      "\n",
      "to estimate the mean of f (X), for some function f , whereX ∼ p(). Denote the true mean by\n",
      "\n",
      "f ∗ (cid:2) E [f (X)]\n",
      "\n",
      "(24.75)\n",
      "\n",
      "A Monte Carlo estimate is given by\n",
      "\n",
      "f =\n",
      "\n",
      "1 S\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "fs\n",
      "\n",
      "(24.76)\n",
      "\n",
      "862\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "where fs (cid:2) f (xs) and xs ∼ p(x). An MCMC estimate of the variance of this estimate is given by\n",
      "\n",
      "VarM CM C[f ] =E\n",
      "\n",
      "= VarM C(f ) +\n",
      "\n",
      "=\n",
      "\n",
      "= E\n",
      "\n",
      "1 S2\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "(f − f ∗)2 ⎡ 3\n",
      "\n",
      "⎣\n",
      "\n",
      "E\n",
      "\n",
      "1 S (cid:24)\n",
      "\n",
      "s=1\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "(fs − f ∗)2\n",
      "\n",
      "(fs − f ∗)\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "1 S2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "s(cid:5)=t\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "E [(fs − f ∗)(ft − f ∗)]\n",
      "\n",
      "42\n",
      "\n",
      "+\n",
      "\n",
      "⎤\n",
      "\n",
      "⎦\n",
      "\n",
      "1 S2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "s(cid:5)=t\n",
      "\n",
      "E [(fs − f ∗)(ft − f ∗)]\n",
      "\n",
      "(24.80)\n",
      "\n",
      "(24.79)\n",
      "\n",
      "(24.78)\n",
      "\n",
      "(24.77)\n",
      "\n",
      "where the ﬁrst term is the Monte Carlo estimate of the variance if the samples weren’t correlated, and the second term depends on the correlation of the samples. We can measure this as follows. Deﬁne the sample-based auto-correlation at lag t of a set of samples f1, . . . , fS as follows:\n",
      "\n",
      "ρt (cid:2)\n",
      "\n",
      "1 S−t\n",
      "\n",
      "(cid:7)S−t\n",
      "\n",
      "s=1 (fs − f )(fs+t − f ) 1 s=1(fs − f )2 S−1\n",
      "\n",
      "(cid:7)S\n",
      "\n",
      "(24.81)\n",
      "\n",
      "This is called the autocorrelation function (ACF). This is plotted in Figure 24.13 for our four samplers for the Gaussian mixture model. We see that the ACF of the Gibbs sampler (bottom right) dies off to 0 much more rapidly than the MH samplers, indicating that each Gibbs sample is “worth” more than each MH sample.\n",
      "\n",
      "A simple method to reduce the autocorrelation is to use thinning, in which we keep every n’th sample. This does not increase the efficiency of the underlying sampler, but it does save space, since it avoids storing highly correlated samples.\n",
      "\n",
      "sample size (ESS) S\n",
      "\n",
      "We can estimate the information content of a set of samples by computing the effective\n",
      "\n",
      "eff, deﬁned by\n",
      "\n",
      "S\n",
      "\n",
      "eff\n",
      "\n",
      "(cid:2) VarM C(f ) VarM CM C(f )\n",
      "\n",
      "(24.82)\n",
      "\n",
      "From Figure 24.12, it is clear that the effective sample size of the Gibbs sampler is higher than that of the other samplers (in this example).\n",
      "\n",
      "24.4.5\n",
      "\n",
      "How many chains?\n",
      "\n",
      "A natural question to ask is: how many chains should we run? We could either run one long chain to ensure convergence, and then collect samples spaced far apart, or we could run many In practice it is common to run a medium short chains, but that wastes the burnin time. number of chains (say 3) of medium length (say 100,000 steps), and to take samples from each after discarding the ﬁrst half of the samples. If we initialize at a local mode, we may be able to use all the samples, and not wait for burn-in.\n",
      "\n",
      "24.5. Auxiliary variable MCMC *\n",
      "\n",
      "863\n",
      "\n",
      "Model Goal Method Probit MAP Gradient EM Probit MAP EP Post Probit Gibbs+ Post Probit Post Probit Gibbs with ARS Post MH using IRLS proposal Probit MAP Gradient Logit Post Logit Post Logit\n",
      "\n",
      "Gibbs+ with Student Gibbs+ with KS\n",
      "\n",
      "Reference Section 9.4.1 Section 11.4.6 (Nickisch and Rasmussen 2008) Exercise 24.6 (Dellaportas and Smith 1993) (Gamerman 1997) Section 8.3.4 (Fruhwirth-Schnatter and Fruhwirth 2010) (Holmes and Held 2006)\n",
      "\n",
      "Table 24.1 Summary of some possible algorithms for estimation and inference for binary classiﬁcation problems using Gaussian priors. Abbreviations: Aux. = auxiliary variable sampling, ARS = adaptive rejection sampling, EP = expectation propagation, Gibbs+ = Gibbs sampling with auxiliary variables, IRLS = iterative reweighted least squares, KS = Kolmogorov Smirnov, MAP = maximum a posteriori, MH = Metropolis Hastings, Post = posterior.\n",
      "\n",
      "24.5\n",
      "\n",
      "Auxiliary variable MCMC *\n",
      "\n",
      "Sometimes we can dramatically improve the efficiency of sampling by introducing dummy auxiliary variables, in order to reduce correlation between the original variables. If the original variables are denoted by x, and the auxiliary variables by z, we require that z p(x, z) = p(x), and that p(x, z) is easier to sample from than just p(x). If we meet these two conditions, we can sample in the enlarged model, and then throw away the sampled z values, thereby recovering samples from p(x). We give some examples below.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "24.5.1\n",
      "\n",
      "Auxiliary variable sampling for logistic regression\n",
      "\n",
      "In Section 9.4.2, we discussed the latent variable interpretation of probit regression. Recall that this had the form\n",
      "\n",
      "zi (cid:2) wT xi + (cid:16)i (cid:16)i ∼ N (0, 1)\n",
      "\n",
      "(24.83)\n",
      "\n",
      "(24.84)\n",
      "\n",
      "yi = 1 = I(zi ≥ 0)\n",
      "\n",
      "(24.85)\n",
      "\n",
      "We exploited this representation in Section 11.4.6, where we used EM to ﬁnd an ML estimate. It is straightforward to convert this into an auxiliary variable Gibbs sampler (Exercise 24.6), since p(w|D) is Gaussian and p(zi|xi, yi, w) is truncated Gaussian, both of which are easy to sample from.\n",
      "\n",
      "Let (cid:16)i follow a logistic distribution, with pdf\n",
      "\n",
      "Now let us discuss how to derive an auxiliary variable Gibbs sampler for logistic regression.\n",
      "\n",
      "pLogistic((cid:16)) =\n",
      "\n",
      "e−(cid:16) (1 + e−(cid:16))2\n",
      "\n",
      "(24.86)\n",
      "\n",
      "with mean E [(cid:16)] = 0 and variance var [(cid:16)] = π2/3. The cdf has the form F ((cid:16)) = sigm((cid:16)), which\n",
      "\n",
      "864\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "is the logistic function. Since yi = 1 iff wT xi + (cid:16) > 0, we have, by symmetry, that\n",
      "\n",
      "p(yi = 1|xi, w) =\n",
      "\n",
      "(cid:12) ∞\n",
      "\n",
      "−wT xi\n",
      "\n",
      "f ((cid:16))d(cid:16) =\n",
      "\n",
      "(cid:12) wT xi\n",
      "\n",
      "−∞\n",
      "\n",
      "f ((cid:16))d(cid:16) = F (wT xi) = sigm(wT xi) (24.87)\n",
      "\n",
      "as required.\n",
      "\n",
      "We can derive an auxiliary variable Gibbs sampler by sampling from p(z|w, D) and p(w|z, D). Unfortunately, sampling directly from p(w|z, D) is not possible. One approach is to deﬁne (cid:16)i ∼ N (0, λi), where λi = (2ψi)2 and ψi ∼ KS, the Kolmogorov Smirnov distribution, and then to sample w, z, λ and ψ (Holmes and Held 2006).\n",
      "\n",
      "A simpler approach is to approximate the logistic distribution by the Student distribution (Albert and Chib 1993). Speciﬁcally, we will make the approximation (cid:16)i ∼ T (0, 1, ν), where ν ≈ 8. We can now use the scale mixture of Gaussians representation of the Student to simplify inference. In particular, we write\n",
      "\n",
      "λi ∼ Ga(ν/2, ν/2) (cid:16)i ∼ N (0, λ−1 ) zi (cid:2) wT xi + (cid:16)i yi = 1|zi = I(zi ≥ 0)\n",
      "\n",
      "i\n",
      "\n",
      "(24.88)\n",
      "\n",
      "(24.89)\n",
      "\n",
      "(24.90)\n",
      "\n",
      "(24.91)\n",
      "\n",
      "All of the full conditionals now have a simple form; see Exercise 24.7 for the details.\n",
      "\n",
      "Note that if we set ν = 1, then zi ∼ N (wT xi, 1), which is equivalent to probit regression (see Section 9.4). Rather than choosing between probit or logit regression, we can simply estimate the ν parameter. There is no convenient conjugate prior, but we can consider a ﬁnite range of possible values and evaluate the posterior as follows:\n",
      "\n",
      "p(ν|λ) ∝ p(ν)\n",
      "\n",
      "N(cid:20)\n",
      "\n",
      "i=1\n",
      "\n",
      "1 Γ(ν/2)(ν/2)ν/2\n",
      "\n",
      "λν/2−1 i\n",
      "\n",
      "e−νλi/2\n",
      "\n",
      "(24.92)\n",
      "\n",
      "Furthermore, if we deﬁne V0 = v0I, we can sample v0 as well. For example, suppose we use a IG(δ1, δ2) prior for v0. The posterior is given by p(v0|w) = IG(δ1 + 1 j=1 w2 j ). This can be interleaved with the other Gibbs sampling steps, and provides an appealing Bayesian alternative to cross validation for setting the strength of the regularizer.\n",
      "\n",
      "2 D, δ2 + 1\n",
      "\n",
      "2\n",
      "\n",
      "(cid:7)D\n",
      "\n",
      "See Table 24.1 for a summary of various algorithms for ﬁtting probit and logit models. Many of these methods can also be extended to the multinomial logistic regression case. For details, see (Scott 2009; Fruhwirth-Schnatter and Fruhwirth 2010).\n",
      "\n",
      "24.5.2\n",
      "\n",
      "Slice sampling\n",
      "\n",
      "Consider sampling from a univariate, but multimodal, distribution ˜p(x). We can sometimes improve the ability to make large moves by adding an auxiliary variable u. We deﬁne the joint distribution as follows: 1/Zp 0\n",
      "\n",
      "ˆp(x, u) =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "if 0 ≤ u ≤ ˜p(x) otherwise\n",
      "\n",
      "(24.93)\n",
      "\n",
      "24.5. Auxiliary variable MCMC *\n",
      "\n",
      "865\n",
      "\n",
      "180\n",
      "\n",
      "160\n",
      "\n",
      "140\n",
      "\n",
      "f(x )(i)\n",
      "\n",
      "120\n",
      "\n",
      "100\n",
      "\n",
      "u\n",
      "\n",
      "(i+1)\n",
      "\n",
      "x\n",
      "\n",
      "(i+1)\n",
      "\n",
      "80\n",
      "\n",
      "60\n",
      "\n",
      "40\n",
      "\n",
      "20\n",
      "\n",
      "x\n",
      "\n",
      "(i)\n",
      "\n",
      "x\n",
      "\n",
      "0 −5\n",
      "\n",
      "−4\n",
      "\n",
      "−3\n",
      "\n",
      "−2\n",
      "\n",
      "−1\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(a) Illustration of the principle behind slice sampling. Given a previous sample xi, we Figure 24.14 sample ui+1 uniformly on [0, f (xi)], where f is the target density. We then sample xi+1 along the slice where f (x) ≥ ui+1. Source: Figure 15 of (Andrieu et al. 2003) . Used with kind permission of Nando de Freitas. (b) Slice sampling in action. Figure generated by sliceSamplingDemo1d.\n",
      "\n",
      "x 10\n",
      "\n",
      "−11\n",
      "\n",
      "4\n",
      "\n",
      "y t i s n e d\n",
      "\n",
      "r o i r e t s o P\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0 6\n",
      "\n",
      "5\n",
      "\n",
      "4\n",
      "\n",
      "Slope\n",
      "\n",
      "3\n",
      "\n",
      "−1\n",
      "\n",
      "−1.5\n",
      "\n",
      "Intercept\n",
      "\n",
      "−2\n",
      "\n",
      "−2.5\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 24.15 Binomial regression for 1d data. approximation. Figure generated by sliceSamplingDemo2d.\n",
      "\n",
      "(a) Grid approximation to posterior.\n",
      "\n",
      "(b) Slice sampling\n",
      "\n",
      "where Zp =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "ˆp(x, u)du =\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "˜p(x)dx. The marginal distribution over x is given by\n",
      "\n",
      "(cid:12) ˜p(x)\n",
      "\n",
      "0\n",
      "\n",
      "1 Zp\n",
      "\n",
      "du =\n",
      "\n",
      "˜p(x) Zp\n",
      "\n",
      "= p(x)\n",
      "\n",
      "(24.94)\n",
      "\n",
      "so we can sample from p(x) by sampling from ˆp(x, u) and then ignoring u. The full conditionals have the form\n",
      "\n",
      "(24.96) where A = {x : ˜p(x) ≥ u} is the set of points on or above the chosen height u. This corresponds to a slice through the distribution, hence the term slice sampling (Neal 2003a). See Figure 24.14(a).\n",
      "\n",
      "In practice, it can be difficult to identify the set A. So we can use the following approach: construct an interval xmin ≤ x ≤ xmax around the current point xs of some width. We then\n",
      "\n",
      "p(u|x) = U[0, ˜p(x)](u) p(x|u) = UA(x)\n",
      "\n",
      "(24.95)\n",
      "\n",
      "866\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "test to see if each end point lies within the slice. If it does, we keep extending in that direction until it lies outside the slice. This is called stepping out. A candidate value x(cid:2) is then chosen If it lies within the slice, it is kept, so xs+1 = x(cid:2). Otherwise we uniformly from this region. shrink the region such that x(cid:2) forms one end and such that the region still contains xs. Then another sample is drawn. We continue in this way until a sample is accepted.\n",
      "\n",
      "To apply the method to multivariate distributions, we can sample one extra auxiliary variable for each dimension. The advantage of slice sampling over Gibbs is that it does not need a speciﬁcation of the full-conditionals, just the unnormalized joint. The advantage of slice sampling over MH is that it does not need a user-speciﬁed proposal distribution (although it does require a speciﬁcation of the width of the stepping out interval).\n",
      "\n",
      "Figure 24.14(b) illustrates the algorithm in action on a synthetic 1d problem. Figure 24.15 illustrates its behavior on a slightly harder problem, namely binomial logistic regression. The model has the form\n",
      "\n",
      "yi ∼ Bin(ni, logit(β1 + β2xi))\n",
      "\n",
      "(24.97)\n",
      "\n",
      "We use a vague Gaussian prior for the βj’s. Figure 24.15(a) shows a grid-based approximation to the posterior, and Figure 24.15(b) shows a sample-based approximation. In this example, the grid is faster to compute, but for any problem with more than 2 dimensions, the grid approach is infeasible.\n",
      "\n",
      "24.5.3\n",
      "\n",
      "Swendsen Wang\n",
      "\n",
      "Consider an Ising model of the following form:\n",
      "\n",
      "p(x) =\n",
      "\n",
      "1 Z\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "e\n",
      "\n",
      "fe(xe)\n",
      "\n",
      "(24.98)\n",
      "\n",
      "where xe = (xi, xj) for edge e = (i, j), xi ∈ {+1, −1}, and the edge factor fe is deﬁned by (cid:8)\n",
      "\n",
      ", where J is the edge strength. Gibbs sampling in such models can be slow when J is large in absolute value, because neighboring states can be highly correlated. The Swendsen Wang algorithm (Swendsen and Wang 1987) is a auxiliary variable MCMC sampler which mixes much faster, at least for the case of attractive or ferromagnetic models, with J > 0.\n",
      "\n",
      "eJ e−J\n",
      "\n",
      "e−J eJ\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "variables, and will be denoted by z. We then deﬁne an extended model p(x, z) of the form\n",
      "\n",
      "Suppose we introduce auxiliary binary variables, one per edge.\n",
      "\n",
      "5 These are called bond\n",
      "\n",
      "p(x, z) =\n",
      "\n",
      "1 Z (cid:2)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "e\n",
      "\n",
      "ge(xe, ze)\n",
      "\n",
      "(24.99)\n",
      "\n",
      "where ze ∈ {0, 1}, and we deﬁne the new factor as follows: ge(xe, ze = 0) =\n",
      "\n",
      "and ge(xe, ze = 1) =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "eJ − e−J 0\n",
      "\n",
      "0 eJ − e−J\n",
      "\n",
      "(cid:9)\n",
      "\n",
      ".\n",
      "\n",
      "It is clear that\n",
      "\n",
      "(cid:7)1\n",
      "\n",
      "ze=0 ge(xe, ze) = fe(xe),\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "e−J e−J\n",
      "\n",
      "e−J e−J\n",
      "\n",
      "(cid:9)\n",
      "\n",
      ",\n",
      "\n",
      "5. Our presentation of the method is based on some notes by David Mackay, available from http://www.inference .phy.cam.ac.uk/mackay/itila/swendsen.pdf.\n",
      "\n",
      "24.5. Auxiliary variable MCMC *\n",
      "\n",
      "867\n",
      "\n",
      "Figure 24.16 Illustration of the Swendsen Wang algorithm on a 2d grid. Used with kind permission of Kevin Tang.\n",
      "\n",
      "and hence that throw away the z samples and get valid x samples from the original distribution.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "z p(x, z) = p(x). So if we can sample from this extended model, we can just\n",
      "\n",
      "Fortunately, it is easy to apply Gibbs sampling to this extended model. The full conditional p(z|x) factorizes over the edges, since the bond variables are conditionally independent given the node variables. Furthermore, the full conditional p(ze|xe) is simple to compute: if the nodes on either end of the edge are in the same state (xi = xj), we set the bond ze to 1 with probability p = 1 − e−2J , otherwise we set it to 0. In Figure 24.16 (top right), the bonds that could be turned on (because their corresponding nodes are in the same state) are represented by dotted edges. In Figure 24.16 (bottom right), the bonds that are randomly turned on are represented by solid edges.\n",
      "\n",
      "To sample p(x|z), we proceed as follows. Find the connected components deﬁned by the graph induced by the bonds that are turned on. (Note that a connected component may consist of a singleton node.) Pick one of these components uniformly at random. All the nodes in each such component must have the same state, since the off-diagonal terms in the ge(xe, ze = 1) factor are 0. Pick a state ±1 uniformly at random, and force all the variables in this component to adopt this new state. This is illustrated in Figure 24.16 (bottom left), where the green square\n",
      "\n",
      "868\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "denotes the selected connected component, and we choose to force all nodes within in to enter the white state.\n",
      "\n",
      "The validity of this algorithm is left as an exercise, as is the extension to handle local evidence\n",
      "\n",
      "and non-stationary potentials.\n",
      "\n",
      "It should be intuitively clear that Swendsen Wang makes much larger moves through the state space than Gibbs sampling. In fact, SW mixes much faster than Gibbs sampling on 2d lattice Ising models for a variety of values of the coupling parameter, provided J > 0. More precisely, let the edge strength be parameterized by J/T , where T > 0 is a computational temperature. For large T , the nodes are roughly independent, so both methods work equally well. However, as T approaches a critical temperature Tc, the typical states of the system have very long correlation lengths, and Gibbs sampling takes a very long time to generate independent samples. As the temperature continues to drop, the typical states are either all on or all off. The frequency with which Gibbs sampling moves between these two modes is exponentiall small. By contrast, SW mixes rapidly at all temperatures.\n",
      "\n",
      "Unfortunately, if any of the edge weights are negative, J < 0, the system is frustrated, and there are exponentially many modes, even at low temperature. SW does not work very well in this setting, since it tries to force many neighboring variables to have the same state. In fact, computation in this regime is provably hard for any algorithm (Jerrum and Sinclair 1993, 1996).\n",
      "\n",
      "24.5.4\n",
      "\n",
      "Hybrid/Hamiltonian MCMC *\n",
      "\n",
      "In this section, we brieﬂy mention a way to perform MCMC sampling for continuous state spaces, for which we can compute the gradient of the (unnormalized) log-posterior. This is the case in neural network models, for example.\n",
      "\n",
      "The basic idea is to think of the parameters as a particle in space, and to create auxiliary variables which represent the “momentum” of this particle. We then update this parameter/ momentum pair according to certain rules (see e.g., (Duane et al. 1987; Neal 1993; MacKay 2003; Neal 2010) for details). The resulting method is called hybrid MCMC or Hamiltonian MCMC. The two main parameters that the user must specify are how many leapfrog steps to take when updating the position/ momentum, and how big to make these steps. Performance can be quite sensitive to these parameters (although see (Hoffman and Gelman 2011) for a recent way to set them automatically). This method can be combined with stochastic gradient descent (Section 8.5.2) in order to handle large datasets, as explained in (Ahn et al. 2012).\n",
      "\n",
      "Recently, a more powerful extension of this method has been developed, that exploits second-\n",
      "\n",
      "order gradient information. See (Girolami et al. 2010) for details.\n",
      "\n",
      "24.6\n",
      "\n",
      "Annealing methods\n",
      "\n",
      "Many distributions are multimodal and hence hard to sample from. However, by analogy to the way metals are heated up and then cooled down in order to make the molecules align, we can imagine using a computational temperature parameter to smooth out a distribution, gradually cooling it to recover the original “bumpy” distribution. We ﬁrst explain this idea in more detail in the context of an algorithm for MAP estimation. We then discuss extensions to the sampling case.\n",
      "\n",
      "24.6. Annealing methods\n",
      "\n",
      "869\n",
      "\n",
      "temp 1.000\n",
      "\n",
      "temp 0.200\n",
      "\n",
      "5 x 10\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "8\n",
      "\n",
      "8\n",
      "\n",
      "6\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0 60\n",
      "\n",
      "0 60\n",
      "\n",
      "40\n",
      "\n",
      "y\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "x\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "40\n",
      "\n",
      "y\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "x\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 24.17 An energy surface at different temperatures. Note the different vertical scales. (b) T = 0.5. Figure generated by saDemoPeaks.\n",
      "\n",
      "(a) T = 1.\n",
      "\n",
      "24.6.1\n",
      "\n",
      "Simulated annealing\n",
      "\n",
      "Simulated annealing (Kirkpatrick et al. 1983) is a stochastic algorithm that attempts to ﬁnd the global optimum of a black-box function f (x). It is closely related to the Metropolis- Hastings algorithm for generating samples from a probability distribution, which we discussed in Section 24.3. SA can be used for both discrete and continuous optimization.\n",
      "\n",
      "which speciﬁes that the probability of being in any particular state x is given by\n",
      "\n",
      "The method is inspired by statistical physics. The key quantity is the Boltzmann distribution,\n",
      "\n",
      "p(x) ∝ exp(−f (x)/T )\n",
      "\n",
      "(24.100)\n",
      "\n",
      "where f (x) is the “energy” of the system and T is the computational temperature. As the temperature approaches 0 (so the system is cooled), the system spends more and more time in its minimum energy (most probable) state.\n",
      "\n",
      "Figure 24.17 gives an example of a 2d function at different temperatures. At high temperatures, T (cid:30) 1, the surface is approximately ﬂat, and hence it is easy to move around (i.e., to avoid local optima). As the temperature cools, the largest peaks become larger, and the smallest peaks disappear. By cooling slowly enough, it is possible to “track” the largest peak, and thus ﬁnd the global optimum. This is an example of a continuation method.\n",
      "\n",
      "We can generate an algorithm from this as follows. At each step, sample a new state according to some proposal distribution x(cid:2) ∼ q(·|xk). For real-valued parameters, this is often simply a random walk proposal, x(cid:2) = xk + (cid:11)k, where (cid:11)k ∼ N (0, Σ). For discrete optimization, other kinds of local moves must be deﬁned.\n",
      "\n",
      "Having proposed a new state, we compute α = exp ((f (x) − f (x(cid:2)))/T )\n",
      "\n",
      "(24.101)\n",
      "\n",
      "We then accept the new state (i.e., set xk+1 = x(cid:2)) with probability min(1, α), otherwise we stay in the current state (i.e., set xk+1 = xk). This means that if the new state has lower energy (is more probable), we will deﬁnitely accept it, but it it has higher energy (is less probable), we might still accept, depending on the current temperature. Thus the algorithm allows “down-hill” moves in probability space (up-hill in energy space), but less frequently as the temperature drops.\n",
      "\n",
      "870\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "1\n",
      "\n",
      "temperature vs iteration\n",
      "\n",
      "−4.5\n",
      "\n",
      "energy vs iteration\n",
      "\n",
      "0.9\n",
      "\n",
      "−5\n",
      "\n",
      "0.8\n",
      "\n",
      "−5.5\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "−6\n",
      "\n",
      "0.5\n",
      "\n",
      "−6.5\n",
      "\n",
      "0.4\n",
      "\n",
      "−7\n",
      "\n",
      "0.3\n",
      "\n",
      "−7.5\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "−8\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "600\n",
      "\n",
      "800\n",
      "\n",
      "1000\n",
      "\n",
      "1200\n",
      "\n",
      "−8.5\n",
      "\n",
      "0\n",
      "\n",
      "200\n",
      "\n",
      "400\n",
      "\n",
      "600\n",
      "\n",
      "800\n",
      "\n",
      "1000\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 24.18 A run of simulated annealing on the energy surface in Figure 24.17. iteration. (b) Energy vs iteration. Figure generated by saDemoPeaks.\n",
      "\n",
      "(a) Temperature vs\n",
      "\n",
      "iter 550, temp 0.064\n",
      "\n",
      "iter 1000, temp 0.007\n",
      "\n",
      "60\n",
      "\n",
      "150\n",
      "\n",
      "50\n",
      "\n",
      "40\n",
      "\n",
      "100\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "50\n",
      "\n",
      "10\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "30\n",
      "\n",
      "28\n",
      "\n",
      "26\n",
      "\n",
      "y\n",
      "\n",
      "24\n",
      "\n",
      "22\n",
      "\n",
      "35\n",
      "\n",
      "36\n",
      "\n",
      "37\n",
      "\n",
      "x\n",
      "\n",
      "38\n",
      "\n",
      "39\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "28\n",
      "\n",
      "26\n",
      "\n",
      "y\n",
      "\n",
      "24\n",
      "\n",
      "22\n",
      "\n",
      "35\n",
      "\n",
      "36\n",
      "\n",
      "37\n",
      "\n",
      "x\n",
      "\n",
      "38\n",
      "\n",
      "39\n",
      "\n",
      "40\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 24.19 Histogram of samples from the annealed “posterior” at 2 different time points produced by simulated annealing on the energy surface shown in Figure 24.17. Note that at cold temperatures, most of the samples are concentrated near the peak at (38,25). Figure generated by saDemoPeaks.\n",
      "\n",
      "The rate at which the temperature changes over time is called the cooling schedule. It has been shown (Kirkpatrick et al. 1983) that if one cools sufficiently slowly, the algorithm will provably ﬁnd the global optimum. However, it is not clear what “sufficient slowly” means. In practice it is common to use an exponential cooling schedule of the following form: Tk = T0C k, where T0 is the initial temperature (often T0 ∼ 1) and C is the cooling rate (often C ∼ 0.8). See Figure 24.18(a) for a plot of this cooling schedule. Cooling too quickly means one can get stuck in a local maximum, but cooling too slowly just wastes time. The best cooling schedule is difficult to determine; this is one of the main drawbacks of simulated annealing.\n",
      "\n",
      "Figure 24.18(b) shows an example of simulated annealing applied to the function in Figure 24.17 using a random walk proposal. We see that the method stochastically reduces the energy over time. Figures 24.19 illustrate (a histogram of) samples drawn from the cooled probability distribution over time. We see that most of the samples are concentrated near the global maximum. When the algorithm has converged, we just return the largest value found.\n",
      "\n",
      "24.6. Annealing methods\n",
      "\n",
      "871\n",
      "\n",
      "24.6.2\n",
      "\n",
      "Annealed importance sampling\n",
      "\n",
      "We now describe a method known as annealed importance sampling (Neal 2001) that com- bines ideas from simulated annealing and importance sampling in order to draw independent samples from difficult (e.g., multimodal) distributions.\n",
      "\n",
      "Suppose we want to sample from p0(x) ∝ f0(x), but we cannot do so easily; for example, this might represent a multimodal posterior. Suppose however that there is an easier distribution which we can sample from, call it pn(x) ∝ fn(x); for example, this might be the prior. We can now construct a sequence of intermediate distributions than move slowly from pn to p0 as follows:\n",
      "\n",
      "fj(x) = f0(x)βj fn(x)1−βj\n",
      "\n",
      "(24.102)\n",
      "\n",
      "where 1 =β 0 > β1 > · · · > βn = 0, where βj is an inverse temperature. (Contrast this to the scheme used by simulated annealing which has the form fj(x) =f 0(x)βj ; this makes it hard to sample from pn.) Furthermore, suppose we have a series of Markov chains Tj(x, x(cid:2)) (from x to x(cid:2)) which leave each pj invariant. Given this, we can sample x from p0 by ﬁrst sampling a sequence z = (zn−1, . . . , z0) as follows: sample zn−1 ∼ pn; sample zn−2 ∼ Tn−1(zn−1, ·); ...; sample z0 ∼ T1(z1, ·). Finally we set x = z0 and give it weight\n",
      "\n",
      "w =\n",
      "\n",
      "fn−1(zn−1) fn(zn−1)\n",
      "\n",
      "fn−2(zn−2) fn−1(zn−2)\n",
      "\n",
      "· ·\n",
      "\n",
      "f1(z1) f2(z1)\n",
      "\n",
      "f0(z0) f1(z0)\n",
      "\n",
      "(24.103)\n",
      "\n",
      "This can be shown to be correct by viewing the algorithm as a form of importance sampling in an extended state space z = (z0, . . . , zn−1). Consider the following distribution on this state space:\n",
      "\n",
      "p(z) ∝ f (z) = f0(z0) ˜T1(z0, z1) ˜T2(z1, z2) · · · ˜Tn−1(zn−2, zn−1)\n",
      "\n",
      "(24.104)\n",
      "\n",
      "where ˜Tj is the reversal of Tj:\n",
      "\n",
      "˜Tj(z, z(cid:2)) = Tj(z(cid:2), z)pj(z(cid:2))/pj(z) = Tj(z(cid:2), z)fj(z(cid:2))/fj(z)\n",
      "\n",
      "(24.105)\n",
      "\n",
      "It is clear that z1,...,zn−1 sequences to recover the original ditribution.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "f (z) = f0(z0), so we can safely just use the z0 part of these\n",
      "\n",
      "Now consider the proposal distribution deﬁned by the algorithm:\n",
      "\n",
      "q(z) ∝ g(z) = fn(zn−1)Tn−1(zn−1, zn−2) · · ·T 2(z2, z1)T1(z1, z0)\n",
      "\n",
      "(24.106)\n",
      "\n",
      "One can show that the importance weights w =\n",
      "\n",
      "f (z0,...,zn−1) g(z0,...,zn−1) are given by Equation 24.103.\n",
      "\n",
      "24.6.3\n",
      "\n",
      "Parallel tempering\n",
      "\n",
      "Another way to combine MCMC and annealing is to run multiple chains in parallel at different temperatures, and allow one chain to sample from another chain at a neighboring temperature. In this way, the high temperature chain can make long distance moves through the state space, and have this inﬂuence lower temperature chains. This is known as parallel tempering. See e.g., (Earl and Deem 2005) for details.\n",
      "\n",
      "872\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "24.7\n",
      "\n",
      "Approximating the marginal likelihood\n",
      "\n",
      "The marginal likelihood p(D|M ) is a key quantity for Bayesian model selection, and is given by\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(D|M ) =\n",
      "\n",
      "p(D|θ, M )p(θ|M )dθ\n",
      "\n",
      "(24.107)\n",
      "\n",
      "Unfortunately, this integral is often intractable to compute, for example if we have non conjugate In this section, we brieﬂy discuss some ways to priors, and/or we have hidden variables. approximate this expression using Monte Carlo. See (Gelman and Meng 1998) for a more extensive review.\n",
      "\n",
      "24.7.1\n",
      "\n",
      "The candidate method\n",
      "\n",
      "There is a simple method for approximating the marginal likelihood known as the Candidate method (Chib 1995). This exploits the following identity:\n",
      "\n",
      "p(D|M ) =\n",
      "\n",
      "p(D|θ, M )p(θ|M ) p(θ|D, M )\n",
      "\n",
      "(24.108)\n",
      "\n",
      "This holds for any value of θ. Once we have picked some value, we can evaluate p(D|θ, M ) and p(θ|M ) quite easily. If we have some estimate of the posterior near θ, we can then evaluate the denominator as well. This posterior is often approximated using MCMC.\n",
      "\n",
      "The ﬂaw with this method is that it relies on the assumption that p(θ|D, M ) has marginalized over all the modes of the posterior, which in practice is rarely possible. Consequently the method can give very inaccurate results in practice (Neal 1998).\n",
      "\n",
      "24.7.2\n",
      "\n",
      "Harmonic mean estimate\n",
      "\n",
      "Newton and Raftery (1994) proposed a simple method for approximating p(D) using the output of MCMC, as follows:\n",
      "\n",
      "1/p(D) ≈ 1 S\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "1 p(D|θs)\n",
      "\n",
      "(24.109)\n",
      "\n",
      "where θs ∼ p(θ|D). This expression is the harmonic mean of the likelihood of the data under each sample. The theoretical correctness of this expression follows from the following identity:\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "1 p(D|θ)\n",
      "\n",
      "p(θ|D)dθ =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "1 p(D|θ)\n",
      "\n",
      "p(D|θ)p(θ) p(D)\n",
      "\n",
      "dθ =\n",
      "\n",
      "1 p(D)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(θ|D)dθ =\n",
      "\n",
      "1 p(D)\n",
      "\n",
      "(24.110)\n",
      "\n",
      "Unfortunately, in practice this method works very poorly. Indeed, Radford Neal called this “the worst Monte Carlo method ever”.6. The reason it is so bad is that it depends only on samples drawn from the posterior. But the posterior is often very insensitive to the prior, whereas the marginal likelihood is not. We only mention this method in order to warn against its use. We present a better method below.\n",
      "\n",
      "6. Source: radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-mon te-carlo-method-ever.\n",
      "\n",
      "24.7. Approximating the marginal likelihood\n",
      "\n",
      "873\n",
      "\n",
      "24.7.3\n",
      "\n",
      "Annealed importance sampling\n",
      "\n",
      "We can use annealed importance sampling (Section 24.6.2) to evaluate a ratio of partition functions. Notice that Z0 = g(z)dz. Hence\n",
      "\n",
      "Z0 Zn\n",
      "\n",
      "=\n",
      "\n",
      "(cid:21) (cid:21)\n",
      "\n",
      "f (z)dz g(z)dz\n",
      "\n",
      "=\n",
      "\n",
      "(cid:21) f (z) g(z) g(z)dz (cid:21) g(z)dz\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "f0(x)dx =\n",
      "\n",
      "= Eq\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "f (z)dz, and Zn = (cid:17)\n",
      "\n",
      "f (z) g(z)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "≈ 1 S\n",
      "\n",
      "S(cid:4)\n",
      "\n",
      "s=1\n",
      "\n",
      "ws\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "fn(x)dx =\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(24.111)\n",
      "\n",
      "If fn is a prior and f0 is the posterior, we can estimate Zn = p(D) using the above equation, provided the prior has a known normalization constant Z0. This is generally considered the method of choice for evaluating difficult partition functions.\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 24.1 Gibbs sampling from a 2D Gaussian Suppose x ∼ N (μ, Σ), where μ = (1, 1) and Σ = (1, −0.5; −0.5, 1). Derive the full condition- als p(x1|x2) and p(x2|x1). Implement the algorithm and plot the 1d marginals p(x1) and p(x2) as histograms. Superimpose a plot of the exact marginals.\n",
      "\n",
      "Exercise 24.2 Gibbs sampling for a 1D Gaussian mixture model Consider applying Gibbs sampling to a univariate mixture of Gaussians, as in Section 24.2.3. Derive the expressions for the full conditionals. Hint: if we know zn = j (say), then μj gets “connected” to xn, but all other values of μi, for all i (cid:13)= j, are irrelevant. (This is an example of context-speciﬁc independence, where the structure of the graph simpliﬁes once we have assigned values to some of the nodes.) Hence, given all the zn values, the posteriors of the μ’s should be independent, so the conditional of μj should be independent of μ−j. (Similarly for σj.)\n",
      "\n",
      "Exercise 24.3 Gibbs sampling from the Potts model Modify the code in gibbsDemoIsing to draw samples from a Potts prior at different temperatures, as in Figure 19.8.\n",
      "\n",
      "Exercise 24.4 Full conditionals for hierarchical model of Gaussian means Let us reconsider the Gaussian-Gaussian model parameters θj. 2009, p134)), that we use the following conjugate priors on the hyper-parameters:\n",
      "\n",
      "in Section 5.6.2 for modelling multiple related mean In this exercise we derive a Gibbs sampler instead of using EB. Suppose, following (Hoff\n",
      "\n",
      "μ ∼ N (μ0, γ2 0 ) τ 2 ∼ IG(η0/2, η0τ 2 σ2 ∼ IG(ν0/2, ν0σ2\n",
      "\n",
      "0 /2) 0/2)\n",
      "\n",
      "(24.112)\n",
      "\n",
      "(24.113)\n",
      "\n",
      "(24.114)\n",
      "\n",
      "874\n",
      "\n",
      "Chapter 24. Markov chain Monte Carlo (MCMC) inference\n",
      "\n",
      "We can set η = (μ0, γ0, η0, τ0, ν0, σ0) to uninformative values. Given this model speciﬁcation, show that the full conditionals for μ, τ , σ and the θj are as follows:\n",
      "\n",
      "p(θj|μ, τ 2, Dj, σ2) =N (θj|\n",
      "\n",
      "p(μ|θ1:D, τ 2) =N (μ|\n",
      "\n",
      "p(τ 2|θ1:D, μ) = IG(τ 2|\n",
      "\n",
      "Dθ/τ 2 + μ0/γ2 0 D/τ 2 + 1/γ2 0 Njxj/σ2 + 1/τ 2 Nj/σ2 + 1/τ 2 η0τ 2 0 + η0 + D 2\n",
      "\n",
      ",\n",
      "\n",
      ", [D/τ 2 + 1/γ2\n",
      "\n",
      ", [Nj/σ2 + 1/τ 2]−1) (cid:2)\n",
      "\n",
      "j(θj − μ)2 2\n",
      "\n",
      "0 ]−1)\n",
      "\n",
      ")\n",
      "\n",
      "(24.115)\n",
      "\n",
      "(24.116)\n",
      "\n",
      "(24.117)\n",
      "\n",
      "p(σ2|θ1:D, D) = IG(σ2| 1 2\n",
      "\n",
      "[ν0 +\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "j=1\n",
      "\n",
      "Nj], 1 2\n",
      "\n",
      "[ν0σ2\n",
      "\n",
      "0 +\n",
      "\n",
      "D(cid:12)\n",
      "\n",
      "j=1\n",
      "\n",
      "Nj(cid:12)\n",
      "\n",
      "(xij − θj)2])\n",
      "\n",
      "i=1\n",
      "\n",
      "(24.118)\n",
      "\n",
      "Exercise 24.5 Gibbs sampling for robust linear regression with a Student t likelihood Modify the EM algorithm in Exercise 11.12 to perform Gibbs sampling for p(w, σ2, z|D, ν).\n",
      "\n",
      "Exercise 24.6 Gibbs sampling for probit regression Modify the EM algorithm in Section 11.4.6 to perform Gibbs sampling for p(w, z|D). Hint: we can sample from a truncated Gaussian, N (z|μ, σ)I(a ≤ z ≤ b) in two steps: ﬁrst sample u ∼ U (Φ((a − μ)/σ), Φ((b − μ)/σ)), then set z = μ + σΦ−1(u) (Robert 1995).\n",
      "\n",
      "Exercise 24.7 Gibbs sampling for logistic regression with the Student approximation Derive the full conditionals for the joint model deﬁned by Equations 24.88 to 24.91.\n",
      "\n",
      "25 Clustering\n",
      "\n",
      "25.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Clustering is the process of grouping similar objects together. There are two kinds of inputs we might use. In similarity-based clustering, the input to the algorithm is an N ×N dissimilarity matrix or distance matrix D. In feature-based clustering, the input to the algorithm is an N × D feature matrix or design matrix X. Similarity-based clustering has the advantage that it allows for easy inclusion of domain-speciﬁc similarity or kernel functions (Section 14.2). Feature- based clustering has the advantage that it is applicable to “raw”, potentially noisy data. We will see examples of both below.\n",
      "\n",
      "In addition to the two types of input, there are two possible types of output: ﬂat cluster- ing, also called partitional clustering, where we partition the objects into disjoint sets; and hierarchical clustering, where we create a nested tree of partitions. We will discuss both of these below. Not surprisingly, ﬂat clusterings are usually faster to create (O(N D) for ﬂat vs O(N 2 log N ) for hierarchical), but hierarchical clusterings are often more useful. Furthermore, most hierarchical clustering algorithms are deterministic and do not require the speciﬁcation of K, the number of clusters, whereas most ﬂat clustering algorithms are sensitive to the initial conditions and require some model selection method for K. (We will discuss how to choose K in more detail below.)\n",
      "\n",
      "The ﬁnal distinction we will make in this chapter is whether the method is based on a probabilistic model or not. One might wonder why we even bother discussing non-probabilistic methods for clustering. The reason is two-fold: ﬁrst, they are widely used, so readers should know about them; second, they often contain good ideas, which can be used to speed up inference in a probabilistic models.\n",
      "\n",
      "25.1.1 Measuring (dis)similarity\n",
      "\n",
      "A dissimilarity matrix D is a matrix where di,i = 0 and di,j ≥ 0 is a measure of “distance” between objects i and j. Subjectively judged dissimilarities are seldom distances in the strict sense, since the triangle inequality, di,j ≤ di,k + dj,k, often does not hold. Some algorithms require D to be a true distance matrix, but many do not. If we have a similarity matrix S, we can convert it to a dissimilarity matrix by applying any monotonically decreasing function, e.g., D = max(S) − S.\n",
      "\n",
      "The most common way to deﬁne dissimilarity between objects is in terms of the dissimilarity\n",
      "\n",
      "876\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "of their attributes:\n",
      "\n",
      "Δ(xi, xi(cid:2) ) =\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "Δj(xij, xi(cid:2)j)\n",
      "\n",
      "(25.1)\n",
      "\n",
      "j=1\n",
      "\n",
      "Some common attribute dissimilarity functions are as follows:\n",
      "\n",
      "Squared (Euclidean) distance:\n",
      "\n",
      "Δj(xij, xi(cid:2)j) = (xij − xi(cid:2)j)2\n",
      "\n",
      "(25.2)\n",
      "\n",
      "Of course, this only makes sense if attribute j is real-valued.\n",
      "\n",
      "Squared distance strongly emphasizes large differences (because differences are squared). A\n",
      "\n",
      "more robust alternative is to use an (cid:2)1 distance:\n",
      "\n",
      "Δj(xij, xi(cid:2)j) = |xij − xi(cid:2)j|\n",
      "\n",
      "(25.3)\n",
      "\n",
      "This is also called city block distance, since, in 2D, the distance can be computed by counting how many rows and columns we have to move horizontally and vertically to get from xi to xi(cid:2) .\n",
      "\n",
      "\n",
      "\n",
      "If xi is a vector (e.g., a time-series of real-valued data), it is common to use the correlation j xijxi(cid:2)j, coefficient (see Section 2.5.1). j(xij − xi(cid:2)j)2 = 2(1 − corr [xi, xi(cid:2) ]). So clustering based on correlation and hence (similarity) is equivalent to clustering based on squared distance (dissimilarity).\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "If the data is standardized, then corr [xi, xi(cid:2) ] =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "For ordinal variables, such as {low, medium, high}, it is standard to encode the values as real-valued numbers, say 1/3, 2/3, 3/3 if there are 3 possible values. One can then apply any dissimilarity function for quantitative variables, such as squared distance.\n",
      "\n",
      "For categorical variables, such as {red, green, blue}, we usually assign a distance of 1 if the features are different, and a distance of 0 otherwise. Summing up over all the categorical features gives\n",
      "\n",
      "Δ(xi, xi) =\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "I(xij (cid:8)= xi(cid:2)j)\n",
      "\n",
      "(25.4)\n",
      "\n",
      "j=1\n",
      "\n",
      "This is called the hamming distance.\n",
      "\n",
      "25.1.2\n",
      "\n",
      "Evaluating the output of clustering methods *\n",
      "\n",
      "The validation of clustering structures is the most difficult and frustrating part of cluster analysis. Without a strong effort in this direction, cluster analysis will remain a black art accessible only to those true believers who have experience and great courage. — Jain and Dubes (Jain and Dubes 1988)\n",
      "\n",
      "25.1.\n",
      "\n",
      "Introduction\n",
      "\n",
      "877\n",
      "\n",
      "(cid:36)(cid:3)(cid:36)(cid:3)(cid:36)(cid:3) (cid:36)(cid:3)(cid:36)(cid:3)(cid:37)\n",
      "\n",
      "(cid:36)(cid:3)(cid:37)(cid:3)(cid:37) (cid:37)(cid:3)(cid:37)(cid:3)(cid:38)\n",
      "\n",
      "(cid:36)(cid:3)(cid:36)(cid:3) (cid:38)(cid:3)(cid:38)(cid:3)(cid:38)\n",
      "\n",
      "Figure 25.1 Three clusters with labeled objects inside. Based on Figure 16.4 of (Manning et al. 2008).\n",
      "\n",
      "Clustering is an unupervised learning technique, so it is hard to evaluate the quality of the output of any given method. If we use probabilistic models, we can always evaluate the likelihood of a test set, but this has two drawbacks: ﬁrst, it does not directly assess any clustering that is discovered by the model; and second, it does not apply to non-probabilistic methods. So now we discuss some performance measures not based on likelihood.\n",
      "\n",
      "Intuitively, the goal of clustering is to assign points that are similar to the same cluster, and to ensure that points that are dissimilar are in different clusters. There are several ways of measuring these quantities e.g., see (Jain and Dubes 1988; Kaufman and Rousseeuw 1990). However, these internal criteria may be of limited use. An alternative is to rely on some external form of data with which to validate the method. For example, suppose we have labels for each object, as in Figure 25.1. (Equivalently, we can have a reference clustering; given a clustering, we can induce a set of labels and vice versa.) Then we can compare the clustering with the labels using various metrics which we describe below. We will use some of these metrics later, when we compare clustering methods.\n",
      "\n",
      "25.1.2.1\n",
      "\n",
      "Purity\n",
      "\n",
      "Let Nij be the number of objects in cluster i that belong to class j, and let Ni = j=1 Nij be the total number of objects in cluster i. Deﬁne pij = Nij/Ni; this is the empirical distribution over class labels for cluster i. We deﬁne the purity of a cluster as pi (cid:2) maxj pij, and the overall purity of a clustering as (cid:4)\n",
      "\n",
      "purity (cid:2)\n",
      "\n",
      "i\n",
      "\n",
      "Ni N\n",
      "\n",
      "pi\n",
      "\n",
      "(cid:7)C\n",
      "\n",
      "(25.5)\n",
      "\n",
      "For example, in Figure 25.1, we have that the purity is\n",
      "\n",
      "6 17\n",
      "\n",
      "5 6\n",
      "\n",
      "+\n",
      "\n",
      "6 17\n",
      "\n",
      "4 6\n",
      "\n",
      "+\n",
      "\n",
      "5 17\n",
      "\n",
      "3 5\n",
      "\n",
      "=\n",
      "\n",
      "5 + 4 + 3 17\n",
      "\n",
      "= 0.71\n",
      "\n",
      "(25.6)\n",
      "\n",
      "The purity ranges between 0 (bad) and 1 (good). However, we can trivially achieve a purity of 1 by putting each object into its own cluster, so this measure does not penalize for the number of clusters.\n",
      "\n",
      "25.1.2.2\n",
      "\n",
      "Rand index\n",
      "\n",
      "Let U = {u1, . . . , uR} and V = {v1, . . . , VC} be two different partitions of the N data points, i.e., two different (ﬂat) clusterings. For example, U might be the estimated clustering and V is reference clustering derived from the class labels. Now deﬁne a 2 × 2 contingency table,\n",
      "\n",
      "878\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "containing the following numbers: T P is the number of pairs that are in the same cluster in both U and V (true positives); T N is the number of pairs that are in the different clusters in both U and V (true negatives); F N is the number of pairs that are in the different clusters in U but the same cluster in V (false negatives); and F P is the number of pairs that are in the same cluster in U but different clusters in V (false positives). A common summary statistic is the Rand index:\n",
      "\n",
      "R (cid:2)\n",
      "\n",
      "T P + T N T P + F P + F N + T N\n",
      "\n",
      "(25.7)\n",
      "\n",
      "This can be interpreted as the fraction of clustering decisions that are correct. Clearly 0 ≤ R ≤ 1.\n",
      "\n",
      "For example, consider Figure 25.1, The three clusters contain 6, 6 and 5 points, so the number\n",
      "\n",
      "of “positives” (i.e., pairs of objects put in the same cluster, regardless of label) is\n",
      "\n",
      "T P + F P =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "6 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "6 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "5 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "= 40\n",
      "\n",
      "(25.8)\n",
      "\n",
      "Of these, the number of true positives is given by\n",
      "\n",
      "T P =\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "5 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "5 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "3 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "+\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "2 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "= 20\n",
      "\n",
      "(25.9)\n",
      "\n",
      "where the last two terms come from cluster 3: there are pairs labeled A. So F P = 40 − 20 = 20. Similarly, one can show F N = 24 and T N = 72. So the Rand index is (20 + 72)/(20 + 20 + 24 + 72) = 0.68.\n",
      "\n",
      "The Rand index only achieves its lower bound of 0 if T P = T N = 0, which is a rare event.\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "3 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "pairs labeled C and\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "2 2\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "One can deﬁne an adjusted Rand index (Hubert and Arabie 1985) as follows:\n",
      "\n",
      "AR (cid:2) index − expected index\n",
      "\n",
      "max index − expected index\n",
      "\n",
      "(25.10)\n",
      "\n",
      "Here the model of randomness is based on using the generalized hyper-geometric distribution, i.e., the two partitions are picked at random subject to having the original number of classes and objects in each, and then the expected value of T P + T N is computed. This model can be used to compute the statistical signiﬁcance of the Rand index.\n",
      "\n",
      "The Rand index weights false positives and false negatives equally. Various other summary statistics for binary decision problems, such as the F-score (Section 5.7.2.2), can also be used. One can compute their frequentist sampling distribution, and hence their statistical signiﬁcance, using methods such as bootstrap.\n",
      "\n",
      "25.1.2.3 Mutual information\n",
      "\n",
      "Another way to measure cluster quality is to compute the mutual information between U and V (Vaithyanathan and Dom 1999). To do this, let pU V (i, j) = be the probability that a randomly chosen object belongs to cluster ui in U and vj in V . Also, let pU (i) = |ui|/N be the be the probability that a randomly chosen object belongs to cluster ui in U ; deﬁne\n",
      "\n",
      "|ui∩vj | N\n",
      "\n",
      "25.2. Dirichlet process mixture models\n",
      "\n",
      "879\n",
      "\n",
      "pV (j) = |vj|/N similarly. Then we have C(cid:4)\n",
      "\n",
      "I(U, V ) =\n",
      "\n",
      "R(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "pU V (i, j) log\n",
      "\n",
      "pU V (i, j) pU (i)pV (j)\n",
      "\n",
      "(25.11)\n",
      "\n",
      "This lies between 0 and min{H (U ) , H (V )}. Unfortunately, the maximum value can be achieved by using lots of small clusters, which have low entropy. To compensate for this, we can use the normalized mutual information,\n",
      "\n",
      "N M I(U, V ) (cid:2)\n",
      "\n",
      "I(U, V ) (H (U ) +H (V ))/2\n",
      "\n",
      "(25.12)\n",
      "\n",
      "This lies between 0 and 1. A version of this that is adjusted for chance (under a particular random data model) is described in (Vinh et al. 2009). Another variant, called variation of information, is described in (Meila 2005).\n",
      "\n",
      "25.2 Dirichlet process mixture models\n",
      "\n",
      "The simplest approach to (ﬂat) clustering is to use a ﬁnite mixture model, as we discussed in Section 11.2.3. This is sometimes called model-based clustering, since we deﬁne a probabilistic model of the data, and optimize a well-deﬁned objective (the likelihood or posterior), as opposed to just using some heuristic algorithm.\n",
      "\n",
      "The principle problem with ﬁnite mixture models is how to choose the number of components K. We discussed several techniques in Section 11.5. However, in many cases, there is no well- deﬁned number of clusters. Even in the simple 2d height-weight data (Figure 1.8), it is not clear if the “correct” value of K should be 2, 3, or 4. It would be much better if we did not have to choose K at all.\n",
      "\n",
      "In this section, we discuss inﬁnite mixture models, in which we do not impose any a priori bound on K. To do this, we will use a non-parametric prior based on the Dirichlet process It will also (DP). This allows the number of clusters to grow as the amount of data increases. prove useful later when we discuss hiearchical clustering.\n",
      "\n",
      "The topic of non-parametric Bayes is currently very active, and we do not have space to go into details (see (Hjort et al. 2010) for a recent book on the topic). Instead we just give a brief review of the DP and its application to mixture modeling, based on the presentation in (Sudderth 2006, sec 2.2).\n",
      "\n",
      "25.2.1\n",
      "\n",
      "From ﬁnite to inﬁnite mixture models\n",
      "\n",
      "Consider a ﬁnite mixture model, as shown in Figure 25.2(a). The usual representation is as follows:\n",
      "\n",
      "p(xi|zi = k, θ) =p( xi|θk)\n",
      "\n",
      "(25.13)\n",
      "\n",
      "p(zi = k|π) =π k\n",
      "\n",
      "(25.14)\n",
      "\n",
      "(25.15) The form of p(θk|λ) is chosen to be conjugate to p(xi|θk). We can write p(xi|θk) as xi ∼ F (θzi ), where F is the observation distribution. Similarly, we can write θk ∼ H(λ), where H is the prior.\n",
      "\n",
      "p(π|α) = Dir(π|(α/K)1K)\n",
      "\n",
      "880\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.2 Two different representations of a ﬁnite mixture model. Left: traditional representation. Right: representation where parameters are samples from G, a discrete measure. The picture on the right illustrates the case where K = 4, and we sample 4 Gaussian means θk from a Gaussian prior H(.|λ). The height of the spikes reﬂects the mixing weights πk. This weighted sum of delta functions is G. We then generate two parameters, θ1 and θ2, from G, one per data point. Finally, we generate two data points, x1 and x2, from N (θ1, σ2) and N (θ2, σ2). Source: Figure 2.9 of (Sudderth 2006) . Used with kind permission of Erik Sudderth.\n",
      "\n",
      "is the parameter used to generate observation xi; these parameters are sampled from distribution G, which has the form\n",
      "\n",
      "An equivalent representation for this model\n",
      "\n",
      "is shown in Figure 25.2(b). Here θi\n",
      "\n",
      "G(θ) =\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "πkδθk (θ)\n",
      "\n",
      "(25.16)\n",
      "\n",
      "k=1\n",
      "\n",
      "where π ∼ Dir( α K 1), and θk ∼ H. Thus we see that G is a ﬁnite mixture of delta functions, centered on the cluster parameters θk. The probability that θi is equal to θk is exactly πk, the prior probability for that cluster.\n",
      "\n",
      "If we sample from this model, we will always (with probability one) get exactly K clusters, with data points scattered around the cluster centers. We would like a more ﬂexible model, that can generate a variable number of clusters. Furthermore, the more data we generate, the more likely we should be to see a new cluster. The way to do this is to replace the discrete distribution G with a random probability measure. Below we will show that the Dirichlet process, denoted G ∼ DP(α, H), is one way to do this.\n",
      "\n",
      "Before we go into the details, we show some samples from this non-parametric model in Figure 25.3. We see that it has the desired properties of generating a variable number of clusters, with more clusters as the amount of data increases. The resulting samples look much more like real data than samples from a ﬁnite mixture model.\n",
      "\n",
      "Of course, working with an “inﬁnite” model sounds scary. Fortunately, as we show below, even though this model is potentially inﬁnite, we can perform inference using an amount of computation that is not only tractable, but is often much less than that required to ﬁt a set\n",
      "\n",
      "25.2. Dirichlet process mixture models\n",
      "\n",
      "881\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(e)\n",
      "\n",
      "(f)\n",
      "\n",
      "Figure 25.3 Some samples from a Dirichlet process mixture model of 2D Gaussians, with concentration parameter α = 1. From left to right, we show N = 50, N = 500 and N = 1000 samples. Each row is a different run. We also show the model parameters as ellipses, which are sampled from a vague NIW base distribution. Based on Figure 2.25 of (Sudderth 2006). Figure generated by dpmSampleDemo, written by Yee-Whye Teh.\n",
      "\n",
      "of ﬁnite mixture models for different K. The intuitive reason is that we can get evidence that certain values of K are appropriate (have high posterior support) long before we have been able to estimate the parameters, so we can focus our computational efforts on models of appropriate complexity. Thus going to the inﬁnite limit can sometimes be faster. This is especially true when we have multiple model selection problems to solve.\n",
      "\n",
      "882\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(a) A base measure H on a 2d space Θ. Figure 25.4 where the shading of cell Tk is proportional to E [G(Tk)] = H(Tk). regions. Source: Figure 2.21 of (Sudderth 2006). Used with kind permission of Erik Sudderth.\n",
      "\n",
      "(b) One possible partition into K = 3 regions, (c) A reﬁned partition into K = 5\n",
      "\n",
      "25.2.2\n",
      "\n",
      "The Dirichlet process\n",
      "\n",
      "Recall from Chapter 15 that a Gaussian process is a distribution over functions of the form It is deﬁned implicitly by the requirement that p(f (x1), . . . , f (xN )) be jointly f : X → R. Gaussian, for any set of points xi ∈ X . The parameters of this Gaussian can be computed using a mean function μ() and covariance (kernel) function K(). We write f ∼ GP(μ(), K()). Fur- thermore, the GP is consistently deﬁned, so that p(f (x1)) can be derived from p(f (x1), f (x2)), etc.\n",
      "\n",
      "require G(θ) ≥ 0 and Θ (G(T1), . . . , G(TK)) has a joint Dirichlet distribution\n",
      "\n",
      "A Dirichlet process is a distribution over probability measures G : Θ → R+, where we G(θ)dθ = 1. The DP is deﬁned implicitly by the requirement that\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(25.17) for any ﬁnite partition (T1, . . . , TK) of Θ. If this is the case, we write G ∼ DP(α, H), where α is called the concentration parameter and H is called the base measure.1\n",
      "\n",
      "Dir(αH(T1), . . . , αH(TK))\n",
      "\n",
      "An example of a DP is shown in Figure 25.4, where the base measure is a 2d Gaussian. The distribution over all the cells, p(G(T1), . . . , G(TK)), is Dirichlet, so the marginals in each cell are beta distributed:\n",
      "\n",
      "Beta(αH(Ti), α\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "H(Tj))\n",
      "\n",
      "(25.18)\n",
      "\n",
      "j(cid:5)=i\n",
      "\n",
      "The DP is consistently deﬁned in the sense that if T1 and T2 form a partition of ˜T1, then G(T1) + G(T2) and G( ˜T1) both follow the same beta distribution.\n",
      "\n",
      "Recall that if π ∼ Dir(α), and z|π ∼ Cat(π), then we can integrate out π to get the\n",
      "\n",
      "predictive distribution for the Dirichlet-multinoulli model:\n",
      "\n",
      "z ∼ Cat(α1/α0, . . . , αK/α0)\n",
      "\n",
      "(25.19)\n",
      "\n",
      "1. Unlike a GP, knowing something about G(Tk) does not tell us anything about G(Tk(cid:2) ), beyond the sum-to-one constraint; we say that the DP is a neutral process. Other stochastic processes can be deﬁned that do not have this property, but they are not so computationally convenient.\n",
      "\n",
      "25.2. Dirichlet process mixture models\n",
      "\n",
      "883\n",
      "\n",
      "β1 π1\n",
      "\n",
      "β2 π2\n",
      "\n",
      "β3 π3\n",
      "\n",
      "1−β1\n",
      "\n",
      "1−β2\n",
      "\n",
      "β4 π4\n",
      "\n",
      "1−β3\n",
      "\n",
      "1−β4\n",
      "\n",
      "β5 π5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "10\n",
      "\n",
      "α = 2\n",
      "\n",
      "α = 5\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "10\n",
      "\n",
      "α = 2\n",
      "\n",
      "α = 5\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.5 (a) We have a unit length stick, which we break at a random point β1; the length of the piece we keep is called π1; we then recursively break off pieces of the remaining stick, to generate π2, π3, . . .. Source: Figure 2.22 of (Sudderth 2006). Used with (b) Samples of πk from this process for α = 2 (top row) and α = 5 kind permission of Erik Sudderth. (bottom row). Figure generated by stickBreakingDemo, written by Yee-Whye Teh.\n",
      "\n",
      "Illustration of the stick breaking construction.\n",
      "\n",
      "where α0 = given one observation is given by\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "k αk. In other words, p(z = k|α) = αk/α0. Also, the updated posterior for π\n",
      "\n",
      "π|z ∼ Dir(α1 + I(z = 1), . . . , αK + I(z = K))\n",
      "\n",
      "(25.20)\n",
      "\n",
      "The DP generalizes this to arbitrary partitions. If G ∼ DP(α, H), then p(θ ∈ Ti) = H(Ti) and the posterior is\n",
      "\n",
      "p(G(T1), . . . , G(TK)|θ, α, H) = Dir(αH(T1) +I (θ ∈ T1), . . . , αH(TK) +I (θ ∈ TK))(25.21)\n",
      "\n",
      "This holds for any set of partitions. Hence if we observe multiple samples θi ∼ G, the new posterior is given by\n",
      "\n",
      "G|θ1, . . . , θN , α, H ∼ DP\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "α + N,\n",
      "\n",
      "1 α + N\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "αH +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "δθi\n",
      "\n",
      "(cid:11)(cid:11)\n",
      "\n",
      "(25.22)\n",
      "\n",
      "Thus we see that the DP effectively deﬁnes a conjugate prior for arbitrary measurable spaces. The concentration parameter α is like the effective sample size of the base measure H.\n",
      "\n",
      "25.2.2.1\n",
      "\n",
      "Stick breaking construction of the DP\n",
      "\n",
      "Our discussion so far has been very abstract. We now give a constructive deﬁnition for the DP, known as the stick-breaking construction.\n",
      "\n",
      "Let π = {πk}∞\n",
      "\n",
      "k=1 be an inﬁnite sequence of mixture weights derived from the following\n",
      "\n",
      "process:\n",
      "\n",
      "βk ∼ Beta(1, α)\n",
      "\n",
      "(25.23)\n",
      "\n",
      "πk = βk\n",
      "\n",
      "k−1(cid:20)\n",
      "\n",
      "(1 − βl) = βk(1 −\n",
      "\n",
      "k−1(cid:4)\n",
      "\n",
      "πl)\n",
      "\n",
      "(25.24)\n",
      "\n",
      "l=1\n",
      "\n",
      "l=1\n",
      "\n",
      "884\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "This is often denoted by\n",
      "\n",
      "π ∼ GEM(α)\n",
      "\n",
      "(25.25)\n",
      "\n",
      "where GEM stands for Griffiths, Engen and McCloskey (this term is due to (Ewens 1990)). Some samples from this process are shown in Figure 25.5. One can show that this process process will terminate with probability 1, although the number of elements it generates increases with α. Furthermore, the size of the πk components decreases on average.\n",
      "\n",
      "Now deﬁne\n",
      "\n",
      "G(θ) =\n",
      "\n",
      "∞(cid:4)\n",
      "\n",
      "πkδθk (θ)\n",
      "\n",
      "(25.26)\n",
      "\n",
      "k=1\n",
      "\n",
      "where π ∼ GEM(α) and θk ∼ H. Then one can show that G ∼ DP(α, H).\n",
      "\n",
      "As a consequence of this construction, we see that samples from a DP are discrete with probability one. In other words, if you keep sampling it, you will get more and more repetitions of previously generated values. So if we sample θi ∼ G, we will see repeated values; let us number the unique values θ1, θ2, etc. Data sampled from θi will therefore cluster around the θk. This is evident in Figure 25.3, where most data comes from the Gaussians with large πk values, represented by ellipses with thick borders. This is our ﬁrst indication that the DP might be useful for clustering.\n",
      "\n",
      "25.2.2.2\n",
      "\n",
      "The Chinese restaurant process (CRP)\n",
      "\n",
      "Working with inﬁnite dimensional sticks is problematic. However, we can exploit the clustering property to draw samples form a GP, as we now show.\n",
      "\n",
      "distinct values θk, then the predictive distribution of the next observation is given by\n",
      "\n",
      "The key result is this:\n",
      "\n",
      "p(θN +1 = θ|θ1:N , α, H) =\n",
      "\n",
      "If θi ∼ G are N observations from G ∼ DP(α, H), taking on K\n",
      "\n",
      "1 α + N\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "αH(θ) +\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "Nkδθk (θ)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(25.27)\n",
      "\n",
      "where Nk is the number of previous observations equal to θk. This is called the Polya urn or Blackwell-MacQueen sampling scheme. This provides a constructive way to sample from a DP. It is much more convenient to work with discrete variables zi which specify which value of\n",
      "\n",
      "θk to use. That is, we deﬁne θi = θzi . Based on the above expression, we have\n",
      "\n",
      "p(zN +1 = z|z1:N , α) =\n",
      "\n",
      "1 α + N\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "αI(z = k∗) +\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "NkI(z = k)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(25.28)\n",
      "\n",
      "where k∗ represents a new cluster index that has not yet been used. This is called the Chinese restaurant process or CRP, based on the seemingly inﬁnite supply of tables at certain Chinese restaurants. The analogy is as follows: The tables are like clusters, and the customers are like observations. When a person enters the restaurant, he may choose to join an existing table with probability proportional to the number of people already sitting at this table (the Nk); otherwise, with a probability that diminishes as more people enter the room (due to the 1/(α + N ) term),\n",
      "\n",
      "25.2. Dirichlet process mixture models\n",
      "\n",
      "885\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.6 Two views of a DP mixture model. Left: π ∼ GEM(α). Right: G is drawn from a DP. Compare to Figure 25.2. 2006). Used with kind permission of Erik Sudderth.\n",
      "\n",
      "inﬁnite number of clusters parameters, θk, and Source: Figure 2.24 of (Sudderth\n",
      "\n",
      "he may choose to sit at a new table k∗. The result is a distribution over partitions of the integers, which is like a distribution of customers to tables.\n",
      "\n",
      "The fact that currently occupied tables are more likely to get new customers is sometimes called the rich get richer phenomenon. Indeed, one can derive an expression for the distri- bution of cluster sizes induced by this prior process; it is basically a power law. The number of occupied tables K almost surely approaches α log(N ) as N → ∞, showing that the model complexity will indeed grow logarithmically with dataset size. More ﬂexible priors over cluster sizes can also be deﬁned, such as the two-parameter Pitman-Yor process.\n",
      "\n",
      "25.2.3\n",
      "\n",
      "Applying Dirichlet processes to mixture modeling\n",
      "\n",
      "The DP is not particularly useful as a model for data directly, since data vectors rarely repeat it is useful as a prior for the parameters of a stochastic data generating exactly. However, mechanism, such as a mixture model. To create such a model, we follow exactly the same setup as Section 11.2, but we deﬁne G ∼ DP(α, H). Equivalently, we can write the model as follows:\n",
      "\n",
      "π ∼ GEM(α) zi ∼ π θk ∼ H(λ) xi ∼ F (θzi )\n",
      "\n",
      "(25.29)\n",
      "\n",
      "(25.30)\n",
      "\n",
      "(25.31)\n",
      "\n",
      "(25.32)\n",
      "\n",
      "This is illustrated in Figure 25.6. We see that G is now a random draw of an unbounded number of parameters θk from the base distribution H, each with weight πk. Each data point xi is generated by sampling its own “private” parameter θi from G. As we get more and more data, it becomes increasingly likely that θi will be equal to one of the θk’s we have seen before, and thus xi will be generated close to an existing datapoint.\n",
      "\n",
      "886\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "25.2.4\n",
      "\n",
      "Fitting a DP mixture model\n",
      "\n",
      "The simplest way to ﬁt a DPMM is to modify the collapsed Gibbs sampler of Section 24.2.4. From Equation 24.23 we have\n",
      "\n",
      "p(zi = k|z−i, x, α,λ ) ∝ p(zi = k|z−i, α)p(xi|x−i, zi = k, z−i, λ)\n",
      "\n",
      "(25.33)\n",
      "\n",
      "By exchangeability, we can assume that zi is the last customer to enter the restaurant. Hence the ﬁrst term is given by\n",
      "\n",
      "p(zi|z−i, α) =\n",
      "\n",
      "1 α + N − 1\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "αI(zi = k∗) +\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "Nk,−iI(zi = k)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(25.34)\n",
      "\n",
      "where K is the number of clusters used by z−i, and k∗ is a new cluster. Another way to write this is as follows:\n",
      "\n",
      "p(zi = k|z−i, α) =\n",
      "\n",
      "3\n",
      "\n",
      "Nk,−i α+N −1 α α+N −1\n",
      "\n",
      "if k has been seen before if k is a new cluster\n",
      "\n",
      "(25.35)\n",
      "\n",
      "Interestingly, this is equivalent to Equation 24.26, which has the form p(zi = k|z−i, α) = Nk,−i+α/K α+N −1 To compute the second term, p(xi|x−i, zi = k, z−i, λ), let us partition the data x−i into clusters based on z−i. Let x−i,c = {xj : zj = c, j (cid:8)= i} be the data assigned to cluster c. If zi = k, then xi is conditionally independent of all the data points except those assigned to cluster k. Hence we have\n",
      "\n",
      ", in theK → ∞ limit (Rasmussen 2000; Neal 2000).\n",
      "\n",
      "p(xi|x−i, z−i, zi = k, λ) =p( xi|x−i,k, λ) =\n",
      "\n",
      "p(xi, x−i,k|λ) p(x−i,k|λ)\n",
      "\n",
      "(25.36)\n",
      "\n",
      "where\n",
      "\n",
      "p(xi, x−i,k|λ) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(xi|θk)\n",
      "\n",
      "⎡\n",
      "\n",
      "⎣\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "⎤\n",
      "\n",
      "p(xj|θk)\n",
      "\n",
      "⎦ H(θk|λ)dθk\n",
      "\n",
      "(25.37)\n",
      "\n",
      "j(cid:5)=i:zj =k\n",
      "\n",
      "is the marginal likelihood of all the data assigned to cluster k, including i, and p(x−i,k|λ) is an analogous expression excluding i. Thus we see that the term p(xi|x−i, z−i, zi = k, λ) is the posterior preditive distribution for cluster k evaluated at xi.\n",
      "\n",
      "If zi = k∗, corresponding to a new cluster, we have\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(xi|x−i, z−i, zi = k∗, λ) = p(xi|λ) =\n",
      "\n",
      "p(xi|θ)H(θ|λ)dθ\n",
      "\n",
      "(25.38)\n",
      "\n",
      "which is just the prior predictive distribution for a new cluster evaluated at xi.\n",
      "\n",
      "See Algorithm 1 for the pseudocode. (This is called “Algorithm 3” in (Neal 2000).) This is very similar to collapsed Gibbs for ﬁnite mixtures except that we have to consider the case zi = k∗. An example of this procedure in action is shown in Figure 25.7. The sample clusterings, and the induced posterior over K, seems reasonable. The method tends to rapidly discover a good clustering. By contrast, Gibbs sampling (and EM) for a ﬁnite mixture model often gets stuck in\n",
      "\n",
      "25.3. Affinity propagation\n",
      "\n",
      "887\n",
      "\n",
      "Algorithm 25.1: Collapsed Gibbs sampler for DP mixtures 1 for each i = 1 :N in random order do 2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "Remove xi’s sufficient statistics from old cluster zi ; for each k = 1 :K do\n",
      "\n",
      "Compute pk(xi) = p(xi|x−i(k)); Set Nk,−i = dim(x−i(k)) ; Compute p(zi = k|z−i, D) =\n",
      "\n",
      "Nk,−i α+N −1 ;\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "11\n",
      "\n",
      "12\n",
      "\n",
      "Compute p∗(xi) = p(xi|λ); Compute p(zi = ∗|z−i, D) = Normalize p(zi|·); Sample zi ∼ p(zi|·) ; Add xi’s sufficient statistics to new cluster zi ; If any cluster is empty, remove it and decrease K;\n",
      "\n",
      "α α+N −1 ;\n",
      "\n",
      "poor local optima (not shown). This is because the DPMM is able to create extra redundant clusters early on, and to use them to escape local optima. Figure 25.8 shows that most of the time, the DPMM converges more rapidly than a ﬁnite mixture model.\n",
      "\n",
      "A variety of other ﬁtting methods have been proposed. (Daume 2007a) shows how one can use A star search and beam search to quickly ﬁnd an approximate MAP estimate. (Mansinghka et al. 2007) discusses how to ﬁt a DPMM online using particle ﬁltering, which is a like a stochastic version of beam search. This can be more efficient than Gibbs sampling, particularly for large datasets. (Kurihara et al. 2006) develops a variational approximation that is even faster (see also (Zobay 2009)). Extensions to the case of non-conjugate priors are discussed in (Neal 2000).\n",
      "\n",
      "Another important issue is how to set the hyper-parameters. For the DP, the value of α does not have much impact on predictive accuracy, but it does affect the number of clusters. One approach is to put a Ga(a, b) prior for α, and then to from its posterior, p(α|K, N, a, b), using auxiliary variable methods (Escobar and West 1995). Alternatively, one can use empirical Bayes (McAuliffe et al. 2006). Similarly, for the base distribution, we can either sample the hyper-parameters λ (Rasmussen 2000) or use empirical Bayes (McAuliffe et al. 2006).\n",
      "\n",
      "25.3\n",
      "\n",
      "Affinity propagation\n",
      "\n",
      "Mixture models, whether ﬁnite or inﬁnite, require access to the raw N × D data matrix, and need to specify a generative model of the data. An alternative approach takes as input an N ×N similarity matrix, and then tries to identify examplars, which will act as cluster centers. The K-medoids or K-centers algorithm (Section 14.4.2) is one approach, but it can suffer from local minima. Here we describe an alternative approach called affinity propagation (Frey and Dueck 2007) that works substantially better in practice.\n",
      "\n",
      "The idea is that each data point must choose another data point as its exemplar or centroid; some data points will choose themselves as centroids, and this will automatically determine the number of clusters. More precisely, let ci ∈ {1, . . . , N } represent the centroid for datapoint i.\n",
      "\n",
      "888\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "iter# 50\n",
      "\n",
      "iter# 100\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "iter# 200\n",
      "\n",
      "0.7\n",
      "\n",
      "4\n",
      "\n",
      "0.6\n",
      "\n",
      "2\n",
      "\n",
      "0.5\n",
      "\n",
      "0\n",
      "\n",
      "0.4\n",
      "\n",
      "−2\n",
      "\n",
      "0.3\n",
      "\n",
      "−4\n",
      "\n",
      "0.2\n",
      "\n",
      "−6\n",
      "\n",
      "0.1\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 25.7 100 data points in 2d are clustered using a DP mixture ﬁt with collapsed Gibbs sampling. We show samples from the posterior after 50,100, 200 samples. We also show the posterior over K, based on 200 samples, discarding the ﬁrst 50 as burnin. Figure generated by dpmGauss2dDemo, written by Yee Whye Teh.\n",
      "\n",
      "The goal is to maximize the following function\n",
      "\n",
      "S(c) =\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "s(i, ci) +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "δk(c)\n",
      "\n",
      "(25.39)\n",
      "\n",
      "i=1\n",
      "\n",
      "k=1\n",
      "\n",
      "The ﬁrst term measures the similarity of each point to its centroid. The second term is a penalty term that is −∞ if some data point i has chosen k as its exemplar (i.e., ci = k), but k has not chosen itself as an exemplar (i.e., we do not have ck = k). More formally,\n",
      "\n",
      "δk(c) =\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "−∞ if ck (cid:8)= k but ∃i : ci = k 0\n",
      "\n",
      "otherwise\n",
      "\n",
      "(25.40)\n",
      "\n",
      "The objective function can be represented as a factor graph. We can either use N nodes,\n",
      "\n",
      "25.3. Affinity propagation\n",
      "\n",
      "889\n",
      "\n",
      "−350\n",
      "\n",
      "−350\n",
      "\n",
      "−400\n",
      "\n",
      "−400\n",
      "\n",
      ") θ\n",
      "\n",
      ", π\n",
      "\n",
      "−450\n",
      "\n",
      ") θ\n",
      "\n",
      ", π\n",
      "\n",
      "−450\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "x ( p\n",
      "\n",
      "x ( p\n",
      "\n",
      "g o\n",
      "\n",
      "l\n",
      "\n",
      "−500\n",
      "\n",
      "g o\n",
      "\n",
      "l\n",
      "\n",
      "−500\n",
      "\n",
      "−550\n",
      "\n",
      "−550\n",
      "\n",
      "−600\n",
      "\n",
      "0 10\n",
      "\n",
      "1 10\n",
      "\n",
      "Iteration\n",
      "\n",
      "Dirichlet Process Mixture Finite Mixture\n",
      "\n",
      "2 10\n",
      "\n",
      "3 10\n",
      "\n",
      "−600\n",
      "\n",
      "0 10\n",
      "\n",
      "1 10\n",
      "\n",
      "Iteration\n",
      "\n",
      "Dirichlet Process Mixture Finite Mixture\n",
      "\n",
      "2 10\n",
      "\n",
      "3 10\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.8 Comparison of collapsed Gibbs samplers for a DP mixture (dark blue) and a ﬁnite mixture (light red) with K = 4 applied to N = 300 data points (shown in Figure 25.7). Left: logprob vs iteration for 20 different starting values. Right: median (thick line) and quantiles (dashed lines) over 100 different starting values. Source: Figure 2.27 of (Sudderth 2006). Used with kind permission of Erik Sudderth.\n",
      "\n",
      "A\n",
      "\n",
      "1\n",
      "\n",
      "2 2\n",
      "\n",
      "3\n",
      "\n",
      "…\n",
      "\n",
      "k\n",
      "\n",
      "…\n",
      "\n",
      "N\n",
      "\n",
      "c1\n",
      "\n",
      "c2\n",
      "\n",
      "c3\n",
      "\n",
      "…\n",
      "\n",
      "ci\n",
      "\n",
      "…\n",
      "\n",
      "cN\n",
      "\n",
      "s(1, )\n",
      "\n",
      "s(2, )\n",
      "\n",
      "s(3, )\n",
      "\n",
      "s(i, )\n",
      "\n",
      "s(N, )\n",
      "\n",
      "Figure 25.9 Factor graphs for affinity propagation. Circles are variables, squares are factors. Each ci node has N possible states. From Figure S2 of (Frey and Dueck 2007). Used with kind permission of Brendan Frey.\n",
      "\n",
      "each with N possible values, as shown in Figure 25.9, or we can use N 2 binary nodes (see (Givoni and Frey 2009) for the details). We will assume the former representation.\n",
      "\n",
      "We can ﬁnd a strong local maximum of the objective by using max-product loopy belief propagation (Section 22.2). Referring to the model in Figure 25.9, each variable nodes ci sends a message to each factor node δk. It turns out that this vector of N numbers can be reduced to a scalar message, denote ri→k, known as the responsibility. This is a measure of how much i thinks k would make a good exemplar, compared to all the other exemplars i has looked at. In addition, each factor node δk sends a message to each variable node ci. Again this can be reduced to a scalar message, ai←k, known as the availability. This is a measure of how strongly k believes it should an exemplar for i, based on all the other data points k has looked at.\n",
      "\n",
      "As usual with loopy BP, the method might oscillate, and convergence is not guaranteed.\n",
      "\n",
      "890\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:6)(cid:7)(cid:6)(cid:8)(cid:6)(cid:9)(cid:10)(cid:6)(cid:11)(cid:9)(cid:8)(cid:6)(cid:12)(cid:7) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1)(cid:1)(cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:6)(cid:8)(cid:13)(cid:14)(cid:9)(cid:8)(cid:6)(cid:12)(cid:7)(cid:15)(cid:16)(cid:21) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1)(cid:1)(cid:1) (cid:1)(cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)(cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:6)(cid:8)(cid:13)(cid:14)(cid:9)(cid:8)(cid:6)(cid:12)(cid:7)(cid:15)(cid:16)(cid:20) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:6)(cid:8)(cid:13)(cid:14)(cid:9)(cid:8)(cid:6)(cid:12)(cid:7)(cid:15)(cid:16)(cid:19) (cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1)(cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1) (cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:6)(cid:8)(cid:13)(cid:14)(cid:9)(cid:8)(cid:6)(cid:12)(cid:7)(cid:15)(cid:16)(cid:18) (cid:1) (cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1) (cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)(cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1) (cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)(cid:1) (cid:1) (cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:6)(cid:8)(cid:13)(cid:14)(cid:9)(cid:8)(cid:6)(cid:12)(cid:7)(cid:15)(cid:16)(cid:3) (cid:1) (cid:1)(cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:6)(cid:8)(cid:13)(cid:14)(cid:9)(cid:8)(cid:6)(cid:12)(cid:7)(cid:15)(cid:16)(cid:17)\n",
      "\n",
      "(cid:1)(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:22)(cid:12)(cid:7)(cid:23)(cid:13)(cid:14)(cid:24)(cid:13)(cid:7)(cid:22)(cid:13)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1) (cid:1) (cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:2)(cid:3)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:5)(cid:3)\n",
      "\n",
      "(cid:25)(cid:26)(cid:25)(cid:2)(cid:27)(cid:28)(cid:27)(cid:29)(cid:30)(cid:31) !(cid:15)\n",
      "\n",
      "(cid:27)(cid:28)(cid:27)(cid:29)(cid:30)(cid:31) !\n",
      "\n",
      "Figure 25.10 Example of affinity propagation. Each point is colored coded by how much it wants to be an exemplar (red is the most, green is the least). This can be computed by summing up all the incoming availability messages and the self-similarity term. The darkness of the i → k arrow reﬂects how much point i wants to belong to exemplar k. From Figure 1 of (Frey and Dueck 2007). Used with kind permission of Brendan Frey.\n",
      "\n",
      "However, by using damping, the method is very reliable in practice. If the graph is densely connected, message passing takes O(N 2) time, but with sparse similarity matrices, it only takes O(E) time, where E is the number of edges or non-zero entries in S.\n",
      "\n",
      "The number of clusters can be controlled by scaling the diagonal terms S(i, i), which reﬂect how much each data point wants to be an exemplar. Figure 25.10 gives a simple example of some 2d data, where the negative Euclidean distance was used to measured similarity. The S(i, i) values were set to be the median of all the pairwise similarities. The result is 3 clusters. Many other results are reported in (Frey and Dueck 2007), who show that the method signiﬁcantly outperforms K-medoids.\n",
      "\n",
      "25.4\n",
      "\n",
      "Spectral clustering\n",
      "\n",
      "An alternative view of clustering is in terms of graph cuts. The idea is we create a weighted undirected graph W from the similarity matrix S, typically by using the nearest neighbors of each point; this ensures the graph is sparse, which speeds computation. If we want to ﬁnd a partition into K clusters, say A1, . . . , AK, one natural criterion is to minimize\n",
      "\n",
      "cut(A1, . . . , AK) (cid:2) 1 2\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "W (Ak, Ak)\n",
      "\n",
      "(25.41)\n",
      "\n",
      "25.4. Spectral clustering\n",
      "\n",
      "891\n",
      "\n",
      "where Ak = V \\ Ak is the complement of Ak, and W (A, B) (cid:2) i∈A,j∈B wij. For K = 2 this problem is easy to solve. Unfortunately the optimal solution often just partitions off a single data point from the rest. To ensure the sets are reasonably large, we can deﬁne the normalized cut to be\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "j=1 wij is the weighted degree of node i. This splits where vol(A) (cid:2) the graph into K clusters such that nodes within each cluster are similar to each other, but are different to nodes in other clusters.\n",
      "\n",
      "We can formulate the Ncut problem in terms of searching for binary vectors ci ∈ {0, 1}N , where cik = 1 if point i belongs to cluster k, that minimize the objective. Unfortunately this is NP-hard (Wagner and Wagner 1993). Affinity propagation is one way to solve the problem. Another is to relax the constraints that ci be binary, and allow them to be real-valued. The result turns into an eigenvector problem known as spectral clustering (see e.g., (Shi and Malik In general, the technique of performing eigenalysis of graphs is called spectral graph 2000)). theory (Chung 1997).\n",
      "\n",
      "Ncut(A1, . . . , AK) (cid:2) 1 2\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "i∈A di, and di =\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "cut(Ak, Ak) vol(Ak) (cid:7)N\n",
      "\n",
      "(25.42)\n",
      "\n",
      "Going into the details would take us too far aﬁeld, but below we give a very brief summary,\n",
      "\n",
      "based on (von Luxburg 2007), since we will encounter some of these ideas later on.\n",
      "\n",
      "25.4.1\n",
      "\n",
      "Graph Laplacian\n",
      "\n",
      "Let W be a symmetric weight matrix for a graph, where wij = wji ≥ 0. Let D = diag(di) be a diaogonal matrix containing the weighted degree of each node. We deﬁne the graph Laplacian as follows:\n",
      "\n",
      "L (cid:2) D − W\n",
      "\n",
      "(25.43)\n",
      "\n",
      "This matrix has various important properties. Because each row sums to zero, we have that 1 is an eigenvector with eigenvalue 0. Furthermore, the matrix is symmetric and positive semi-deﬁnite. To see this, note that\n",
      "\n",
      "f T Lf = f T Df − f T Wf =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "dif 2\n",
      "\n",
      "i −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "fifjwij\n",
      "\n",
      "(25.44)\n",
      "\n",
      "⎛\n",
      "\n",
      "i\n",
      "\n",
      "i,j\n",
      "\n",
      "⎞\n",
      "\n",
      "=\n",
      "\n",
      "1 2\n",
      "\n",
      "⎝\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i\n",
      "\n",
      "dif 2\n",
      "\n",
      "i − 2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i,j\n",
      "\n",
      "fifjwij +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "j\n",
      "\n",
      "djf 2 j\n",
      "\n",
      "⎠ =\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i,j\n",
      "\n",
      "wij(fi − fj)2\n",
      "\n",
      "(25.45)\n",
      "\n",
      "Hence f T Lf ≥ 0 for all f ∈ RN . Consequently we see that L has N non-negative, real-valued eigenvalues, 0 ≤ λ1 ≤ λ2 ≤ . . . ≤ λN .\n",
      "\n",
      "To get some intuition as to why L might be useful for graph-based clustering, we note the\n",
      "\n",
      "following result.\n",
      "\n",
      "Theorem 25.4.1. The set of eigenvectors of L with eigenvalue 0 is spanned by the indicator vectors 1A1\n",
      "\n",
      ", . . . , 1AK , where Ak are the K connected components of the graph.\n",
      "\n",
      "892\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "If f is an eigenvector with eigenvalue 0, then Proof. Let us start with the case K = 1. ij wij(fi − fj)2. If two nodes are connected, so wij > 0, we must have that fi = fj. 0 = Hence f is constant for all vertices which are connected by a path in the graph. Now suppose K > 1. In this case, L will be block diagonal. A similar argument to the above shows that we will have K indicator functions, which “select out” the connected components.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "This suggests the following algorithm. Compute the ﬁrst K eigenvectors uk of L. Let U = [u1, . . . , uK] be an N × K matrix with the eigenvectors in its columns. Let yi ∈ RK be the i’th row of U. Since these yi will be piecewise constant, we can apply K-means clustering to them to recover the connected components. Now assign point i to cluster k iff row i of Y was assigned to cluster k.\n",
      "\n",
      "In reality, we do not expect a graph derived from a real similarity matrix to have isolated connected components — that would be too easy. But it is reasonable to suppose the graph is a small “perturbation” from such an ideal. In this case, one can use results from perturbation theory to show that the eigenvectors of the perturbed Laplacian will be close to these ideal indicator functions (Ng et al. 2001).\n",
      "\n",
      "Note that this approach is related to kernel PCA (Section 14.4.4). In particular, KPCA uses the largest eigenvectors of W; these are equivalent to the smallest eigenvectors of I − W. This is similar to the above method, which computes the smallest eigenvectors of L = D − W. See In practice, spectral clustering gives much better results than (Bengio et al. 2004) for details. KPCA.\n",
      "\n",
      "25.4.2\n",
      "\n",
      "Normalized graph Laplacian\n",
      "\n",
      "In practice, it is important to normalize the graph Laplacian, to account for the fact that some nodes are more highly connected than others. There are two comon ways to do this. One method, used in e.g., (Shi and Malik 2000; Meila 2001), creates a stochastic matrix where each row sums to one:\n",
      "\n",
      "Lrw (cid:2) D−1L = I − D−1W\n",
      "\n",
      "(25.46)\n",
      "\n",
      "The eigenvalues and eigenvectors of L and Lrw are closely related to each other (see (von Luxburg 2007) for details). Furthemore, one can show that for Lrw, the eigenspace of 0 is again spanned by the indicator vectors 1Ak . This suggests the following algorithm: ﬁnd the smallest K eigenvectors of Lrw, create U, cluster the rows of U using K-means, then infer the partitioning of the original points (Shi and Malik 2000). (Note that the eigenvectors/ values of Lrw are equivalent to the generalized eigenvectors/ values of L, which solve Lu = λDU.)\n",
      "\n",
      "Another method, used in e.g., (Ng et al. 2001), creates a symmetric matrix Lsym (cid:2) D− 1\n",
      "\n",
      "2 LD− 1\n",
      "\n",
      "2 = I − D− 1\n",
      "\n",
      "2 WD− 1\n",
      "\n",
      "2\n",
      "\n",
      "(25.47)\n",
      "\n",
      "This time the eigenspace of 0 is spanned by D 1 2 1Ak . This suggest the following algorithm: ﬁnd the smallest K eigenvectors of Lsym, create U, normalize each row to unit norm by creating tij = uij/ ik), cluster the rows of T using K-means, then infer the partitioning of the original points (Ng et al. 2001).\n",
      "\n",
      "\"\n",
      "\n",
      "(\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "k u2\n",
      "\n",
      "There is an interesting connection between Ncuts and random walks on a graph (Meila 2001). First note that P = D−1W = I − Lrw is a stochastic matrix, where pij = wij/di\n",
      "\n",
      "25.5. Hierarchical clustering\n",
      "\n",
      "893\n",
      "\n",
      "5\n",
      "\n",
      "k−means clustering\n",
      "\n",
      "5\n",
      "\n",
      "spectral clustering\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "y\n",
      "\n",
      "0\n",
      "\n",
      "y\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−2\n",
      "\n",
      "−3\n",
      "\n",
      "−3\n",
      "\n",
      "−4\n",
      "\n",
      "−4\n",
      "\n",
      "−5\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0 x\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "−5\n",
      "\n",
      "−6\n",
      "\n",
      "−4\n",
      "\n",
      "−2\n",
      "\n",
      "0 x\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "6\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.11 Clustering data consisting of 2 spirals. (a) K-means. (b) Spectral clustering. Figure generated by spectralClusteringDemo, written by Wei-Lwun Lu.\n",
      "\n",
      "can be interpreted as the probability of going from i to j. If the graph is connected and non-bipartite, it possesses a unique stationary distribution π = (π1, . . . , πN ), where πi = di/vol(V ). Furthermore, one can show that\n",
      "\n",
      "Ncut(A, A) = p(A|A) +p( A|A)\n",
      "\n",
      "(25.48)\n",
      "\n",
      "This means that we are looking for a cut such that a random walk rarely makes transitions from A to A or vice versa.\n",
      "\n",
      "25.4.3\n",
      "\n",
      "Example\n",
      "\n",
      "Figure 25.11 illustrates the method in action. In Figure 25.11(a), we see that K-means does a poor job of clustering, since it implicitly assumes each cluster corresponds to a spherical Gaussian. Next we try spectral clustering. We deﬁne a similarity matrix using the Gaussian kernel. We compute the ﬁrst two eigenvectors of the Laplacian. From this we can infer the clustering in Figure 25.11(b).\n",
      "\n",
      "Since the method is based on ﬁnding the smallest K eigenvectors of a sparse matrix, it takes O(N 3) time. However, a variety of methods can be used to scale it up for large datasets (see e.g., (Yan et al. 2009)).\n",
      "\n",
      "25.5 Hierarchical clustering\n",
      "\n",
      "Mixture models, whether ﬁnite or inﬁnite, produce a “ﬂat” clustering. Often we want to learn a hierarchical clustering, where clusters can be nested inside each other.\n",
      "\n",
      "There are two main approaches to hierarchical clustering: bottom-up or agglomerative clus- tering, and top-down or divisive clustering. Both methods take as input a dissimilarity matrix In the bottom-up approach, the most similar groups are merged at each between the objects.\n",
      "\n",
      "894\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "5\n",
      "\n",
      "2.5\n",
      "\n",
      "4.5\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "3.5\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "2.5\n",
      "\n",
      "5\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "3\n",
      "\n",
      "1.5\n",
      "\n",
      "1.5\n",
      "\n",
      "4\n",
      "\n",
      "1\n",
      "\n",
      "0.5\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "1\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.12 (a) An example of single link clustering using city block distance. Pairs (1,3) and (4,5) are both distance 1 apart, so get merged ﬁrst. (b) The resulting dendrogram. Based on Figure 7.5 of (Alpaydin 2004). Figure generated by agglomDemo.\n",
      "\n",
      "Hierarchical Clustering of Profiles\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2\n",
      "\n",
      "−3 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "−0.5 −1 −1.5 −2 −2.5 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "1 0 −1 −2 −3 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "−2 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "−4 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "−2 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2\n",
      "\n",
      "−4 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "4\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "−2 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "2 1 0 −1 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "2.5 2 1.5 1 0.5 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "2 1.5 1 0.5\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "0\n",
      "\n",
      "−1\n",
      "\n",
      "−2 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "−1 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.13 Hierarchical clustering applied to the yeast gene expression data. (a) The rows are permuted according to a hierarchical clustering scheme (average link agglomerative clustering), in order to bring similar rows close together. (b) 16 clusters induced by cutting the average linkage tree at a certain height. Figure generated by hclustYeastDemo.\n",
      "\n",
      "step. details below.\n",
      "\n",
      "In the top-down approach, groups are split using various different criteria. We give the\n",
      "\n",
      "Note that agglomerative and divisive clustering are both just heuristics, which do not optimize any well-deﬁned objective function. Thus it is hard to assess the quality of the clustering they produce in any formal sense. Furthermore, they will always produce a clustering of the input data, even if the data has no structure at all (e.g., it is random noise). Later in this section we will discuss a probabilistic version of hierarchical clustering that solves both these problems.\n",
      "\n",
      "25.5. Hierarchical clustering\n",
      "\n",
      "895\n",
      "\n",
      "Algorithm 25.2: Agglomerative clustering 1 initialize clusters as singletons: for i ← 1 to n do Ci ← {i}; 2 initialize set of clusters available for merging: S ← {1, . . . , n}; 3 repeat 4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "Pick 2 most similar clusters to merge: (j, k) ← arg minj,k∈S dj,k; Create new cluster C(cid:8) ← Cj ∪ Ck; Mark j and k as unavailable: S ← S \\ {j, k}; if C(cid:8) (cid:8)= {1, . . . , n} then\n",
      "\n",
      "Mark (cid:2) as available, S ← S ∪ {(cid:2)};\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "foreach i ∈ S do\n",
      "\n",
      "Update dissimilarity matrix d(i, (cid:2));\n",
      "\n",
      "11 until no more clusters are available for merging;\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 25.14\n",
      "\n",
      "Illustration of (a) Single linkage. (b) Complete linkage. (c) Average linkage.\n",
      "\n",
      "25.5.1\n",
      "\n",
      "Agglomerative clustering\n",
      "\n",
      "Agglomerative clustering starts with N groups, each initially containing one object, and then at each step it merges the two most similar groups until there is a single group, containing all the data. See Algorithm 11 for the pseudocode. Since picking the two most similar clusters to merge takes O(N 2) time, and there are O(N ) steps in the algorithm, the total running time is O(N 3). However, by using a priority queue, this can be reduced to O(N 2 log N ) (see e.g., (Manning et al. 2008, ch. 17) for details). For large N , a common heuristic is to ﬁrst run K-means, which takes O(KN D) time, and then apply hierarchical clustering to the estimated cluster centers.\n",
      "\n",
      "The merging process can be represented by a binary tree, called a dendrogram, as shown in Figure 25.12(b). The initial groups (objects) are at the leaves (at the bottom of the ﬁgure), and every time two groups are merged, we join them in the tree. The height of the branches represents the dissimilarity between the groups that are being joined. The root of the tree (which is at the top) represents a group containing all the data. If we cut the tree at any given height, we induce a clustering of a given size. For example, if we cut the tree in Figure 25.12(b) at height 2, we get the clustering {{{4, 5}, {1, 3}}, {2}}. We discuss the issue of how to choose the height/ number of clusters below.\n",
      "\n",
      "data. Figure 25.13(b).\n",
      "\n",
      "A more complex example is shown in Figure 25.13(a), where we show some gene expression If we cut the tree in Figure 25.13(a) at a certain height, we get the 16 clusters shown in\n",
      "\n",
      "There are actually three variants of agglomerative clustering, depending on how we deﬁne the dissimilarity between groups of objects. These can give quite different results, as shown in\n",
      "\n",
      "896\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "single link\n",
      "\n",
      "0.3\n",
      "\n",
      "0.25\n",
      "\n",
      "0.2\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "0.05\n",
      "\n",
      "(a)\n",
      "\n",
      "complete link\n",
      "\n",
      "2\n",
      "\n",
      "1.8\n",
      "\n",
      "1.6\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "(b)\n",
      "\n",
      "average link\n",
      "\n",
      "1.8\n",
      "\n",
      "1.6\n",
      "\n",
      "1.4\n",
      "\n",
      "1.2\n",
      "\n",
      "1\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 25.15 Hierarchical clustering of yeast gene expression data. (a) Single linkage. (b) Complete linkage. (c) Average linkage. Figure generated by hclustYeastDemo.\n",
      "\n",
      "25.5. Hierarchical clustering\n",
      "\n",
      "897\n",
      "\n",
      "Figure 25.15. We give the details below.\n",
      "\n",
      "25.5.1.1\n",
      "\n",
      "Single link\n",
      "\n",
      "In single link clustering, also called nearest neighbor clustering, the distance between two groups G and H is deﬁned as the distance between the two closest members of each group:\n",
      "\n",
      "dSL(G, H) = min\n",
      "\n",
      "i∈G,i(cid:2)∈H\n",
      "\n",
      "di,i(cid:2)\n",
      "\n",
      "(25.49)\n",
      "\n",
      "See Figure 25.14(a).\n",
      "\n",
      "The tree built using single link clustering is a minimum spanning tree of the data, which is a tree that connects all the objects in a way that minimizes the sum of the edge weights (distances). To see this, note that when we merge two clusters, we connect together the two closest members of the clusters; this adds an edge between the corresponding nodes, and this is guaranteed to be the “lightest weight” edge joining these two clusters. And once two clusters have been merged, they will never be considered again, so we cannot create cycles. As a consequence of this, we can actually implement single link clustering in O(N 2) time, whereas the other variants take O(N 3) time.\n",
      "\n",
      "25.5.1.2\n",
      "\n",
      "Complete link\n",
      "\n",
      "In complete link clustering, also called furthest neighbor clustering, the distance between two groups is deﬁned as the distance between the two most distant pairs:\n",
      "\n",
      "dCL(G, H) = max\n",
      "\n",
      "i∈G,i(cid:2)∈H\n",
      "\n",
      "di,i(cid:2)\n",
      "\n",
      "(25.50)\n",
      "\n",
      "See Figure 25.14(b).\n",
      "\n",
      "Single linkage only requires that a single pair of objects be close for the two groups to be considered close together, regardless of the similarity of the other members of the group. Thus clusters can be formed that violate the compactness property, which says that all the observations within a group should be similar to each other. In particular if we deﬁne the diameter of a group as the largest dissimilarity of its members, dG = maxi∈G,i(cid:2)∈G di,i(cid:2) , then we can see that single linkage can produce clusters with large diameters. Complete linkage represents the opposite extreme: two groups are considered close only if all of the observations in their union are relatively similar. This will tend to produce clusterings with small diameter, i.e., compact clusters.\n",
      "\n",
      "25.5.1.3\n",
      "\n",
      "Average link\n",
      "\n",
      "In practice, the preferred method is average link clustering, which measures the average distance between all pairs:\n",
      "\n",
      "davg(G, H) =\n",
      "\n",
      "1 nGnH\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i∈G\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i(cid:2)∈H\n",
      "\n",
      "di,i(cid:2)\n",
      "\n",
      "(25.51)\n",
      "\n",
      "where nG and nH are the number of elements in groups G and H. See Figure 25.14(c).\n",
      "\n",
      "Average link clustering represents a compromise between single and complete link clustering. It tends to produce relatively compact clusters that are relatively far apart. However, since it\n",
      "\n",
      "898\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "involves averaging of the di,i(cid:2) ’s, any change to the measurement scale can change the result. In contrast, single linkage and complete linkage are invariant to monotonic transformations of di,i(cid:2) , since they leave the relative ordering the same.\n",
      "\n",
      "25.5.2\n",
      "\n",
      "Divisive clustering\n",
      "\n",
      "Divisive clustering starts with all the data in a single cluster, and then recursively divides each cluster into two daughter clusters, in a top-down fashion. Since there are 2N −1 − 1 ways to split a group of N items into 2 groups, it is hard to compute the optimal split, so various heuristics are used. One approach is pick the cluster with the largest diameter, and split it in two using the K-means or K-medoids algorithm with K = 2. This is called the bisecting K-means algorithm (Steinbach et al. 2000). We can repeat this until we have any desired number of clusters. This can be used as an alternative to regular K-means, but it also induces a hierarchical clustering.\n",
      "\n",
      "to make new clusters by breaking the link corresponding to the largest dissimilarity. actually gives the same results as single link agglomerative clustering.)\n",
      "\n",
      "Another method is to build a minimum spanning tree from the dissimilarity graph, and then (This\n",
      "\n",
      "is as follows. We start with a single cluster containing all the data, G = {1, . . . , N }. We then measure the average dissimilarity of i ∈ G to all the other i(cid:2) ∈ G:\n",
      "\n",
      "Yet another method, called dissimilarity analysis (Macnaughton-Smith et al. 1964),\n",
      "\n",
      "dG i =\n",
      "\n",
      "1 nG\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i(cid:2)∈G\n",
      "\n",
      "di,i(cid:2)\n",
      "\n",
      "(25.52)\n",
      "\n",
      "We remove the most dissimilar object and put it in its own cluster H: i , G = G \\ {i∗}, H = {i∗} dG\n",
      "\n",
      "i∗ = arg max i∈G\n",
      "\n",
      "(25.53)\n",
      "\n",
      "We now continue to move objects from G to H until some stopping criterion is met. Speciﬁcally, we pick a point i∗ to move that maximizes the average dissimilarity to each i(cid:2) ∈ G but minimizes the average dissimilarity to each i(cid:2) ∈ H:\n",
      "\n",
      "dH i =\n",
      "\n",
      "1 nH\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "i(cid:2)∈H\n",
      "\n",
      "di,i(cid:2) ,\n",
      "\n",
      "i∗ = arg max i∈G\n",
      "\n",
      "i − dH dG i\n",
      "\n",
      "(25.54)\n",
      "\n",
      "We continue to do this until dG is negative. The ﬁnal result is that we have split G into two daughter clusters, G and H. We can then recursively call the algorithm on G and/or H, or on any other node in the tree. For example, we might choose to split the node G whose average dissimilarity is highest, or whose maximum dissimilarity (i.e., diameter) is highest. We continue the process until the average dissimilarity within each cluster is below some threshold, and/or all clusters are singletons.\n",
      "\n",
      "i − dH i\n",
      "\n",
      "Divisive clustering is less popular than agglomerative clustering, but it has two advantages. First, it can be faster, since if we only split for a constant number of levels, it takes just O(N ) time. Second, the splitting decisions are made in the context of seeing all the data, whereas bottom-up methods make myopic merge decisions.\n",
      "\n",
      "25.5. Hierarchical clustering\n",
      "\n",
      "899\n",
      "\n",
      "25.5.3\n",
      "\n",
      "Choosing the number of clusters\n",
      "\n",
      "It is difficult to choose the “right” number of clusters, since a hierarchical clustering algorithm will always create a hierarchy, even if the data is completely random. But, as with choosing K for K-means, there is the hope that there will be a visible “gap” in the lengths of the links in the dendrogram (represent the dissimilarity between merged groups) between natural clusters and unnatural clusters. Of course, on real data, this gap might be hard to detect. In Section 25.5.4, we will present a Bayesian approach to hierarchical clustering that nicely solves this problem.\n",
      "\n",
      "25.5.4\n",
      "\n",
      "Bayesian hierarchical clustering\n",
      "\n",
      "There are several ways to make probabilistic models which produce results similar to hierarchical clustering, e.g., (Williams 2000; Neal 2003b; Castro et al. 2004; Lau and Green 2006). Here we present one particular approach called Bayesian hierarchical clustering (Heller and Ghahra- mani 2005). Algorithmically it is very similar to standard bottom-up agglomerative clustering, and takes comparable time, whereas several of the other techniques referenced above are much slower. However, it uses Bayesian hypothesis tests to decide which clusters to merge (if any), rather than computing the similarity between groups of points in some ad-hoc way. These hypothesis tests are closely related to the calculations required to do inference in a Dirichlet process mixture model, as we will see. Furthermore, the input to the model is a data matrix, not a dissimilarity matrix.\n",
      "\n",
      "25.5.4.1\n",
      "\n",
      "The algorithm\n",
      "\n",
      "Let D = {x1, . . . , xN } represent all the data, and let Di be the set of datapoints at the leaves of the substree Ti. At each step, we compare two trees Ti and Tj to see if they should be merged into a new tree. Deﬁne Dij as their merged data, and let Mij = 1 if they should be merged, and Mij = 0 otherwise.\n",
      "\n",
      "The probability of a merge is given by\n",
      "\n",
      "rij (cid:2)\n",
      "\n",
      "p(Dij|Mij = 1)p(Mij = 1) p(Dij|Tij)\n",
      "\n",
      "(25.55)\n",
      "\n",
      "(25.56) Here p(Mij = 1) is the prior probability of a merge, which can be computed using a bottom-up algorithm described below. We now turn to the likelihood terms. If Mij = 1, the data in Dij is assumed to come from the same model, and hence ⎡\n",
      "\n",
      "p(Dij|Tij) =p( Dij|Mij = 1)p(Mij = 1) + p(Dij|Mij = 0)p(Mij = 0)\n",
      "\n",
      "p(Dij|Mij = 1) =\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "⎣\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "p(xn|θ)\n",
      "\n",
      "⎤\n",
      "\n",
      "⎦ p(θ|λ)dθ\n",
      "\n",
      "(25.57)\n",
      "\n",
      "xn∈Dij\n",
      "\n",
      "If Mij = 0, the data in Dij is assumed to have been generated by each tree independently, so\n",
      "\n",
      "p(Dij|Mij = 0) = p(Di|Ti)p(Dj|Tj)\n",
      "\n",
      "(25.58)\n",
      "\n",
      "These two terms will have already been computed by the bottom-up process. Consequently we have all the quantities we need to decide which trees to merge. See Algorithm 9 for the pseudocode, assuming p(Mij) is uniform. When ﬁnished, we can cut the tree at points where rij < 0.5.\n",
      "\n",
      "900\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "Algorithm 25.3: Bayesian hierarchical clustering 1 Initialize Di = {xi}, i = 1 :N ; 2 Compute p(Di|Ti), i = 1 :N ; 3 repeat 4\n",
      "\n",
      "5\n",
      "\n",
      "for each pair of clusters i, j do Compute p(Dij|Tij)\n",
      "\n",
      "Find the pair Di and Dj with highest merge probability rij; Merge Dk := Di ∪ Dj; Delete Di, Dj ; 8 9 until all clusters merged;\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "25.5.4.2\n",
      "\n",
      "The connection with Dirichlet process mixture models\n",
      "\n",
      "In this section, we will establish the connection between BHC and DPMMs. This will in turn give us an algorithm to compute the prior probabilities p(Mij = 1).\n",
      "\n",
      "Note that the marginal likelihood of a DPMM, summing over all 2N − 1 partitions, is given by\n",
      "\n",
      "p(Dk) =\n",
      "\n",
      "p(v) =\n",
      "\n",
      "v∈V αmv\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(v)p(Dv) (cid:26)mv\n",
      "\n",
      "Γ(nk+α) Γ(α)\n",
      "\n",
      "l=1 Γ(nv l )\n",
      "\n",
      "(25.60)\n",
      "\n",
      "(25.59)\n",
      "\n",
      "p(Dv) =\n",
      "\n",
      "mv(cid:20)\n",
      "\n",
      "p(Dv l )\n",
      "\n",
      "(25.61)\n",
      "\n",
      "l=1\n",
      "\n",
      "where V is the set of all possible partitions of Dk, p(v) is the probability of partition v, mv is the number of clusters in partition v, nv is the number of points in cluster l of partition v, Dv l l are the points in cluster l of partition v, and nk are the number of points in Dk.\n",
      "\n",
      "One can show (Heller and Ghahramani 2005) that p(Dk|Tk) computed by the BHC algorithm is similar to p(Dk) given above, except for the fact that it only sums over partitions which are consistent with tree Tk. (The number of tree-consistent partitions is exponential in the number of data points for balanced binary trees, but this is obviously a subset of all possible partitions.) In this way, we can use the BHC algorithm to compute a lower bound on the marginal likelihood of the data from a DPMM. Furthermore, we can interpret the algorithm as greedily searching through the exponentially large space of tree-consistent partitions to ﬁnd the best ones of a given size at each step.\n",
      "\n",
      "We are now in a position to compute πk = p(Mk = 1), for each node k with children i and j. This is equal to the probability of cluster Dk coming from the DPMM, relative to all other partitions of Dk consistent with the current tree. This can be computed as follows: initialize di = α and πi = 1 for each leaf i; then as we build the tree, for each internal node k, compute dk = αΓ(nk) +d idj, and πk =\n",
      "\n",
      "αΓ(nk) dk\n",
      "\n",
      ", wherei and j are k’s left and right children.\n",
      "\n",
      "25.6. Clustering datapoints and features\n",
      "\n",
      "901\n",
      "\n",
      "Data Set Synthetic Newsgroups Spambase Digits Fglass\n",
      "\n",
      "Single Linkage 0.599 ± 0.033 0.275 ± 0.001 0.598 ± 0.017 0.224 ± 0.004 0.478 ± 0.009\n",
      "\n",
      "Complete Linkage 0.634 ± 0.024 0.315 ± 0.008 0.699 ± 0.017 0.299 ± 0.006 0.476 ± 0.009\n",
      "\n",
      "Average Linkage 0.668 ± 0.040 0.282 ± 0.002 0.668 ± 0.019 0.342 ± 0.005 0.491 ± 0.009\n",
      "\n",
      "BHC 0.828 ± 0.025 0.465 ± 0.016 0.728 ± 0.029 0.393 ± 0.015 0.467 ± 0.011\n",
      "\n",
      "Table 25.1 Purity scores for various hierarchical clustering schemes applied to various data sets. The synthetic data has N = 200, D = 2, C = 4 and real features. Newsgroups is extracted from the 20 newsgroups dataset (D = 500, N = 800, C = 4, binary features). Spambase has N = 100, C = 2, D = 57 , binary features. Digits is the CEDAR Buffalo digits (N = 200, C = 10, D = 64, binarized features). Fglass is forensic glass dataset (N = 214, C = 6, D = 9, real features). Source: Table 1 of (Heller and Ghahramani 2005). Used with kind permission of Katherine Heller.\n",
      "\n",
      "25.5.4.3\n",
      "\n",
      "Learning the hyper-parameters\n",
      "\n",
      "The model has two free-parameters: α and λ, where λ are the hyper-parameters for the prior on the parameters θ. In (Heller and Ghahramani 2005), they show how one can back-propagate gradients of the form through the tree, and thus perform an empirical Bayes estimate of the hyper-parameters.\n",
      "\n",
      "∂p(Dk|Tk) ∂λ\n",
      "\n",
      "25.5.4.4\n",
      "\n",
      "Experimental results\n",
      "\n",
      "(Heller and Ghahramani 2005) compared BHC with traditional agglomerative clustering algo- rithms on various data sets in terms of purity scores. The results are shown in Table 25.1. We see that BHC did much better than the other methods on all datasets except the forensic glass one.\n",
      "\n",
      "Figure 25.16 visualizes the tree structure estimated by BHC and agglomerative hierarchical clustering (AHC) on the newsgroup data (using a beta-Bernoulli model). The BHC tree is clearly superior (look at the colors at the leaves, which represent class labels). Figure 25.17 is a zoom-in on the top few nodes of these two trees. BHC splits off clusters concerning sports from clusters concerning cars and space. AHC keeps sports and cars merged together. Although sports and cars both fall under the same “rec” newsgroup heading (as opposed to space, that comes under the “sci” newsgroup heading), the BHC clustering still seems more reasonable, and this is borne out by the quantitative purity scores.\n",
      "\n",
      "BHC has also been applied to gene expression data, with good results (Savage et al. 2009).\n",
      "\n",
      "25.6\n",
      "\n",
      "Clustering datapoints and features\n",
      "\n",
      "So far, we have been concentrating on clustering datapoints. But each datapoint is often described by multiple features, and we might be interested in clustering them as well. Below we describe some methods for doing this.\n",
      "\n",
      "902\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "4 Newsgroups Average Linkage Clustering\n",
      "\n",
      "(a)\n",
      "\n",
      "4 Newsgroups Bayesian Hierarchical Clustering\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.16 Hierarchical clustering applied to 800 documents from 4 newsgroups (red is rec.autos, blue is rec.sport.baseball, green is rec.sport.hockey, and magenta is sci.space). Top: average linkage hierarchical clustering. Bottom: Bayesian hierarchical clustering. Each of the leaves is labeled with a color, according to which newsgroup that document came from. We see that the Bayesian method results in a clustering that is more consistent with these labels (which were not used during model ﬁtting). Source: Figure 7 of (Heller and Ghahramani 2005). Used with kind permission of Katherine Heller.\n",
      "\n",
      "25.6. Clustering datapoints and features\n",
      "\n",
      "903\n",
      "\n",
      "354\n",
      "\n",
      "All Data\n",
      "\n",
      "446\n",
      "\n",
      "1\n",
      "\n",
      "All Data\n",
      "\n",
      "799\n",
      "\n",
      "205\n",
      "\n",
      "Game Team Play\n",
      "\n",
      "149\n",
      "\n",
      "284\n",
      "\n",
      "Car Space NASA\n",
      "\n",
      "162\n",
      "\n",
      "Quebec Jet Boston\n",
      "\n",
      "Car Baseball Engine\n",
      "\n",
      "2\n",
      "\n",
      "Pitcher Boston Ball\n",
      "\n",
      "797\n",
      "\n",
      "Car Player Space\n",
      "\n",
      "Baseball Pitch Hit\n",
      "\n",
      "NHL Hockey Round\n",
      "\n",
      "Car Dealer Drive\n",
      "\n",
      "Space NASA Orbit\n",
      "\n",
      "1\n",
      "\n",
      "Vehicle Dealer Driver\n",
      "\n",
      "796\n",
      "\n",
      "Team Game Hockey\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.17 Zoom-in on the top nodes in the trees of Figure 25.16. (b) Average linkage. We show the 3 most probable words per cluster. The number of documents at each cluster is also given. Source: Figure 5 of (Heller and Ghahramani 2005). Used with kind permission of Katherine Heller.\n",
      "\n",
      "(a) Bayesian method.\n",
      "\n",
      "25.6.1\n",
      "\n",
      "Biclustering\n",
      "\n",
      "Clustering the rows and columns is known as biclustering or coclustering. This is widely used in bioinformatics, where the rows often represent genes and the columns represent conditions. It can also be used for collaborative ﬁltering, where the rows represent users and the columns represent movies.\n",
      "\n",
      "A variety of ad hoc methods for biclustering have been proposed; see (Madeira and Oliveira 2004) for a review. Here we present a simple probabilistic generative model, based on (Kemp et al. 2006) (see also (Sheng et al. 2003) for a related approach). The idea is to associate each row and each column with a latent indicator, ri ∈ {1, . . . , K r}, cj ∈ {1, . . . , K c}. We then assume the data are iid across samples and across features within each block:\n",
      "\n",
      "p(x|r, c, θ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "p(xij|ri, cj, θ) = p(xij|θri,cj )\n",
      "\n",
      "(25.62)\n",
      "\n",
      "i\n",
      "\n",
      "j\n",
      "\n",
      "where θa,b are the parameters for row cluster a and column cluster b. Rather than using a ﬁnite number of clusters for the rows and columns, we can use a Dirchlet process, as in the inﬁnite relational model which we discuss in Section 27.6.1. We can ﬁt this model using e.g., (collapsed) Gibbs sampling.\n",
      "\n",
      "The behavior of this model is illustrated in Figure 25.18. The data has the form X(i, j) = 1 iff animal i has feature j, where i = 1 : 50 and j = 1 : 85. The animals represent whales, bears, horses, etc. The features represent properties of the habitat (jungle, tree, coastal), or anatomical properties (has teeth, quadrapedal), or behavioral properties (swims, eats meat), etc. The model, using a Bernoulli likelihood, was ﬁt to the data. It discovered 12 animal clusters and 33 feature clusters. For example, it discovered a bicluster that represents the fact that mammals tend to have aquatic features.\n",
      "\n",
      "25.6.2 Multi-view clustering\n",
      "\n",
      "The problem with biclustering is that each object (row) can only belong to one cluster. Intuitively, an object can have multiple roles, and can be assigned to different clusters depending on which\n",
      "\n",
      "904\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "O1 O2 O3 monkey, gorilla, chimp hippo, elephant, rhino O4 grizzly bear, polar bear O5\n",
      "\n",
      "killer whale, blue whale, humpback, seal, walrus, dolphin antelope, horse, giraffe, zebra, deer\n",
      "\n",
      "F1 F2 F3 F4 F5 meat teeth, eats meat, hunter, ﬁerce F6\n",
      "\n",
      "ﬂippers, strain teeth, swims, arctic, coastal, ocean, water hooves, long neck, horns hands, bipedal, jungle, tree bulbous body shape, slow, inactive\n",
      "\n",
      "walks, quadrapedal, ground\n",
      "\n",
      "O1\n",
      "\n",
      "O2 O3 O4 O5\n",
      "\n",
      "F1 2\n",
      "\n",
      "43\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "Figure 25.18 Illustration of biclustering . We show 5 of the 12 animal clusters, and 6 of the 33 feature clusters. The original data matrix is shown, partitioned according to the discovered clusters. From Figure 3 of (Kemp et al. 2006). Used with kind permission of Charles Kemp.\n",
      "\n",
      "α\n",
      "\n",
      "γ\n",
      "\n",
      "β\n",
      "\n",
      "cj\n",
      "\n",
      "θjk\n",
      "\n",
      "k = 1 : ∞\n",
      "\n",
      "riv\n",
      "\n",
      "v = 1 : ∞\n",
      "\n",
      "xij\n",
      "\n",
      "j = 1 : D\n",
      "\n",
      "i = 1 : N\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 25.19 (a) Illustration of multi-view clustering. Here we have 3 views (column partitions). In the ﬁrst view, we have 2 clusters (row partitions). In the third view, we have 2 clusters. The number of views and partitions are inferred from data. Rows within each colored block are assumed to generated iid; however, each column can have a different distributional form, which is useful for modeling discrete and continuous data. From Figure 1 of (Guan et al. 2010). Used with kind permission of Jennifer Dy. (b) Corresponding DGM.\n",
      "\n",
      "In the second view, we have 3 clusters.\n",
      "\n",
      "subset of features you use. For example, in the animal dataset, we may want to group the animals on the basis of anatomical features (e.g., mammals are warm blooded, reptiles are not), or on the basis of behavioral features (e.g., predators vs prey).\n",
      "\n",
      "We now present a model that can capture this phenomenon. This model was indepen- dently proposed in (Shafto et al. 2006; Mansinghka et al. 2011), who call it crosscat (for cross- categorization), and in (Guan et al. 2010; Cui et al. 2010), who call it (non-parametric) multi-clust. (See also (Rodriguez and Ghosh 2011) for a very similar model.) The idea is that we partition the columns (features) into V groups or views, so cj ∈ {1, . . . , V }, where j ∈ {1, . . . , D} indexes\n",
      "\n",
      "25.6. Clustering datapoints and features\n",
      "\n",
      "905\n",
      "\n",
      "features. We will use a Dirichlet process prior for p(c), which allows V to grow automatically. Then for each partition of the columns (i.e., each view), call it v, we partition the rows, again using a DP, as illustrated in Figure 25.19(a). Let riv ∈ {1, . . . , K(v)} be the cluster to which the i’th row belongs in view v. Finally, having partitioned the rows and columns, we generate the data: we assume all the rows and columns within a block are iid. We can deﬁne the model more precisely as follows:\n",
      "\n",
      "p(c, r, D) =p( c)p(r|c)p(D|r, c)\n",
      "\n",
      "(25.63)\n",
      "\n",
      "p(c) =DP( c|α)\n",
      "\n",
      "(25.64)\n",
      "\n",
      "p(r|c) =\n",
      "\n",
      "p(D|r, c, θ) =\n",
      "\n",
      "V (c)(cid:20)\n",
      "\n",
      "v=1 V (c)(cid:20)\n",
      "\n",
      "DP(rv|β) ⎡\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "⎣\n",
      "\n",
      "K(rv)(cid:20)\n",
      "\n",
      "(cid:12) (cid:20)\n",
      "\n",
      "p(xij|θjk)p(θjk)dθjk\n",
      "\n",
      "⎤\n",
      "\n",
      "⎦\n",
      "\n",
      "(25.65)\n",
      "\n",
      "(25.66)\n",
      "\n",
      "v=1\n",
      "\n",
      "j:cj =v\n",
      "\n",
      "k=1\n",
      "\n",
      "i:riv=k\n",
      "\n",
      "See Figure 25.19(b) for the DGM.2\n",
      "\n",
      "If the data is binary, and we use a Beta(γ, γ) prior for θjk, the likelihood reduces to\n",
      "\n",
      "p(D|r, c, γ) =\n",
      "\n",
      "V (c)(cid:20)\n",
      "\n",
      "v=1\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "j:cj =v\n",
      "\n",
      "K(rv)(cid:20)\n",
      "\n",
      "k=1\n",
      "\n",
      "Beta(nj,k,v + γ, nj,k,v + γ) Beta(γ, γ)\n",
      "\n",
      "(25.67)\n",
      "\n",
      "where nj,k,v = i:ri,v=k I(xij = 1) counts the number of features which are on in the j’th column for view v and for row cluster k. Similarly, nj,k,v counts how many features are off. The model is easily extended to other kinds of data, by replacing the beta-Bernoulli with, say, the Gaussian-Gamma-Gaussian model, as discussed in (Guan et al. 2010; Mansinghka et al. 2011). Approximate MAP estimation can be done using stochastic search (Shafto et al. 2006), and approximate inference can be done using variational Bayes (Guan et al. 2010) or Gibbs sampling (Mansinghka et al. 2011). The hyper-parameter γ for the likelihood can usually be set in a non- informative way, but results are more sensitive to the other two parameters, since α controls the number of column partitions, and β controls the number of row partitions. Hence a more robust technique is to infer the hyper-parameters using MH. This also speeds up convergence (Mansinghka et al. 2011).\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "Figure 25.20 illustrates the model applied to some binary data containing 22 animals and 106 features. The ﬁgures shows the (approximate) MAP partition. The ﬁrst partition of the columns contains taxonomic features, such as “has bones”, “is warm-blooded”, “lays eggs”, etc. This divides the animals into birds, reptiles/ amphibians, mammals, and invertebrates. The second partition of the columns contains features that are treated as noise, with no apparent structure (except for the single row labeled “frog”). The third partition of the columns contains ecological features like “dangerous”, “carnivorous”, “lives in water”, etc. This divides the animals into prey, land predators, sea predators and air predators. Thus each animal (row) can belong to a different\n",
      "\n",
      "2. The dependence between r and c is not shown, since it is not a dependence between the values of riv and cj , but between the cardinality of v and cj . In other words, the number of row partitions we need to specify (the number of views, indexed by v) depends on the number of column partitions (clusters) that we have.\n",
      "\n",
      "906\n",
      "\n",
      "Chapter 25. Clustering\n",
      "\n",
      "Leopard Sheep Seal Dolphin Monkey Bat Alligator Iguana Frog Python Finch Ostrich Seagull Owl Penguin Eagle Grasshopper Ant Bee Jellyfish Octopus Dragonfly\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "Frog\n",
      "\n",
      "C\n",
      "\n",
      "Leopard Alligator Python Seal Dolphin Frog Jellyfish Octopus Penguin Finch Seagull Owl Eagle Dragonfly Bat Grasshopper Ant Bee Sheep Monkey Iguana Ostrich\n",
      "\n",
      "s e n o b s a h\n",
      "\n",
      "s g g e s y a\n",
      "\n",
      "l\n",
      "\n",
      "d e d o o b − m r a w s i\n",
      "\n",
      "l\n",
      "\n",
      "a m m a m a s i\n",
      "\n",
      "l\n",
      "\n",
      "s k w a u q s\n",
      "\n",
      "k a e b a s a h\n",
      "\n",
      "e u g n o t a s a h\n",
      "\n",
      "n e e r g s i\n",
      "\n",
      "d r o c l\n",
      "\n",
      "i\n",
      "\n",
      "a n p s a s a h\n",
      "\n",
      "d r a z i l\n",
      "\n",
      "a s i\n",
      "\n",
      "e a n n e t n a s a h\n",
      "\n",
      "i l f\n",
      "\n",
      "s r e p p\n",
      "\n",
      "s a h\n",
      "\n",
      "s w a p s a h\n",
      "\n",
      "i\n",
      "\n",
      "l\n",
      "\n",
      "n a r b e g r a\n",
      "\n",
      "a s a h\n",
      "\n",
      "a t a s a h\n",
      "\n",
      "l i\n",
      "\n",
      "s i\n",
      "\n",
      "y r r u f\n",
      "\n",
      "s t a e\n",
      "\n",
      "e c i m\n",
      "\n",
      "s t a e\n",
      "\n",
      "s t n e d o r\n",
      "\n",
      "t u o n s a s a h\n",
      "\n",
      "n w o r b s i\n",
      "\n",
      "s e s i o n d u o\n",
      "\n",
      "l\n",
      "\n",
      "s e k a m\n",
      "\n",
      "s a h\n",
      "\n",
      "h t e e t\n",
      "\n",
      "t e e f\n",
      "\n",
      "s a h\n",
      "\n",
      "t r a m\n",
      "\n",
      "s\n",
      "\n",
      "s i\n",
      "\n",
      "s p u o r g n\n",
      "\n",
      "i\n",
      "\n",
      "s l e v a r t\n",
      "\n",
      "s e v\n",
      "\n",
      "l\n",
      "\n",
      "i l\n",
      "\n",
      "i\n",
      "\n",
      "s e k a\n",
      "\n",
      "n\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "n a b h p m a n a s i\n",
      "\n",
      "t n e d o r a s i\n",
      "\n",
      "l l\n",
      "\n",
      "s i\n",
      "\n",
      "a t\n",
      "\n",
      "h s i f a s i\n",
      "\n",
      "i l s\n",
      "\n",
      "y m\n",
      "\n",
      "s i\n",
      "\n",
      "s n r o h s a h\n",
      "\n",
      "s e v o o h s a h\n",
      "\n",
      "i l\n",
      "\n",
      "e f a s i\n",
      "\n",
      "e n\n",
      "\n",
      "s r a o r\n",
      "\n",
      "s n i f\n",
      "\n",
      "s a h\n",
      "\n",
      "t e e f d e b b e w s a h\n",
      "\n",
      "s t u n s t a e\n",
      "\n",
      "s i\n",
      "\n",
      "s\n",
      "\n",
      "h t o o m\n",
      "\n",
      "i\n",
      "\n",
      "s e e r t n\n",
      "\n",
      "s e v\n",
      "\n",
      "i l\n",
      "\n",
      "l\n",
      "\n",
      "e g r a\n",
      "\n",
      "s i\n",
      "\n",
      "i l c d o c n\n",
      "\n",
      "s e v\n",
      "\n",
      "s e t a m\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "i l\n",
      "\n",
      "s u o i c o r e f\n",
      "\n",
      "s i\n",
      "\n",
      "s u o r e g n a d s i\n",
      "\n",
      "e r o v n r a c a s i\n",
      "\n",
      "i\n",
      "\n",
      "r o t a d e r p a s i\n",
      "\n",
      "s e v\n",
      "\n",
      "r e t a w n\n",
      "\n",
      "i l\n",
      "\n",
      "i\n",
      "\n",
      "s e\n",
      "\n",
      "i l f\n",
      "\n",
      "s i\n",
      "\n",
      "g n o\n",
      "\n",
      "l\n",
      "\n",
      "s e v a e\n",
      "\n",
      "l\n",
      "\n",
      "s t a e\n",
      "\n",
      "s l a m n a s t a e\n",
      "\n",
      "i\n",
      "\n",
      "s s a r g n\n",
      "\n",
      "i l\n",
      "\n",
      "i\n",
      "\n",
      "s e v\n",
      "\n",
      "s t a e\n",
      "\n",
      "h s i f\n",
      "\n",
      "s e t a m\n",
      "\n",
      "i l c t o h n\n",
      "\n",
      "i\n",
      "\n",
      "s e v\n",
      "\n",
      "i l\n",
      "\n",
      "Figure 25.20 MAP estimate produced by the crosscat system when applied to a binary data matrix of animals (rows) by features (columns). See text for details. Source: Figure 7 of (Shafto et al. 2006) . Used with kind permission of Vikash Mansingkha.\n",
      "\n",
      "cluster depending on what set of features are considered. Uncertainty about the partitions can be handled by sampling.\n",
      "\n",
      "It is interesting to compare this model to a standard inﬁnite mixture model. While the standard model can represent any density on ﬁxed-sized vectors as N → ∞, it cannot cope with D → ∞, since it has no way to handle irrelevant, noisy or redundant features. By contrast, the crosscat/multi-clust system is robust to irrelevant features: it can just partition them off, and cluster the rows only using the relevant features. Note, however, that it does not need a separate “background” model, since everything is modelled using the same mechanism. This is useful, since one’s person’s noise is another person’s signal. (Indeed, this symmetry may explain why multi-clust outperformed the sparse mixture model approach of (Law et al. 2004) in the experiments reported in (Guan et al. 2010).)\n",
      "\n",
      "26 Graphical model structure learning\n",
      "\n",
      "26.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "We have seen how graphical models can be used to express conditional independence assump- tions between variables. In this chapter, we discuss how to learn the structure of the graphical model itself. That is, we want to compute p(G|D), whereG is the graph structure, represented as an V × V adjacency matrix.\n",
      "\n",
      "As we discussed in Section 1.3.3, there are two main applications of structure learning: knowl- edge discovery and density estimation. The former just requires a graph topology, whereas the latter requires a fully speciﬁed model.\n",
      "\n",
      "The main obstacle in structure learning is that the number of possible graphs is exponential in the number of nodes: a simple upper bound is O(2V (V −1)/2). Thus the full posterior p(G|D) is prohibitively large: even if we could afford to compute it, we could not even store it. So we will seek appropriate summaries of the posterior. These summary statistics depend on our task. is knowledge discovery, we may want to compute posterior edge marginals, p(Gst = 1|D); we can then plot the corresponding graph, where the thickness of each edge represents our conﬁdence in its presence. By setting a threshold, we can generate a sparse graph, which can be useful for visualization purposes (see Figure 1.11).\n",
      "\n",
      "If our goal\n",
      "\n",
      "If our goal is density estimation, we may want to compute the MAP graph, ˆG ∈ argmaxG p(G|D).\n",
      "\n",
      "In most cases, ﬁnding the globally optimal graph will take exponential time, so we will use dis- crete optimization methods such as heuristic search. However, in the case of trees, we can ﬁnd the globally optimal graph structure quite efficiently using exact methods, as we discuss in Section 26.3.\n",
      "\n",
      "it is worth considering whether it would be more appropriate to learn a latent variable model, which can capture correlation between the visible variables via a set of latent common causes (see Chapters 12 and 27). Such models are often easier to learn and, perhaps more importantly, they can be applied (for prediction purposes) much more efficiently, since they do not require performing inference in a learned graph with potentially high treewidth. The downside with such models is that the latent factors are often unidentiﬁable, and hence hard to interpret. Of course, we can combine graphical model structure learning and latent variable learning, as we will show later in this chapter.\n",
      "\n",
      "If density estimation is our only goal,\n",
      "\n",
      "In some cases, we don’t just want to model the observed correlation between variables; instead, we want to model the causal structure behind the data, so we can predict the effects of manipulating variables. This is a much more challenging task, which we brieﬂy discuss in\n",
      "\n",
      "908\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "bible\n",
      "\n",
      "case\n",
      "\n",
      "course\n",
      "\n",
      "evidence\n",
      "\n",
      "children\n",
      "\n",
      "mission\n",
      "\n",
      "launch\n",
      "\n",
      "christian\n",
      "\n",
      "fact\n",
      "\n",
      "israel\n",
      "\n",
      "government\n",
      "\n",
      "earth\n",
      "\n",
      "gun\n",
      "\n",
      "nasa\n",
      "\n",
      "lunar\n",
      "\n",
      "god\n",
      "\n",
      "human\n",
      "\n",
      "jews\n",
      "\n",
      "war\n",
      "\n",
      "president\n",
      "\n",
      "law\n",
      "\n",
      "orbit\n",
      "\n",
      "shuttle\n",
      "\n",
      "moon\n",
      "\n",
      "jesus\n",
      "\n",
      "computer\n",
      "\n",
      "religion\n",
      "\n",
      "world\n",
      "\n",
      "rights\n",
      "\n",
      "solar\n",
      "\n",
      "space\n",
      "\n",
      "science\n",
      "\n",
      "state\n",
      "\n",
      "university\n",
      "\n",
      "Figure 26.1 Part of a relevance network constructed from the 20-news data shown in Figure 1.2. We show edges whose mutual information is greater than or equal to 20% of the maximum pairwise MI. For clarity, the graph has been cropped, so we only show a subset of the nodes and edges. Figure generated by relevanceNetworkNewsgroupDemo.\n",
      "\n",
      "Section 26.6.\n",
      "\n",
      "26.2\n",
      "\n",
      "Structure learning for knowledge discovery\n",
      "\n",
      "Since computing the MAP graph or the exact posterior edge marginals is in general computa- tionally intractable (Chickering 1996), in this section we discuss some “quick and dirty” methods for learning graph structures which can be used to visualize one’s data. The resulting models do not constitute consistent joint probability distributions, so they cannot be used for prediction, and they cannot even be formally evaluated in terms of goodness of ﬁt. Nevertheless, these methods are a useful ad hoc tool to have in one’s data visualization toolbox, in view of their speed and simplicity.\n",
      "\n",
      "26.2.1\n",
      "\n",
      "Relevance networks\n",
      "\n",
      "A relevance network is a way of visualizing the pairwise mutual information between multiple random variables: we simply choose a threshold and draw an edge from node i to node j if I (Xi; Xj) is above this threshold. In the Gaussian case, I (Xi; Xj) =− 1 ij), where ρij is the correlation coefficient (see Exercise 2.13), so we are essentially visualizing Σ; this is known as the covariance graph (Section 19.4.4.1).\n",
      "\n",
      "2 log(1 − ρ2\n",
      "\n",
      "This method is quite popular in systems biology (Margolin et al. 2006), where it is used to visualize the interaction between genes. The trouble with biological examples is that they are hard for non-biologists to understand. So let us instead illustrate the idea using natural language text. Figure 26.1 gives an example, where we visualize the MI between words in the newsgroup dataset from Figure 1.2. The results seem intuitively reasonable.\n",
      "\n",
      "However, relevance networks suffer from a major problem: the graphs are usually very dense, since most variables are dependent on most other variables, even after thresholding the MIs. For example, suppose X1 directly inﬂuences X2 which directly inﬂuences X3 (e.g., these form components of a signalling cascade, X1 − X2 − X3). Then X1 has non-zero MI with X3 (and vice versa), so there will be a 1 − 3 edge in the relevance network. Indeed, most pairs will be\n",
      "\n",
      "26.2. Structure learning for knowledge discovery\n",
      "\n",
      "909\n",
      "\n",
      "children\n",
      "\n",
      "case\n",
      "\n",
      "course\n",
      "\n",
      "fact\n",
      "\n",
      "question\n",
      "\n",
      "earth\n",
      "\n",
      "bible\n",
      "\n",
      "christian\n",
      "\n",
      "food\n",
      "\n",
      "baseball\n",
      "\n",
      "mission\n",
      "\n",
      "god\n",
      "\n",
      "disk\n",
      "\n",
      "mac\n",
      "\n",
      "car\n",
      "\n",
      "aids\n",
      "\n",
      "doctor\n",
      "\n",
      "fans\n",
      "\n",
      "nasa\n",
      "\n",
      "jesus\n",
      "\n",
      "pc\n",
      "\n",
      "dos\n",
      "\n",
      "drive\n",
      "\n",
      "bmw\n",
      "\n",
      "israel\n",
      "\n",
      "government\n",
      "\n",
      "health\n",
      "\n",
      "games\n",
      "\n",
      "hockey\n",
      "\n",
      "hit\n",
      "\n",
      "launch\n",
      "\n",
      "memory\n",
      "\n",
      "scsi\n",
      "\n",
      "jews\n",
      "\n",
      "engine\n",
      "\n",
      "dealer\n",
      "\n",
      "state\n",
      "\n",
      "war\n",
      "\n",
      "computer\n",
      "\n",
      "president\n",
      "\n",
      "medicine\n",
      "\n",
      "season\n",
      "\n",
      "puck\n",
      "\n",
      "nhl\n",
      "\n",
      "shuttle\n",
      "\n",
      "religion\n",
      "\n",
      "data\n",
      "\n",
      "card\n",
      "\n",
      "honda\n",
      "\n",
      "power\n",
      "\n",
      "oil\n",
      "\n",
      "world\n",
      "\n",
      "insurance\n",
      "\n",
      "science\n",
      "\n",
      "studies\n",
      "\n",
      "team\n",
      "\n",
      "software\n",
      "\n",
      "solar\n",
      "\n",
      "graphics\n",
      "\n",
      "driver\n",
      "\n",
      "gun\n",
      "\n",
      "research\n",
      "\n",
      "university\n",
      "\n",
      "water\n",
      "\n",
      "human\n",
      "\n",
      "cancer\n",
      "\n",
      "win\n",
      "\n",
      "league\n",
      "\n",
      "lunar\n",
      "\n",
      "system\n",
      "\n",
      "display\n",
      "\n",
      "video\n",
      "\n",
      "windows\n",
      "\n",
      "law\n",
      "\n",
      "disease\n",
      "\n",
      "won\n",
      "\n",
      "players\n",
      "\n",
      "moon\n",
      "\n",
      "server\n",
      "\n",
      "files\n",
      "\n",
      "rights\n",
      "\n",
      "problem\n",
      "\n",
      "evidence\n",
      "\n",
      "space\n",
      "\n",
      "program\n",
      "\n",
      "format\n",
      "\n",
      "patients\n",
      "\n",
      "msg\n",
      "\n",
      "help\n",
      "\n",
      "mars\n",
      "\n",
      "orbit\n",
      "\n",
      "technology\n",
      "\n",
      "ftp\n",
      "\n",
      "image\n",
      "\n",
      "number\n",
      "\n",
      "vitamin\n",
      "\n",
      "email\n",
      "\n",
      "satellite\n",
      "\n",
      "version\n",
      "\n",
      "phone\n",
      "\n",
      "Figure 26.2 A dependency network constructed from the 20-news data. We show all edges with regres- sion weight above 0.5 in the Markov blankets estimated by (cid:7)1 penalized logistic regression. Undirected edges represent cases where a directed edge was found in both directions. From Figure 4.9 of (Schmidt 2010). Used with kind permission of Mark Schmidt.\n",
      "\n",
      "connected.\n",
      "\n",
      "A better approach is to use graphical models, which represent conditional independence, In the above example, X1 is conditionally independent of X3 given rather than dependence. X2, so there will not be a 1 − 3 edge. Consequently graphical models are usually much sparser than relevance networks, and hence are a more useful way of visualizing interactions between multiple variables.\n",
      "\n",
      "26.2.2\n",
      "\n",
      "Dependency networks\n",
      "\n",
      "A simple and efficient way to learn a graphical model structure is to independently ﬁt D sparse full-conditional distributions p(xt|x−t); this is called a dependency network (Heckerman et al. 2000). The chosen variables constitute the inputs to the node, i.e., its Markov blanket. We can then visualize the resulting sparse graph. The advantage over relevance networks is that redundant variables will not be selected as inputs.\n",
      "\n",
      "We can use any kind of sparse regression or classiﬁcation method to ﬁt each CPD. (Heckerman (Meinshausen and Buhlmann 2006) use (cid:2)1- et al. 2000) uses classiﬁcation/ regression trees, regularized linear regression, (Wainwright et al. 2006) use (cid:2)1-regularized logistic regression (see (Meinshausen depnetFit for some code), (Dobra 2009) uses Bayesian variable selection, etc.\n",
      "\n",
      "910\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "and Buhlmann 2006) discuss theoretical conditions under which (cid:2)1-regularized linear regression can recover the true graph structure, assuming the data was generated from a sparse Gaussian graphical model.\n",
      "\n",
      "Figure 26.2 shows a dependency network that was learned from the 20-newsgroup data using (cid:2)1 regularized logistic regression, where the penalty parameter λ was chosen by BIC. Many of the words present in these estimated Markov blankets represent fairly natural associations (aids:disease, baseball:fans, bible:god, bmw:car, cancer:patients, etc.). However, some of the esti- mated statistical dependencies seem less intuitive, such as baseball:windows and bmw:christian. We can gain more insight if we look not only at the sparsity pattern, but also the values of the regression weights. For example, here are the incoming weights for the ﬁrst 5 words:\n",
      "\n",
      "aids: children (0.53), disease (0.84), fact (0.47), health (0.77), president (0.50), research (0.53)\n",
      "\n",
      "baseball: christian (-0.98), drive (-0.49), games (0.81), god (-0.46), government (-0.69), hit (0.62),\n",
      "\n",
      "memory (-1.29), players (1.16), season (0.31), software (-0.68), windows (-1.45)\n",
      "\n",
      "bible: car (-0.72), card (-0.88), christian (0.49), fact (0.21), god (1.01), jesus (0.68), orbit (0.83),\n",
      "\n",
      "program (-0.56), religion (0.24), version (0.49)\n",
      "\n",
      "bmw: car (0.60), christian (-11.54), engine (0.69), god (-0.74), government (-1.01), help (-0.50),\n",
      "\n",
      "windows (-1.43)\n",
      "\n",
      "cancer: disease (0.62), medicine (0.58), patients (0.90), research (0.49), studies (0.70)\n",
      "\n",
      "Words in italic red have negative weights, which represents a dissociative relationship. For example, the model reﬂects that baseball:windows is an unlikely combination. It turns out that most of the weights are negative (1173 negative, 286 positive, 8541 zero) in this model.\n",
      "\n",
      "In addition to visualizing the data, a dependency network can be used for inference. However, the only algorithm we can use is Gibbs sampling, where we repeatedly sample the nodes with missing values from their full conditionals. Unfortunately, a product of full conditionals does not, in general, constitute a representation of any valid joint distribution (Heckerman et al. 2000), so the output of the Gibbs sampler may not be meaningful. Nevertheless, the method can sometimes give reasonable results if there is not much missing data, and it is a useful method In addition, the method can be used as for data imputation (Gelman and Raghunathan 2001). an initialization technique for more complex structure learning methods that we discuss below.\n",
      "\n",
      "26.3\n",
      "\n",
      "Learning tree structures\n",
      "\n",
      "For the rest of this chapter, we focus on learning fully speciﬁed joint probability models, which can be used for density estimation, prediction and knowledge discovery.\n",
      "\n",
      "Since the problem of structure learning for general graphs is NP-hard (Chickering 1996), we start by considering the special case of trees. Trees are special because we can learn their structure efficiently, as we disuscs below, and because, once we have learned the tree, we can use them for efficient exact inference, as discussed in Section 20.2.\n",
      "\n",
      "26.3. Learning tree structures\n",
      "\n",
      "911\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 26.3 An undirected tree and two equivalent directed trees.\n",
      "\n",
      "26.3.1\n",
      "\n",
      "Directed or undirected tree?\n",
      "\n",
      "Before continuing, we need to discuss the issue of whether we should use directed or undirected trees. A directed tree, with a single root node r, deﬁnes a joint distribution as follows:\n",
      "\n",
      "p(x|T ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "p(xt|xpa(t))\n",
      "\n",
      "(26.1)\n",
      "\n",
      "t∈V\n",
      "\n",
      "where we deﬁne pa(r) = ∅. For example, in Figure 26.3(b-c), we have\n",
      "\n",
      "p(x1, x2, x3, x4|T ) =p( x1)p(x2|x1)p(x3|x2)p(x4|x2) = p(x2)p(x1|x2)p(x3|x2)p(x4|x2)\n",
      "\n",
      "(26.2)\n",
      "\n",
      "(26.3)\n",
      "\n",
      "We see that the choice of root does not matter: both of these models are equivalent.\n",
      "\n",
      "To make the model more symmetric, it is preferable to use an undirected tree. This can be\n",
      "\n",
      "represented as follows:\n",
      "\n",
      "p(x|T ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "t∈V\n",
      "\n",
      "p(xt)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(s,t)∈E\n",
      "\n",
      "p(xs, xt) p(xs)p(xt)\n",
      "\n",
      "(26.4)\n",
      "\n",
      "where p(xs, xt) is an edge marginal and p(xt) is a node marginal. For example, in Figure 26.3(a) we have\n",
      "\n",
      "p(x1, x2, x3, x4|T ) = p(x1)p(x2)p(x3)p(x4)\n",
      "\n",
      "p(x1, x2)p(x2, x3)p(x2, x4) p(x1)p(x2)p(x2)p(x3)p(x2)p(x4)\n",
      "\n",
      "(26.5)\n",
      "\n",
      "To see the equivalence with the directed representation, let us cancel terms to get\n",
      "\n",
      "p(x1, x2, x3, x4|T ) =p( x1, x2)\n",
      "\n",
      "p(x2, x3) p(x2) = p(x1)p(x2|x1)p(x3|x2)p(x4|x2) = p(x2)p(x1|x2)p(x3|x2)p(x4|x2)\n",
      "\n",
      "p(x2, x4) p(x2)\n",
      "\n",
      "(26.6)\n",
      "\n",
      "(26.7)\n",
      "\n",
      "(26.8)\n",
      "\n",
      "where p(xt|xs) = p(xs, xt)/p(xs).\n",
      "\n",
      "the number of parameters is the same, and hence the complexity of learning is the same. And of course, inference is the same in both representations, too. The undirected representation, which is symmetric, is useful for structure learning, but the directed representation is more convenient for parameter learning.\n",
      "\n",
      "Thus a tree can be represented as either an undirected or directed graph:\n",
      "\n",
      "912\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "26.3.2\n",
      "\n",
      "Chow-Liu algorithm for ﬁnding the ML tree structure\n",
      "\n",
      "Using Equation 26.4, we can write the log-likelihood for a tree as follows:\n",
      "\n",
      "log p(D|θ, T ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "Ntk log p(xt = k|θ)\n",
      "\n",
      "t\n",
      "\n",
      "+\n",
      "\n",
      "k (cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "s,t\n",
      "\n",
      "j,k\n",
      "\n",
      "Nstjk log\n",
      "\n",
      "p(xs = j, xt = k|θ) p(xs = j|θ)p(xt = k|θ)\n",
      "\n",
      "(26.9)\n",
      "\n",
      "where Nstjk is the number of times node s is in state j and node t is in state k, and Ntk is the number of times node t is in state k. We can rewrite these counts in terms of the empirical distribution: Nstjk = N pemp(xs = j, xt = k) and Ntk = N pemp(xt = k). Setting θ to the MLEs, this becomes log p(D|θ, T ) N\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "t∈V\n",
      "\n",
      "+\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "k (cid:4)\n",
      "\n",
      "pemp(xt = k) log pemp(xt = k)\n",
      "\n",
      "I(xs, xt|ˆθst)\n",
      "\n",
      "(26.10)\n",
      "\n",
      "(26.11)\n",
      "\n",
      "(s,t)∈E(T )\n",
      "\n",
      "where I(xs, xt|ˆθst) ≥ 0 is the mutual information between xs and xt given the empirical distribution:\n",
      "\n",
      "I(xs, xt|ˆθst) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "j\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "k\n",
      "\n",
      "pemp(xs = j, xt = k) log\n",
      "\n",
      "pemp(xs = j, xt = k) pemp(xs = j)pemp(xt = k)\n",
      "\n",
      "(26.12)\n",
      "\n",
      "Since the ﬁrst term in Equation 26.11 is independent of the topology T , we can ignore it when learning structure. Thus the tree topology that maximizes the likelihood can be found by computing the maximum weight spanning tree, where the edge weights are the pairwise mutual informations, I(ys, yt|ˆθst). This is called the Chow-Liu algorithm (Chow and Liu 1968).\n",
      "\n",
      "There are several algorithms for ﬁnding a max spanning tree (MST). The two best known are Prim’s algorithm and Kruskal’s algorithm. Both can be implemented to run in O(E log V ) time, where E = V 2 is the number of edges and V is the number of nodes. See e.g., (Sedgewick and Wayne 2011, 4.3) for details. Thus the overall running time is O(N V 2 + V 2 log V ), where the ﬁrst term is the cost of computing the sufficient statistics.\n",
      "\n",
      "Figure 26.4 gives an example of the method in action, applied to the binary 20 newsgroups data shown in Figure 1.2. The tree has been arbitrarily rooted at the node representing “email”. The connections that are learned seem intuitively reasonable.\n",
      "\n",
      "26.3.3\n",
      "\n",
      "Finding the MAP forest\n",
      "\n",
      "Since all trees have the same number of parameters, we can safely used the maximum likelihood score as a model selection criterion without worrying about overﬁtting. However, sometimes we may want to ﬁt a forest rather than a single tree, since inference in a forest is much faster than in a tree (we can run belief propagation in each tree in the forest in parallel). The MLE criterion will never choose to omit an edge. However, if we use the marginal likelihood or a penalized likelihood (such as BIC), the optimal solution may be a forest. Below we give the details for the marginal likelihood case.\n",
      "\n",
      "26.3. Learning tree structures\n",
      "\n",
      "913\n",
      "\n",
      "email\n",
      "\n",
      "ftp\n",
      "\n",
      "phone\n",
      "\n",
      "files\n",
      "\n",
      "number\n",
      "\n",
      "disk\n",
      "\n",
      "format\n",
      "\n",
      "windows\n",
      "\n",
      "drive\n",
      "\n",
      "memory\n",
      "\n",
      "system\n",
      "\n",
      "image\n",
      "\n",
      "card\n",
      "\n",
      "dos\n",
      "\n",
      "driver\n",
      "\n",
      "pc\n",
      "\n",
      "program\n",
      "\n",
      "version\n",
      "\n",
      "win\n",
      "\n",
      "car\n",
      "\n",
      "scsi\n",
      "\n",
      "data\n",
      "\n",
      "problem\n",
      "\n",
      "display\n",
      "\n",
      "graphics\n",
      "\n",
      "video\n",
      "\n",
      "software\n",
      "\n",
      "space\n",
      "\n",
      "team\n",
      "\n",
      "won\n",
      "\n",
      "bmw\n",
      "\n",
      "dealer\n",
      "\n",
      "engine\n",
      "\n",
      "honda\n",
      "\n",
      "mac\n",
      "\n",
      "help\n",
      "\n",
      "server\n",
      "\n",
      "launch\n",
      "\n",
      "moon\n",
      "\n",
      "nasa\n",
      "\n",
      "orbit\n",
      "\n",
      "shuttle\n",
      "\n",
      "technology\n",
      "\n",
      "fans\n",
      "\n",
      "games\n",
      "\n",
      "hockey\n",
      "\n",
      "league\n",
      "\n",
      "players\n",
      "\n",
      "puck\n",
      "\n",
      "season\n",
      "\n",
      "oil\n",
      "\n",
      "lunar\n",
      "\n",
      "mars\n",
      "\n",
      "earth\n",
      "\n",
      "satellite\n",
      "\n",
      "solar\n",
      "\n",
      "mission\n",
      "\n",
      "nhl\n",
      "\n",
      "baseball\n",
      "\n",
      "god\n",
      "\n",
      "hit\n",
      "\n",
      "bible\n",
      "\n",
      "christian\n",
      "\n",
      "jesus\n",
      "\n",
      "religion\n",
      "\n",
      "jews\n",
      "\n",
      "government\n",
      "\n",
      "israel\n",
      "\n",
      "children\n",
      "\n",
      "power\n",
      "\n",
      "president\n",
      "\n",
      "rights\n",
      "\n",
      "state\n",
      "\n",
      "war\n",
      "\n",
      "health\n",
      "\n",
      "human\n",
      "\n",
      "law\n",
      "\n",
      "university\n",
      "\n",
      "world\n",
      "\n",
      "aids\n",
      "\n",
      "food\n",
      "\n",
      "insurance\n",
      "\n",
      "medicine\n",
      "\n",
      "fact\n",
      "\n",
      "gun\n",
      "\n",
      "research\n",
      "\n",
      "science\n",
      "\n",
      "msg\n",
      "\n",
      "water\n",
      "\n",
      "patients\n",
      "\n",
      "studies\n",
      "\n",
      "case\n",
      "\n",
      "course\n",
      "\n",
      "evidence\n",
      "\n",
      "question\n",
      "\n",
      "computer\n",
      "\n",
      "cancer\n",
      "\n",
      "disease\n",
      "\n",
      "doctor\n",
      "\n",
      "vitamin\n",
      "\n",
      "Figure 26.4 The MLE tree on the 20-newsgroup data. From Figure 4.11 of (Schmidt 2010). Used with kind permission of Mark Schmidt. (A topologically equivalent tree can be produced using chowliuTreeDemo.)\n",
      "\n",
      "In Section 26.4.2.2, we explain how to compute the marginal likelihood of any DAG using a\n",
      "\n",
      "Dirichlet prior for the CPTs. The resulting expression can be written as follows:\n",
      "\n",
      "log p(D|T ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "log\n",
      "\n",
      "(cid:12) N(cid:20)\n",
      "\n",
      "p(xit|xi,pa(t)|θt)p(θt)dθt =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "score(Nt,pa(t))\n",
      "\n",
      "(26.13)\n",
      "\n",
      "t∈V\n",
      "\n",
      "i=1\n",
      "\n",
      "t\n",
      "\n",
      "where Nt,pa(t) are the counts (sufficient statistics) for node t and its parents, and score is deﬁned in Equation 26.28.\n",
      "\n",
      "Now suppose we only allow DAGs with at most one parent. Following (Heckerman et al. 1995, p227), let us associate a weight with each s → t edge, ws,t (cid:2) score(t|s) − score(t|0), where score(t|0) is the score when t has no parents. Note that the weights might be negative (unlike the MLE case, where edge weights are aways non-negative because they correspond to mutual information). Then we can rewrite the objective as follows: (cid:4)\n",
      "\n",
      "log p(D|T ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "score(t|pa(t)) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "wpa(t),t +\n",
      "\n",
      "score(t|0)\n",
      "\n",
      "(26.14)\n",
      "\n",
      "t\n",
      "\n",
      "t\n",
      "\n",
      "t\n",
      "\n",
      "The last term is the same for all trees T , so we can ignore it. Thus ﬁnding the most probable tree amounts to ﬁnding a maximal branching in the corresponding weighted directed graph. This can be found using the algorithm in (Gabow et al. 1984).\n",
      "\n",
      "914\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "If the scoring function is prior and likelihood equivalent (these terms are explained in Sec-\n",
      "\n",
      "tion 26.4.2.3), we have\n",
      "\n",
      "score(s|t) +score( t|0) = score(t|s) +score( s|0)\n",
      "\n",
      "(26.15)\n",
      "\n",
      "and hence the weight matrix is symmetric. In this case, the maximal branching is the same as the maximal weight forest. We can apply a slightly modiﬁed version of the MST algorithm to ﬁnd this (Edwards et al. 2010). To see this, let G = (V, E) be a graph with both positive and negative edge weights. Now let G(cid:2) be a graph obtained by omitting all the negative edges from G. This cannot reduce the total weight, so we can ﬁnd the maximum weight forest of G by ﬁnding the MST for each connected component of G(cid:2). We can do this by running Kruskal’s algorithm directly on G(cid:2): there is no need to ﬁnd the connected components explicitly.\n",
      "\n",
      "26.3.4 Mixtures of trees\n",
      "\n",
      "A single tree is rather limited in its expressive power. Later in this chapter we discuss ways to learn more general graphs. However, the resulting graphs can be expensive to do inference in. An interesting alternative is to learn a mixture of trees (Meila and Jordan 2000), where each mixture component may have a different tree topology. This is like an unsupervised version of the TAN classiﬁer discussed in Section 10.2.1. We can ﬁt a mixture of trees by using EM: in the E step, we compute the responsibilities of each cluster for each data point, and in the M step, we use a weighted version of the Chow-Liu algorithm. See (Meila and Jordan 2000) for details.\n",
      "\n",
      "In fact, it is possible to create an “inﬁnite mixture of trees”, by integrating out over all possible trees. Remarkably, this can be done in V 3 time using the matrix tree theorem. This allows us to perform exact Bayesian inference of posterior edge marginals etc. However, it is not tractable to use this inﬁnite mixture for inference of hidden nodes. See (Meila and Jaakkola 2006) for details.\n",
      "\n",
      "26.4\n",
      "\n",
      "Learning DAG structures\n",
      "\n",
      "In this section, we discuss how to compute (functions of) p(G|D), where G is constrained to be a DAG. This is often called Bayesian network structure learning. In this section, we assume there is no missing data, and that there are no hidden variables. This is called the complete data assumption. For simplicity, we will focus on the case where all the variables are categorical and all the CPDs are tables, although the results generalize to real-valued data and other kinds of CPDs, such as linear-Gaussian CPDs.\n",
      "\n",
      "Our presentation is based in part on (Heckerman et al. 1995), although we will follow the notation of Section 10.4.2. In particular, let xit ∈ {1, . . . , Kt} be the value of node t in case i, where Kt is the number of states for node t. Let θtck (cid:2) p(xt = k|xpa(t) = c), for k = 1 :K t, and c = 1 :C t, where Ct is the number of parent combinations (possible conditioning cases). For notational simplicity, we will often assume Kt = K, so all nodes have the same number of states. We will also let dt = dim(pa(t)) be the degree or fan-in of node t, so that Ct = K dt .\n",
      "\n",
      "26.4.1 Markov equivalence\n",
      "\n",
      "In this section, we discuss some fundamental limits to our ability to learn DAG structures from data.\n",
      "\n",
      "26.4. Learning DAG structures\n",
      "\n",
      "915\n",
      "\n",
      "G1\n",
      "\n",
      "G2\n",
      "\n",
      "G3\n",
      "\n",
      "X1\n",
      "\n",
      "X3\n",
      "\n",
      "X1\n",
      "\n",
      "X3\n",
      "\n",
      "X1\n",
      "\n",
      "X3\n",
      "\n",
      "X2\n",
      "\n",
      "X2\n",
      "\n",
      "X2\n",
      "\n",
      "X5\n",
      "\n",
      "X5\n",
      "\n",
      "X5\n",
      "\n",
      "X4\n",
      "\n",
      "X4\n",
      "\n",
      "X4\n",
      "\n",
      "Figure 26.5 Three DAGs. G1 and G3 are Markov equivalent, G2 is not.\n",
      "\n",
      "Consider the following 3 DGMs: X → Y → Z, X ← Y ← Z and X ← Y → Z. These all\n",
      "\n",
      "represent the same set of CI statements, namely\n",
      "\n",
      "X ⊥ Z|Y, X (cid:8)⊥ Z\n",
      "\n",
      "(26.16)\n",
      "\n",
      "We say these graphs are Markov equivalent, since they encode the same set of CI assumptions. That is, they all belong to the same Markov equivalence class. However, the v-structure X → Y ← Z encodes X ⊥ Z and X (cid:8)⊥ Z|Y , which represents the opposite set of CI assumptions.\n",
      "\n",
      "One can prove the following theorem.\n",
      "\n",
      "Theorem 26.4.1 (Verma and Pearl (Verma and Pearl 1990)). Two structures are Markov equivalent iff they have the same undirected skeleton and the same set of v-structures.\n",
      "\n",
      "For example, referring to Figure 26.5, we see that G1 (cid:8)≡ G2, since reversing the 2 → 4 arc creates a new v-structure. However, G1 ≡ G3, since reversing the 1 → 5 arc does not create a new v-structure.\n",
      "\n",
      "We can represent a Markov equivalence class using a single partially directed acyclic graph (PDAG), also called an essential graph or pattern, in which some edges are directed and some undirected. The undirected edges represent reversible edges; any combination is possible so long as no new v-structures are created. The directed edges are called compelled edges, since changing their orientation would change the v-structures and hence change the equivalence class. For example, the PDAG X − Y − Z represents {X → Y → Z, X ← Y ← Z, X ← Y → Z} which encodes X (cid:8)⊥ Z and X ⊥ Z|Y . See Figure 26.6.\n",
      "\n",
      "The signiﬁcance of the above theorem is that, when we learn the DAG structure from data, we will not be able to uniquely identify all of the edge directions, even given an inﬁnite amount of data. We say that we can learn DAG structure “up to Markov equivalence”. This also cautions us not to read too much into the meaning of particular edge orientations, since we can often change them without changing the model in any observable way.\n",
      "\n",
      "916\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "Y\n",
      "\n",
      "Y\n",
      "\n",
      "Y\n",
      "\n",
      "≡\n",
      "\n",
      "Y\n",
      "\n",
      "Y\n",
      "\n",
      "≡\n",
      "\n",
      "Y\n",
      "\n",
      "Z\n",
      "\n",
      "Z\n",
      "\n",
      "Z\n",
      "\n",
      "Z\n",
      "\n",
      "Z\n",
      "\n",
      "Z\n",
      "\n",
      "Figure 26.6 PDAG representation of Markov equivalent DAGs.\n",
      "\n",
      "26.4.2\n",
      "\n",
      "Exact structural inference\n",
      "\n",
      "In this section, we discuss how to compute the exact posterior over graphs, p(G|D), ignoring for now the issue of computational tractability.\n",
      "\n",
      "26.4.2.1\n",
      "\n",
      "Deriving the likelihood\n",
      "\n",
      "Assuming there is no missing data, and that all CPDs are tabular, the likelihood can be written as follows:\n",
      "\n",
      "p(D|G, θ) =\n",
      "\n",
      "N(cid:20)\n",
      "\n",
      "V(cid:20)\n",
      "\n",
      "Cat(xit|xi,pa(t), θt)\n",
      "\n",
      "(26.17)\n",
      "\n",
      "=\n",
      "\n",
      "i=1 N(cid:20)\n",
      "\n",
      "t=1 V(cid:20)\n",
      "\n",
      "Ct(cid:20)\n",
      "\n",
      "Cat(xit|θtc)I(xi,pa(t)=c)\n",
      "\n",
      "(26.18)\n",
      "\n",
      "=\n",
      "\n",
      "i=1 N(cid:20)\n",
      "\n",
      "t=1 V(cid:20)\n",
      "\n",
      "c=1 Ct(cid:20)\n",
      "\n",
      "Kt(cid:20)\n",
      "\n",
      "θI(xi,t=k,xi,pa(t)=c) tck\n",
      "\n",
      "(26.19)\n",
      "\n",
      "=\n",
      "\n",
      "i=1 V(cid:20)\n",
      "\n",
      "t=1 Ct(cid:20)\n",
      "\n",
      "c=1 Kt(cid:20)\n",
      "\n",
      "k=1\n",
      "\n",
      "θNtck tck\n",
      "\n",
      "(26.20)\n",
      "\n",
      "t=1\n",
      "\n",
      "c=1\n",
      "\n",
      "k=1\n",
      "\n",
      "where Ntck is the number of times node t is in state k and its parents are in state c. (Technically these counts depend on the graph structure G, but we drop this from the notation.)\n",
      "\n",
      "26.4.2.2\n",
      "\n",
      "Deriving the marginal likelihood\n",
      "\n",
      "Of course, choosing the graph with the maximum likelihood will always pick a fully connected graph (subject to the acyclicity constraint), since this maximizes the number of parameters. To avoid such overﬁtting, we will choose the graph with the maximum marginal likelihood, p(D|G); the magic of the Bayesian Occam’s razor will then penalize overly complex graphs.\n",
      "\n",
      "To compute the marginal likelihood, we need to specify priors on the parameters. We will make two standard assumptions. First, we assume global prior parameter independence, which means\n",
      "\n",
      "p(θ) =\n",
      "\n",
      "V(cid:20)\n",
      "\n",
      "p(θt)\n",
      "\n",
      "(26.21)\n",
      "\n",
      "t=1\n",
      "\n",
      "26.4. Learning DAG structures\n",
      "\n",
      "917\n",
      "\n",
      "Second, we assume local prior parameter independence, which means\n",
      "\n",
      "p(θt) =\n",
      "\n",
      "Ct(cid:20)\n",
      "\n",
      "p(θtc)\n",
      "\n",
      "(26.22)\n",
      "\n",
      "c=1\n",
      "\n",
      "for each t. must be a Dirichlet (Geiger and Heckerman 1997), that is,\n",
      "\n",
      "It turns out that these assumtions imply that the prior for each row of each CPT\n",
      "\n",
      "p(θtc) = Dir(θtc|αtc)\n",
      "\n",
      "(26.23)\n",
      "\n",
      "Given these assumptions, and using the results of Section 5.3.2.2, we can write down the marginal likelihood of any DAG as follows:\n",
      "\n",
      "p(D|G) =\n",
      "\n",
      "V(cid:20)\n",
      "\n",
      "Ct(cid:20)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "⎡\n",
      "\n",
      "⎣\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Cat(xit|θtc)\n",
      "\n",
      "⎤\n",
      "\n",
      "⎦ Dir(θtc)dθtc\n",
      "\n",
      "(26.24)\n",
      "\n",
      "t=1\n",
      "\n",
      "c=1\n",
      "\n",
      "i:xi,pa(t)=c\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "=\n",
      "\n",
      "V(cid:20)\n",
      "\n",
      "t=1 V(cid:20)\n",
      "\n",
      "t=1 V(cid:20)\n",
      "\n",
      "Ct(cid:20)\n",
      "\n",
      "c=1 Ct(cid:20)\n",
      "\n",
      "c=1\n",
      "\n",
      "B(Ntc + αtc) B(αtc)\n",
      "\n",
      "Γ(Ntc) Γ(Ntc + αtc)\n",
      "\n",
      "Kt(cid:20)\n",
      "\n",
      "k=1\n",
      "\n",
      "score(Nt,pa(t))\n",
      "\n",
      "Γ(Ntck + αG Γ(αG\n",
      "\n",
      "ijk)\n",
      "\n",
      "tck)\n",
      "\n",
      "(26.25)\n",
      "\n",
      "(26.26)\n",
      "\n",
      "(26.27)\n",
      "\n",
      "where Ntc = node t and its parents, and score() is a local scoring function deﬁned by\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "k Ntck, αtc =\n",
      "\n",
      "t=1\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "k αtck, Nt,pa(t) is the vector of counts (sufficient statistics) for\n",
      "\n",
      "score(Nt,pa(t)) (cid:2)\n",
      "\n",
      "Ct(cid:20)\n",
      "\n",
      "c=1\n",
      "\n",
      "B(Ntc + αtc) B(αtc)\n",
      "\n",
      "(26.28)\n",
      "\n",
      "We say that the marginal likelihood decomposes or factorizes according to the graph structure.\n",
      "\n",
      "26.4.2.3\n",
      "\n",
      "Setting the prior\n",
      "\n",
      "How should we set the hyper-parameters αtck? It is tempting to use a Jeffreys prior of the form αtck = 1 2 (Equation 5.62). However, it turns out that this violates a property called likelihood equivalence, which is sometimes considered desirable. This property says that if G1 and G2 are Markov equivalent (Section 26.4.1), they should have the same marginal likelihood, since they are essentially equivalent models. Geiger and Heckerman (1997) proved that, for complete graphs, the only prior that satisﬁes likelihood equivalence and parameter independence is the Dirichlet prior, where the pseudo counts have the form\n",
      "\n",
      "αtck = α p0(xt = k, xpa(t) = c)\n",
      "\n",
      "(26.29)\n",
      "\n",
      "where α > 0 is called the equivalent sample size, and p0 is some prior joint probability dis- tribution. This is called the BDe prior, which stands for Bayesian Dirichlet likelihood equivalent.\n",
      "\n",
      "918\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "To derive the hyper-parameters for other graph structures, Geiger and Heckerman (1997) invoked an additional assumption called parameter modularity, which says that if node Xt has the same parents in G1 and G2, then p(θt|G1) =p( θt|G2). With this assumption, we can always derive αt for a node t in any other graph by marginalizing the pseudo counts in Equation 26.29.\n",
      "\n",
      "Typically the prior distribution p0 is assumed to be uniform over all possible joint conﬁgura-\n",
      "\n",
      "tions. In this case, we have α KtCt\n",
      "\n",
      "αtck =\n",
      "\n",
      "(26.30)\n",
      "\n",
      "since p0(xt = k, xpa(t) = c) = 1 KtCt . Thus if we sum the pseudo counts over all Ct × Kt entries in the CPT, we get a total equivalent sample size of α. This is called the BDeu prior, where the “u” stands for uniform. This is the most widely used prior for learning Bayes net structures. For advice on setting the global tuning parameter α, see (Silander et al. 2007).\n",
      "\n",
      "26.4.2.4\n",
      "\n",
      "Simple worked example\n",
      "\n",
      "We now give a very simple worked example from (Neapolitan 2003, p.438). Suppose we have just 2 binary nodes, and the following 8 data cases: X1 X2 1 1 1 2 1 2 1 2\n",
      "\n",
      "graph. The empirical counts for node 1 in G1 are N1 = (5, 3) and for node 2 are\n",
      "\n",
      "The BDeu prior for G1 is α1 = (α/2, α/2), α2|x1=1 = (α/4, α/4) and α2|x1=2 = (α/4, α/4). For G2, the prior for θ1 is the same, and for θ2 it is α2|x1=1 = (α/2, α/2) If we setα = 4, and use the BDeu prior, we ﬁnd p(D|G1) = and α2|x1=2 = (α/2, α/2). 7.2150 × 10−6 and p(D|G2) = 6.7465 × 10−6. Hence the posterior probabilites, under a uniform graph prior, are p(G1|D) = 0.51678 and p(G2|D) = 0.48322.\n",
      "\n",
      "1 2 1 2 1 1 1 2 Suppose we are interested in two possible graphs: G1 is X1 → X2 and G2 is the disconnected\n",
      "\n",
      "X1 = 1 X1 = 2\n",
      "\n",
      "X2 = 1 X2 = 2 4 1\n",
      "\n",
      "1 2\n",
      "\n",
      "26.4.2.5\n",
      "\n",
      "Example: analysis of the college plans dataset\n",
      "\n",
      "We now consider a more interesting example from (Heckerman et al. 1997). Consider the data set collected in 1968 by Sewell and Shah which measured 5 variables that might inﬂuence the decision of high school students about whether to attend college. Speciﬁcally, the variables are as follows:\n",
      "\n",
      "26.4. Learning DAG structures\n",
      "\n",
      "919\n",
      "\n",
      "Figure 26.7 The two most probable DAGs learned from the Sewell-Shah data. Source: (Heckerman et al. 1997) . Used with kind permission of David Heckerman\n",
      "\n",
      "Sex Male or female • SES Socio economic status: low, lower middle, upper middle or high. • • PE Parental encouragment: low or high • CP College plans: yes or no.\n",
      "\n",
      "IQ Intelligence quotient: discretized into low, lower middle, upper middle or high.\n",
      "\n",
      "4 × 2× = 128 possible joint conﬁgurations.\n",
      "\n",
      "These variables were measured for 10,318 Wisconsin high school seniors. There are 2 × 4 ×\n",
      "\n",
      "Heckerman et al. computed the exact posterior over all 29,281 possible 5 node DAGs, except for ones in which SEX and/or SES have parents, and/or CP have children. (The prior probability of these graphs was set to 0, based on domain knowledge.) They used the BDeu score with α = 5, although they said that the results were robust to any α in the range 3 to 40. The top two graphs are shown in Figure 26.7. We see that the most probable one has approximately all of the probability mass, so the posterior is extremely peaked.\n",
      "\n",
      "In particular, it seems that socio-economic status, IQ and parental encouragment all causally inﬂuence the decision about whether to go to college, which makes sense. Also, sex inﬂuences college plans only indirectly through parental encouragement, which also makes sense. However, the direct link from socio economic status to IQ seems surprising; this may be due to a hidden common cause. In Section 26.5.1.4 we will re-examine this dataset allowing for the presence of hidden variables.\n",
      "\n",
      "It is tempting to interpret this graph in terms of causality (see Section 26.6).\n",
      "\n",
      "26.4.2.6\n",
      "\n",
      "The K2 algorithm\n",
      "\n",
      "Suppose we know a total ordering of the nodes. Then we can compute the distribution over parents for each node independently, without the risk of introducing any directed cycles: we\n",
      "\n",
      "920\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "simply enumerate over all possible subsets of ancestors and compute their marginal likelihoods.1 If we just return the best set of parents for each node, we get the the K2 algorithm (Cooper and Herskovits 1992).\n",
      "\n",
      "26.4.2.7\n",
      "\n",
      "Handling non-tabular CPDs\n",
      "\n",
      "If all CPDs are linear Gaussian, we can replace the Dirichlet-multinomial model with the normal- gamma model, and thus derive a different exact expression for the marginal likelihood. See (Geiger and Heckerman 1994) for the details. In fact, we can easily combine discrete nodes and Gaussian nodes, as long as the discrete nodes always have discrete parents; this is called a conditional Gaussian DAG. Again, we can compute the marginal likelihood in closed form. See (Bottcher and Dethlefsen 2003) for the details.\n",
      "\n",
      "In the general case (i.e., everything except Gaussians and CPTs), we need to approximate the marginal likelihood. The simplest approach is to use the BIC approximation, which has the form\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "t\n",
      "\n",
      "log p(Dt|ˆθt) −\n",
      "\n",
      "KtCt 2\n",
      "\n",
      "log N\n",
      "\n",
      "(26.31)\n",
      "\n",
      "26.4.3\n",
      "\n",
      "Scaling up to larger graphs\n",
      "\n",
      "The main challenge in computing the posterior over DAGs is that there are so many possible graphs. More precisely, (Robinson 1973) showed that the number of DAGs on D nodes satisﬁes the following recurrence:\n",
      "\n",
      "f (D) =\n",
      "\n",
      "D(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(−1)i+1\n",
      "\n",
      "(cid:8)\n",
      "\n",
      "D i\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "2i(D−i)f (D − i)\n",
      "\n",
      "(26.32)\n",
      "\n",
      "for D > 2. The base case is f (1) = 1. Solving this recurrence yields the following sequence: 1, 3, 25, 543, 29281, 3781503, etc.2 In view of the enormous size of the hypothesis space, we are generally forced to use approximate methods, some of which we review below.\n",
      "\n",
      "26.4.3.1\n",
      "\n",
      "Approximating the mode of the posterior\n",
      "\n",
      "We can use dynamic programming to ﬁnd the globally optimal MAP DAG (up to Markov equiv- alence) (Koivisto and Sood 2004; Silander and Myllmaki 2006). Unfortunately this method takes V 2V time and space, making it intractable beyond about 16 nodes. Indeed, the general problem of ﬁnding the globally optimal MAP DAG is provably NP-complete (Chickering 1996),\n",
      "\n",
      "Consequently, we must settle for ﬁnding a locally optimal MAP DAG. The most common method is greedy hill climbing: at each step, the algorithm proposes small changes to the current graph, such as adding, deleting or reversing a single edge; it then moves to the neigh- boring graph which most increases the posterior. The method stops when it reaches a lo- It is important that the method only proposes local changes to the graph, cal maximum.\n",
      "\n",
      "1. We can make this method more efficient by using (cid:7)1-regularization to select the parents (Schmidt et al. 2007). In this case, we need to approximate the marginal likelhood as we discuss below. 2. A longer list of values can be found at http://www.research.att.com/~njas/sequences/A003024. Interest- ingly, the number of DAGs is equal to the number of (0,1) matrices all of whose eigenvalues are positive real numbers (McKay et al. 2004).\n",
      "\n",
      "26.4. Learning DAG structures\n",
      "\n",
      "921\n",
      "\n",
      "evidence\n",
      "\n",
      "case\n",
      "\n",
      "course\n",
      "\n",
      "question\n",
      "\n",
      "msg\n",
      "\n",
      "fact\n",
      "\n",
      "drive\n",
      "\n",
      "god\n",
      "\n",
      "nasa\n",
      "\n",
      "scsi\n",
      "\n",
      "gun\n",
      "\n",
      "christian\n",
      "\n",
      "shuttle\n",
      "\n",
      "disk\n",
      "\n",
      "government\n",
      "\n",
      "religion\n",
      "\n",
      "jesus\n",
      "\n",
      "car\n",
      "\n",
      "disease\n",
      "\n",
      "mission\n",
      "\n",
      "space\n",
      "\n",
      "law\n",
      "\n",
      "jews\n",
      "\n",
      "engine\n",
      "\n",
      "patients\n",
      "\n",
      "orbit\n",
      "\n",
      "games\n",
      "\n",
      "program\n",
      "\n",
      "rights\n",
      "\n",
      "power\n",
      "\n",
      "bible\n",
      "\n",
      "honda\n",
      "\n",
      "computer\n",
      "\n",
      "bmw\n",
      "\n",
      "medicine\n",
      "\n",
      "earth\n",
      "\n",
      "solar\n",
      "\n",
      "season\n",
      "\n",
      "launch\n",
      "\n",
      "technology\n",
      "\n",
      "dos\n",
      "\n",
      "dealer\n",
      "\n",
      "science\n",
      "\n",
      "moon\n",
      "\n",
      "system\n",
      "\n",
      "team\n",
      "\n",
      "satellite\n",
      "\n",
      "files\n",
      "\n",
      "problem\n",
      "\n",
      "studies\n",
      "\n",
      "mars\n",
      "\n",
      "lunar\n",
      "\n",
      "players\n",
      "\n",
      "version\n",
      "\n",
      "human\n",
      "\n",
      "hockey\n",
      "\n",
      "hit\n",
      "\n",
      "windows\n",
      "\n",
      "israel\n",
      "\n",
      "university\n",
      "\n",
      "nhl\n",
      "\n",
      "puck\n",
      "\n",
      "baseball\n",
      "\n",
      "won\n",
      "\n",
      "email\n",
      "\n",
      "memory\n",
      "\n",
      "ftp\n",
      "\n",
      "president\n",
      "\n",
      "war\n",
      "\n",
      "state\n",
      "\n",
      "research\n",
      "\n",
      "league\n",
      "\n",
      "fans\n",
      "\n",
      "win\n",
      "\n",
      "phone\n",
      "\n",
      "format\n",
      "\n",
      "video\n",
      "\n",
      "mac\n",
      "\n",
      "children\n",
      "\n",
      "world\n",
      "\n",
      "oil\n",
      "\n",
      "cancer\n",
      "\n",
      "number\n",
      "\n",
      "image\n",
      "\n",
      "data\n",
      "\n",
      "driver\n",
      "\n",
      "software\n",
      "\n",
      "water\n",
      "\n",
      "health\n",
      "\n",
      "pc\n",
      "\n",
      "food\n",
      "\n",
      "aids\n",
      "\n",
      "insurance\n",
      "\n",
      "doctor\n",
      "\n",
      "card\n",
      "\n",
      "help\n",
      "\n",
      "server\n",
      "\n",
      "graphics\n",
      "\n",
      "vitamin\n",
      "\n",
      "display\n",
      "\n",
      "Figure 26.8 A locally optimal DAG learned from the 20-newsgroup data. From Figure 4.10 of (Schmidt 2010). Used with kind permission of Mark Schmidt.\n",
      "\n",
      "since this enables the change in marginal likelihood (and hence the posterior) to be computed in constant time (assuming we cache the sufficient statistics). This is because all but one or two of the terms in Equation 26.25 will cancel out when computing the log Bayes factor δ(G → G(cid:2)) = log p(G(cid:2)|D) − log p(G|D).\n",
      "\n",
      "We can initialize the search from the best tree, which can be found using exact methods discussed in Section 26.3. For speed, we can restrict the search so it only adds edges which are part of the Markov blankets estimated from a dependency network (Schmidt 2010). Figure 26.8 gives an example of a DAG learned in this way from the 20-newsgroup data.\n",
      "\n",
      "We can use techniques such as multiple random restarts to increase the chance of ﬁnding a good local maximum. We can also use more sophisticated local search methods, such as genetic algorithms or simulated annealing, for structure learning.\n",
      "\n",
      "26.4.3.2\n",
      "\n",
      "Approximating other functions of the posterior\n",
      "\n",
      "If our goal is knowledge discovery, the MAP DAG can be misleading, for reasons we discussed in Section 5.2.1. A better approach is to compute the probability that each edge is present, p(Gst = 1|D), of the probability there is a path from s to t. We can do this exactly using dynamic programming (Koivisto 2006; Parviainen and Koivisto 2011). Unfortunately these methods take V 2V time in the general case, making them intractable for graphs with more than about 16\n",
      "\n",
      "922\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "nodes.\n",
      "\n",
      "An approximate method is to sample DAGs from the posterior, and then to compute the fraction of times there is an s → t edge or path for each (s, t) pair. The standard way to draw samples is to use the Metropolis Hastings algorithm (Section 24.3), where we use the same local proposal as we did in greedy search (Madigan and Raftery 1994).\n",
      "\n",
      "A faster-mixing method is to use a collapsed MH sampler, as suggested in (Friedman and Koller 2003). This exploits the fact that, if a total ordering of the nodes is known, we can select the parents for each node independently, without worrying about cycles, as discussed in Section 26.4.2.6. By summing over all possible choice of parents, we can marginalize out this part of the problem, and just sample total orders. (Ellis and Wong 2008) also use order-space (collapsed) MCMC, but this time with a parallel tempering MCMC algorithm.\n",
      "\n",
      "26.5\n",
      "\n",
      "Learning DAG structure with latent variables\n",
      "\n",
      "Sometimes the complete data assumption does not hold, either because we have missing data, and/ or because we have hidden variables. In this case, the marginal likelihood is given by\n",
      "\n",
      "p(D|G) =\n",
      "\n",
      "(cid:12) (cid:4)\n",
      "\n",
      "p(D, h|θ, G)p(θ|G)dθ =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "p(D, h|θ, G)p(θ|G)dθ\n",
      "\n",
      "(26.33)\n",
      "\n",
      "h\n",
      "\n",
      "h\n",
      "\n",
      "where h represents the hidden or missing data.\n",
      "\n",
      "In general this is intractable to compute. For example, consider a mixture model, where In this case, there are KN possible completions of the we don’t observe the cluster label. data (assuming we have K clusters); we can evaluate the inner integral for each one of these assignments to h, but we cannot afford to evaluate all of the integrals. (Of course, most of these integrals will correspond to hypotheses with little posterior support, such as assigning single data points to isolated clusters, but we don’t know ahead of time the relative weight of these assignments.)\n",
      "\n",
      "In this section, we discuss some ways for learning DAG structure when we have latent variables\n",
      "\n",
      "and/or missing data.\n",
      "\n",
      "26.5.1\n",
      "\n",
      "Approximating the marginal likelihood when we have missing data\n",
      "\n",
      "The simplest approach is to use standard structure learning methods for fully visible DAGs, but to approximate the marginal likelihood. In Section 24.7, we discussed some Monte Carlo methods for approximating the marginal likelihood. However, these are usually too slow to use inside of a search over models. Below we mention some faster deterministic approximations.\n",
      "\n",
      "26.5.1.1\n",
      "\n",
      "BIC approximation\n",
      "\n",
      "A simple approximation is to use the BIC score, which is given by\n",
      "\n",
      "BIC(G) (cid:2) log p(D|ˆθ, G) − log N 2\n",
      "\n",
      "dim(G)\n",
      "\n",
      "(26.34)\n",
      "\n",
      "where dim(G) is the number of degrees of freedom in the model and ˆθ is the MAP or ML estimate. However, the BIC score often severely underestimates the true marginal likelihood (Chickering and Heckerman 1997), resulting in it selecting overly simple models.\n",
      "\n",
      "26.5. Learning DAG structure with latent variables\n",
      "\n",
      "923\n",
      "\n",
      "26.5.1.2\n",
      "\n",
      "Cheeseman-Stutz approximation\n",
      "\n",
      "We now present a better method known as the Cheeseman-Stutz approximation (CS) (Cheese- man and Stutz 1996). We ﬁrst compute a MAP estimate of the parameters ˆθ (e.g., using EM). Denote the expected sufficient statistics of the data by D = D(ˆθ); in the case of discrete variables, we just “ﬁll in” the hidden variables with their expectation. We then use the exact marginal likelihood equation on this ﬁlled-in data: (cid:12)\n",
      "\n",
      "p(D|G) ≈ p(D|G) =\n",
      "\n",
      "p(D|θ, G)p(θ|G)dθ\n",
      "\n",
      "(26.35)\n",
      "\n",
      "However, comparing this to Equation 26.33, we can see that the value will be exponentially smaller, since it does not sum over all values of h. To correct for this, we ﬁrst write\n",
      "\n",
      "log p(D|G) = log p(D|G) + log p(D|G) − log p(D|G)\n",
      "\n",
      "(26.36)\n",
      "\n",
      "and then we apply a BIC approximation to the last two terms:\n",
      "\n",
      "log p(D|G) − log p(D|G) ≈\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "−\n",
      "\n",
      "log p(D|ˆθ, G) − (cid:17)\n",
      "\n",
      "log p(D|ˆθ, G) −\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "N\n",
      "\n",
      "2\n",
      "\n",
      "dim(ˆθ)\n",
      "\n",
      "dim(ˆθ)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(26.37)\n",
      "\n",
      "(26.38)\n",
      "\n",
      "= log p(D|ˆθ, G) − log p(D|ˆθ, G)\n",
      "\n",
      "(26.39)\n",
      "\n",
      "Putting it altogether we get\n",
      "\n",
      "log p(D|G) ≈ log p(D|G) + log p(D|ˆθ, G) − log p(D|ˆθ, G)\n",
      "\n",
      "(26.40)\n",
      "\n",
      "The ﬁrst term p(D|G) can be computed by plugging in the ﬁlled-in data into the exact marginal likelihood. The second term p(D|ˆθ, G), which involves an exponential sum (thus matching the “dimensionality” of the left hand side) can be computed using an inference algorithm. The ﬁnal term p(D|ˆθ, G) can be computed by plugging in the ﬁlled-in data into the regular likelihood.\n",
      "\n",
      "26.5.1.3\n",
      "\n",
      "Variational Bayes EM\n",
      "\n",
      "An even more accurate approach is to use the variational Bayes EM algorithm. Recall from Section 21.6 that the key idea is to make the following factorization assumption: (cid:20)\n",
      "\n",
      "p(θ, z1:N |D) ≈ q(θ)q(z) = q(θ)\n",
      "\n",
      "q(zi)\n",
      "\n",
      "(26.41)\n",
      "\n",
      "i\n",
      "\n",
      "where zi are the hidden variables in case i. In the E step, we update the q(zi), and in the M step, we update q(θ). The corresponding variational free energy provides a lower bound on In (Beal and Ghahramani 2006), it is shown that this bound is a the log marginal likelihood. much better approximation to the true log marginal likelihood (as estimated by a slow annealed importance sampling procedure) than either BIC or CS. In fact, one can prove that the variational bound will always be more accurate than CS (which in turn is always more accurate than BIC).\n",
      "\n",
      "924\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "PE\n",
      "\n",
      "low low high high\n",
      "\n",
      "H\n",
      "\n",
      "0 1 0 1\n",
      "\n",
      "p(IQ=high|PE,H)\n",
      "\n",
      "0.098 0.22 0.21 0.49\n",
      "\n",
      "p(male) = 0.48\n",
      "\n",
      "H\n",
      "\n",
      "SEX\n",
      "\n",
      "PE\n",
      "\n",
      "p(H=0) = 0.63 p(H=1) = 0.37\n",
      "\n",
      "SES\n",
      "\n",
      "H\n",
      "\n",
      "0 1\n",
      "\n",
      "p(SES=high|H)\n",
      "\n",
      "0.088 0.51\n",
      "\n",
      "IQ\n",
      "\n",
      "SES\n",
      "\n",
      "IQ\n",
      "\n",
      "PE\n",
      "\n",
      "p(CP=yes|SES,IQ,PE)\n",
      "\n",
      "SES\n",
      "\n",
      "low low high high\n",
      "\n",
      "SEX\n",
      "\n",
      "male female male female\n",
      "\n",
      "p(PE=high|SES,SEX)\n",
      "\n",
      "0.32 0.166 0.86 0.81\n",
      "\n",
      "CP\n",
      "\n",
      "low low low low high high high high\n",
      "\n",
      "low low high high low low high high\n",
      "\n",
      "low high low high low high low high\n",
      "\n",
      "0.011 0.170 0.124 0.53 0.093 0.39 0.24 0.84\n",
      "\n",
      "Figure 26.9 The most probable DAG with a single binary hidden variable learned from the Sewell-Shah data. MAP estimates of the CPT entries are shown for some of the nodes. Source: (Heckerman et al. 1997). Used with kind permission of David Heckerman.\n",
      "\n",
      "26.5.1.4\n",
      "\n",
      "Example: college plans revisited\n",
      "\n",
      "Let us revisit the college plans dataset from Section 26.4.2.5. Recall that if we ignore the possibility of hidden variables there was a direct link from socio economic status to IQ in the MAP DAG. Heckerman et al. decided to see what would happen if they introduced a hidden variable H, which they made a parent of both SES and IQ, representing a hidden common cause. They also considered a variant in which H points to SES, IQ and PE. For both such cases, they considered dropping none, one, or both of the SES-PE and PE-IQ edges. They varied the number of states for the hidden node from 2 to 6. Thus they computed the approximate posterior over 8 × 5 = 40 different models, using the CS approximation.\n",
      "\n",
      "The most probable model which they found is shown in Figure 26.9. This is 2 · 1010 times It is also 5 · 109 times more more likely than the best model containing no hidden variable. likely than the second most probable model with a hidden variable. So again the posterior is very peaked.\n",
      "\n",
      "These results suggests that there is indeed a hidden common cause underlying both the socio-economic status of the parents and the IQ of the children. By examining the CPT entries, we see that both SES and IQ are more likely to be high when H takes on the value 1. They interpret this to mean that the hidden variable represents “parent quality” (possibly a genetic factor). Note, however, that the arc between H and SES can be reversed without changing the v- structures in the graph, and thus without affecting the likelihood; this underscores the difficulty in interpreting hidden variables.\n",
      "\n",
      "Interestingly, the hidden variable model has the same conditional independence assumptions amongst the visible variables as the most probable visible variable model. So it is not pos- sible to distinguish between these hypotheses by merely looking at the empirical conditional independencies in the data (which is the basis of the constraint-based approach to structure learning (Pearl and Verma 1991; Spirtes et al. 2000)). Instead, by adopting a Bayesian approach, which takes parsimony into account (and not just conditional independence), we can discover\n",
      "\n",
      "26.5. Learning DAG structure with latent variables\n",
      "\n",
      "925\n",
      "\n",
      "the possible existence of hidden factors. This is the basis of much of scientiﬁc and everday human reasoning (see e.g. (Griffiths and Tenenbaum 2009) for a discussion).\n",
      "\n",
      "26.5.2\n",
      "\n",
      "Structural EM\n",
      "\n",
      "One way to perform structural inference in the presence of missing data is to use a standard search procedure (deterministic or stochastic), and to use the methods from Section 26.5.1 to estimate the marginal likelihood. However, this approach is very efficient, because the marginal likelihood does not decompose when we have missing data, and nor do its approximations. For example, if we use the CS approximation or the VBEM approximation, we have to perform inference in every neighboring model, just to evaluate the quality of a single move!\n",
      "\n",
      "(Friedman 1997b; Thiesson et al. 1998) presents a much more efficient approach called the structural EM algorithm. The basic idea is this: instead of ﬁtting each candidate neighboring graph and then ﬁlling in its data, ﬁll in the data once, and use this ﬁlled-in data to evaluate the score of all the neighbors. Although this might be a bad approximation to the marginal likelihood, it can be a good enough approximation of the difference in marginal likelihoods between different models, which is all we need in order to pick the best neighbor.\n",
      "\n",
      "ˆθ0. Now deﬁne a modiﬁed BIC score as follows: scoreBIC(G, D) (cid:2) log p(D|ˆθ, G) − log N\n",
      "\n",
      "More precisely, deﬁne D(G0, ˆθ0) to be the data ﬁlled in using model G0 with MAP parameters\n",
      "\n",
      "2\n",
      "\n",
      "dim(G) + log p(G) + log p(ˆθ|G)\n",
      "\n",
      "(26.42)\n",
      "\n",
      "where we have included the log prior for the graph and parameters. One can show (Friedman 1997b) that if we pick a graph G which increases the BIC score relative to G0 on the expected data, it will also increase the score on the actual data, i.e., scoreBIC(G, D(G0, ˆθ0)) − scoreBIC(G0, D(G0, ˆθ0) ≤ scoreBIC(G, D) − scoreBIC(G0, D)(26.43) To convert this into an algorithm, we proceed as follows. First we initialize with some graph G0 and some set of parameters θ0. Then we ﬁll-in the data using the current parameters — in practice, this means when we ask for the expected counts for any particular family, we perform inference using our current model. (If we know which counts we will need, we can precompute all of them, which is much faster.) We then evaluate the BIC score of all of our neighbors using the ﬁlled-in data, and we pick the best neighbor. We then reﬁt the model parameters, ﬁll-in the data again, and repeat. For increased speed, we may choose to only reﬁt the model every few steps, since small changes to the structure hopefully won’t invalidate the parameter estimates and the ﬁlled-in data too much.\n",
      "\n",
      "One interesting application is to learn a phylogenetic tree structure. Here the observed leaves are the DNA or protein sequences of currently alive species, and the goal is to infer the topology of the tree and the values of the missing internal nodes. There are many classical algorithms for this task (see e.g., (Durbin et al. 1998)), but one that uses SEM is discussed in (Friedman et al. 2002).\n",
      "\n",
      "Another interesting application of this method is to learn sparse mixture models (Barash and Friedman 2002). The idea is that we have one hidden variable C specifying the cluster, and we have to choose whether to add edges C → Xt for each possible feature Xt. Thus some features (See also (Law et al. 2004) will be dependent on the cluster id, and some will be independent.\n",
      "\n",
      "926\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "(cid:25)(cid:33)(cid:32)\n",
      "\n",
      "(cid:25)(cid:34)(cid:25)\n",
      "\n",
      "(cid:25)(cid:34)(cid:28)\n",
      "\n",
      "(cid:1)(cid:2)(cid:3)(cid:4)\n",
      "\n",
      "(cid:25)(cid:33)(cid:29)\n",
      "\n",
      "(cid:25)(cid:33)(cid:25)\n",
      "\n",
      "(cid:25)(cid:33)(cid:27)\n",
      "\n",
      "(cid:9)(cid:1)(cid:14)(cid:6)(cid:12)\n",
      "\n",
      "(cid:25)(cid:33)(cid:26)\n",
      "\n",
      "(cid:25)(cid:27)(cid:31)\n",
      "\n",
      "(cid:25)(cid:32)(cid:31)\n",
      "\n",
      "(cid:13)(cid:2)(cid:14)\n",
      "\n",
      "(cid:16)(cid:15)(cid:9)(cid:6)(cid:12)\n",
      "\n",
      "(cid:16)(cid:17)(cid:10)(cid:18)\n",
      "\n",
      "(cid:25)(cid:31)(cid:27)\n",
      "\n",
      "(cid:25)(cid:32)(cid:27)\n",
      "\n",
      "(cid:13)(cid:6)(cid:1)(cid:7)(cid:14)(cid:13)\n",
      "\n",
      "(cid:2)(cid:11)(cid:4)(cid:17)(cid:12)(cid:1)(cid:11)(cid:10)(cid:6)\n",
      "\n",
      "(cid:4)(cid:14)(cid:17)(cid:3)(cid:2)(cid:6)(cid:4)\n",
      "\n",
      "(cid:25)(cid:31)(cid:33)\n",
      "\n",
      "(cid:25)(cid:29)(cid:27)\n",
      "\n",
      "(cid:25)(cid:29)(cid:29)\n",
      "\n",
      "(cid:25)(cid:28)(cid:29)\n",
      "\n",
      "(cid:25)(cid:31)(cid:30)\n",
      "\n",
      "(cid:3)(cid:15)(cid:10)(cid:14)(cid:15)(cid:12)\n",
      "\n",
      "(cid:25)(cid:31)(cid:28)\n",
      "\n",
      "(cid:9)(cid:2)(cid:11)\n",
      "\n",
      "(cid:9)(cid:15)(cid:11)\n",
      "\n",
      "(cid:22)(cid:1)(cid:11)(cid:4)\n",
      "\n",
      "(cid:25)(cid:28)(cid:31)\n",
      "\n",
      "(cid:12)(cid:6)(cid:7)(cid:2)(cid:21)(cid:2)(cid:15)(cid:11)\n",
      "\n",
      "(cid:25)(cid:26)(cid:31)\n",
      "\n",
      "(cid:24)(cid:17)(cid:6)(cid:4)(cid:14)(cid:2)(cid:15)(cid:11)\n",
      "\n",
      "(cid:10)(cid:1)(cid:11)(cid:10)(cid:6)(cid:12)\n",
      "\n",
      "(cid:25)(cid:28)(cid:30)\n",
      "\n",
      "(cid:25)(cid:27)(cid:27)\n",
      "\n",
      "(cid:25)(cid:27)(cid:30)\n",
      "\n",
      "(cid:10)(cid:13)(cid:12)(cid:2)(cid:4)(cid:14)(cid:2)(cid:1)(cid:11)\n",
      "\n",
      "(cid:25)(cid:26)(cid:28)\n",
      "\n",
      "(cid:10)(cid:15)(cid:17)(cid:12)(cid:4)(cid:6)\n",
      "\n",
      "(cid:8)(cid:6)(cid:3)(cid:2)(cid:10)(cid:2)(cid:11)(cid:6)\n",
      "\n",
      "(cid:25)(cid:27)(cid:25)\n",
      "\n",
      "(cid:25)(cid:26)(cid:34)\n",
      "\n",
      "(cid:25)(cid:25)(cid:29)\n",
      "\n",
      "(cid:7)(cid:6)(cid:1)(cid:21)(cid:17)(cid:6)\n",
      "\n",
      "(cid:25)(cid:25)(cid:27)\n",
      "\n",
      "(cid:5)(cid:2)(cid:5)(cid:7)(cid:6)\n",
      "\n",
      "(cid:25)(cid:26)(cid:25)\n",
      "\n",
      "(cid:3)(cid:2)(cid:4)(cid:6)(cid:1)(cid:4)(cid:6)\n",
      "\n",
      "(cid:16)(cid:1)(cid:14)(cid:2)(cid:6)(cid:11)(cid:14)(cid:4)\n",
      "\n",
      "(cid:13)(cid:15)(cid:10)(cid:18)(cid:6)(cid:19)\n",
      "\n",
      "(cid:11)(cid:13)(cid:7)\n",
      "\n",
      "(cid:21)(cid:1)(cid:8)(cid:6)(cid:4)\n",
      "\n",
      "(cid:25)(cid:26)(cid:29)\n",
      "\n",
      "(cid:5)(cid:1)(cid:4)(cid:6)(cid:5)(cid:1)(cid:7)(cid:7)\n",
      "\n",
      "(cid:16)(cid:7)(cid:1)(cid:19)(cid:6)(cid:12)(cid:4)\n",
      "\n",
      "(cid:21)(cid:15)(cid:3)\n",
      "\n",
      "(cid:23)(cid:6)(cid:4)(cid:17)(cid:4)\n",
      "\n",
      "(cid:4)(cid:6)(cid:1)(cid:4)(cid:15)(cid:11)\n",
      "\n",
      "(cid:14)(cid:6)(cid:1)(cid:8)\n",
      "\n",
      "Figure 26.10 Part of a hierarchical latent tree learned from the 20-newsgroup data. From Figure 2 of (Harmeling and Williams 2011). Used with kind permission of Stefan Harmeling.\n",
      "\n",
      "for a different way to perform this task, using regular EM and a set of bits, one per feature, that are free to change across data cases.)\n",
      "\n",
      "26.5.3\n",
      "\n",
      "Discovering hidden variables\n",
      "\n",
      "In Section 26.5.1.4, we introduced a hidden variable “by hand”, and then ﬁgured out the local topology by ﬁtting a series of different models and computing the one with the best marginal likelihood. How can we automate this process? Figure 11.1 provides one useful intuition:\n",
      "\n",
      "if there is a hidden variable in the “true model”, then its children are likely to be densely connected. This suggest the following heuristic (Elidan et al. 2000): perform structure learning in the visible domain, and then look for structural signatures, such as sets of densely connected nodes (near-cliques); introduce a hidden variable and connect it to all nodes in this near-clique; and then let structural EM sort out the details. Unfortunately, this technique does not work too well, since structure learning algorithms are biased against ﬁtting models with densely connected cliques.\n",
      "\n",
      "Another useful intuition comes from clustering. In a ﬂat mixture model, also called a latent class model, the discrete latent variable provides a compressed representation of its children. Thus we want to create hidden variables with high mutual information with their children.\n",
      "\n",
      "One way to do this is to create a tree-structured hierarchy of latent variables, each of which (Zhang 2004) calls this a hierarchical latent class only has to explain a small set of children. model. They propose a greedy local search algorithm to learn such structures, based on adding (Note that learning the optimal latent or deleting hidden nodes, adding or deleting edges, etc.\n",
      "\n",
      "26.5. Learning DAG structure with latent variables\n",
      "\n",
      "927\n",
      "\n",
      "h3\n",
      "\n",
      "h17\n",
      "\n",
      "president\n",
      "\n",
      "government\n",
      "\n",
      "power\n",
      "\n",
      "h4\n",
      "\n",
      "children\n",
      "\n",
      "war\n",
      "\n",
      "h20\n",
      "\n",
      "religion\n",
      "\n",
      "h14\n",
      "\n",
      "earth\n",
      "\n",
      "lunar\n",
      "\n",
      "orbit\n",
      "\n",
      "satellite\n",
      "\n",
      "solar\n",
      "\n",
      "law\n",
      "\n",
      "state\n",
      "\n",
      "human\n",
      "\n",
      "rights\n",
      "\n",
      "world\n",
      "\n",
      "israel\n",
      "\n",
      "jews\n",
      "\n",
      "h8\n",
      "\n",
      "bible\n",
      "\n",
      "god\n",
      "\n",
      "moon\n",
      "\n",
      "technology\n",
      "\n",
      "mission\n",
      "\n",
      "gun\n",
      "\n",
      "mars\n",
      "\n",
      "h1\n",
      "\n",
      "h2\n",
      "\n",
      "christian\n",
      "\n",
      "jesus\n",
      "\n",
      "space\n",
      "\n",
      "launch\n",
      "\n",
      "shuttle\n",
      "\n",
      "nasa\n",
      "\n",
      "health\n",
      "\n",
      "case\n",
      "\n",
      "course\n",
      "\n",
      "evidence\n",
      "\n",
      "fact\n",
      "\n",
      "question\n",
      "\n",
      "program\n",
      "\n",
      "h9\n",
      "\n",
      "food\n",
      "\n",
      "aids\n",
      "\n",
      "h21\n",
      "\n",
      "insurance\n",
      "\n",
      "version\n",
      "\n",
      "h12\n",
      "\n",
      "ftp\n",
      "\n",
      "email\n",
      "\n",
      "msg\n",
      "\n",
      "water\n",
      "\n",
      "studies\n",
      "\n",
      "h13\n",
      "\n",
      "medicine\n",
      "\n",
      "car\n",
      "\n",
      "h25\n",
      "\n",
      "files\n",
      "\n",
      "format\n",
      "\n",
      "phone\n",
      "\n",
      "dealer\n",
      "\n",
      "h15\n",
      "\n",
      "cancer\n",
      "\n",
      "disease\n",
      "\n",
      "doctor\n",
      "\n",
      "patients\n",
      "\n",
      "vitamin\n",
      "\n",
      "windows\n",
      "\n",
      "h18\n",
      "\n",
      "h11\n",
      "\n",
      "image\n",
      "\n",
      "number\n",
      "\n",
      "bmw\n",
      "\n",
      "engine\n",
      "\n",
      "honda\n",
      "\n",
      "oil\n",
      "\n",
      "h5\n",
      "\n",
      "card\n",
      "\n",
      "driver\n",
      "\n",
      "h10\n",
      "\n",
      "dos\n",
      "\n",
      "h19\n",
      "\n",
      "h26\n",
      "\n",
      "h6\n",
      "\n",
      "puck\n",
      "\n",
      "season\n",
      "\n",
      "team\n",
      "\n",
      "h7\n",
      "\n",
      "win\n",
      "\n",
      "video\n",
      "\n",
      "h16\n",
      "\n",
      "disk\n",
      "\n",
      "memory\n",
      "\n",
      "h22\n",
      "\n",
      "pc\n",
      "\n",
      "software\n",
      "\n",
      "display\n",
      "\n",
      "server\n",
      "\n",
      "games\n",
      "\n",
      "baseball\n",
      "\n",
      "league\n",
      "\n",
      "players\n",
      "\n",
      "fans\n",
      "\n",
      "hockey\n",
      "\n",
      "nhl\n",
      "\n",
      "won\n",
      "\n",
      "graphics\n",
      "\n",
      "h23\n",
      "\n",
      "system\n",
      "\n",
      "data\n",
      "\n",
      "scsi\n",
      "\n",
      "drive\n",
      "\n",
      "computer\n",
      "\n",
      "h24\n",
      "\n",
      "hit\n",
      "\n",
      "problem\n",
      "\n",
      "help\n",
      "\n",
      "mac\n",
      "\n",
      "science\n",
      "\n",
      "university\n",
      "\n",
      "research\n",
      "\n",
      "Figure 26.11 A partially latent tree learned from the 20-newsgroup data. Note that some words can have multiple meanings, and get connected to different latent variables, representing different “topics”. For example, the word “win” can refer to a sports context (represented by h5) or the Microsoft Windows context (represented by h25). From Figure 12 of (Choi et al. 2011). Used with kind permission of Jin Choi.\n",
      "\n",
      "tree is NP-hard (Roch 2006).)\n",
      "\n",
      "Recently (Harmeling and Williams 2011) proposed a faster greedy algorithm for learning such models based on agglomerative hierarchical clustering. Rather than go into details, we just give an example of what this system can learn. Figure 26.10 shows part of a latent forest learned from the 20-newsgroup data. The algorithm imposes the constraint that each latent node has exactly two children, for speed reasons. Nevertheless, we see interpretable clusters arising. For example, Figure 26.10 shows separate clusters concerning medicine, sports and religion. This provides an alternative to LDA and other topic models (Section 4.2.2), with the added advantage that inference in latent trees is exact and takes time linear in the number of nodes.\n",
      "\n",
      "An alternative approach is proposed in (Choi et al. 2011), in which the observed data is not constrained to be at the leaves. This method starts with the Chow-Liu tree on the observed data, and then adds hidden variables to capture higher-order dependencies between internal nodes. This results in much more compact models, as shown in Figure 26.11. This model also has better predictive accuracy than other approaches, such as mixture models, or trees where all the observed data is forced to be at the leaves. Interestingly, one can show that this method can recover the exact latent tree structure, providing the data is generated from a tree. See\n",
      "\n",
      "928\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "Figure 26.12 Google’s rephil model. Leaves represent presence or absence of words. Internal nodes represent clusters of co-occuring words, or “concepts”. All nodes are binary, and all CPDs are noisy-OR. The model contains 12 million word nodes, 1 million latent cluster nodes, and 350 million edges. Used with kind permission of Brian Milch.\n",
      "\n",
      "(Choi et al. 2011) for details. Note, however, that this approach, unlike (Zhang 2004; Harmeling and Williams 2011), requires that the cardinality of all the variables, hidden and observed, be the same. Furthermore, if the observed variables are Gaussian, the hidden variables must be Gaussian also.\n",
      "\n",
      "26.5.4\n",
      "\n",
      "Case study: Google’s Rephil\n",
      "\n",
      "In this section, we describe a huge DGM called Rephil, which was automatically learned from data.3 The model is widely used inside Google for various purposes, including their famous AdSense system.4\n",
      "\n",
      "The model structure is shown in Figure 26.12. The leaves are binary nodes, and represent the presence or absence of words or compounds (such as “New York City”) in a text document or query. The latent variables are also binary, and represent clusters of co-occuring words. All CPDs are noisy-OR, since some leaf nodes (representing words) can have many parents. This means each edge can be augmented with a hidden variable specifying if the link was activated or not; if the link is not active, then the parent cannot turn the child on. (A very similar model was proposed independently in (Singliar and Hauskrecht 2006).)\n",
      "\n",
      "Parameter learning is based on EM, where the hidden activation status of each edge needs to be inferred (Meek and Heckerman 1997). Structure learning is based on the old neuroscience\n",
      "\n",
      "3. The original system, called “Phil”, was developed by Georges Harik and Noam Shazeer,. It has been published as US Patent #8024372, “Method and apparatus for learning a probabilistic generative model for text”, ﬁled in 2004. Rephil is a more probabilistically sound version of the method, developed by Uri Lerner et al. The summary below is based on notes by Brian Milch (who also works at Google). 4. AdSense is Google’s system for matching web pages with content-appropriate ads in an automatic way, by extracting semantic keywords from web pages. These keywords play a role analogous to the words that users type in when searching; this latter form of information is used by Google’s AdWords system. The details are secret, but (Levy 2011) gives an overview.\n",
      "\n",
      "26.5. Learning DAG structure with latent variables\n",
      "\n",
      "929\n",
      "\n",
      "idea that “nodes that ﬁre together should wire together”. To implement this, we run inference and check for cluster-word and cluster-cluster pairs that frequently turn on together. We then add an edge from parent to child if the link can signiﬁcantly increase the probability of the child. Links that are not activated very often are pruned out. We initialize with one cluster per “document” (corresponding to a set of semantically related phrases). We then merge clusters A and B if A explains B’s top words and vice versa. We can also discard clusters that are used too rarely.\n",
      "\n",
      "The model was trained on about 100 billion text snippets or search queries; this takes several weeks, even on a parallel distributed computing architecture. The resulting model contains 12 million word nodes and about 1 million latent cluster nodes. There are about 350 million links in the model, including many cluster-cluster dependencies. The longest path in the graph has length 555, so the model is quite deep.\n",
      "\n",
      "Exact inference in this model is obviously infeasible. However note that most leaves will be off, since most words do not occur in a given query; such leaves can be analytically removed, as shown in Exercise 10.7. We an also prune out unlikely hidden nodes by following the strongest links from the words that are on up to their parents to get a candidate set of concepts. We then perform iterative conditional modes to ﬁnd a good set of local maxima. At each step of ICM, each node sets its value to its most probable state given the values of its neighbors in its Markov blanket. This continues until it reaches a local maximum. We can repeat this process a few times from random starting conﬁgurations. At Google, this can be made to run in 15 milliseconds!\n",
      "\n",
      "26.5.5\n",
      "\n",
      "Structural equation models *\n",
      "\n",
      "A structural equation model (Bollen 1989) is a special kind of directed mixed graph (Sec- tion 19.4.4.1), possibly cyclic, in which all CPDs are linear Gaussian, and in which all bidirected edges represent correlated Gaussian noise. Such models are also called path diagrams. SEMs are widely used, especially in economics and social science. It is common to interpret the edge directions in terms of causality, where directed cycles are interpreted is in terms of feedback loops (see e.g., (Pearl 2000, Ch.5)). However, the model is really just a way of specifying a joint Gaussian, as we show below. There is nothing inherently “causal” about it at all. (We discuss causality in Section 26.6.)\n",
      "\n",
      "We can deﬁne an SEM as a series of full conditionals as follows:\n",
      "\n",
      "xi = μi +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "wijxj + (cid:16)i\n",
      "\n",
      "(26.44)\n",
      "\n",
      "j(cid:5)=i\n",
      "\n",
      "where (cid:11) ∼ N (0, Ψ). We can rewrite the model in matrix form as follows:\n",
      "\n",
      "x = Wx + μ + (cid:11) ⇒ x = (I − W)−1((cid:11) + μ)\n",
      "\n",
      "(26.45)\n",
      "\n",
      "Hence the joint distribution is given by p(x) = N (μ, Σ) where\n",
      "\n",
      "Σ = (I − W)−1Ψ(I − W)−T\n",
      "\n",
      "(26.46)\n",
      "\n",
      "We draw an arc Xi ← Xj if |wij| > 0. If W is lower triangular then the graph is acyclic. If, in addition, Ψ is diagonal, then the model is equivalent to a Gaussian DGM, as discussed in Section 10.2.5; such models are called recursive. If Ψ is not diagonal, then we draw a bidirected\n",
      "\n",
      "930\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "Y1\n",
      "\n",
      "Y2\n",
      "\n",
      "Z1\n",
      "\n",
      "Z1\n",
      "\n",
      "Z3\n",
      "\n",
      "Y3\n",
      "\n",
      "Figure 26.13 A cyclic directed mixed graphical model (non-recursive SEM). Note the Z1 → Z2 → Z3 → Z1 feedback loop.\n",
      "\n",
      "arc Xi ↔ Xj for each non-zero off-diagonal term. Such edges represent correlation, possibly due to a hidden common cause.\n",
      "\n",
      "When using structural equation models, it is common to partition the variables into latent variables, Zt, and observed or manifest variables Yt. For example, Figure 26.13 illustrates the following model: ⎛ X1 X2 X3 X4 X5 X6\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "=\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "Z1 Z2 Z3 Y1 Y2 Y3\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "=\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "⎛\n",
      "\n",
      "0 w21 0 w41 0 0\n",
      "\n",
      "0 0 w32 0 w52 0\n",
      "\n",
      "w13 0 0 0 0 w63\n",
      "\n",
      "0 0 0 0 0 0\n",
      "\n",
      "0 0 0 0 0 0\n",
      "\n",
      "0 ⎟ 0 ⎟ ⎟ 0 ⎟ ⎟ 0 ⎟ ⎠ 0 0\n",
      "\n",
      "⎞\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "⎛\n",
      "\n",
      "Z1 Z2 Z3 Y1 Y2 Y3\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "+\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "(cid:16)1 (cid:16)2 (cid:16)3 (cid:16)4 (cid:16)5 (cid:16)6\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠\n",
      "\n",
      ",\n",
      "\n",
      "(26.47)\n",
      "\n",
      "where\n",
      "\n",
      "Ψ =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝\n",
      "\n",
      "Ψ11 0 0 0 0 0\n",
      "\n",
      "0 Ψ22 0 0 0 0\n",
      "\n",
      "0 0 Ψ33 0 0 0\n",
      "\n",
      "0 0 0\n",
      "\n",
      "0 0 0\n",
      "\n",
      "Ψ44 Ψ45 Ψ54 Ψ55\n",
      "\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "0 0 0 0 0 Ψ66\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠\n",
      "\n",
      "(26.48)\n",
      "\n",
      "The presence of a feedback loop Z1 → Z2 → Z3 is evident from the fact that W is not lower triangular. Also the presence of confounding between Y1 and Y2 is evident in the off-diagonal terms in Ψ.\n",
      "\n",
      "Often we assume there are multiple observations for each latent variable. To ensure identiﬁa- bility, we can set the mean of the latent variables Zt to 0, and we can set the regression weights of Zt → Yt to 1. This essentially deﬁnes the scale of each latent variable. (In addition to the Z’s, there are the extra hidden variables implied by the presence of the bidirected edges.)\n",
      "\n",
      "The standard practice in the SEM community, as exempliﬁed by the popular commercial software package called LISREL (available from http://www.ssicentral.com/lisrel/), is to\n",
      "\n",
      "26.6. Learning causal DAGs\n",
      "\n",
      "931\n",
      "\n",
      "build the structure by hand, to estimate the parameters by maximum likelihood, and then to test if any of the regression weights are signiﬁcantly different from 0, using standard frequentist methods. However, one can also use Bayesian inference for the parameters (see e.g., (Dunson et al. 2005)). Structure learning in SEMs is rare, but since recursive SEMs are equivalent to Gaussian DAGs, many of the techniques we have been discussing in this section can be applied. SEMs are closely related to factor analysis (FA) models (Chapter 12). The basic difference is that in an FA model, the latent Gaussian has a low-rank covariance matrix, and the observed noise has a diagonal covariance (hence no bidirected edges). In an SEM, the covariance of the latent Gaussian has a sparse Cholesky decomposition (at least if W is acyclic), and the observed noise might have a full covariance matrix.\n",
      "\n",
      "Note that SEMs can be extended in many ways. For example, we can add covariates/ input variables (possibly noisily observed), we can make some of the observations be discrete (e.g., by using probit links), and so on.\n",
      "\n",
      "26.6\n",
      "\n",
      "Learning causal DAGs\n",
      "\n",
      "Causal models are models which can predict the effects of interventions to, or manipulations of, a system. For example, an electronic circuit diagram implicitly provides a compact encoding of what will happen if one removes any given component, or cuts any wire. A causal medical model might predict that if I continue to smoke, I am likely to get lung cancer (and hence if I cease smoking, I am less likely to get lung cancer). Causal claims are inherently stronger, yet more useful, than purely associative claims, such as “people who smoke often have lung cancer”.\n",
      "\n",
      "Causal models are often represented by DAGs (Pearl 2000), although this is somewhat contro- versial (Dawid 2010). We explain this causal interpretation of DAGs below. We then show how to use a DAG to do causal reasoning. Finally, we brieﬂy discuss how to learn the structure of causal DAGs. A more detailed description of this topic can be found in (Pearl 2000) and (Koller and Friedman 2009, Ch.21).\n",
      "\n",
      "26.6.1\n",
      "\n",
      "Causal interpretation of DAGs\n",
      "\n",
      "In this section, we deﬁne a directed edge A → B in a DAG to mean that “A directly causes B”, so if we manipulate A, then B will change. This is known as the causal Markov assumption. (Of course, we have not deﬁned the word “causes”, and we cannot do that by appealing to a DAG, lest we end up with a cyclic deﬁnition; see (Dawid 2010) for further disussion of this point.) We will also assume that all relevant variables are included in the model, i.e., there are no unknown confounders, reﬂecting hidden common causes. This is called the causal sufficiency assumption. (If there are known to be confounders, they should be added to the model, although one can sometimes use mixed directed graphs (Section 26.5.5) as a way to avoid having to model confounders explicitly.)\n",
      "\n",
      "Assuming we are willing to make the causal Markov and causal sufficiency assumptions, we can use DAGs to answer causal questions. The key abstraction is that of a perfect intervention; this represents the act of setting a variable to some known value, say setting Xi to xi. A real world example of such a perfect intervention is a gene knockout experiment, in which a gene is “silenced”. We need some notational convention to distinguish this from observing that Xi\n",
      "\n",
      "932\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "G\n",
      "\n",
      "Gdo(X=x)\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "Figure 26.14\n",
      "\n",
      "Surgical intervention on X. Based on (Pe’er 2005).\n",
      "\n",
      "happens to have value xi. We use Pearl’s do calculus notation (as in the verb “to do”) and write do(Xi = xi) to denote the event that we set Xi to xi. A causal model can be used to make inferences of the form p(x|do(Xi = xi)), which is different from making inferences of the form p(x|Xi = xi).\n",
      "\n",
      "To understand the difference between conditioning on interventions and conditioning on observations (i.e., the difference between doing and seeing), consider a 2 node DGM S → Y , in which S = 1 if you smoke and S = 0 otherwise, and Y = 1 if you have yellow-stained ﬁngers, and Y = 0 otherwise. If I observe you have yellow ﬁngers, I am licensed to infer that you are probably a smoker (since nicotine causes yellow stains):\n",
      "\n",
      "p(S = 1|Y = 1)> p (S = 1)\n",
      "\n",
      "(26.49)\n",
      "\n",
      "However, if I intervene and paint your ﬁngers yellow, I am no longer licensed to infer this, since I have disrupted the normal causal mechanism. Thus\n",
      "\n",
      "p(S = 1|do(Y = 1)) =p( S = 1)\n",
      "\n",
      "(26.50)\n",
      "\n",
      "One way to model perfect interventions is to use graph surgery: represent the joint distri- bution by a DGM, and then cut the arcs coming into any nodes that were set by intervention. See Figure 26.14 for an example. This prevents any information ﬂow from the nodes that were intervened on from being sent back up to their parents. Having perform this surgery, we can then perform probabilistic inference in the resulting “mutilated” graph in the usual way to reason about the effects of interventions. We state this formally as follows.\n",
      "\n",
      "Theorem 26.6.1 (Manipulation theorem (Pearl 2000; Spirtes et al. 2000)). . To compute p(Xi|do(Xj)) for sets of nodes i, j, we can perform surgical intervention on the Xj nodes and then use standard probabilistic inference in the mutilated graph.\n",
      "\n",
      "We can generalize the notion of a perfect intervention by adding interventions as explicit action nodes to the graph. The result is like an inﬂuence diagram, except there are no utility nodes (Lauritzen 2000; Dawid 2002). This has been called the augmented DAG (Pearl 2000). We\n",
      "\n",
      "26.6. Learning causal DAGs\n",
      "\n",
      "933\n",
      "\n",
      "y\n",
      "\n",
      "x\n",
      "\n",
      "Figure 26.15\n",
      "\n",
      "Illustration of Simpson’s paradox. Figure generated by simpsonsParadoxGraph.\n",
      "\n",
      "can then deﬁne the CPD p(Xi|do(Xi)) to be anything we want. We can also allow an action to affect multiple nodes. This is called a fat hand intervention, a reference to someone trying to change a single component of some system (e.g., an electronic circuit), but accidently touching multiple components and thereby causing various side effects (see (Eaton and Murphy 2007) for a way to model this using augmented DAGs).\n",
      "\n",
      "26.6.2\n",
      "\n",
      "Using causal DAGs to resolve Simpson’s paradox\n",
      "\n",
      "In this section, we assume we know the causal DAG. We can then do causal reasoning by applying d-separation to the mutilated graph. In this section, we give an example of this, and show how causal reasoning can help resolve a famous paradox, known as Simpon’s paradox.\n",
      "\n",
      "Simpson’s paradox says that any statistical relationship between two variables can be reversed by including additional factors in the analysis. For example, suppose some cause C (say, taking a drug) makes some effect E (say getting better) more likely\n",
      "\n",
      "P (E|C) > P (E|¬C)\n",
      "\n",
      "and yet, when we condition on the gender of the patient, we ﬁnd that taking the drug makes the effect less likely in both females (F ) and males (¬F ):\n",
      "\n",
      "P (E|C, F ) < P (E|¬C, F ) P (E|C, ¬F ) < P (E|¬C, ¬F )\n",
      "\n",
      "This seems impossible, but by the rules of probability, this is perfectly possible, because the event space where we condition on (¬C, F ) or (¬C, ¬F ) can be completely different to the event space when we just condition on ¬C. The table of numbers below shows a concrete example (from (Pearl 2000, p175)):\n",
      "\n",
      "C ¬C Total\n",
      "\n",
      "E 20 16 36\n",
      "\n",
      "Combined ¬E 20 24 44\n",
      "\n",
      "Total 40 40 80\n",
      "\n",
      "Rate E 18 50% 7 40% 25\n",
      "\n",
      "Male ¬E 12 3 15\n",
      "\n",
      "Total 30 10 40\n",
      "\n",
      "Rate E 2 60% 9 70% 11\n",
      "\n",
      "¬E 8 21 29\n",
      "\n",
      "Female Total 10 30 40\n",
      "\n",
      "Rate 20% 30%\n",
      "\n",
      "934\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "From this table of numbers, we see that\n",
      "\n",
      "p(E|C) = 20/40 = 0.5 > p(E|¬C) = 16/40 = 0.4 p(E|C, F ) = 2/10 = 0.2 < p(E|¬C, F ) = 9/30 = 0.3 p(E|C, ¬F ) = 18/30 = 0.6 < p(E|¬, ¬F ) = 7/10 = 0.7\n",
      "\n",
      "(26.51)\n",
      "\n",
      "(26.52)\n",
      "\n",
      "(26.53)\n",
      "\n",
      "A visual representation of the paradox is given in in Figure 26.15. The line which goes up and to the right shows that the effect (y-axis) increases as the cause (x-axis) increases. However, the dots represent the data for females, and the crosses represent the data for males. Within each subgroup, we see that the effect decreases as we increase the cause.\n",
      "\n",
      "It is clear that the effect is real, but it is still very counter-intuitive. The reason the paradox arises is that we are interpreting the statements causally, but we are not using proper causal reasoning when performing our calculations. The statement that the drug C causes recovery E is\n",
      "\n",
      "P (E|do(C)) > P (E|do(¬C))\n",
      "\n",
      "(26.54)\n",
      "\n",
      "whereas the data merely tell us P (E|C) > P (E|¬C)\n",
      "\n",
      "(26.55) This is not a contradiction. Observing C is positive evidence for E, since more males than females take the drug, and the male recovery rate is higher (regardless of the drug). Thus Equation 26.55 does not imply Equation 26.54.\n",
      "\n",
      "Nevertheless, we are left with a practical question: should we use the drug or not? It seems like if we don’t know the patient’s gender, we should use the drug, but as soon as we discover if they are male or female, we should stop using it. Obviously this conclusion is ridiculous.\n",
      "\n",
      "To answer the question, we need to make our assumptions more explicit. Suppose reality can be modeled by the causal DAG in Figure 26.16(a). To compute the causal effect of C on E, we need to adjust for (i.e., condition on) the confounding variable F . This is necessary because there is a backdoor path from C to E via F , so we need to check the C → E relationship for each value of F separately, to make sure the relationship between C and E is not affected by any value of F .\n",
      "\n",
      "Suppose that for each value of F , taking the drug is harmful, that is,\n",
      "\n",
      "p(E|do(C), F ) < p(E|do(¬C), F ) p(E|do(C), ¬F ) < p(E|do(¬C), ¬F )\n",
      "\n",
      "(26.56) (26.57)\n",
      "\n",
      "Then we can show that taking the drug is harmful overall:\n",
      "\n",
      "p(E|do(C)) < p(E|do(¬C))\n",
      "\n",
      "(26.58)\n",
      "\n",
      "The proof is as follows (Pearl 2000, p181). First, from our assumptions in Figure 26.16(a), we see that drugs have no effect on gender\n",
      "\n",
      "p(F |do(C)) = p(F |do(¬C)) = p(F )\n",
      "\n",
      "(26.59)\n",
      "\n",
      "Now using the law of total probability,\n",
      "\n",
      "p(E|do(C)) = p(E|do(C), F )p(F |do(C)) + p(E|do(C), ¬F )p(¬F |do(C)) = p(E|do(C), F )p(F ) +p(E |do(C), ¬F )p(¬F )\n",
      "\n",
      "(26.60)\n",
      "\n",
      "(26.61)\n",
      "\n",
      "26.6. Learning causal DAGs\n",
      "\n",
      "935\n",
      "\n",
      "Treatment\n",
      "\n",
      "Gender\n",
      "\n",
      "Treatment\n",
      "\n",
      "Blood Pressure\n",
      "\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "E Recovery\n",
      "\n",
      "E Recovery\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 26.16 Two different models uses to illustrate Simpson’s paradox. (a) F is gender and is a confounder for C and E. (b) F is blood pressure and is caused by C.\n",
      "\n",
      "Similarly,\n",
      "\n",
      "p(E|do(¬C)) = p(E|do(¬C), F )p(F ) +p( E|do(¬C), ¬F )p(¬F )\n",
      "\n",
      "(26.62)\n",
      "\n",
      "Since every term in Equation 26.61 is less than the corresponding term in Equation 26.62, we conclude that\n",
      "\n",
      "p(E|do(C)) < p(E|do(¬C))\n",
      "\n",
      "(26.63)\n",
      "\n",
      "So if the model in Figure 26.16(a) is correct, we should not administer the drug, since it reduces the probability of the effect.\n",
      "\n",
      "Now consider a different version of this example. Suppose we keep the data the same but interpret F as something that is affected by C, such as blood pressure. See Figure 26.16(b). In this case, we can no longer assume\n",
      "\n",
      "p(F |do(C)) = p(F |do(¬C)) = p(F )\n",
      "\n",
      "(26.64)\n",
      "\n",
      "and the above proof breaks down. So p(E|do(C)) − p(E|do(¬C)) may be positive or negaitve. In the true model is Figure 26.16(b), then we should not condition on F when assessing the effect of C on E, since there is no backdoor path in this case, because of the v-structure at F . That is, conditioning on F might block one of the causal pathways. In other words, by comparing patients with the same post-treatment blood pressure (value of F ), we may mask the effect of one of the two pathways by which the drug operates to bring about recovery.\n",
      "\n",
      "Thus we see that different causal assumptions lead to different causal conclusions, and hence different courses of action. This raises the question on whether we can learn the causal model from data. We discuss this issue below.\n",
      "\n",
      "26.6.3\n",
      "\n",
      "Learning causal DAG structures\n",
      "\n",
      "In this section, we discuss some ways to learn causal DAG structures.\n",
      "\n",
      "936\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "26.6.3.1\n",
      "\n",
      "Learning from observational data\n",
      "\n",
      "In Section 26.4, we discussed various methods for learning DAG structures from observational data. It is natural to ask whether these methods can recover the “true” DAG structure that was used to generate the data. Clearly, even if we have inﬁnite data, an optimal method can only identify the DAG up to Markov equivalence (Section 26.4.1). That is, it can identify the PDAG (partially directed acylic graph), but not the complete DAG structure, because all DAGs which are Markov equivalent have the same likelihood.\n",
      "\n",
      "There are several algorithms (e.g., the greedy equivalence search method of (Chickering 2002)) that are consistent estimators of PDAG structure, in the sense that they identify the true Markov equivalence class as the sample size goes to inﬁnity, assuming we observe all the variables. However, we also have to assume that the generating distribution p is faithful to the generating DAG G. This means that all the conditional indepence (CI) properties of p are exactly captured by the graphical structure, so I(p) = I(G); this means there cannot be any CI properties in p that are due to particular settings of the parameters (such as zeros in a regression matrix) that are not graphically explicit. For this reason, a faithful distribution is also called a stable distribution.\n",
      "\n",
      "Instead of recovering the full graph, we can focus on the causal analog of edge marginals, by computing the magnitude of the causal effect of one node on another (say A on B). If we know the DAG, we can do this using techniques described in (Pearl 2000). If the DAG is unknown, we can compute a lower bound on the effect as follows (Maathuis et al. 2009): learn an equivalence class (PDAG) from data; enumerate all the DAGs in the equivalence class; apply Pearl’s do-calculus to compute the magnitude of the causal effect of A on B in each DAG; ﬁnally, take the minimum of these effects as the lower bound. It is usually computationally infeasible to compute all DAGs in the equivalence class, but fortunately one only needs to be able to identify the local neighborhood of A and B, which can be esimated more efficiently, as described in (Maathuis et al. 2009). This technique is called IDA, which is short for “intervention-calculus when the DAG is absent”.\n",
      "\n",
      "Suppose the assumptions hold and we learn a PDAG. What can we do with it?\n",
      "\n",
      "In (Maathuis et al. 2010), this technique was applied to some yeast gene expression data. Gene knockout data was used to estimate the “ground truth” effect of each 234 single-gene deletions on the remaining 5,361 genes. Then the algorithm was applied to 63 unperturbed (wild-type) samples, and was used to rank order the likely targets of each of the 234 genes. The method had a precision of 66% when the recall was set to 10%; while low, this is substantially more than rival variable-selection methods, such as lasso and elastic net, which were only slightly above chance.\n",
      "\n",
      "26.6.3.2\n",
      "\n",
      "Learning from interventional data\n",
      "\n",
      "If we want to distinguish between DAGs within the equivalence class, we need to use interven- tional data, where certain variables have been set, and the consequences have been measured. An example of this is the dataset in Figure 26.17(a), where proteins in a signalling pathway were perturbed, and their phosphorylation status was measured using a technique called ﬂow cytometry (Sachs et al. 2005).\n",
      "\n",
      "It is straightforward to modify the standard Bayesian scoring criteria, such as the marginal likelihood or BIC score, to handle learning from mixed observational and experimental data: we\n",
      "\n",
      "26.6. Learning causal DAGs\n",
      "\n",
      "937\n",
      "\n",
      "(a)\n",
      "\n",
      "Psitect\n",
      "\n",
      "AKT inh\n",
      "\n",
      "U0126\n",
      "\n",
      "PMA\n",
      "\n",
      "pkc\n",
      "\n",
      "plcy\n",
      "\n",
      "pip2\n",
      "\n",
      "pip3\n",
      "\n",
      "akt\n",
      "\n",
      "raf f\n",
      "\n",
      "pka\n",
      "\n",
      "G06967\n",
      "\n",
      "mek12\n",
      "\n",
      "Present Missing Int. edge\n",
      "\n",
      "jnk\n",
      "\n",
      "p38\n",
      "\n",
      "erk\n",
      "\n",
      "B2cAMP\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 26.17 (a) A design matrix consisting of 5400 data points (rows) measuring the status (using ﬂow cytometry) of 11 proteins (columns) under different experimental conditions. The data has been discretized low (black), medium (grey) and high (white). Some proteins were explicitly controlled using into 3 states: (b) A directed graphical model representing dependencies between activating or inhibiting chemicals. various proteins (blue circles) and various experimental interventions (pink ovals), which was inferred from this data. We plot all edges for which p(Gst = 1|D) > 0.5. Dotted edges are believed to exist in nature but were not discovered by the algorithm (1 false negative). Solid edges are true positives. The light colored edges represent the effects of intervention. Source: Figure 6d of (Eaton and Murphy 2007) . This ﬁgure can be reproduced using the code at http://www.cs.ubc.ca/~murphyk/Software/BDAGL/index.html.\n",
      "\n",
      "938\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "just compute the sufficient statistics for a CPD’s parameter by skipping over the cases where that node was set by intervention (Cooper and Yoo 1999). For example, when using tabular CPDs, we modify the counts as follows:\n",
      "\n",
      "Ntck (cid:2)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "I(xi,t = k, xi,pa(t) = c)\n",
      "\n",
      "(26.65)\n",
      "\n",
      "i:xit not set\n",
      "\n",
      "The justiﬁcation for this is that in cases where node t is set by force, it is not sampled from its usual mechanism, so such cases should be ignored when inferring the parameter θt. The mod- iﬁed scoring criterion can be combined with any of the standard structure learning algorithms. (He and Geng 2009) discusses some methods for choosing which interventions to perform, so as to reduce the posterior uncertainty as quickly as possible (a form of active learning).\n",
      "\n",
      "The preceeding method assumes the interventions are perfect. In reality, experimenters can rarely control the state of individual molecules. Instead, they inject various stimulant or inhibitor chemicals which are designed to target speciﬁc molecules, but which may have side effects. We can model this quite simply by adding the intervention nodes to the DAG, and then learning a larger augmented DAG structure, with the constraint that there are no edges between the intervention nodes, and no edges from the “regular” nodes back to the intervention nodes.\n",
      "\n",
      "Figure 26.17(b) shows the augmented DAG that was learned from the interventional ﬂow In particular, we plot the median graph, which cytometry data depicted in Figure 26.17(a). includes all edges for which p(Gij = 1|D) > 0.5. These were computed using the exact algorithm of (Koivisto 2006). It turns out that, in this example, the median model has exactly the same structure as the optimal MAP model, argmaxG p(G|D), which was computed using the algorithm of (Koivisto and Sood 2004; Silander and Myllmaki 2006).\n",
      "\n",
      "26.7\n",
      "\n",
      "Learning undirected Gaussian graphical models\n",
      "\n",
      "Learning the structured of undirected graphical models is easier than learning DAG structure because we don’t need to worry about acyclicity. On the other hand, it is harder than learning DAG structure since the likelihood does not decompose (see Section 19.5). This precludes the kind of local search methods (both greedy search and MCMC sampling) we used to learn DAG structures, because the cost of evaluating each neighboring graph is too high, since we have to reﬁt each model from scratch (there is no way to incrementally update the score of a model).\n",
      "\n",
      "in the context of Gaussian random ﬁelds or undirected Gaussian graphical models (GGM)s. We consider structure learning for discrete undirected models in Section 26.8.\n",
      "\n",
      "In this section, we discuss several solutions to this problem,\n",
      "\n",
      "26.7.1 MLE for a GGM\n",
      "\n",
      "Before discussing structure learning, we need to discuss parameter estimation. The task of computing the MLE for a (non-decomposable) GGM is called covariance selection (Dempster 1972).\n",
      "\n",
      "From Equation 4.19, the log likelihood can be written as\n",
      "\n",
      "(cid:2)(Ω) = log det Ω − tr(SΩ)\n",
      "\n",
      "(26.66)\n",
      "\n",
      "26.7. Learning undirected Gaussian graphical models\n",
      "\n",
      "939\n",
      "\n",
      "where Ω = Σ−1 is the precision matrix, and S = 1 N covariance matrix. One can show that the gradient of this is given by\n",
      "\n",
      "i=1(xi − x)(xi − x)T is the empirical (For notational simplicity, we assume we have already estimated ˆμ = x.)\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "(26.67) However, we have to enforce the constraints that Ωst = 0 if Gst = 0 (structural zeros), and that Ω is positive deﬁnite. The former constraint is easy to enforce, but the latter is somewhat challenging (albeit still a convex constraint). One approach is to add a penalty term to the objective if Ω leaves the positive deﬁnite cone; this is the approach used in ggmFitMinfunc (see also (Dahl et al. 2008)). Another approach is to use a coordinate descent method, described in (Hastie et al. 2009, p633), and implemented in ggmFitHtf. Yet another approach is to use iterative proportional ﬁtting, described in Section 19.5.7. However, IPF requires identifying the cliques of the graph, which is NP-hard in general.\n",
      "\n",
      "Interestingly, one can show that the MLE must satisfy the following property: Σst = Sst if Gst = 1 or s = t, i.e., the covariance of a pair that are connected by an edge must match the In addition, we have Ωst = 0 if Gst = 0, by deﬁnition of a GGM, i.e., empirical covariance. the precision of a pair that are not connected must be 0. We say that Σ is a positive deﬁnite matrix completion of S, since it retains as many of the entries in S as possible, corresponding to the edges in the graph, subject to the required sparsity pattern on Σ−1, corresponding to the absent edges; the remaining entries in Σ are ﬁlled in so as to maximize the likelihood.\n",
      "\n",
      "∇(cid:2)(Ω) =Ω −1 − S\n",
      "\n",
      "Let us consider a worked example from (Hastie et al. 2009, p652). We will use the following adjacency matrix, representing the cyclic structure, X1 − X2 − X3 − X4 − X1, and the following empirical covariance matrix:\n",
      "\n",
      "G =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "0 1 0 1\n",
      "\n",
      "1 0 1 0 1 0 1 0 1 0 1 0\n",
      "\n",
      "⎟ ⎟ ⎠ , S =\n",
      "\n",
      "⎞\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "10 1 5 4\n",
      "\n",
      "1 10 2 6\n",
      "\n",
      "5 2 10 3\n",
      "\n",
      "4 6 3 10\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠\n",
      "\n",
      "(26.68)\n",
      "\n",
      "The MLE is given by ⎛\n",
      "\n",
      "Σ =\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "10.00 1.00 1.31 4.00\n",
      "\n",
      "1.00 10.00 2.00 0.87\n",
      "\n",
      "1.31 2.00 10.00 3.00\n",
      "\n",
      "4.00 0.87 3.00 10.00\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠ , Ω =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎜ ⎜ ⎝\n",
      "\n",
      "0.12 −0.01 −0.01 0 −0.05\n",
      "\n",
      "0.11 −0.02 −0.02 0\n",
      "\n",
      "0.11 −0.03 0.13 −0.03\n",
      "\n",
      "0\n",
      "\n",
      "−0.05 0\n",
      "\n",
      "⎞\n",
      "\n",
      "⎟ ⎟ ⎠ (26.69)\n",
      "\n",
      "(See ggmFitDemo for the code to reproduce these numbers.) The constrained elements in Ω, and the free elements in Σ, both of which correspond to absent edges, have been highlighted.\n",
      "\n",
      "26.7.2\n",
      "\n",
      "Graphical lasso\n",
      "\n",
      "We now discuss one way to learn a sparse GRF structure, which exploits the fact that there is a 1:1 correspondence between zeros in the precision matrix and absent edges in the graph. This suggests that we can learn a sparse graph structure by using an objective that encourages zeros in the precision matrix. By analogy to lasso (see Section 13.3), one can deﬁne the following (cid:2)1 penalized NLL:\n",
      "\n",
      "J(Ω) =− log det Ω + tr(SΩ) +λ||Ω ||1\n",
      "\n",
      "(26.70)\n",
      "\n",
      "940\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "lambda=36.00, nedges=8\n",
      "\n",
      "lambda=27.00, nedges=11\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "lambda=7.00, nedges=18\n",
      "\n",
      "lambda=0.00, nedges=55\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 26.18 Sparse GGMs learned using graphical lasso applied to the ﬂow cytometry data. (a) λ = 36. (b) λ = 27. (c) λ = 7. (d) λ = 0. Figure generated by ggmLassoDemo.\n",
      "\n",
      "where ||Ω||1 = Glasso.\n",
      "\n",
      "it is non-smooth (because of the non-differentiable (cid:2)1 penalty) and is constrained (because Ω must be a positive deﬁnite matrix). Several algorithms have been proposed for optimizing this objective (Yuan and Lin 2007; Banerjee et al. 2008; Duchi et al. 2008), although arguably the simplest is the one in (Friedman et al. 2008), which uses a coordinate descent algorithm similar to the shooting algorithm for lasso. See ggmLassoHtf for an implementation. (See also (Mazumder and Hastie 2012) for a more recent version of this algorithm.)\n",
      "\n",
      "Although the objective is convex,\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "j,k |ωjk| is the 1-norm of the matrix. This is called the graphical lasso or\n",
      "\n",
      "As an example, let us apply the method to the ﬂow cytometry dataset from (Sachs et al. 2005). A discretized version of the data is shown in Figure 26.17(a). Here we use the original continuous data. However, we are ignoring the fact that the data was sampled under intervention. In Figure 26.18, we illustrate the graph structures that are learned as we sweep λ from 0 to a large value. These represent a range of plausible hypotheses about the connectivity of these proteins. It is worth comparing this with the DAG that was learned in Figure 26.17(b). The DAG has the advantage that it can easily model the interventional nature of the data, but the disadvantage that it cannot model the feedback loops that are known to exist in this biological pathway (see the discussion in (Schmidt and Murphy 2009)). Note that the fact that we show many UGMs and only one DAG is incidental: we could easily use BIC to pick the “best” UGM, and conversely, we\n",
      "\n",
      "26.7. Learning undirected Gaussian graphical models\n",
      "\n",
      "941\n",
      "\n",
      "could easily display several DAG structures, sampled from the posterior.\n",
      "\n",
      "26.7.3\n",
      "\n",
      "Bayesian inference for GGM structure *\n",
      "\n",
      "Although the graphical lasso is reasonably fast, it only gives a point estimate of the structure. Furthermore, it is not model-selection consistent (Meinshausen 2005), meaning it cannot recover the true graph even as N → ∞. It would be preferable to integrate out the parameters, and perform posterior inference in the space of graphs, i.e., to compute p(G|D). We can then extract summaries of the posterior, such as posterior edge marginals, p(Gij = 1|D), just as we did for DAGs. In this section, we discuss how to do this.\n",
      "\n",
      "Note that the situation is analogous to Chapter 13, where we discussed variable selection. In Section 13.2, we discussed Bayesian variable selection, where we integrated out the regression weights and computed p(γ|D) and the marginal inclusion probabilities p(γj = 1|D). Then in Section 13.3, we discussed methods based on (cid:2)1 regularization. Here we have the same dichotomy, but we are presenting them in the opposite order.\n",
      "\n",
      "If the graph is decomposable, and if we use conjugate priors, we can compute the marginal likelihood in closed form (Dawid and Lauritzen 1993). Furthermore, we can efficiently identify the decomposable neighbors of a graph (Thomas and Green 2009), i.e., the set of legal edge additions and removals. This means that we can perform relatively efficient stochastic local (Giudici and Green 1999; Armstrong et al. 2008; search to approximate the posterior (see e.g. Scott and Carvalho 2008)).\n",
      "\n",
      "However, the restriction to decomposable graphs is rather limiting if one’s goal is knowledge discovery, since the number of decomposable graphs is much less than the number of general undirected graphs.5\n",
      "\n",
      "A few authors have looked at Bayesian inference for GGM structure in the non-decomposable case (e.g., (Dellaportas et al. 2003; Wong et al. 2003; Jones et al. 2005)), but such methods cannot scale to large models because they use an expensive Monte Carlo approximation to the marginal likelihood (Atay-Kayis and Massam 2005). (Lenkoski and Dobra 2008) suggested using a Laplace approxmation. This requires computing the MAP estimate of the parameters for Ω under a G- Wishart prior (Roverato 2002). In (Lenkoski and Dobra 2008), they used the iterative proportional scaling algorithm (Speed and Kiiveri 1986; Hara and Takimura 2008) to ﬁnd the mode. However, this is very slow, since it requires knowing the maximal cliques of the graph, which is NP-hard in general.\n",
      "\n",
      "In particular, they modify the gradient-based methods from Section 26.7.1 to ﬁnd the MAP estimate; these algorithms do not need to know the cliques of the graph. A further speedup is obtained by just using a diagonal Laplace approximation, which is more accurate than BIC, but has essentially the same cost. This, plus the lack of restriction to decomposable graphs, enables fairly fast stochastic search methods to be used to approximate p(G|D) and its mode. This approach signiﬁcantly outperfomed graphical lasso, both in terms of predictive accuracy and structural recovery, for a comparable computational cost.\n",
      "\n",
      "In (Moghaddam et al. 2009), a much faster method is proposed.\n",
      "\n",
      "5. The number of decomposable graphs on V nodes, for V = 2, . . . , 8, is as follows ((Armstrong 2005, p158)): 2; 8; 61; 822; 18,154; 61,7675; 30,888,596. If we divide these numbers by the number of undirected graphs, which is 2V (V −1)/2, we ﬁnd the ratios are: 1, 1, 0.95, 0.8, 0.55, 0.29, 0.12. So we see that decomposable graphs form a vanishing fraction of the total hypothesis space.\n",
      "\n",
      "942\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "26.7.4\n",
      "\n",
      "Handling non-Gaussian data using copulas *\n",
      "\n",
      "The graphical lasso and variants is inhertently limited to data that is jointly Gaussian, which is a rather severe restriction. Fortunately the method can be generalized to handle non-Gaussian, but still continuous, data in a fairly simple fashion. The basic idea is to estimate a set of D univariate monotonic transformations fj, one per variable j, such that the resulting transformed If this is possible, we say the data belongs to the nonparametric data is jointly Gaussian. Normal distribution, or nonparanormal distribution (Liu et al. 2009). This is equivalent to the family of Gaussian copulas (Klaassen and Wellner 1997). Details on how to estimate the fj transformations from the empirical cdf’s of each variable can be found in (Liu et al. 2009). After transforming the data, we can compute the correlation matrix and then apply glasso in the usual way. One can show, under various assumptions, that this is a consistent estimator of the graph structure, representing the CI assumptions of the original distribution(Liu et al. 2009).\n",
      "\n",
      "26.8\n",
      "\n",
      "Learning undirected discrete graphical models\n",
      "\n",
      "The problem of learning the structure for UGMs with discrete variables is harder than the Gaussian case, because computing the partition function Z(θ), which is needed for parameter estimation, has complexity comparable to computing the permanent of a matrix, which in general is intractable (Jerrum et al. 2004). By contrast, in the Gaussian case, computing Z only requires computing a matrix determinant, which is at most O(V 3).\n",
      "\n",
      "Since stochastic local search is not tractable for general discrete UGMs, below we mention\n",
      "\n",
      "some possible alternative approaches that have been tried.\n",
      "\n",
      "26.8.1\n",
      "\n",
      "Graphical lasso for MRFs/CRFs\n",
      "\n",
      "It is possible to extend the graphical lasso idea to the discrete MRF and CRF case. However, now there is a set of parameters associated with each edge in the graph, so we have to use the graph analog of group lasso (see Section 13.5.1). For example, consider a pairwise CRF with ternary nodes, and node and edge potentials given by ⎞ ⎛\n",
      "\n",
      "ψt(yt, x) =\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "vT t1x vT t2x vT t3x\n",
      "\n",
      "⎠ , ψst(ys, yt, x) =\n",
      "\n",
      "⎝\n",
      "\n",
      "wT t11x wT st21x wT wT st31x wT wT\n",
      "\n",
      "st12x wT st22x wT st32x wT\n",
      "\n",
      "st13x st23x st33x\n",
      "\n",
      "⎞\n",
      "\n",
      "⎠\n",
      "\n",
      "(26.71)\n",
      "\n",
      "where we assume x begins with a constant 1 term, to account for the offset. (If x only contains 1, the CRF reduces to an MRF.) Note that we may choose to set some of the vtk and wstjk weights to 0, to ensure identiﬁability, although this can also be taken care of by the prior, as shown in Exercise 8.5.\n",
      "\n",
      "To learn sparse structure, we can minimize the following objective:\n",
      "\n",
      "J = −\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "log ψt(yit, xi, vt) +\n",
      "\n",
      "V(cid:4)\n",
      "\n",
      "V(cid:4)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "log ψst(yis, yit, xi, wst)\n",
      "\n",
      "i=1\n",
      "\n",
      "t\n",
      "\n",
      "s=1\n",
      "\n",
      "t=s+1\n",
      "\n",
      "+λ1\n",
      "\n",
      "V(cid:4)\n",
      "\n",
      "V(cid:4)\n",
      "\n",
      "||wst||p + λ2\n",
      "\n",
      "V(cid:4)\n",
      "\n",
      "||vt||2 2\n",
      "\n",
      "(26.72)\n",
      "\n",
      "s=1\n",
      "\n",
      "t=s+1\n",
      "\n",
      "t=1\n",
      "\n",
      "26.8. Learning undirected discrete graphical models\n",
      "\n",
      "943\n",
      "\n",
      "case\n",
      "\n",
      "children\n",
      "\n",
      "bible\n",
      "\n",
      "health\n",
      "\n",
      "course\n",
      "\n",
      "christian\n",
      "\n",
      "insurance\n",
      "\n",
      "computer\n",
      "\n",
      "evidence\n",
      "\n",
      "disk\n",
      "\n",
      "email\n",
      "\n",
      "display\n",
      "\n",
      "card\n",
      "\n",
      "fact\n",
      "\n",
      "earth\n",
      "\n",
      "files\n",
      "\n",
      "graphics\n",
      "\n",
      "government\n",
      "\n",
      "god\n",
      "\n",
      "dos\n",
      "\n",
      "format\n",
      "\n",
      "help\n",
      "\n",
      "data\n",
      "\n",
      "image\n",
      "\n",
      "video\n",
      "\n",
      "gun\n",
      "\n",
      "human\n",
      "\n",
      "car\n",
      "\n",
      "president\n",
      "\n",
      "israel\n",
      "\n",
      "jesus\n",
      "\n",
      "drive\n",
      "\n",
      "memory\n",
      "\n",
      "number\n",
      "\n",
      "power\n",
      "\n",
      "law\n",
      "\n",
      "engine\n",
      "\n",
      "dealer\n",
      "\n",
      "jews\n",
      "\n",
      "baseball\n",
      "\n",
      "ftp\n",
      "\n",
      "mac\n",
      "\n",
      "scsi\n",
      "\n",
      "problem\n",
      "\n",
      "rights\n",
      "\n",
      "war\n",
      "\n",
      "religion\n",
      "\n",
      "games\n",
      "\n",
      "fans\n",
      "\n",
      "pc\n",
      "\n",
      "program\n",
      "\n",
      "phone\n",
      "\n",
      "nasa\n",
      "\n",
      "state\n",
      "\n",
      "question\n",
      "\n",
      "hockey\n",
      "\n",
      "software\n",
      "\n",
      "research\n",
      "\n",
      "shuttle\n",
      "\n",
      "league\n",
      "\n",
      "nhl\n",
      "\n",
      "launch moon\n",
      "\n",
      "science\n",
      "\n",
      "orbit\n",
      "\n",
      "players\n",
      "\n",
      "space\n",
      "\n",
      "university\n",
      "\n",
      "world\n",
      "\n",
      "season\n",
      "\n",
      "system\n",
      "\n",
      "driver\n",
      "\n",
      "team\n",
      "\n",
      "version\n",
      "\n",
      "technology\n",
      "\n",
      "win\n",
      "\n",
      "windows\n",
      "\n",
      "won\n",
      "\n",
      "Figure 26.19 An MRF estimated from the 20-newsgroup data using group (cid:7)1 regularization with λ = 256. Isolated nodes are not plotted. From Figure 5.9 of (Schmidt 2010). Used with kind permission of Mark Schmidt.\n",
      "\n",
      "where ||wst||p is the p-norm; common choices are p = 2 or p = ∞, as explained in Sec- tion 13.5.1. This method of CRF structure learning was ﬁrst suggested in (Schmidt et al. 2008). (The use of (cid:2)1 regularization for learning the structure of binary MRFs was proposed in (Lee et al. 2006).)\n",
      "\n",
      "Although this objective is convex, it can be costly to evaluate, since we need to perform inference to compute its gradient, as explained in Section 19.6.3 (this is true also for MRFs). We should therefore use an optimizer that does not make too many calls to the objective function or its gradient, such as the projected quasi-Newton method in (Schmidt et al. 2009). In addition, we can use approximate inference, such as convex belief propagation (Section 22.4.2), to compute an approximate objective and gradient more quickly. Another approach is to apply the group lasso penalty to the pseudo-likelihood discussed in Section 19.5.4. This is much faster, since inference is no longer required (Hoeﬂing and Tibshirani 2009). Figure 26.19 shows the result of applying this procedure to the 20-newsgroup data, where yit indicates the presence of word t in document i, and xi = 1 (so the model is an MRF).\n",
      "\n",
      "944\n",
      "\n",
      "Chapter 26. Graphical model structure learning\n",
      "\n",
      "(cid:51)(cid:3)(cid:11)(cid:38)(cid:32)(cid:41)(cid:12)\n",
      "\n",
      "(cid:51)(cid:11)(cid:38)(cid:32)(cid:55)(cid:12)\n",
      "\n",
      "(cid:19)(cid:17)(cid:24)\n",
      "\n",
      "(cid:19)(cid:17)(cid:24)\n",
      "\n",
      "(cid:38)(cid:79)(cid:82)(cid:88)(cid:71)(cid:92)\n",
      "\n",
      "(cid:38)\n",
      "\n",
      "(cid:51)(cid:11)(cid:54)(cid:32)(cid:41)(cid:12)\n",
      "\n",
      "(cid:51)(cid:11)(cid:54)(cid:32)(cid:55)(cid:12)\n",
      "\n",
      "(cid:38)\n",
      "\n",
      "(cid:51)(cid:11)(cid:53)(cid:32)(cid:41)(cid:12)\n",
      "\n",
      "(cid:51)(cid:11)(cid:53)(cid:32)(cid:55)(cid:12)\n",
      "\n",
      "(cid:41)\n",
      "\n",
      "(cid:19)(cid:17)(cid:24)\n",
      "\n",
      "(cid:19)(cid:17)(cid:24)\n",
      "\n",
      "(cid:54)(cid:83)(cid:85)(cid:76)(cid:81)(cid:78)(cid:79)(cid:72)(cid:85)\n",
      "\n",
      "(cid:53)(cid:68)(cid:76)(cid:81)\n",
      "\n",
      "(cid:41)\n",
      "\n",
      "(cid:19)(cid:17)(cid:27)\n",
      "\n",
      "(cid:19)(cid:17)(cid:21)\n",
      "\n",
      "(cid:55)\n",
      "\n",
      "(cid:19)(cid:17)(cid:28)\n",
      "\n",
      "(cid:19)(cid:17)(cid:20)\n",
      "\n",
      "(cid:55)\n",
      "\n",
      "(cid:19)(cid:17)(cid:21)\n",
      "\n",
      "(cid:19)(cid:17)(cid:27)\n",
      "\n",
      "(cid:58)(cid:72)(cid:87)(cid:3) (cid:42)(cid:85)(cid:68)(cid:86)(cid:86)\n",
      "\n",
      "(cid:54)(cid:3)(cid:3)(cid:53)\n",
      "\n",
      "(cid:51)(cid:11)(cid:58)(cid:32)(cid:41)(cid:12)\n",
      "\n",
      "(cid:51)(cid:11)(cid:58)(cid:32)(cid:55)(cid:12)\n",
      "\n",
      "(cid:41)(cid:3)(cid:3)(cid:41)\n",
      "\n",
      "(cid:20)(cid:17)(cid:19)\n",
      "\n",
      "(cid:19)(cid:17)(cid:19)\n",
      "\n",
      "(cid:55)(cid:3)(cid:3)(cid:41)\n",
      "\n",
      "(cid:19)(cid:17)(cid:20)\n",
      "\n",
      "(cid:19)(cid:17)(cid:28)\n",
      "\n",
      "(cid:41)(cid:3)(cid:3)(cid:55)\n",
      "\n",
      "(cid:19)(cid:17)(cid:20)\n",
      "\n",
      "(cid:19)(cid:17)(cid:28)\n",
      "\n",
      "(cid:55)(cid:3)(cid:3)(cid:55)\n",
      "\n",
      "(cid:19)(cid:17)(cid:19)(cid:20)\n",
      "\n",
      "(cid:19)(cid:17)(cid:28)(cid:28)\n",
      "\n",
      "Figure 26.20 Water sprinkler DGM with corresponding binary CPTs. T and F stand for true and false.\n",
      "\n",
      "26.8.2\n",
      "\n",
      "Thin junction trees\n",
      "\n",
      "So far, we have been concerned with learning “sparse” graphs, but these do not necessarily have low treewidth. For example, a D × D grid is sparse, but has treewidth O(D). This means that the models we learn may be intractable to use for inference purposes, which defeats one of the two main reasons to learn graph structure in the ﬁrst place (the other reason being “knowledge discovery”). There have been various attempts to learn graphical models with bounded treewidth (e.g., (Bach and Jordan 2001; Srebro 2001; Elidan and Gould 2008; Shahaf et al. 2009)), also known as thin junction trees, but the exact problem in general is hard.\n",
      "\n",
      "An alternative approach is to learn a model with low circuit complexity (Gogate et al. 2010; Poon and Domingos 2011). Such models may have high treewidth, but they exploit context- speciﬁc independence and determinism to enable fast exact inference (see e.g., (Darwiche 2009)).\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 26.1 Causal reasoning in the sprinkler network Consider the causal network in Figure 26.20. Let T represent true and F represent false.\n",
      "\n",
      "a. Suppose I perform a perfect intervention and make the grass wet. What is the probability the sprinkler\n",
      "\n",
      "is on, p(S = T |do(W = T ))?\n",
      "\n",
      "b. Suppose I perform a perfect intervention and make the grass dry. What is the probability the sprinkler\n",
      "\n",
      "is on, p(S = T |do(W = F ))?\n",
      "\n",
      "c. Suppose I perform a perfect intervention and make the clouds “turn on” (e.g., by seeding them). What\n",
      "\n",
      "is the probability the sprinkler is on, p(S = T |do(C = T ))?\n",
      "\n",
      "27 Latent variable models for discrete data\n",
      "\n",
      "27.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In this chapter, we are concerned with latent variable models for discrete data, such as bit vectors, sequences of categorical variables, count vectors, graph structures, relational data, etc. These models can be used to analyze voting records, text and document collections, low-intensity images, movie ratings, etc. However, we will mostly focus on text analysis, and this will be reﬂected in our terminology.\n",
      "\n",
      "Since we will be dealing with so many different kinds of data, we need some precise notation to keep things clear. When modeling variable-length sequences of categorical variables (i.e., let yil ∈ {1, . . . , V } represent symbols or tokens), such as words in a document, we will the identity of the l’th word in document i, where V is the number of possible words in the vocabulary. We assume l = 1 : Li, where Li is the (known) length of document i, and i = 1 :N , whereN is the number of documents.\n",
      "\n",
      "We will often ignore the word order, resulting in a bag of words. This can be reduced to a ﬁxed length vector of counts (a histogram). We will use niv ∈ {0, 1, . . . , Li} to denote the number of times word v occurs in document i, for v = 1 :V . Note that the N × V count matrix N is often large but sparse, since we typically have many documents, but most words do not occur in any given document.\n",
      "\n",
      "In some cases, we might have multiple different bags of words, e.g., bags of text words and bags of visual words. These correspond to different “channels” or types of features. We will denote these by yirl, for r = 1 :R (the number of responses) and l = 1 :L ir. If Lir = 1, it means we have a single token (a bag of length 1); in this case, we just write yir ∈ {1, . . . , Vr} for brevity. If every channel is just a single token, we write the ﬁxed-size response vector as yi,1:R; in this case, the N × R design matrix Y will not be sparse. For example, in social science surveys, yir could be the response of person i to the r’th multi-choice question.\n",
      "\n",
      "Out goal is to build joint probability models of p(yi) or p(ni) using latent variables to capture the correlations. We will then try to interpret the latent variables, which provide a compressed representation of the data. We provide an overview of some approaches in Section 27.2, before going into more detail in later sections.\n",
      "\n",
      "Towards the end of the chapter, we will consider modeling graphs and relations, which can also be represented as sparse discrete matrices. For example, we might want to model the graph of which papers mycite which other papers. We will denote these relations by R, reserving the symbol Y for any categorical data (e.g., text) associated with the nodes.\n",
      "\n",
      "946\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "27.2 Distributed state LVMs for discrete data\n",
      "\n",
      "In this section, we summarize a variety of possible approaches for constructing models of the form p(yi,1:Li ), for bags of tokens; p(y1:R), for vectors of tokens; and p(ni), for vectors of integer counts.\n",
      "\n",
      "27.2.1 Mixture models\n",
      "\n",
      "The simplest approach is to use a ﬁnite mixture model (Chapter 11). This associates a single discrete latent variable, qi ∈ {1, . . . , K}, with every document, where K is the number of clusters. We will use a discrete prior, qi ∼ Cat(π). For variable length documents, we can deﬁne p(yil|qi = k) = bkv, where bkv is the probability that cluster k generates word v. The value of qi is known as a topic, and the vector bk is the k’th topic’s word distribution. That is, the likelihood has the form\n",
      "\n",
      "p(yi,1:Li\n",
      "\n",
      "|qi = k) =\n",
      "\n",
      "Li(cid:20)\n",
      "\n",
      "Cat(yil|bk)\n",
      "\n",
      "(27.1)\n",
      "\n",
      "l=1\n",
      "\n",
      "The induced distribution on the visible data is given by\n",
      "\n",
      "p(yi,1:Li ) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "πk\n",
      "\n",
      "(cid:24)\n",
      "\n",
      "Li(cid:20)\n",
      "\n",
      "Cat(yil|bk)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "(27.2)\n",
      "\n",
      "k\n",
      "\n",
      "l=1\n",
      "\n",
      "The “generative story” which this encodes is as follows: for document i, pick a topic qi from π, call it k, and then for each word l = 1 :L i, pick a word from bk. We will consider more sophisticated generative models later in this chapter.\n",
      "\n",
      "If we have a ﬁxed set of categorical observations, we can use a different topic matrix for each\n",
      "\n",
      "output variable:\n",
      "\n",
      "p(yi,1:R|qi = k) =\n",
      "\n",
      "R(cid:20)\n",
      "\n",
      "Cat(yil|b(r) k )\n",
      "\n",
      "(27.3)\n",
      "\n",
      "r=1\n",
      "\n",
      "This is an unsupervised analog of naive Bayes classiﬁcation. If the sum Li =\n",
      "\n",
      "We can also model count vectors.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "v niv is known, we can use a\n",
      "\n",
      "multinomial:\n",
      "\n",
      "p(ni|Li, qi = k) = Mu(ni|Li, bk)\n",
      "\n",
      "(27.4)\n",
      "\n",
      "If the sum is unknown, we can use a Poisson class-conditional density to give\n",
      "\n",
      "p(ni|qi = k) =\n",
      "\n",
      "V(cid:20)\n",
      "\n",
      "Poi(niv|λvk)\n",
      "\n",
      "(27.5)\n",
      "\n",
      "In this case, Li|qi = k ∼ Poi(\n",
      "\n",
      "v=1\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "v λvk).\n",
      "\n",
      "27.2. Distributed state LVMs for discrete data\n",
      "\n",
      "947\n",
      "\n",
      "27.2.2\n",
      "\n",
      "Exponential family PCA\n",
      "\n",
      "Unfortunately, ﬁnite mixture models are very limited in their expressive power. A more ﬂexible model is to use a vector of real-valued continuous latent variables, similar to the factor analysis In PCA, we use a Gaussian prior of the form p(zi) = (FA) and PCA models in Chapter 12. N (μ, Σ), where zi ∈ RK, and a Gaussian likelihood of the form p(yi|zi) = N (Wzi, σ2I). Indeed, the method known This method can certainly be applied to discrete or count data. as latent semantic analysis (LSA) orlatent semantic indexing (LSI) (Deerwester et al. 1990; Dumais and Landauer 1997) is exactly equivalent to applying PCA to a term by document count matrix.\n",
      "\n",
      "A better method for modeling categorical data is to use a multinoulli or multinomial distribu-\n",
      "\n",
      "tion. We just have to change the likelihood to\n",
      "\n",
      "p(yi,1:Li\n",
      "\n",
      "|zi) =\n",
      "\n",
      "Li(cid:20)\n",
      "\n",
      "Cat(yil|S(Wzi))\n",
      "\n",
      "(27.6)\n",
      "\n",
      "l=1\n",
      "\n",
      "where W ∈ RV ×K is a weight matrix and S is the softmax function. If we have a ﬁxed number of categorical responses, we can use\n",
      "\n",
      "p(y1:R|zi) =\n",
      "\n",
      "R(cid:20)\n",
      "\n",
      "Cat(yir|S(Wrzi))\n",
      "\n",
      "(27.7)\n",
      "\n",
      "r=1\n",
      "\n",
      "where Wr ∈ RV ×K is the weight matrix for the r’th response variable. This model is called categorical PCA, and is illustrated in Figure 27.1(a); see Section 12.4 for further discussion. If we have counts, we can use a multinomial model\n",
      "\n",
      "p(ni|Li, zi) = Mu(ni|Li, S(Wzi))\n",
      "\n",
      "(27.8)\n",
      "\n",
      "or a Poisson model V(cid:20)\n",
      "\n",
      "p(ni|zi) =\n",
      "\n",
      "Poi(niv| exp(wT\n",
      "\n",
      "v,:zi))\n",
      "\n",
      "(27.9)\n",
      "\n",
      "v=1\n",
      "\n",
      "All of these models are examples of exponential family PCA or ePCA (Collins et al. 2002; Mohamed et al. 2008), which is an unsupervised analog of GLMs. The corresponding induced distribution on the visible variables has the form\n",
      "\n",
      "p(yi,1:Li ) =\n",
      "\n",
      "(cid:12) (cid:24)\n",
      "\n",
      "Li(cid:20)\n",
      "\n",
      "p(yil|zi, W)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "N (zi|μ, Σ)dzi\n",
      "\n",
      "(27.10)\n",
      "\n",
      "l=1\n",
      "\n",
      "Fitting this model is tricky, due to the lack of conjugacy. (Collins et al. 2002) proposed a coordinate ascent method that alternates between estimating the zi and W. This can be regarded as a degenerate version of EM, that computes a point estimate of zi in the E step. The problem with the degenerate approach is that it is very prone to overﬁtting, since the number of latent variables is proportional to the number of datacases (Welling et al. 2008). A true EM algorithm would marginalize out the latent variables zi. A way to do this for categorical PCA, using variational EM, is discussed in Section 12.4. For more general models, one can use MCMC (Mohamed et al. 2008).\n",
      "\n",
      "948\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "μ\n",
      "\n",
      "Σ\n",
      "\n",
      "α\n",
      "\n",
      "zi\n",
      "\n",
      "πi\n",
      "\n",
      "yi,1\n",
      "\n",
      ". . .\n",
      "\n",
      "yi,R\n",
      "\n",
      "N\n",
      "\n",
      "ni,1\n",
      "\n",
      ". . .\n",
      "\n",
      "ni,V\n",
      "\n",
      "Li\n",
      "\n",
      "W 1,K,V\n",
      "\n",
      ". . .\n",
      "\n",
      "W R,K,V\n",
      "\n",
      "N\n",
      "\n",
      "BK,V\n",
      "\n",
      "γ\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.1 Two LVMs for discrete data. Circles are scalar nodes, ellipses are vector nodes, squares are matrix nodes. (a) Categorical PCA. (b) Multinomial PCA.\n",
      "\n",
      "27.2.3\n",
      "\n",
      "LDA and mPCA\n",
      "\n",
      "In ePCA, the quantity Wzi represents the natural parameters of the exponential family. Some- times it is more convenient to use the dual parameters. For example, for the multinomial, the dual parameter is the probability vector, whereas the natural parameter is the vector of log odds. If we want to use the dual parameters, we need to constrain the latent variables so they live in the appropriate parameter space. In the case of categorical data, we will need to ensure the latent vector lives in SK, the K-dimensional probability simplex. To avoid confusion with ePCA, we will denote such a latent vector by πi. In this case, the natural prior for the latent variables is the Dirichlet, πi ∼ Dir(α). Typically we set α = α1K. If we set α (cid:22) 1, we encourage πi to be sparse, as shown in Figure 2.14.\n",
      "\n",
      "When we have a count vector whose total sum is known, the likelihood is given by\n",
      "\n",
      "p(ni|Li, πi) = Mu(ni|Li, Bπi)\n",
      "\n",
      "(27.11)\n",
      "\n",
      "This model is called multinomial PCA or mPCA (Buntine 2002; Buntine and Jakulin 2004, 2006). See Figure 27.1(b). Since we are assuming niv = k bvkπiv, this can be seen as a form of matrix factorization for the count matrix. Note that we use bv,k to denote the parameter vector, rather than wv,k, since we impose the constraints that 0 ≤ bv,k ≤ 1 and v bv,k = 1. The corresponding marginal distribution has the form\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "p(ni|Li) =\n",
      "\n",
      "Mu(ni|Li, Bπi)Dir(πi|α)dπi\n",
      "\n",
      "(27.12)\n",
      "\n",
      "Unfortunately, this integral cannot be computed analytically.\n",
      "\n",
      "If we have a variable length sequence (of known length), we can use\n",
      "\n",
      "p(yi,1:Li |πi) =\n",
      "\n",
      "Li(cid:20)\n",
      "\n",
      "Cat(yil|Bπi)\n",
      "\n",
      "(27.13)\n",
      "\n",
      "l=1\n",
      "\n",
      "27.2. Distributed state LVMs for discrete data\n",
      "\n",
      "949\n",
      "\n",
      "This is called latent Dirichlet allocation or LDA (Blei et al. 2003), and will be described in much greater detail below. LDA can be thought of as a probabilistic extension of LSA, where the latent quantities πik are non-negative and sum to one. By contrast, in LSA, zik can be negative which makes interpetation difficult.\n",
      "\n",
      "A predecessor to LDA, known as probabilistic latent semantic indexing or PLSI (Hofmann 1999), uses the same model but computes a point estimate of πi for each document (similar to ePCA), rather than integrating it out. Thus in PLSI, there is no prior for πi.\n",
      "\n",
      "We can modify LDA to handle a ﬁxed number of different categorical responses as follows:\n",
      "\n",
      "p(yi,1:R|πi) =\n",
      "\n",
      "R(cid:20)\n",
      "\n",
      "Cat(yil|B(r)πi)\n",
      "\n",
      "(27.14)\n",
      "\n",
      "r=1\n",
      "\n",
      "This has been called the user rating proﬁle (URP) model (Marlin 2003), and the simplex factor model (Bhattacharya and Dunson 2011).\n",
      "\n",
      "27.2.4\n",
      "\n",
      "GaP model and non-negative matrix factorization\n",
      "\n",
      "Now consider modeling count vectors where we do not constrain the sum to be observed. this case, the latent variables just need to be non-negative, so we will denote them by z+ can be ensured by using a prior of the form\n",
      "\n",
      "In i . This\n",
      "\n",
      "p(z+\n",
      "\n",
      "i ) =\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "Ga(z+\n",
      "\n",
      "ik|αk, βk)\n",
      "\n",
      "(27.15)\n",
      "\n",
      "k=1\n",
      "\n",
      "The likelihood is given by\n",
      "\n",
      "p(ni|z+\n",
      "\n",
      "i ) =\n",
      "\n",
      "V(cid:20)\n",
      "\n",
      "Poi(niv|bT\n",
      "\n",
      "v,:z+ i )\n",
      "\n",
      "(27.16)\n",
      "\n",
      "v=1\n",
      "\n",
      "This is called the GaP (Gamma-Poisson) model (Canny 2004). See Figure 27.2(a).\n",
      "\n",
      "In (Buntine and Jakulin 2006), it is shown that the GaP model, when conditioned on a ﬁxed Li, reduces to the mPCA model. This follows since a set of Poisson random variables, when conditioned on their sum, becomes a multinomial distribution (see e.g., (Ross 1989)).\n",
      "\n",
      "If we set αk = βk = 0 in the GaP model, we recover a method known as non-negative matrix factorization or NMF (Lee and Seung 2001), as shown in (Buntine and Jakulin 2006). NMF is not a probabilistic generative model, since it does not specify a proper prior for z+ i . Furthermore, the algorithm proposed in (Lee and Seung 2001) is another degenerate EM algo- rithm, so suffers from overﬁtting. Some procedures to ﬁt the GaP model, which overcome these problems, are given in (Buntine and Jakulin 2006).\n",
      "\n",
      "To encourage z+ i\n",
      "\n",
      "to be sparse, we can modify the prior to be a spike-and-Gamma type prior\n",
      "\n",
      "as follows: p(z+\n",
      "\n",
      "ik|αk, βk) where ρk is the probability of the spike at 0. This is called the conditional Gamma Poisson model (Buntine and Jakulin 2006). It is simple to modify Gibbs sampling to handle this kind of prior, although we will not go into detail here.\n",
      "\n",
      "ik) = ρkI(z+\n",
      "\n",
      "ik = 0) + (1 − ρk)Ga(z+\n",
      "\n",
      "(27.17)\n",
      "\n",
      "950\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "α1 β1\n",
      "\n",
      ". . .\n",
      "\n",
      "αK βK\n",
      "\n",
      "α\n",
      "\n",
      "πi\n",
      "\n",
      "z+ i,1\n",
      "\n",
      ". . .\n",
      "\n",
      "z+ i,K\n",
      "\n",
      "qil\n",
      "\n",
      "ni,1\n",
      "\n",
      ". . .\n",
      "\n",
      "ni,V\n",
      "\n",
      "N\n",
      "\n",
      "yil\n",
      "\n",
      "Li\n",
      "\n",
      "N\n",
      "\n",
      "B\n",
      "\n",
      "B\n",
      "\n",
      "γ\n",
      "\n",
      "γ\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.2 (a) Gaussian-Poisson (GAP) model. (b) Latent Dirichlet allocation (LDA) model.\n",
      "\n",
      "27.3\n",
      "\n",
      "Latent Dirichlet allocation (LDA)\n",
      "\n",
      "In this section, we explain the latent Dirichlet allocation or LDA (Blei et al. 2003) model in detail.\n",
      "\n",
      "27.3.1\n",
      "\n",
      "Basics\n",
      "\n",
      "In a mixture of multinoullis, every document is assigned to a single topic, qi ∈ {1, . . . , K}, In LDA, every word is assigned to its own topic, qil ∈ drawn from a global distribution π. {1, . . . , K}, drawn from a document-speciﬁc distribution πi. Since a document belongs to a distribution over topics, rather than a single topic, the model is called an admixture mixture or mixed membership model (Erosheva et al. 2004). This model has many other applications beyond text analysis, e.g., genetics (Pritchard et al. 2000), health science (Erosheva et al. 2007), social network analysis (Airoldi et al. 2008), etc.\n",
      "\n",
      "Adding conjugate priors to the parameters, the full model is as follows:1\n",
      "\n",
      "πi|α ∼ Dir(α1K) qil|πi ∼ Cat(πi) bk|γ ∼ Dir(γ1V ) yil|qil = k, B ∼ Cat(bk)\n",
      "\n",
      "(27.18)\n",
      "\n",
      "(27.19)\n",
      "\n",
      "(27.20)\n",
      "\n",
      "(27.21)\n",
      "\n",
      "This is illustrated in Figure 27.2(b). We can marginalize out the qi variables, thereby creating a\n",
      "\n",
      "1. Our notation is similar to the one we use elsewhere in this book, but is different from that used by most LDA papers. They typically use wnd for the identity of word n in document d, znd to represent the discrete indicator, θd as the continuous latent vector for document d, and βk as the k’th topic vector.\n",
      "\n",
      "27.3. Latent Dirichlet allocation (LDA)\n",
      "\n",
      "951\n",
      "\n",
      "1\n",
      "\n",
      "P(word1)\n",
      "\n",
      "= topic\n",
      "\n",
      "= observed document\n",
      "\n",
      "= generated document\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "P(word2)\n",
      "\n",
      "1\n",
      "\n",
      "P(word3)\n",
      "\n",
      "Figure 27.3 Geometric interpretation of LDA. We have K = 2 topics and V = 3 words. Each document Source: Figure 5 of (Steyvers and (white dots), and each topic (black dots), is a point in the 3d simplex. Griffiths 2007). Used with kind permission of Tom Griffiths.\n",
      "\n",
      "direct arc from πi to yil, with the following CPD: (cid:4)\n",
      "\n",
      "p(yil = v|πi) =\n",
      "\n",
      "p(yil = v|qil = k)p(qil = k) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "πikbkv\n",
      "\n",
      "(27.22)\n",
      "\n",
      "k\n",
      "\n",
      "k\n",
      "\n",
      "As we mentioned in the introduction, this is very similar to the multinomial PCA model proposed in (Buntine 2002), which in turn is closely related to categorical PCA, GaP, NMF, etc.\n",
      "\n",
      "LDA has an interesting geometric interpretation. Each vector bk deﬁnes a distribution over V words; each k is known as a topic. Each document vector πi deﬁnes a distribution over K topics. So we model each document as an admixture over topics. Equivalently, we can think of LDA as a form of dimensionality reduction (assuming K < V , as is usually the case), where we project a point in the V -dimensional simplex (a normalized document count vector xi) onto the K-dimensional simplex. This is illustrated in Figure 27.3, where we have V = 3 words and K = 2 topics. The observed documents (which live in the 3d simplex) are approximated as living on a 2d simplex spanned by the 2 topic vectors, each of which lives in the 3d simplex.\n",
      "\n",
      "One advantage of using the simplex as our latent space rather than Euclidean space is that the simplex can handle ambiguity. This is importance since in natural language, words can often have multiple meanings, a phenomomen known as polysemy. For example, “play” might refer to a verb (e.g., “to play ball” or “to play the coronet”), or to a noun (e.g., “Shakespeare’s play”). In LDA, we can have multiple topics, each of which can generate the word “play”, as shown in Figure 27.4, reﬂecting this ambiguity.\n",
      "\n",
      "Given word l in document i, we can compute p(qil = k|yi, θ), and thus infer its most likely topic. By looking at the word in isolation, it might be hard to know what sense of the word is meant, but we can disambiguate this by looking at other words in the document. In particular, given xi, we can infer the topic distribution πi for the document; this acts as a prior for disambiguating qil. This is illustrated in Figure 27.5, where we show three documents from the TASA corpus.2 In the ﬁrst document, there are a variety of music related words, which suggest\n",
      "\n",
      "2. The TASA corpus is a collection of 37,000 high-school level English documents, comprising over 10 million words,\n",
      "\n",
      "952\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "Topic 77\n",
      "\n",
      "Topic 82\n",
      "\n",
      "Topic 166\n",
      "\n",
      "prob. MUSIC .090 .034 DANCE SONG .033 PLAY .030 SING .026 SINGING .026 BAND .026 PLAYED .023 SANG .022 .021 DANCING .020 PIANO .017 PLAYING .016 RHYTHM .015 .013 ALBERT .013 MUSICAL\n",
      "\n",
      "SONGS\n",
      "\n",
      "word\n",
      "\n",
      "SHAKESPEARE\n",
      "\n",
      "word LITERATURE\n",
      "\n",
      "prob. .031 POEM .028 POETRY .027 .020 .019 .019 PLAY .015 LITERARY .013 .013 DRAMA .012 .012 WROTE .011 POETS WRITER .011 .010 WRITTEN .009 .009\n",
      "\n",
      "WRITERS\n",
      "\n",
      "POET PLAYS POEMS\n",
      "\n",
      "STAGE\n",
      "\n",
      "prob. PLAY .136 .129 BALL .065 GAME PLAYING .042 HIT .032 PLAYED .031 .027 BASEBALL .025 GAMES BAT .019 RUN .019 THROW .016 .015 BALLS .011 TENNIS HOME .010 CATCH .010 FIELD .010\n",
      "\n",
      "word\n",
      "\n",
      "Figure 27.4 Three topics related to the word play. Used with kind permission of Tom Griffiths.\n",
      "\n",
      "Source: Figure 9 of (Steyvers and Griffiths 2007).\n",
      "\n",
      "Document #29795 Bix beiderbecke, at age060 fifteen207, sat174 on the slope071 of a bluff055 overlooking027 the mississippi137 river137. He was listening077 to music077 coming009 from a passing043 riverboat. The music077 had already captured006 his heart157 as well as his ear119. It was jazz077. Bix beiderbecke had already had music077 lessons077. He showed002 promise134 on the piano077, and his parents035 hoped268 he might consider118 becoming a concert077 pianist077. But bix was interested268 in another kind050 of music077. He wanted268 to play077 the cornet. And he wanted268 to play077 jazz077...\n",
      "\n",
      "Document #1883 There is a simple050 reason106 why there are so few periods078 of really great theater082 in our whole western046 world. Too many things300 have to come right at the very same time. The dramatists must have the right actors082, the actors082 must have the right playhouses, the playhouses must have the right audiences082. We must remember288 that plays082 exist143 to be performed077, not merely050 to be read254. ( even when you read254 a play082 to yourself, try288 to perform062 it, to put174 it on a stage078, as you go along.) as soon028 as a play082 has to be performed082, then some kind126 of theatrical082...\n",
      "\n",
      "Document #21359 Jim296 has a game166 book254. Jim296 reads254 the book254. Jim296 sees081 a game166 for one. Jim296 plays166 the game166. Jim296 likes081 the game166 for one. The game166 book254 helps081 jim296. Don180 comes040 into the house038. Don180 and jim296 read254 the game166 book254. The boys020 see a game166 for two. The two boys020 play166 the game166. The boys020 play166 the game166 for two. The boys020 like the game166. Meg282 comes040 into the house282. Meg282 and don180 and jim296 read254 the book254. They see a game166 for three. Meg282 and don180 and jim296 play166 the game166. They play166...\n",
      "\n",
      "Figure 27.5 Three documents from the TASA corpus containing different senses of the word play. Grayed out words were ignored by the model, because they correspond to uninteresting stop words (such as “and”, “the”, etc.) or very low frequency words. Source: Figure 10 of (Steyvers and Griffiths 2007). Used with kind permission of Tom Griffiths.\n",
      "\n",
      "27.3. Latent Dirichlet allocation (LDA)\n",
      "\n",
      "953\n",
      "\n",
      "πi will put most of its mass on the music topic (number 77); this in turn makes the music interpretation of “play” the most likely, as shown by the superscript. The second document interprets play in the theatrical sense, and the third in the sports sense. Note that is crucial that πi be a latent variable, so information can ﬂow between the qil’s, thus enabling local disambiguation to use the full set of words.\n",
      "\n",
      "27.3.2\n",
      "\n",
      "Unsupervised discovery of topics\n",
      "\n",
      "One of the main purposes of LDA is discover topics in a large collection or corpus of docu- ments (see Figure 27.12 for an example). Unfortunately, since the model is unidentiﬁable, the interpertation of the topics can be difficult (Chang et al. 2009).. One approach, known as la- beled LDA (Ramage et al. 2009), exploits the existence of tags on documents as a way to ensure identiﬁability. In particular, it forces the topics to correspond to the tags, and then it learns a distribution over words for each tag. This can make the results easier to interpret.\n",
      "\n",
      "27.3.3\n",
      "\n",
      "Quantitatively evaluating LDA as a language model\n",
      "\n",
      "In order to evaluate LDA quantitatively, we can treat it as a language model, i.e., a probability distribution over sequences of words. Of course, it is not a very good language model, since it ignores word order and just looks at single words (unigrams), but it is interesting to compare LDA to other unigram-based models, such as mixtures of multinoullis, and pLSI. Such simple language models are sometimes useful for information retrieval purposes. The standard way to measure the quality of a language model is to use perplexity, which we now deﬁne below.\n",
      "\n",
      "27.3.3.1\n",
      "\n",
      "Perplexity\n",
      "\n",
      "The perplexity of language model q given a stochastic process3 p is deﬁned as\n",
      "\n",
      "perplexity(p, q) (cid:2) 2H(p,q)\n",
      "\n",
      "(27.23)\n",
      "\n",
      "where H(p, q) is the cross-entropy of the two stochastic processes, deﬁned as\n",
      "\n",
      "H(p, q) (cid:2) lim N→∞\n",
      "\n",
      "− 1 N\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "y1:N\n",
      "\n",
      "p(y1:N ) log q(y1:N )\n",
      "\n",
      "(27.24)\n",
      "\n",
      "The cross entropy (and hence perplexity) is minimized if q = p; in this case, the model can predict as well as the “true” distribution.\n",
      "\n",
      "We can approximate the stochastic process by using a single long test sequence (composed of multiple documents and multiple sentences, complete with end-of-sentence markers), call it y∗ (This approximation becomes more and more accurate as the sequence gets longer, provided the process is stationary and ergodic (Cover and Thomas 2006).) Deﬁne the empirical distribution (an approximation to the stochastic process) as\n",
      "\n",
      "1:N .\n",
      "\n",
      "pemp(y1:N ) = δy∗\n",
      "\n",
      "1:N (y1:N )\n",
      "\n",
      "(27.25)\n",
      "\n",
      "collated by a company formerly known as Touchstone Applied Science Associates, but now known as Questar Assessment Inc www.questarai.com. 3. A stochastic process is one which can deﬁne a joint distribution over an arbitrary number of random variables. We can think of natural language as a stochastic process, since it can generate an inﬁnite stream of words.\n",
      "\n",
      "954\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "In this case, the cross-entropy becomes\n",
      "\n",
      "H(pemp, q) = − 1 N\n",
      "\n",
      "log q(y∗\n",
      "\n",
      "1:N )\n",
      "\n",
      "(27.26)\n",
      "\n",
      "and the perplexity becomes\n",
      "\n",
      "perplexity(pemp, q) = 2H(pemp,q) = q(y∗\n",
      "\n",
      "9 : : ; 1:N )−1/N = N\n",
      "\n",
      "N(cid:20)\n",
      "\n",
      "i=1\n",
      "\n",
      "q(y∗\n",
      "\n",
      "1 i |y∗ 1:i−1)\n",
      "\n",
      "(27.27)\n",
      "\n",
      "We see that this is the geometric mean of the inverse predictive probabilities, which is the usual deﬁnition of perplexity (Jurafsky and Martin 2008, p96).\n",
      "\n",
      "In the case of unigram models, the cross entropy term is given by\n",
      "\n",
      "H = − 1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "1 Li\n",
      "\n",
      "Li(cid:4)\n",
      "\n",
      "l=1\n",
      "\n",
      "log q(yil)\n",
      "\n",
      "(27.28)\n",
      "\n",
      "where N is the number of documents and Li is the number of words in document i. Hence the perplexity of model q is given by\n",
      "\n",
      "perplexity(pemp, p) = exp\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "− 1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "1 Li\n",
      "\n",
      "Li(cid:4)\n",
      "\n",
      "l=1\n",
      "\n",
      "log q(yil)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(27.29)\n",
      "\n",
      "Intuitively, perplexity mesures the weighted average branching factor of the model’s predic- tive distribution. Suppose the model predicts that each symbol (letter, word, whatever) is equally likely, so p(yi|y1:i−1) = 1/K. Then the perplexity is ((1/K)N )−1/N = K. If some symbols are more likely than others, and the model correctly reﬂects this, its perplexity will be lower than K. Of course, H(p, p) =H (p) ≤ H(p, q), so we can never reduce the perplexity below the entropy of the underlying stochastic process.\n",
      "\n",
      "27.3.3.2\n",
      "\n",
      "Perplexity of LDA\n",
      "\n",
      "The key quantity is p(v), the predictive distribution of the model over possible words. (It is implicitly conditioned on the training set.) For LDA, this can be approximated by plugging in B (e.g., the posterior mean estimate) and approximately integrating out q using mean ﬁeld inference (see (Wallach et al. 2009) for a more accurate way to approximate the predictive likelihood).\n",
      "\n",
      "In Figure 27.6, we compare LDA to several other simple unigram models, namely MAP estima- tion of a multinoulli, MAP estimation of a mixture of multinoullis, and pLSI. (When performing MAP estimation, the same Dirichlet prior on B was used as in the LDA model.) The metric is perplexity, as in Equation 27.29, and the data is a subset of the TREC AP corpus containing 16,333 newswire articles with 23,075 unique terms. We see that LDA signiﬁcantly outperforms these other methods.\n",
      "\n",
      "27.3. Latent Dirichlet allocation (LDA)\n",
      "\n",
      "955\n",
      "\n",
      "7000\n",
      "\n",
      "6500\n",
      "\n",
      "6000\n",
      "\n",
      "Unigram Mixtures of Unigrams LDA Fold in pLSI\n",
      "\n",
      "5500\n",
      "\n",
      "y t i x e p r e P\n",
      "\n",
      "l\n",
      "\n",
      "5000\n",
      "\n",
      "4500\n",
      "\n",
      "4000\n",
      "\n",
      "3500\n",
      "\n",
      "3000\n",
      "\n",
      "2500\n",
      "\n",
      "0\n",
      "\n",
      "20\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "120 100 80 Number of Topics\n",
      "\n",
      "140\n",
      "\n",
      "160\n",
      "\n",
      "180\n",
      "\n",
      "200\n",
      "\n",
      "Figure 27.6 Perplexity vs number of topics on the TREC AP corpus for various language models. Based on Figure 9 of (Blei et al. 2003). Figure generated by bleiLDAperplexityPlot.\n",
      "\n",
      "α\n",
      "\n",
      "α\n",
      "\n",
      "π1\n",
      "\n",
      ". . .\n",
      "\n",
      ". . .\n",
      "\n",
      "πN\n",
      "\n",
      "q1,1\n",
      "\n",
      "q1,L1 . . .\n",
      "\n",
      ". . .\n",
      "\n",
      "qN,1\n",
      "\n",
      "qN,LN . . .\n",
      "\n",
      "q1,1\n",
      "\n",
      "q1,L1 . . .\n",
      "\n",
      ". . .\n",
      "\n",
      "qN,1\n",
      "\n",
      "qN,LN . . .\n",
      "\n",
      "y1,1\n",
      "\n",
      "y1,L1 . . .\n",
      "\n",
      ". . .\n",
      "\n",
      "yN,1\n",
      "\n",
      "yN,LN . . .\n",
      "\n",
      "y1,1\n",
      "\n",
      "y1,L1 . . .\n",
      "\n",
      ". . .\n",
      "\n",
      "yN,1\n",
      "\n",
      "yN,LN . . .\n",
      "\n",
      "b1 . . .b K\n",
      "\n",
      "γ\n",
      "\n",
      "γ\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.7 the bk.\n",
      "\n",
      "(a) LDA unrolled for N documents.\n",
      "\n",
      "(b) Collapsed LDA, where we integrate out the πi and\n",
      "\n",
      "27.3.4\n",
      "\n",
      "Fitting using (collapsed) Gibbs sampling\n",
      "\n",
      "It is straightforward to derive a Gibbs sampling algorithm for LDA. The full conditionals are as follows:\n",
      "\n",
      "p(qil = k|·) ∝ exp[log πik + log bk,xil ]\n",
      "\n",
      "p(πi|·) = Dir({αk +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "I(zil = k)})\n",
      "\n",
      "(27.30)\n",
      "\n",
      "(27.31)\n",
      "\n",
      "p(bk|·) = Dir({γv +\n",
      "\n",
      "l (cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "I(xil = v, zil = k)})\n",
      "\n",
      "(27.32)\n",
      "\n",
      "i\n",
      "\n",
      "l\n",
      "\n",
      "However, one can get better performance by analytically integrating out the πi’s and the bk’s,\n",
      "\n",
      "956\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "both of which have a Dirichlet distribution, and just sampling the discrete qil’s. This approach was ﬁrst suggested in (Griffiths and Steyvers 2004), and is an example of collapsed Gibbs sampling. Figure 27.7(b) shows that now all the qil variables are fully correlated. However, we can sample them one at a time, as we explain below. (cid:7)Li\n",
      "\n",
      "word v is assigned to topic k in document i. Let cik = word from document i has been assigned to topic k. Let cvk = word v has been assigned to topic k in any document. Let niv = times word v occurs in document i; this is observed. Let ck = assigned to topic k. Finally, let Li = observed.\n",
      "\n",
      "First, we need some notation. Let civk =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "l=1 I(qil = k, yil = v) be the number of times v civk be the number of times any i civk be the number of times k civk be the number of v cvk be the number of words k cik be the number of words in document i; this is\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "We can now derive the marginal prior. By applying Equation 5.24, one can show that\n",
      "\n",
      "p(q|α) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:12) (cid:24)\n",
      "\n",
      "Li(cid:20)\n",
      "\n",
      "Cat(qil|πi)\n",
      "\n",
      "(cid:25)\n",
      "\n",
      "Dir(πi|α1K)dπi\n",
      "\n",
      "(27.33)\n",
      "\n",
      "=\n",
      "\n",
      "i (cid:8)\n",
      "\n",
      "Γ(Kα) Γ(α)K\n",
      "\n",
      "l=1\n",
      "\n",
      "(cid:9)N N(cid:20)\n",
      "\n",
      "i=1\n",
      "\n",
      "(cid:26)K\n",
      "\n",
      "k=1 Γ(cik + α) Γ(Li + Kα)\n",
      "\n",
      "(27.34)\n",
      "\n",
      "By similar reasoning, one can show\n",
      "\n",
      "p(y|q, γ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "(cid:12)\n",
      "\n",
      "⎡\n",
      "\n",
      "⎣\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Cat(yil|bk)\n",
      "\n",
      "⎤\n",
      "\n",
      "⎦ Dir(bk|γ1V )dbk\n",
      "\n",
      "(27.35)\n",
      "\n",
      "=\n",
      "\n",
      "k (cid:8)\n",
      "\n",
      "Γ(V β) Γ(β)V\n",
      "\n",
      "il:qil=k (cid:9)K K(cid:20)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:26)V\n",
      "\n",
      "v=1 Γ(cvk + β) Γ(ck + V β)\n",
      "\n",
      "(27.36)\n",
      "\n",
      "From the above equations, and using the fact that Γ(x + 1)/Γ(x) = x, we can derive the full conditional for p(qil|q−i,l). Deﬁne c− ivk to be the same as civk except it is compute by summing over all locations in document i except for qil. Also, let yil = v. Then\n",
      "\n",
      "p(qi,l = k|q−i,l, y, α, γ) ∝\n",
      "\n",
      "c− v,k + γ c− k + V γ\n",
      "\n",
      "c− i,k + α Li + Kα\n",
      "\n",
      "(27.37)\n",
      "\n",
      "We see that a word in a document is assigned to a topic based both on how often that word is generated by the topic (ﬁrst term), and also on how often that topic is used in that document (second term).\n",
      "\n",
      "Given Equation 27.37, we can implement the collapsed Gibbs sampler as follows. We randomly assign a topic to each word, qil ∈ {1, . . . , K}. We can then sample a new topic as follows: for a given word in the corpus, decrement the relevant counts, based on the topic assigned to the current word; draw a new topic from Equation 27.37, update the count matrices; and repeat. This algorithm can be made efficient since the count matrices are very sparse.\n",
      "\n",
      "27.3.5\n",
      "\n",
      "Example\n",
      "\n",
      "This process is illustrated in Figure 27.8 on a small example with two topics, and ﬁve words. The left part of the ﬁgure illustrates 16 documents that were sampled from the LDA model using\n",
      "\n",
      "27.3. Latent Dirichlet allocation (LDA)\n",
      "\n",
      "957\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
      "\n",
      "River\n",
      "\n",
      "Stream\n",
      "\n",
      "Bank\n",
      "\n",
      "Money\n",
      "\n",
      "Loan\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
      "\n",
      "River\n",
      "\n",
      "Stream\n",
      "\n",
      "Bank\n",
      "\n",
      "Money\n",
      "\n",
      "Loan\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.8 Illustration of (collapsed) Gibbs sampling applied to a small LDA example. There are N = 16 documents, each containing a variable number of words drawn from a vocabulary of V = 5 words, There are two topics. A white dot means word the word is assigned to topic 1, a black dot means the word is assigned to topic 2. (b) A sample from the posterior after 64 steps of Gibbs sampling. Source: Figure 7 of (Steyvers and Griffiths 2007). Used with kind permission of Tom Griffiths.\n",
      "\n",
      "(a) The initial random assignment of states.\n",
      "\n",
      "p(money|k = 1) =p(loan| k = 1) =p(bank| k = 1) = 1/3 and p(river|k = 2) =p(stream| k = 2) = p(bank|k = 2) = 1/3. For example, we see that the ﬁrst document contains the word “bank” 4 times (indicated by the four dots in row 1 of the “bank” column), as well as various other ﬁnancial terms. The right part of the ﬁgure shows the state of the Gibbs sampler after 64 iterations. The “correct” topic has been assigned to each token in most cases. For example, in document 1, we see that the word “bank” has been correctly assigned to the ﬁnancial topic, based on the presence of the words “money” and “loan”. The posterior mean estimate of the parameters is given by ˆp(money|k = 1) = 0.32, ˆp(loan|k = 1) = 0.29, ˆp(bank|k = 1) = 0.39, ˆp(river|k = 2) = 0.25, ˆp(stream|k = 2) = 0.4, and ˆp(bank|k = 2) = 0.35, which is impressively accurate, given that there are only 16 training examples.\n",
      "\n",
      "27.3.6\n",
      "\n",
      "Fitting using batch variational inference\n",
      "\n",
      "A faster alternative to MCMC is to use variational EM. (We cannot use exact EM since exact inference of πi and qi is intractable.) We give the details below.\n",
      "\n",
      "27.3.6.1\n",
      "\n",
      "Sequence version\n",
      "\n",
      "Following (Blei et al. 2003), we will use a fully factorized (mean ﬁeld) approximation of the form\n",
      "\n",
      "q(πi, qi) = Dir(πi| ˜πi)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Cat(qil|˜qil)\n",
      "\n",
      "(27.38)\n",
      "\n",
      "l\n",
      "\n",
      "We will follow the usual mean ﬁeld recipe. For q(qil), we use Bayes’ rule, but where we need to take expectations over the prior:\n",
      "\n",
      "˜qilk ∝ byi,l,k exp(E [log πik])\n",
      "\n",
      "(27.39)\n",
      "\n",
      "where\n",
      "\n",
      "E [log πik] = ψk( ˜πi.) (cid:2) Ψ(˜πik) − Ψ(\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "˜πik(cid:2) )\n",
      "\n",
      "(27.40)\n",
      "\n",
      "k(cid:2)\n",
      "\n",
      "958\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "where Ψ is the digamma function. The update for q(πi) is obtained by adding up the expected counts:\n",
      "\n",
      "˜πik = αk +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "˜qilk\n",
      "\n",
      "(27.41)\n",
      "\n",
      "l\n",
      "\n",
      "The M step is obtained by adding up the expected counts and normalizing:\n",
      "\n",
      "ˆbvk ∝ γv +\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "Li(cid:4)\n",
      "\n",
      "˜qilkI(yil = v)\n",
      "\n",
      "(27.42)\n",
      "\n",
      "i=1\n",
      "\n",
      "l=1\n",
      "\n",
      "27.3.6.2\n",
      "\n",
      "Count version\n",
      "\n",
      "Note that the E step takes O(( It is much more space efficient to perform inference in the mPCA version of the model, which works with counts; these only take O(N V K) space, which is a big savings if documents are long. (By contrast, the collapsed Gibbs sampler must work explicitly with the qil variables.)\n",
      "\n",
      "We will focus on approximating p(πi, ci|ni, Li), where we write ci as shorthand for ci... We\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "i Li)V K) space to store the ˜qilk.\n",
      "\n",
      "will again use a fully factorized (mean ﬁeld) approximation of the form\n",
      "\n",
      "q(πi, ci) = Dir(πi| ˜πi)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Mu(civ.|niv, ˜civ.)\n",
      "\n",
      "(27.43)\n",
      "\n",
      "v\n",
      "\n",
      "The new E step becomes\n",
      "\n",
      "˜πik = αk +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "niv˜civk\n",
      "\n",
      "(27.44)\n",
      "\n",
      "v\n",
      "\n",
      "˜civk ∝ bvk exp(E [log πik])\n",
      "\n",
      "(27.45)\n",
      "\n",
      "The new M step becomes\n",
      "\n",
      "ˆbvk ∝ γv +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "niv˜civk\n",
      "\n",
      "(27.46)\n",
      "\n",
      "i\n",
      "\n",
      "27.3.6.3\n",
      "\n",
      "VB version\n",
      "\n",
      "We now modify the algorithm to use VB instead of EM, so that we infer the parameters as well as the latent variables. There are two advantages to this. First, by setting γ (cid:22) 1, VB will encourage B to be sparse (as in Section 21.6.1.6). Second, we will be able to generalize this to the online learning setting, as we discuss below. Our new posterior approximation becomes\n",
      "\n",
      "q(πi, ci, B) = Dir(πi| ˜πi)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Mu(civ.|niv, ˜civ.)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Dir(b.k|˜b.k)\n",
      "\n",
      "(27.47)\n",
      "\n",
      "v\n",
      "\n",
      "k\n",
      "\n",
      "The update for ˜civk changes, to the following:\n",
      "\n",
      "˜civk ∝ exp (E [log bvk] +E [log πik])\n",
      "\n",
      "(27.48)\n",
      "\n",
      "27.3. Latent Dirichlet allocation (LDA)\n",
      "\n",
      "959\n",
      "\n",
      "Algorithm 27.1: Batch VB for LDA 1 Input: niv, K, αk, γv; 2 Estimate ˜bvk using EM for multinomial mixtures; 3 Initialize counts niv; 4 while not converged do 5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "// E step ; svk = 0 // expected sufficient statistics; for each document i = 1 :N do ( ˜πi, ˜ci) = Estep(ni, ˜B, α); svk+ = niv˜civk;\n",
      "\n",
      "10\n",
      "\n",
      "11\n",
      "\n",
      "12\n",
      "\n",
      "// M step ; for each topic k = 1 :K do\n",
      "\n",
      "˜bvk = γv + svk;\n",
      "\n",
      "13 function ( ˜πi, ˜ci) = Estep(ni, ˜B, α); 14 Initialize ˜πik = αk; 15 repeat ˜πold i. = ˜πi., ˜πik = αk; 16 for each word v = 1 :V do\n",
      "\n",
      "18\n",
      "\n",
      "19\n",
      "\n",
      "17\n",
      "\n",
      "for each topic k = 1 :K do\n",
      "\n",
      "˜civk = exp\n",
      "\n",
      "!\n",
      "\n",
      "ψk(˜bv.) +ψ k( ˜πold i. )\n",
      "\n",
      "#\n",
      "\n",
      ";\n",
      "\n",
      "20\n",
      "\n",
      "21 22 until 1 K\n",
      "\n",
      "˜civ. = normalize(˜civ.); ˜πik+ = niv˜civk (cid:7) k |˜πik − ˜πold\n",
      "\n",
      "ik | < thresh;\n",
      "\n",
      "Also, the M step becomes\n",
      "\n",
      "˜bvk = γv +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "˜civk\n",
      "\n",
      "(27.49)\n",
      "\n",
      "i\n",
      "\n",
      "No normalization is required, since we are just updating the pseudcounts. The overall algorithm is summarized in Algorithm 22.\n",
      "\n",
      "27.3.7\n",
      "\n",
      "Fitting using online variational inference\n",
      "\n",
      "In the bathc version, the E step clearly takes O(N KV T ) time, where T is the number of mean ﬁeld updates (typically T ∼ 5). This can be slow if we have many documents. This can be reduced by using stochastic gradient descent (Section 8.5.2) to perform online variational inference, as we now explain.\n",
      "\n",
      "We can derive an online version, following (Hoffman et al. 2010). We perform an E step in the usual way. We then compute the variational parameters for B treating the expected sufficient statistics from the single data case as if the whole data set had those statistics. Finally, we make\n",
      "\n",
      "960\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "Algorithm 27.2: Online variational Bayes for LDA 1 Input: niv, K, αk, γv, τ0, κ; 2 Initialize ˜bvk randomly; 3 for t = 1 :∞ do 4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "Set step size ρt = (τ0 + t)−κ; Pick document i = i(t); ; ( ˜πi, ˜ci) = Estep(ni, ˜B, α); ˜bnew vk = γv + N niv˜civk; ˜bvk = (1 − ρt)˜bvk + ρt˜bnew vk ;\n",
      "\n",
      "900\n",
      "\n",
      "Online 98K\n",
      "\n",
      "850\n",
      "\n",
      "y t i x e p r e P\n",
      "\n",
      "l\n",
      "\n",
      "800\n",
      "\n",
      "750\n",
      "\n",
      "700\n",
      "\n",
      "650\n",
      "\n",
      "Online 3.3M\n",
      "\n",
      "Batch 98K\n",
      "\n",
      "600\n",
      "\n",
      "103.5\n",
      "\n",
      "104 Documents seen (log scale)\n",
      "\n",
      "104.5\n",
      "\n",
      "105\n",
      "\n",
      "105.5\n",
      "\n",
      "106\n",
      "\n",
      "106.5\n",
      "\n",
      "Figure 27.9 Test perplexity vs number of training documents for batch and online VB-LDA. From Figure 1 of (Hoffman et al. 2010). Used with kind permission of David Blei.\n",
      "\n",
      "a partial update for the variational parameters for B, putting weight ρt on the new estimate and weight 1 − ρt on the old estimate. The step size ρt decays over time, as in Equation 8.83. In practice, we should use mini-batches, The overall algorithm is summarized in Algorithm 3. as explained in Section 8.5.2.3. In (Hoffman et al. 2010), they used a batch of size 256–4096.\n",
      "\n",
      "Figure 27.9 plots the perplexity on a test set of size 1000 vs number of analyzed documents (E steps), where the data is drawn from (English) Wikipedia. The ﬁgure shows that online variational inference is much faster than offline inference, yet produces similar results.\n",
      "\n",
      "27.3.8\n",
      "\n",
      "Determining the number of topics\n",
      "\n",
      "Choosing K, the number of topics, is a standard model selection problem. Here are some approaches that have been taken:\n",
      "\n",
      "Use annealed importance sampling (Section 24.6.2) to approximate the evidence (Wallach\n",
      "\n",
      "et al. 2009).\n",
      "\n",
      "Cross validation, using the log likelihood on a test set.\n",
      "\n",
      "27.4. Extensions of LDA\n",
      "\n",
      "961\n",
      "\n",
      "Use the variational lower bound as a proxy for log p(D|K). • Use non-parametric Bayesian methods (Teh et al. 2006).\n",
      "\n",
      "27.4\n",
      "\n",
      "Extensions of LDA\n",
      "\n",
      "Many extensions of LDA have been proposed since the ﬁrst paper came out in 2003. We brieﬂy discuss a few of these below.\n",
      "\n",
      "27.4.1\n",
      "\n",
      "Correlated topic model\n",
      "\n",
      "One weakness of LDA is that it cannot capture correlation between topics. For example, if a document has the “business” topic, it is reasonable to expect the “ﬁnance” topic to co-occcur. The source of the problem is the use of a Dirichlet prior for πi. The problem with the Dirichelt it that it is characterized by just a mean vector and a strength parameter, but its covariance is ﬁxed (Σij = −αiαj), rather than being a free parameter.\n",
      "\n",
      "One way around this is to replace the Dirichlet prior with the logistic normal distribution, as\n",
      "\n",
      "in categorical PCA (Section 27.2.2). The model becomes\n",
      "\n",
      "bk|γ ∼ Dir(γ1V ) zi ∼ N (μ, Σ)\n",
      "\n",
      "(27.50)\n",
      "\n",
      "(27.51)\n",
      "\n",
      "πi|zi = S(zi) qil|πi ∼ Cat(πi) yil|qil = k, B ∼ Cat(bk)\n",
      "\n",
      "(27.52)\n",
      "\n",
      "(27.53)\n",
      "\n",
      "(27.54)\n",
      "\n",
      "This is known as the correlated topic model (Blei and Lafferty 2007). This is very similar to categorical PCA, but slightly different. To see the difference, let us marginalize out the qil and πi. Then in the CTM we have\n",
      "\n",
      "yil ∼ Cat(BS(zi))\n",
      "\n",
      "(27.55)\n",
      "\n",
      "where B is a stochastic matrix. By contrast, in catPCA we have\n",
      "\n",
      "yil ∼ Cat(S(Wzi))\n",
      "\n",
      "(27.56)\n",
      "\n",
      "where W is an unconstrained matrix.\n",
      "\n",
      "Fitting this model is tricky, since the prior for πi is no longer conjugate to the multinomial likelihood for qil. However, we can use any of the variational methods in Section 21.8.1.1, where In the CTM case, things are even harder we discussed Bayesian multiclass logistic regression. since the categorical response variables qi are hidden, but we can handle this by using an additional mean ﬁeld approximation. See (Blei and Lafferty 2007) for details.\n",
      "\n",
      "by pruning low-strength edges, to get a sparse Gaussian graphical model. This allows you to visualize the correlation between topics. Figure 27.10 shows the result of applying this procedure to articles from Science magazine, from 1990-1999. (This corpus contains 16,351 documents, and 5.7M words (19,088 of them unique), after stop-word and low-frequency removal.) Nodes represent topics, with the top 5 words per topic listed inside. The font size reﬂects the overall prevalence of the topic in the corpus. Edges represent signiﬁcant elements of the precision matrix.\n",
      "\n",
      "Having ﬁt the model, one can then convert ˆΣ to a sparse precision matrix ˆΣ\n",
      "\n",
      "−1\n",
      "\n",
      "962\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "activated tyrosine phosphorylation activation phosphorylation kinase\n",
      "\n",
      "p53 cell cycle activity cyclin regulation\n",
      "\n",
      "research funding support nih program\n",
      "\n",
      "science scientists says research people\n",
      "\n",
      "receptor receptors ligand ligands apoptosis\n",
      "\n",
      "amino acids cdna sequence isolated protein\n",
      "\n",
      "united states women universities students education\n",
      "\n",
      "cells cell expression cell lines bone marrow\n",
      "\n",
      "wild type mutant mutations mutants mutation\n",
      "\n",
      "mice antigen t cells antigens immune response\n",
      "\n",
      "virus hiv aids infection viruses\n",
      "\n",
      "bacteria bacterial host resistance parasite\n",
      "\n",
      "gene disease mutations families mutation\n",
      "\n",
      "patients disease treatment drugs clinical\n",
      "\n",
      "cells proteins researchers protein found\n",
      "\n",
      "development embryos drosophila genes expression\n",
      "\n",
      "enzyme enzymes iron active site reduction\n",
      "\n",
      "proteins protein binding domain domains\n",
      "\n",
      "plants plant gene genes arabidopsis\n",
      "\n",
      "genetic population populations differences variation\n",
      "\n",
      "rna dna rna polymerase cleavage site\n",
      "\n",
      "sequence sequences genome dna sequencing\n",
      "\n",
      "fossil record birds fossils dinosaurs fossil\n",
      "\n",
      "ancient found impact million years ago africa\n",
      "\n",
      "magnetic magnetic ﬁeld spin superconductivity superconducting\n",
      "\n",
      "surface liquid surfaces ﬂuid model\n",
      "\n",
      "computer problem information computers problems\n",
      "\n",
      "brain memory subjects left task\n",
      "\n",
      "species forest forests populations ecosystems\n",
      "\n",
      "volcanic deposits magma eruption volcanism\n",
      "\n",
      "laser optical light electrons quantum\n",
      "\n",
      "pressure high pressure pressures core inner core\n",
      "\n",
      "surface tip image sample device\n",
      "\n",
      "reaction reactions molecule molecules transition state\n",
      "\n",
      "neurons stimulus motor visual cortical\n",
      "\n",
      "materials organic polymer polymers molecules\n",
      "\n",
      "earthquake earthquakes fault images data\n",
      "\n",
      "climate ocean ice changes climate change\n",
      "\n",
      "mantle crust upper mantle meteorites ratios\n",
      "\n",
      "physicists particles physics particle experiment\n",
      "\n",
      "synapses ltp glutamate synaptic neurons\n",
      "\n",
      "co2 carbon carbon dioxide methane water\n",
      "\n",
      "stars astronomers universe galaxies galaxy\n",
      "\n",
      "sun solar wind earth planets planet\n",
      "\n",
      "ozone atmospheric measurements stratosphere concentrations\n",
      "\n",
      "Figure 27.10 Output of the correlated topic model (with K = 50 topics) when applied to articles from Science. Nodes represent topics, with the 5 most probable phrases from each topic shown inside. Font size reﬂects overall prevalence of the topic. See http://www.cs.cmu.edu/~lemur/science/ for an interactive version of this model with 100 topics. Source: Figure 2 of (Blei and Lafferty 2007). Used with kind permission of David Blei.\n",
      "\n",
      "27.4.2\n",
      "\n",
      "Dynamic topic model\n",
      "\n",
      "In LDA, the topics (distributions over words) are assumed to be static. In some cases, it makes sense to allow these distributions to evolve smoothly over time. For example, an article might use the topic “neuroscience”, but if it was written in the 1900s, it is more likely to use words like “nerve”, whereas if it was written in the 2000s, it is more likely to use words like “calcium receptor” (this reﬂects the general trend of neuroscience towards molecular biology).\n",
      "\n",
      "One way to model this is use a dynamic logistic normal model, as illustrated in Figure 27.11. In particular, we assume the topic distributions evolve according to a Gaussian random walk, and then we map these Gaussian vectors to probabilities via the softmax function:\n",
      "\n",
      "il|qt yt\n",
      "\n",
      "bt,k|bt−1,k ∼ N (bt−1,k, σ21V ) i ∼ Dir(α1K) i ∼ Cat(πt i) il = k, Bt ∼ Cat(S(bt\n",
      "\n",
      "πt il|πt qt\n",
      "\n",
      "k))\n",
      "\n",
      "(27.57)\n",
      "\n",
      "(27.58)\n",
      "\n",
      "(27.59)\n",
      "\n",
      "(27.60)\n",
      "\n",
      "This is known as a dynamic topic model (Blei and Lafferty 2006b).\n",
      "\n",
      "27.4. Extensions of LDA\n",
      "\n",
      "963\n",
      "\n",
      "α\n",
      "\n",
      "πt−1 i\n",
      "\n",
      "πt i\n",
      "\n",
      "πt+1 i\n",
      "\n",
      "qt−1 il\n",
      "\n",
      "qt il\n",
      "\n",
      "qt+1 il\n",
      "\n",
      "yt−1 il\n",
      "\n",
      "yt il\n",
      "\n",
      "yt+1 il\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "bt−1 k\n",
      "\n",
      "bt k\n",
      "\n",
      "bt+1 k\n",
      "\n",
      "K\n",
      "\n",
      "Figure 27.11 The dynamic topic model.\n",
      "\n",
      "One can perform approximate infernece in this model using a structured mean ﬁeld method (Section 21.4), that exploits the Kalman smoothing algorithm (Section 18.3.1) to perform exact inference on the linear-Gaussian chain between the bt,k nodes (see (Blei and Lafferty 2006b) for details).\n",
      "\n",
      "Figure 27.12 illustrates a typical output of the system when applied to 100 years of articles from Science. On the top, we visualize the top 10 words from a speciﬁc topic (which seems to be related to neuroscience) after 10 year intervals. On the bottom left, we plot the probability of some speciﬁc words belonging to this topic. On the bottom right, we list the titles of some articles that contained this topic.\n",
      "\n",
      "One interesting application of this model is to perform temporally-corrected document re- trieval. That is, suppose we look for documents about the inheritance of disease. Modern articles will use words like “DNA”, but older articles (before the discovery of DNA) may use other terms such as “heritable unit”. But both articles are likely to use the same topics. Similar ideas can be used to perform cross-language information retrieval, see e.g., (Cimiano et al. 2009).\n",
      "\n",
      "27.4.3\n",
      "\n",
      "LDA-HMM\n",
      "\n",
      "The LDA model assumes words are exchangeable, which is clearly not true. A simple way to model sequential dependence between words is to use a hidden Markov model or HMM. The trouble with HMMs is that they can only model short-range dependencies, so they cannot capture the overall gist of a document. Hence they can generate syntactically correct sentences (see e.g., Table 17.1). but not semantically plausible ones.\n",
      "\n",
      "It is possible to combine LDA with HMM to create a model called LDA-HMM (Griffiths et al.\n",
      "\n",
      "964\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "1881 brain movement action right eye hand left muscle nerve sound\n",
      "\n",
      "1890 movement eye right hand brain left action muscle sound experiment\n",
      "\n",
      "1900 brain eye movement right left hand nerve vision sound muscle\n",
      "\n",
      "1910 movement brain sound nerve active muscle left eye right nervous\n",
      "\n",
      "1920 movement sound muscle active nerve stimulate ﬁber reaction brain response\n",
      "\n",
      "1930 stimulate muscle sound movement response nerve frequency ﬁber active brain\n",
      "\n",
      "1940 record nerve stimulate response muscle electrode active brain ﬁber potential\n",
      "\n",
      "1950 respons record stimulate nerve muscle active frequency electrode potential study\n",
      "\n",
      "1960 response stimulate record condition active potential stimulus nerve subject eye\n",
      "\n",
      "1970 respons cell potential stimul neuron active nerve eye record abstract\n",
      "\n",
      "1980 cell neuron response active brain stimul muscle system nerve receptor\n",
      "\n",
      "1990 cell channel neuron ca2 active brain receptor muscle respons current\n",
      "\n",
      "2000 neuron active brain cell ﬁg response channel receptor synapse signal\n",
      "\n",
      "\"Neuroscience\"\n",
      "\n",
      "nerve\n",
      "\n",
      "neuron\n",
      "\n",
      "ca2\n",
      "\n",
      "1887 Mental Science 1900 Hemianopsia in Migraine 1912 A Defence of the ``New Phrenology'' 1921 The Synchronal Flashing of Fireﬂies 1932 Myoesthesis and Imageless Thought 1943 Acetylcholine and the Physiology of the Nervous System 1952 Brain Waves and Unit Discharge in Cerebral Cortex 1963 Errorless Discrimination Learning in the Pigeon 1974 Temporal Summation of Light by a Vertebrate Visual Receptor 1983 Hysteresis in the Force-Calcium Relation in Muscle 1993 GABA-Activated Chloride Channels in Secretory Nerve Endings\n",
      "\n",
      "1880\n",
      "\n",
      "1900\n",
      "\n",
      "1920\n",
      "\n",
      "1940\n",
      "\n",
      "1960\n",
      "\n",
      "1980\n",
      "\n",
      "2000\n",
      "\n",
      "Figure 27.12 Part of the output of the dynamic topic model when applied to articles from Science. We show the top 10 words for the neuroscience topic over time. We also show the probability of three words within this topic over time, and some articles that contained this topic. Source: Figure 4 of (Blei and Lafferty 2006b). Used with kind permission of David Blei.\n",
      "\n",
      "2004). This model uses the HMM states to model function or syntactic words, such as “and” or “however”, and uses the LDA to model content or semantic words, which are harder to predict. There is a distinguished HMM state which speciﬁes when the LDA model should be used to generate the word; the rest of the time, the HMM generates the word.\n",
      "\n",
      "More formally, for each document i, the model deﬁnes an HMM with states zil ∈ {0, . . . , C}. In addition, each document has an LDA model associated with it. If zil = 0, we generate word yil from the semantic LDA model, with topic speciﬁed by qil; otherwise we generate word yil from the syntactic HMM model. The DGM is shown in Figure 27.13. The CPDs are as follows:\n",
      "\n",
      "p(πi) = Dir(πi|α1K)\n",
      "\n",
      "(27.61)\n",
      "\n",
      "p(yil = v|qil = k, zil = c) =\n",
      "\n",
      "p(qil = k|πi) =π ik p(zil = c(cid:2)|zi,l−1 = c) =A HMM (c, c(cid:2))\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "BLDA(k, v) BHMM (c, v)\n",
      "\n",
      "if c = 0 if c > 0\n",
      "\n",
      "(27.62)\n",
      "\n",
      "(27.63)\n",
      "\n",
      "(27.64)\n",
      "\n",
      "where BLDA is the usual topic-word matrix, BHMM is the state-word HMM emission matrix and AHMM is the state-state HMM transition matrix.\n",
      "\n",
      "Inference in this model can be done with collapsed Gibbs sampling, analytically integrating\n",
      "\n",
      "out all the continuous quantities. See (Griffiths et al. 2004) for the details.\n",
      "\n",
      "The results of applying this model (with K = 200 LDA topics and C = 20 HMM states) to the combined Brown and TASA corpora4 are shown in Table 27.1. We see that the HMM generally is\n",
      "\n",
      "4. The Brown corpus consists of 500 documents and 1,137,466 word tokens, with part-of-speech tags for each token.\n",
      "\n",
      "27.4. Extensions of LDA\n",
      "\n",
      "965\n",
      "\n",
      "α\n",
      "\n",
      "πi\n",
      "\n",
      "qi,l−1\n",
      "\n",
      "qi,l\n",
      "\n",
      "qi,l+1 . . .\n",
      "\n",
      "yi,l−1\n",
      "\n",
      "yi,l\n",
      "\n",
      "yi,l+1\n",
      "\n",
      ". . .\n",
      "\n",
      "BLDA\n",
      "\n",
      "zi,l−1\n",
      "\n",
      "zi,l\n",
      "\n",
      "zi,l+1 . . .\n",
      "\n",
      "N\n",
      "\n",
      "AHMM\n",
      "\n",
      "BHMM\n",
      "\n",
      "Figure 27.13 LDA-HMM model.\n",
      "\n",
      "1.\n",
      "\n",
      "In contrast to this approach, we study here how the overall network activity can control single cell parameters such as input resistance, as well as time and space constants, parameters that are crucial for excitability and spariotemporal (sic) integration.\n",
      "\n",
      "The integrated architecture in this paper combines feed forward control and error feedback adaptive\n",
      "\n",
      "control using neural networks.\n",
      "\n",
      "2.\n",
      "\n",
      "In other words, for our proof of convergence, we require the softassign algorithm to return a doubly stochastic matrix as *sinkhorn theorem guarantees that it will instead of a matrix which is merely close to being doubly stochastic based on some reasonable metric.\n",
      "\n",
      "The aim is to construct a portfolio with a maximal expected return for a given risk level and time horizon while simultaneously obeying *institutional or *legally required constraints.\n",
      "\n",
      "The left graph is the standard experiment the right from a training with # samples.\n",
      "\n",
      "3.\n",
      "\n",
      "The graph G is called the *guest graph, and H is called the host graph.\n",
      "\n",
      "Figure 27.14 Function and content words in the NIPS corpus, as distinguished by the LDA-HMM model. Graylevel indicates posterior probability of assignment to LDA component, with black being highest. The boxed word appears as a function word in one sentence, and as a content word in another sentence. Asterisked words had low frequency, and were treated as a single word type by the model. Source: Figure 4 of (Griffiths et al. 2004). Used with kind permission of Tom Griffiths.\n",
      "\n",
      "966\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "the blood , of body heart and in to is blood heart pressure body lungs oxygen vessels arteries * breathing the a his this their these your her my some\n",
      "\n",
      "the , and of a in trees tree with on forest trees forests land soil areas park wildlife area rain in for to on with at by from as into\n",
      "\n",
      "the , and of in land to farmers for farm farmers land crops farm food people farming wheat farms corn he it you they i she we there this who\n",
      "\n",
      "the of , to in and classes government a state government state federal public local act states national laws department * new other ﬁrst same great good small little old\n",
      "\n",
      "the a of , in to picture ﬁlm image lens light eye lens image mirror eyes glass object objects lenses be have see make do know get go take ﬁnd\n",
      "\n",
      "a the of , in water is and matter are water matter molecules liquid particles gas solid substance temperature changes said made used came went found called\n",
      "\n",
      "the , of a and in story is to as story stories poem characters poetry character author poems life poet can would will could may had must do have did\n",
      "\n",
      "the , a of and drink alcohol to bottle in drugs drug alcohol people drinking person effects marijuana body use time way years day part number kind place\n",
      "\n",
      "the , a in game ball and team to play ball game team * baseball players football player ﬁeld basketball , ; ( : )\n",
      "\n",
      "Table 27.1 Upper row: Topics extracted by the LDA model when trained on the combined Brown and TASA corpora. Middle row: topics extracted by LDA part of LDA-HMM model. Bottom row: topics extracted by HMM part of LDA-HMM model. Each column represents a single topic/class, and words appear in order of probability in that topic/class. Since some classes give almost all probability to only a few words, a list is terminated when the words account for 90% of the probability mass. Source: Figure 2 of (Griffiths et al. 2004). Used with kind permission of Tom Griffiths.\n",
      "\n",
      "responsible for syntactic words, and the LDA for semantics words. If we did not have the HMM, the LDA topics would get “polluted” by function words (see top of ﬁgure), which is why such words are normally removed during preprocessing.\n",
      "\n",
      "The model can also help disambiguate when the same word is being used syntactically or semantically. Figure 27.14 shows some examples when the model was applied to the NIPS corpus.5 We see that the roles of words are distinguished, e.g., “we require the algorithm to return a matrix” (verb) vs “the maximal expected return” (noun). In principle, a part of speech tagger could disambiguate these two uses, but note that (1) the LDA-HMM method is fully unsupervised (no POS tags were used), and (2) sometimes a word can have the same POS tag, but different senses, e.g., “the left graph” (a synactic role) vs “the graph G” (a semantic role).\n",
      "\n",
      "The topic of probabilistic models for syntax and semantics is a vast one, which we do not\n",
      "\n",
      "The TASA corpus is an untagged collection of educational materials consisting of 37,651 documents and 12,190,931 word tokens. Words appearing in fewer than 5 documents were replaced with an asterisk, but punctuation was included. The combined vocabulary was of size 37,202 unique words. 5. NIPS stands for “Neural Information Processing Systems”. NIPS corpus volumes 1–12 contains 1713 documents.\n",
      "\n",
      "It is one of the top machine learning conferences. The\n",
      "\n",
      "27.4. Extensions of LDA\n",
      "\n",
      "967\n",
      "\n",
      "α\n",
      "\n",
      "α\n",
      "\n",
      "πi\n",
      "\n",
      "πi\n",
      "\n",
      "qil\n",
      "\n",
      "¯qi\n",
      "\n",
      "ci\n",
      "\n",
      "qil\n",
      "\n",
      "A\n",
      "\n",
      "yil\n",
      "\n",
      "ci\n",
      "\n",
      "yil\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "B\n",
      "\n",
      "w\n",
      "\n",
      "B\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.15\n",
      "\n",
      "(a) Supervised LDA. (b) Discriminative LDA.\n",
      "\n",
      "have space to delve into any more. See e.g., (Jurafsky and Martin 2008) for further information.\n",
      "\n",
      "27.4.4\n",
      "\n",
      "Supervised LDA\n",
      "\n",
      "In this section, we discuss extensions of LDA to handle side information of various kinds beyond just words.\n",
      "\n",
      "27.4.4.1\n",
      "\n",
      "Generative supervised LDA\n",
      "\n",
      "Suppose we have a variable length sequence of words yil ∈ {1, . . . , V } as usual, but we also have a class label ci ∈ {1, . . . , C}. How can we predict ci from yi? There are many possible approaches, but most are direct mappings from the words to the class. In some cases, such as sentiment analysis, we can get better performance by ﬁrst performing inference, to try to disambiguate the meaning of words. For example, suppose the goal is to determine if a If we encounter the phrase “Brad Pitt was document is a favorable review of a movie or not. excellent until the middle of the movie”, the word “excellent” may lead us to think the review is positive, but clearly the overall sentiment is negative.\n",
      "\n",
      "(Blei and McAuliffe 2010) proposes an approach, called supervised LDA, where the class label ci is generated from the topics as follows:\n",
      "\n",
      "One way to tackle such problems is to build a joint model of the form p(ci, yi|θ).\n",
      "\n",
      "p(ci|qi) = Ber(sigm(wT qi))\n",
      "\n",
      "(27.65)\n",
      "\n",
      "Here qi is the empirical topic distribution for document i:\n",
      "\n",
      "qik (cid:2) 1 Li\n",
      "\n",
      "Li(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "qilk\n",
      "\n",
      "(27.66)\n",
      "\n",
      "See Figure 27.15(a) for an illustration.\n",
      "\n",
      "968\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "xi\n",
      "\n",
      "μ\n",
      "\n",
      "Σ\n",
      "\n",
      "xi\n",
      "\n",
      "πi\n",
      "\n",
      "W\n",
      "\n",
      "xi\n",
      "\n",
      "W i\n",
      "\n",
      "αi\n",
      "\n",
      "W\n",
      "\n",
      "πi\n",
      "\n",
      "πi\n",
      "\n",
      "qil\n",
      "\n",
      "qil\n",
      "\n",
      "qil\n",
      "\n",
      "yil\n",
      "\n",
      "yil\n",
      "\n",
      "yil\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "N\n",
      "\n",
      "B\n",
      "\n",
      "B\n",
      "\n",
      "B\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 27.16 Discriminative variants of LDA. (a) Mixture of experts aka MR-LDA. The double ring denotes a node that πi a deterministic function of its parents. (c) DMR-LDA.\n",
      "\n",
      "(b) Mixture of experts with random effects.\n",
      "\n",
      "We can ﬁt this model using Monte Carlo EM: run the collapsed Gibbs sampler in the E step, to compute E [qik], and then use this as the input feature to a standard logistic regression package.\n",
      "\n",
      "27.4.4.2\n",
      "\n",
      "Discriminative supervised LDA\n",
      "\n",
      "An alternative approach, known as discriminative LDA (Lacoste-Julien et al. 2009), is shown in Figure 27.15(b). This is a discriminative model of the form p(yi|ci, θ). The only change from regular LDA is that the topic prior becomes input dependent, as follows:\n",
      "\n",
      "p(qil|πi, ci = c, θ) = Cat(Acπ)\n",
      "\n",
      "(27.67)\n",
      "\n",
      "where Ac is a K × K stochastic matrix.\n",
      "\n",
      "So far, we have assumed the “side information” is a single categorical variable ci. Often we have high dimensional covariates xi ∈ RD. For example, consider the task of image tagging. The idea is that yil represent correlated tags or labels, which we want to predict given xi. We now discuss several attempts to extend LDA so that it can generate tags given the inputs.\n",
      "\n",
      "The simplest approach is to use a mixture of experts (Section 11.2.4) with multiple outputs. This is just like LDA except we replace the Dirichlet prior on πi with a deterministic function of the input:\n",
      "\n",
      "πi = S(Wxi)\n",
      "\n",
      "(27.68)\n",
      "\n",
      "In (Law et al. 2010), this is called multinomial regression LDA. See Figure 27.16(a). Eliminating the deterministic πi we have\n",
      "\n",
      "p(qil|xi, W) = Cat(S(Wxi))\n",
      "\n",
      "(27.69)\n",
      "\n",
      "We can ﬁt this with EM in the usual way. However, (Law et al. 2010) suggest an alternative. First ﬁt an unsupervised LDA model based only on yi; then treat the inferred πi as data, and\n",
      "\n",
      "27.4. Extensions of LDA\n",
      "\n",
      "969\n",
      "\n",
      "ﬁt a multinomial logistic regression model mapping xi to πi. Although this is fast, ﬁtting LDA in an unsupervised fashion does not necessarily result in a discriminative set of latent variables, as discussed in (Blei and McAuliffe 2010).\n",
      "\n",
      "There is a more subtle problem with this model. Since πi is a deterministic function of the inputs, it is effectively observed, rendering the qil (and hence the tags yil) independent. In other words,\n",
      "\n",
      "p(yi|xi, θ) =\n",
      "\n",
      "Li(cid:20)\n",
      "\n",
      "p(yil|xi, θ) =\n",
      "\n",
      "Li(cid:20)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "p(yil|qil = k, B)p(qil = k|xi, W)\n",
      "\n",
      "(27.70)\n",
      "\n",
      "l=1\n",
      "\n",
      "l=1\n",
      "\n",
      "k\n",
      "\n",
      "This means that if we observe the value of one tag, it will have no inﬂuence on any of the others. This may explain why the results in (Law et al. 2010) only show negligible improvement over predicting each tag independently.\n",
      "\n",
      "One way to induce correlations is to make W a random variable. The resulting model is shown in Figure 27.16(b). We call this a random effects mixture of experts. We typically assume a Gaussian prior on Wi. If xi = 1, then p(qil|xi, wi) = Cat(S(wi)), so we recover the correlated topic model. It is possible to extend this model by adding Markovian dynamics to the qil variables. This is called a conditional topic random ﬁeld (Zhu and Xing 2010).\n",
      "\n",
      "A closely related approach, known as Dirichlet multinomial regression LDA (Mimno and McCallum 2008), is shown in Figure 27.16(c). This is identical to standard LDA except we make α a function of the input\n",
      "\n",
      "αi = exp(Wxi)\n",
      "\n",
      "(27.71)\n",
      "\n",
      "where W is a K × D matrix. Eliminating the deterministic αi we have\n",
      "\n",
      "πi ∼ Dir(exp(Wxi))\n",
      "\n",
      "(27.72)\n",
      "\n",
      "Unlike (Law et al. 2010), this model allows information to ﬂow between tags via the latent πi.\n",
      "\n",
      "A variant of this model, where xi corresponds to a bag of discrete labels and πi ∼ Dir(α $ In this case, the labels xi are in 1:1 xi), correspondence with the latent topics, which makes the resulting topics much more interpretable. An extension, known as partially labeled LDA (Ramage et al. 2011), allows each label to have multiple latent sub-topics; this model includes LDA, labeled LDA and a multinomial mixture model as special cases.\n",
      "\n",
      "is known as labeled LDA (Ramage et al. 2009).\n",
      "\n",
      "27.4.4.3\n",
      "\n",
      "Discriminative categorical PCA\n",
      "\n",
      "An alternative to using LDA is to expand the categorical PCA model with inputs, as shown in Figure 27.17(a). Since the latent space is now real-valued, we can use simple linear regression for the input-hidden mapping. For the hidden-output mapping, we use traditional catPCA:\n",
      "\n",
      "p(zi|xi, V) =N (Vxi, Σ) p(yi|zi, W) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Cat(yil|S(Wzi))\n",
      "\n",
      "(27.73)\n",
      "\n",
      "(27.74)\n",
      "\n",
      "l\n",
      "\n",
      "This model is essentially a probabilistic neural network with one hidden layer, as shown in Figure 27.17(b), but with exchangeable output (e.g., to handle variable numbers of tags). The\n",
      "\n",
      "970\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "xi\n",
      "\n",
      "xi1\n",
      "\n",
      ". . .\n",
      "\n",
      ". . .\n",
      "\n",
      "xiD\n",
      "\n",
      "zi\n",
      "\n",
      "V\n",
      "\n",
      "v1\n",
      "\n",
      "zi1\n",
      "\n",
      ". . .\n",
      "\n",
      "ziK\n",
      "\n",
      "vK\n",
      "\n",
      "yil\n",
      "\n",
      "W\n",
      "\n",
      "yi1\n",
      "\n",
      ". . .\n",
      "\n",
      ". . .\n",
      "\n",
      "yiLi N\n",
      "\n",
      "N\n",
      "\n",
      "W\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.17 vector nodes expanded out.\n",
      "\n",
      "(a) Categorical PCA with inputs and exchangeable outputs.\n",
      "\n",
      "(b) Same as (a), but with the\n",
      "\n",
      "key difference from a neural net is that information can ﬂow between the yil’s via the latent bottleneck layer zi. This should work better than a conventional neural net when the output labels are highly correlated, even after conditioning on the features; this problem frequently arises in multi label classiﬁcation. Note that we could allow a direct xi to yi arc, but this would require too many parameters if the number of labels is large.6\n",
      "\n",
      "We can ﬁt this model with a small modiﬁcation of the variational EM algorithm in Section 12.4. If we use this model for regression, rather than classiﬁcation, we can perform the E step exactly, by modifying the EM algorithm for factor analysis. (Ma et al. 1997) reports that this method converges faster than standard backpropagation.\n",
      "\n",
      "We can also extend the model so that the prior on zi is a mixture of Gaussians using input- If the output is Gaussian, this corresponds to a mixture of discriminative dependent means. factor analysers (Fokoue 2005; Zhou and Liu 2008). If the output is categorical, this would be an (as yet unpublished) model, which we could call “discriminative mixtures of categorical factor analyzers”.\n",
      "\n",
      "27.5\n",
      "\n",
      "LVMs for graph-structured data\n",
      "\n",
      "Another source of discrete data is when modeling graph or network structures. To see the connection, recall that any graph on D nodes can be represented as a D × D adjacency matrix G, where G(i, j) = 1 iff there is an edge from node i to node j. Such matrices are binary, and often very sparse. See Figure 27.19 for an example.\n",
      "\n",
      "Graphs arise in many application areas, such as modeling social networks, protein-protein interaction networks, or patterns of disease transmission between people or animals. There are usually two primary goals when analysing such data: ﬁrst, try to discover some “interesting\n",
      "\n",
      "6. A non-probabilistic version of this idea, using squared loss, was proposed in (Ji et al. 2010). This is similar to a linear feed-forward neural network with an additional edge from xi directly to yi.\n",
      "\n",
      "27.5. LVMs for graph-structured data\n",
      "\n",
      "971\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "1\n",
      "\n",
      "6\n",
      "\n",
      "4\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "9\n",
      "\n",
      "7\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.18 (a) A directed graph. (b) The same graph, with the nodes partitioned into 3 groups, making the block structure more apparent.\n",
      "\n",
      "1 2 3 4 5 6 7 8 9\n",
      "\n",
      "1 2 3 4 5 6 7 8 9\n",
      "\n",
      "1 6 4\n",
      "\n",
      "8 2 3 5\n",
      "\n",
      "9 7\n",
      "\n",
      "z\n",
      "\n",
      "1 6 4 8 2 3 5 9 7\n",
      "\n",
      "0.1 0.1 0.9\n",
      "\n",
      "0.9 0.1 0.1\n",
      "\n",
      "0.1 0.9 0.1\n",
      "\n",
      "1 6 4 8 2 3 5 9 7\n",
      "\n",
      "η\n",
      "\n",
      "R\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.19 (a) Adjacency matrix for the graph in Figure 27.18(a). (b) Rows and columns are shown permuted to show the block structure. We also sketch of how the stochastic block model can generate this graph. From Figure 1 of (Kemp et al. 2006). Used with kind permission of Charles Kemp.\n",
      "\n",
      "structure” in the graph, such as clusters or communities; second, try to predict which links might occur in the future (e.g., who will make friends with whom). Below we summarize some models that have been proposed for these tasks, some of which are related to LDA. Futher details on these and other approaches can be found in e.g., (Goldenberg et al. 2009) and the references therein.\n",
      "\n",
      "27.5.1\n",
      "\n",
      "Stochastic block model\n",
      "\n",
      "In Figure 27.18(a) we show a directed graph on 9 nodes. There is no apparent structure. However, if we look more deeply, we see it is possible to partition the nodes into three groups or blocks, B1 = {1, 4, 6}, B2 = {2, 3, 5, 8}, and B3 = {7, 9}, such that most of the connections go from nodes in B1 to B2, or from B2 to B3, or from B3 to B1. This is illustrated in Figure 27.18(b).\n",
      "\n",
      "972\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "m e\n",
      "\n",
      "t s y s\n",
      "\n",
      "l\n",
      "\n",
      "a n o\n",
      "\n",
      "i t\n",
      "\n",
      "a e R\n",
      "\n",
      "l\n",
      "\n",
      "A\n",
      "\n",
      "D\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "D\n",
      "\n",
      "D\n",
      "\n",
      "G\n",
      "\n",
      "B\n",
      "\n",
      "E\n",
      "\n",
      "H\n",
      "\n",
      "A\n",
      "\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "A\n",
      "\n",
      "C\n",
      "\n",
      "E\n",
      "\n",
      "B\n",
      "\n",
      "D\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "D\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "D\n",
      "\n",
      "A B\n",
      "\n",
      "C D\n",
      "\n",
      "E\n",
      "\n",
      "F G H\n",
      "\n",
      "A\n",
      "\n",
      "EDCB\n",
      "\n",
      "x i r t a m d e t r o S\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "D\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "D\n",
      "\n",
      "A B C D E F G H\n",
      "\n",
      "A\n",
      "\n",
      "B\n",
      "\n",
      "C\n",
      "\n",
      "D\n",
      "\n",
      "E\n",
      "\n",
      "Figure 27.20 Some examples of graphs generated using the stochastic block model with different kinds of connectivity patterns between the blocks. The abstract graph (between blocks) represent a ring, a dominance hierarchy, a common-cause structure, and a common-effect structure. From Figure 4 of (Kemp et al. 2010). Used with kind permission of Charles Kemp.\n",
      "\n",
      "The problem is easier to understand if we plot the adjacency matrices. Figure 27.19(a) shows the matrix for the graph with the nodes in their original ordering. Figure 27.19(b) shows the matrix for the graph with the nodes in their permtuted ordering. It is clear that there is block structure.\n",
      "\n",
      "We can make a generative model of block structured graphs as follows. First, for every node, sample a latent block qi ∼ Cat(π), where πk is the probability of choosing block k, for k = 1 : K. Second, choose the probability of connecting group a to group b, for all pairs of groups; let us denote this probability by ηa,b. This can come from a beta prior. Finally, generate each edge Rij using the following model:\n",
      "\n",
      "p(Rij = r|qi = a, qj = b, η) = Ber(r|ηa,b)\n",
      "\n",
      "(27.75)\n",
      "\n",
      "This is called the stochastic block model (Nowicki and Snijders 2001). Figure 27.21(a) illustrates the model as a DGM, and Figure 27.19(c) illustrates how this model can be used to cluster the nodes in our example.\n",
      "\n",
      "Note that this is quite different from a conventional clustering problem. For example, we see that all the nodes in block 3 are grouped together, even though there are no connections between them. What they share is the property that they “like to” connect to nodes in block 1, and to receive connections from nodes in block 2. Figure 27.20 illustrates the power of the model for generating many different kinds of graph structure. For example, some social networks have hierarchical structure, which can be modeled by clustering people into different social strata, whereas others consist of a set of cliques.\n",
      "\n",
      "Unlike a standard mixture model, it is not possible to ﬁt this model using exact EM, because all the latent qi variables become correlated. However, one can use variational EM (Airoldi et al.\n",
      "\n",
      "27.5. LVMs for graph-structured data\n",
      "\n",
      "973\n",
      "\n",
      "π\n",
      "\n",
      "qj\n",
      "\n",
      "α\n",
      "\n",
      "πj\n",
      "\n",
      "qi←j\n",
      "\n",
      "qi\n",
      "\n",
      "Ri,j I\n",
      "\n",
      "J\n",
      "\n",
      "πi\n",
      "\n",
      "qi→j\n",
      "\n",
      "Ri,j I\n",
      "\n",
      "J\n",
      "\n",
      "ηa,b\n",
      "\n",
      "ηa,b\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.21\n",
      "\n",
      "(a) Stochastic block model. (b) Mixed membership stochastic block model.\n",
      "\n",
      "2008), collapsed Gibbs sampling (Kemp et al. 2006), etc. We omit the details (which are similar to the LDA case).\n",
      "\n",
      "In (Kemp et al. 2006), they lifted the restriction that the number of blocks K be ﬁxed, by replacing the Dirichlet prior on π by a Dirichlet process (see Section 25.2.2). This is known as the inﬁnite relational model. See Section 27.6.1 for details.\n",
      "\n",
      "If we have features associated with each node, we can make a discriminative version of this\n",
      "\n",
      "model, for example by deﬁning\n",
      "\n",
      "p(Rij = r|qi = a, qj = b, xi, xj, θ) = Ber(r|wT\n",
      "\n",
      "a,bf (xi, xj))\n",
      "\n",
      "(27.76)\n",
      "\n",
      "where f (xi, xj) is some way of combining the feature vectors. For example, we could use concatenation, [xi, xj], or elementwise product xi ⊗ xj as in supervised LDA. The overall model is like a relational extension of the mixture of experts model.\n",
      "\n",
      "27.5.2 Mixed membership stochastic block model\n",
      "\n",
      "In (Airoldi et al. 2008), they lifted the restriction that each node only belong to one cluster. That is, they replaced qi ∈ {1, . . . , K} with πi ∈ SK. This is known as the mixed membership stochastic block model, and is similar in spirit to fuzzy clustering or soft clustering. Note that πik is not the same as p(zi = k|D); the former represents ontological uncertainty (to what degree does each object belong to a cluster) wheras the latter represents epistemological uncertainty (which cluster does an object belong to). If we want to combine epistemological and ontological uncertainty, we can compute p(πi|D).\n",
      "\n",
      "In more detail, the generative process is as follows. First, each node picks a distribution over blocks, πi ∼ Dir(α). Second, choose the probability of connecting group a to group b, for all pairs of groups, ηa,b ∼ β(α, β). Third, for each edge, sample two discrete variables, one for each direction:\n",
      "\n",
      "qi→j ∼ Cat(πi), qi←j ∼ Cat(πj)\n",
      "\n",
      "(27.77)\n",
      "\n",
      "Finally, generate each edge Rij using the following model:\n",
      "\n",
      "p(Rij = 1|qi→j = a, qi←j = b, η) = ηa,b\n",
      "\n",
      "(27.78)\n",
      "\n",
      "974\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "Waverers\n",
      "\n",
      "15\n",
      "\n",
      "Loyal Opposition\n",
      "\n",
      "13\n",
      "\n",
      "8\n",
      "\n",
      "1 16\n",
      "\n",
      "11\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "7\n",
      "\n",
      "5 6\n",
      "\n",
      "Outcasts\n",
      "\n",
      "17\n",
      "\n",
      "9\n",
      "\n",
      "Young Turks\n",
      "\n",
      "3\n",
      "\n",
      "14\n",
      "\n",
      "4 18\n",
      "\n",
      "2\n",
      "\n",
      "1 Ambrose 2 Boniface 3 Mark 4 Winfrid 5 Elias 6 Basil 7 Simplicius 8 Berthold 9 John Bosco 10 Victor 11 Bonaventure 12 Amand 13 Louis 14 Albert 15Ramuald 16 Peter 17 Gregory 18 Hugh\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.22 (a) Who-likes-whom graph for Sampson’s monks. one of three groups. From Figures 2-3 of (Airoldi et al. 2008). Used with kind permission of Edo Airoldi.\n",
      "\n",
      "(b) Mixed membership of each monk in\n",
      "\n",
      "See Figure 27.21(b) for the DGM.\n",
      "\n",
      "Unlike the regular stochastic block model, each node can play a different role, depending on who it is connecting to. As an illustration of this, we will consider a data set that is widely used in the social networks analysis literature. The data concerns who-likes-whom amongst of group of 18 monks. It was collected by hand in 1968 by Sampson (Sampson 1968) over a period of months. (These days, in the era of social media such as Facebook, a social network with only 18 people is trivially small, but the methods we are discussing can be made to scale.) Figure 27.22(a) plots the raw data, and Figure 27.22(b) plots E [π]i for each monk, where K = 3. We see that most of the monk belong to one of the three clusters, known as the “young turks”, the “outcasts” and the “loyal opposition”. However, some individuals, notably monk 15, belong to two clusters; Sampson called these monks the “waverers”. It is interesting to see that the model can recover the same kinds of insights as Sampson derived by hand.\n",
      "\n",
      "One prevalent problem in social network analysis is missing data. For example, if Rij = 0, it may be due to the fact that person i and j have not had an opportunity to interact, or that data is not available for that interaction, as opposed to the fact that these people don’t want to interact. In other words, absence of evidence is not evidence of absence. We can model this by modifying the observation model so that with probability ρ, we generate a 0 from the background model, and we only force the model to explain observed 0s with probability 1 − ρ. In other words, we robustify the observation model to allow for outliers, as follows:\n",
      "\n",
      "p(Rij = r|qi→j = a, qi←j = b, η) = ρδ0(r) + (1− ρ)Ber(r|ηa,b)\n",
      "\n",
      "(27.79)\n",
      "\n",
      "See (Airoldi et al. 2008) for details.\n",
      "\n",
      "27.5.3\n",
      "\n",
      "Relational topic model\n",
      "\n",
      "In many cases, the nodes in our network have atttributes. For example, if the nodes represent academic papers, and the edges represent citations, then the attributes include the text of the document itself. It is therefore desirable to create a model that can explain the text and the link structure concurrently. Such a model can predict links given text, or even vice versa.\n",
      "\n",
      "The relational topic model (RTM) (Chang and Blei 2010) is one way to do this. This is a\n",
      "\n",
      "27.6. LVMs for relational data\n",
      "\n",
      "975\n",
      "\n",
      "B\n",
      "\n",
      "yjl\n",
      "\n",
      "α\n",
      "\n",
      "πj\n",
      "\n",
      "qjl\n",
      "\n",
      "¯qj\n",
      "\n",
      "πi\n",
      "\n",
      "yil\n",
      "\n",
      "qil\n",
      "\n",
      "¯qi\n",
      "\n",
      "Rij\n",
      "\n",
      "I\n",
      "\n",
      "J\n",
      "\n",
      "w\n",
      "\n",
      "Figure 27.23 DGM for the relational topic model.\n",
      "\n",
      "simple extension of supervised LDA (Section 27.4.4.1), where the response variable Rij (which represents whether there is an edge between nodes i and j) is modeled as follows:\n",
      "\n",
      "p(Rij = 1|qi, qj, θ) = sigm(wT (qi ⊗ qj) +w 0)\n",
      "\n",
      "(27.80)\n",
      "\n",
      "Recall that qi is the empirical topic distribution for document i, qik (cid:2) 1 Li Figure 27.23\n",
      "\n",
      "Note that it is important that Rij depend on the actual topics chosen, qi and qj, and not on the topic distributions, πi and πj, otherwise predictive performance is not as good. The if Rij is a child of πi and πj, it will be treated as just intuitive reason for this is as follows: another word, similar to the qil’s and yil’s; but since there are many more words than edges, the graph structure information will get “washed out”. By making Rij a child of qi and qj, the graph information can inﬂuence the choice of topics more directly.\n",
      "\n",
      "(cid:7)Li\n",
      "\n",
      "i=1 qilk. See\n",
      "\n",
      "One can ﬁt this model in a manner similar to SLDA. See (Chang and Blei 2010) for details. The method does better at predicting missing links than the simpler approach of ﬁrst ﬁtting an LDA model, and then using the qi’s as inputs to a logistic regression problem. The reason is analogous to the superiority of partial least squares (Section 12.5.2) to PCA+ linear regression, namely that the RTM learns a latent space that is forced to be predictive of the graph structure and words, whereas LDA might learn a latent space that is not useful for predicting the graph.\n",
      "\n",
      "27.6\n",
      "\n",
      "LVMs for relational data\n",
      "\n",
      "Graphs can be used to represent data which represents the relation amongst variables of a certain type, e.g., friendship relationships between people. But often we have multiple types of objects, and multiple types of relations. For example, Figure 27.24 illustrates two relations, one between people and people, and one between people and movies.\n",
      "\n",
      "In general, we deﬁne a k-ary relation R as a subset of k-tuples of the appropriate types:\n",
      "\n",
      "R ⊆ T1 × T2 × · · · ×T k\n",
      "\n",
      "(27.81)\n",
      "\n",
      "976\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "Figure 27.24 Example of relational data. There are two types of objects, people and movies; one 2-ary relation, friends: people × people → {0, 1} and one 2-ary function, rates: people × movie → R. Age and sex are attributes (unary functions) of the people class.\n",
      "\n",
      "where Ti are sets or types. A binary, pairwise or dyadic relation is a relation deﬁned on pairs of objects. For example, the seen relation between people and movies might be represented as the set of movies that people have seen. We can either represent this explicitly as a set, such as\n",
      "\n",
      "seen\n",
      "\n",
      "= { (Bob, StarWars), (Bob, TombRaider), (Alice, Jaws)}\n",
      "\n",
      "or implicitly, using an indicator function for the set:\n",
      "\n",
      "seen(Bob, StarWars)=1, seen(Bob, TombRaider)=1, seen(Alice, Jaws)=1\n",
      "\n",
      "A relation between two entities of types T 1 and T 2 can be represented as a binary function R : T 1 × T 2 → {0, 1}, and hence as a binary matrix. This can also be represented as a bipartite graph, in which we have nodes of two types. If T 1 = T 2, this becomes a regular directed graph, as in Section 27.5. However, there are some situations that are not so easily modelled by graphs, but which can still be modelled by relations. For example, we might have a ternary relation, R : T 1 × T 1 × T 2 → {0, 1}, where, say, R(i, j, k) = 1 iff protein i interacts with protein j when chemical k is present. This can be modelled by a 3d binary matrix. We will give some examples of this in Section 27.6.1.\n",
      "\n",
      "Making probabilistic models of relational data is called statistical relational learning (Getoor and Taskar 2007). One approach is to directly model the relationship between the variables using graphical models; this is known as probabilistic relational modeling. Another approach is to use latent variable models, as we discuss below.\n",
      "\n",
      "27.6.1\n",
      "\n",
      "Inﬁnite relational model\n",
      "\n",
      "It is straightforward to extend the stochastic block model to model relational data: we just associate a latent variable qt i ∈ {1, . . . , Kt} with each entity i of each type t. We then deﬁne the probability of the relation holding between speciﬁc entities by looking up the probability of the relation holding between entities of that type. For example, if R : T 1 × T 1 × T 2 → {0, 1}, we have\n",
      "\n",
      "p(R(i, j, k) = 1|q1\n",
      "\n",
      "i = a, q1\n",
      "\n",
      "j = b, q2\n",
      "\n",
      "k = c, η) = ηa,b,c\n",
      "\n",
      "(27.82)\n",
      "\n",
      "If we allow the number of clusters Kt for each type to be unbounded, by using a Dirichlet pro- cess, the model is called the inﬁnite relational model (IRM) (Kemp et al. 2006). An essentially\n",
      "\n",
      "27.6. LVMs for relational data\n",
      "\n",
      "977\n",
      "\n",
      "(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:2)(cid:7)(cid:8)(cid:9)\n",
      "\n",
      "interact with\n",
      "\n",
      "(cid:15)(cid:6)(cid:13)(cid:1)(cid:7)(cid:2)(cid:16)(cid:6)(cid:17)(cid:4)(cid:18)(cid:9)(cid:19)(cid:15)(cid:9)(cid:16)(cid:7)(cid:12)(cid:2)(cid:4)(cid:9)\n",
      "\n",
      "affects, causes\n",
      "\n",
      "causes\n",
      "\n",
      "affects\n",
      "\n",
      "affects, causes, complicates\n",
      "\n",
      "causes, complicates\n",
      "\n",
      "affects, complicates disrupts\n",
      "\n",
      "affects, complicates, process of, manifestation of\n",
      "\n",
      "(cid:10)(cid:6)(cid:9)(cid:4)(cid:7)(cid:9)(cid:4)(cid:9)\n",
      "\n",
      "affects, process of, result of, manifestation of\n",
      "\n",
      "result of\n",
      "\n",
      "(cid:15)(cid:6)(cid:13)(cid:8)(cid:13)(cid:11)(cid:6)(cid:2)(cid:7)(cid:8)(cid:18)(cid:20)(cid:19)(cid:12)(cid:2)(cid:16)(cid:6)(cid:13)(cid:12)(cid:9)\n",
      "\n",
      "affects, process of, result of\n",
      "\n",
      "result of\n",
      "\n",
      "(cid:7)(cid:15)(cid:12)(cid:13)(cid:14)(cid:5)(cid:7)(cid:8)(cid:6)(cid:16)(cid:6)(cid:4)(cid:9)\n",
      "\n",
      "result of\n",
      "\n",
      "affects, process of\n",
      "\n",
      "manifestation of, associated with\n",
      "\n",
      "manifestation of\n",
      "\n",
      "affects, process of\n",
      "\n",
      "manifestation of, associated with\n",
      "\n",
      "(cid:13)(cid:14)(cid:11)(cid:7)(cid:12)(cid:6)(cid:9)(cid:5)(cid:9)\n",
      "\n",
      "(cid:9)(cid:6)(cid:11)(cid:12)(cid:9)\n",
      "\n",
      "Figure 27.25 Illustration of an ontology learned by IRM applied to the Uniﬁed Medical Language System. The boxes represent 7 of the 14 concept clusters. Predicates that belong to the same cluster are grouped together, and associated with edges to which they pertain. All links with weight above 0.8 have been included. From Figure 9 of (Kemp et al. 2010). Used with kind permission of Charles Kemp.\n",
      "\n",
      "identical model, under the name inﬁnite hidden relational model (IHRM), was concurrently proposed in (Xu et al. 2006). We can ﬁt this model with variational Bayes (Xu et al. 2006, 2007) or collapsed Gibbs sampling (Kemp et al. 2006). Rather than go into algorithmic detail, we just sketch some interesting applications.\n",
      "\n",
      "27.6.1.1\n",
      "\n",
      "Learning ontologies\n",
      "\n",
      "An ontology refers to an organisation of knowledge. (see e.g., (Russell and Norvig 2010)), but it is interesting to try and learn them from data. (Kemp et al. 2006), they show how this can be done using the IRM.\n",
      "\n",
      "In AI, ontologies are often built by hand In\n",
      "\n",
      "The data comes from the Uniﬁed Medical Language System (McCray 2003), which deﬁnes a semantic network with 135 concepts (such as “disease or syndrome”, “diagnostic procedure”, “animal”), and 49 binary predicates (such as “affects”, “prevents”). We can represent this as a ternary relation R : T 1 × T 1 × T 2 → {0, 1}, where T 1 is the set of concepts and T 2 is the set of binary predicates. The result is a 3d cube. We can then apply the IRM to partition the cube into regions of roughly homogoneous response. The system found 14 concept clusters and 21 predicate clusters. Some of these are shown in Figure 27.25. The system learns, for example, that biological functions affect organisms (since ηa,b,c ≈ 1 where a represents the biological function cluster, b represents the organism cluster, and c represents the affects cluster).\n",
      "\n",
      "27.6.1.2\n",
      "\n",
      "Clustering based on relations and features\n",
      "\n",
      "We can also use IRM to cluster objects based on their relations and their features. For example, (Kemp et al. 2006) consider a political dataset (from 1965) consisting of 14 countries, 54 binary\n",
      "\n",
      "978\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "a)\n",
      "\n",
      "t s n u m m o c n o n\n",
      "\n",
      "i\n",
      "\n",
      "c o b\n",
      "\n",
      "n r e t s e w\n",
      "\n",
      "l\n",
      "\n",
      "l\n",
      "\n",
      "t v o g\n",
      "\n",
      "a n o i t u t i t s n o c\n",
      "\n",
      "e e r f\n",
      "\n",
      "l\n",
      "\n",
      "s n o i t c e e\n",
      "\n",
      "i\n",
      "\n",
      "c o b\n",
      "\n",
      "t s n u m m o c\n",
      "\n",
      "l\n",
      "\n",
      "i\n",
      "\n",
      "s t s n u m m o c\n",
      "\n",
      "n a i r a t i l\n",
      "\n",
      "a t o t\n",
      "\n",
      "t s i t i l\n",
      "\n",
      "e\n",
      "\n",
      "p h s r o s n e c\n",
      "\n",
      "i\n",
      "\n",
      "h g h\n",
      "\n",
      "i\n",
      "\n",
      "s n o i t c e e\n",
      "\n",
      "l\n",
      "\n",
      "e e r f\n",
      "\n",
      "o n\n",
      "\n",
      "y c a r e t i l l i\n",
      "\n",
      "i\n",
      "\n",
      "l\n",
      "\n",
      "c i t s e m o d\n",
      "\n",
      "e c n e o v\n",
      "\n",
      "s e g r u p\n",
      "\n",
      "S U m o r f\n",
      "\n",
      "r a f\n",
      "\n",
      "l l\n",
      "\n",
      "i\n",
      "\n",
      "a f n a r\n",
      "\n",
      "s u o g\n",
      "\n",
      "i\n",
      "\n",
      "s k o o b\n",
      "\n",
      "e r\n",
      "\n",
      "i l\n",
      "\n",
      "P N G / s t r o p x e\n",
      "\n",
      "n o i t a c u d e\n",
      "\n",
      "$\n",
      "\n",
      "t v o g\n",
      "\n",
      "m\n",
      "\n",
      "e n n o s r e p\n",
      "\n",
      "l\n",
      "\n",
      "y r a t i l i\n",
      "\n",
      "e n r o b a e s\n",
      "\n",
      "s d o o g\n",
      "\n",
      "i\n",
      "\n",
      "s s i r c\n",
      "\n",
      "t v o g\n",
      "\n",
      "t n e u q n\n",
      "\n",
      "e d N U\n",
      "\n",
      "i l\n",
      "\n",
      "l\n",
      "\n",
      "a r t u e n\n",
      "\n",
      "l\n",
      "\n",
      "c o b\n",
      "\n",
      "s n o i t a n s s a s s a\n",
      "\n",
      "i\n",
      "\n",
      "n o i t u o v e r\n",
      "\n",
      "t v o g\n",
      "\n",
      "l\n",
      "\n",
      "i l\n",
      "\n",
      "i\n",
      "\n",
      "s n o g\n",
      "\n",
      "m u n\n",
      "\n",
      "e r\n",
      "\n",
      "i\n",
      "\n",
      "y r a t i l i\n",
      "\n",
      "i\n",
      "\n",
      "m g n n e v r e t n\n",
      "\n",
      "p h s r o s n e c\n",
      "\n",
      "i\n",
      "\n",
      "e m o s\n",
      "\n",
      "d e m u s n o c\n",
      "\n",
      "y g r e n e\n",
      "\n",
      "l\n",
      "\n",
      "e n o h p e e t\n",
      "\n",
      "l\n",
      "\n",
      "n o i t a u p o p\n",
      "\n",
      "$\n",
      "\n",
      "e s n e f e d\n",
      "\n",
      "s t a e r h t\n",
      "\n",
      "P N G\n",
      "\n",
      "s t s e t o r p\n",
      "\n",
      "o h t a C\n",
      "\n",
      "s c\n",
      "\n",
      "i l\n",
      "\n",
      "i\n",
      "\n",
      "n e k a t\n",
      "\n",
      "d a S U\n",
      "\n",
      ". n p o p\n",
      "\n",
      "y t i s n e d\n",
      "\n",
      "l\n",
      "\n",
      "a e r a\n",
      "\n",
      "d n a\n",
      "\n",
      "l\n",
      "\n",
      "d a o r l i\n",
      "\n",
      "a r\n",
      "\n",
      "h t g n e\n",
      "\n",
      "s t n e d u t s\n",
      "\n",
      "n g e r o f\n",
      "\n",
      "i\n",
      "\n",
      "f o\n",
      "\n",
      "e g a\n",
      "\n",
      "y r t n u o c\n",
      "\n",
      "l\n",
      "\n",
      "s O G N w a\n",
      "\n",
      "m u n\n",
      "\n",
      "s e g a u g n a\n",
      "\n",
      "l\n",
      "\n",
      "n e k a t\n",
      "\n",
      "$\n",
      "\n",
      "d a\n",
      "\n",
      "i\n",
      "\n",
      "s r e k r o w e a m e f\n",
      "\n",
      "l\n",
      "\n",
      "t n e s\n",
      "\n",
      "i\n",
      "\n",
      "l i\n",
      "\n",
      "a m n g e r o f\n",
      "\n",
      "n\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "t e d\n",
      "\n",
      "i\n",
      "\n",
      "n e t o r p\n",
      "\n",
      "i\n",
      "\n",
      "s t n e m t s e v n\n",
      "\n",
      "s O G N s t r a\n",
      "\n",
      "y h c r a n o m\n",
      "\n",
      "l\n",
      "\n",
      "d a o r\n",
      "\n",
      "h t g n e\n",
      "\n",
      "l\n",
      "\n",
      "e b a r a\n",
      "\n",
      "i\n",
      "\n",
      "s t n a r g m e\n",
      "\n",
      "d e y o p m e n u\n",
      "\n",
      "l\n",
      "\n",
      "l\n",
      "\n",
      "s e i r o a c\n",
      "\n",
      "i\n",
      "\n",
      "n\n",
      "\n",
      "t e d\n",
      "\n",
      "i\n",
      "\n",
      "Brazil Netherlands UK USA Burma Indonesia Jordan Egypt India Israel China Cuba Poland USSR\n",
      "\n",
      "b)\n",
      "\n",
      "military alliance\n",
      "\n",
      "sends tourists to\n",
      "\n",
      "exports books to\n",
      "\n",
      "exports to\n",
      "\n",
      "c)\n",
      "\n",
      "treaties\n",
      "\n",
      "conferences\n",
      "\n",
      "d)\n",
      "\n",
      "joint membership of IGOs\n",
      "\n",
      "joint membership of NGOs\n",
      "\n",
      "e)\n",
      "\n",
      "negative behavior\n",
      "\n",
      "negative\n",
      "\n",
      "communications accusations\n",
      "\n",
      "protests\n",
      "\n",
      "f)\n",
      "\n",
      "book translations\n",
      "\n",
      "g)\n",
      "\n",
      "economic aid\n",
      "\n",
      "h)\n",
      "\n",
      "emigration\n",
      "\n",
      "i)\n",
      "\n",
      "common bloc membership\n",
      "\n",
      "Figure 27.26 Illustration of IRM applied to some political data containing features and pairwise interac- tions. Top row (a). the partition of the countries into 5 clusters and the features into 5 clusters. Every second column is labelled with the name of the corresponding feature. Small squares at bottom (a-i): these are 8 of the 18 clusters of interaction types. From Figure 6 of (Kemp et al. 2006). Used with kind permission of Charles Kemp.\n",
      "\n",
      "predicates representing interaction types between countries (e.g., “sends tourists to”, “economic aid”), and 90 features (e.g., “communist”, “monarchy”). To create a binary dataset, real-valued features were thresholded at their mean, and categorical variables were dummy-encoded. The data has 3 types: T 1 represents countries, T 2 represents interactions, and T 3 represents features. We have two relations: R1 : T 1 ×T 1 ×T 2 → {0, 1}, and R2 : T 1 ×T 3 → {0, 1}. (This problem therefore combines aspects of both the biclustering model and the ontology discovery model.) When given multiple relations, the IRM treats them as conditionally independent. In this case, we have\n",
      "\n",
      "p(R1, R2, q1, q2, q3|θ) = p(R1|q1, q2, θ)p(R2|q1, q3, θ)\n",
      "\n",
      "(27.83)\n",
      "\n",
      "The results are shown in Figure 27.26. The IRM divides the 90 features into 5 clusters, the ﬁrst of which contains “noncommunist”, which captures one of the most important aspects of this Cold-War era dataset. It also clusters the 14 countries into 5 clusters, reﬂecting natural geo-political groupings (e.g., US and UK, or the Communist Bloc), and the 54 predicates into 18 clusters, reﬂecting similar relationships (e.g., “negative behavior and “accusations”).\n",
      "\n",
      "27.6. LVMs for relational data\n",
      "\n",
      "979\n",
      "\n",
      "27.6.2\n",
      "\n",
      "Probabilistic matrix factorization for collaborative ﬁltering\n",
      "\n",
      "As discussed in Section 1.3.4.2, collaborative ﬁltering (CF) requires predicting entries in a matrix R : T 1 × T 2 → R, where for example R(i, j) is the rating that user i gave to movie j. Thus we see that CF is a kind of relational learning problem (and one with particular commercial importance).\n",
      "\n",
      "Much of the work in this area makes use of the data that Netﬂix made available in their competition. In particular, a large 17,770 × 480,189 movie x user ratings matrix is provided. The full matrix would have ∼ 8.6 × 109 entries, but only 100,480,507 (about 1%) of the entries are observed, so the matrix is extremely sparse. In addition the data is quite imbalanced, with many users rating fewer than 5 movies, and a few users rating over 10,000 movies. The validation set is 1,408,395 (movie,user) pairs. Finally, there is a separate test set with 2,817,131 (movie,user) pairs, for which the ranking is known but withheld from contestants. The performance measure is root mean square error:\n",
      "\n",
      "RM SE =\n",
      "\n",
      "9 : : ; 1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "(X(mi, ui) − ˆX(mi, ui))2\n",
      "\n",
      "(27.84)\n",
      "\n",
      "where X(mi, ui) is the true rating of user ui on movie mi, and ˆX(mi, ui) is the prediction. The baseline system, known as Cinematch, had an RMSE on the training set of 0.9514, and on the test set of 0.9525. To qualify for the grand prize, teams needed to reduce the test RMSE by 10%, i.e., get a test RMSE of 0.8563 or less. We will discuss some of the basic methods used byt the winning team below.\n",
      "\n",
      "Since the ratings are drawn from the set {0, 1, 2, 3, 4, 5}, it is tempting to use a categorical observation model. However, this does not capture the fact that the ratings are ordered. Although we could use an ordinal observation model, in practice people use a Gaussian observation model for simplicity. One way to make the model better match the data is to pass the model’s predicted mean response through a sigmoid, and then to map the [0, 1] interval to [0, 5] (Salakhutdinov and Mnih 2008). Alternatively we can make the data a better match to the Gaussian model by transforming the data using Rij =\n",
      "\n",
      "\"\n",
      "\n",
      "6 − Rij (Aggarwal and Merugu 2007).\n",
      "\n",
      "We could use the IRM for the CF task, by associating a discrete latent variable for each user i and for each movie or video qv qu\n",
      "\n",
      "j , and then deﬁning\n",
      "\n",
      "p(Rij = r|qu\n",
      "\n",
      "i = a, qv\n",
      "\n",
      "j = b, θ) = N (r|μa,b, σ2)\n",
      "\n",
      "(27.85)\n",
      "\n",
      "This is just another example of co-clustering. We can also extend the model to generate side information, such as attributes about each user and/or movie. See Figure 27.27 for an illustration. Another possibility is to replace the discrete latent variables with continuous latent variables πu i ∈ SKu and πv j ∈ SKv . However, it has been found (see e.g., (Shan and Banerjee 2010)) that one obtains much better results by using unconstrained real-valued latent factors for each user ui ∈ RK and each movie vj ∈ RK.7 We then use a likelihood of the form i vj, σ2)\n",
      "\n",
      "p(Rij = r|ui, vj) = N (r|uT\n",
      "\n",
      "(27.86)\n",
      "\n",
      "7. Good results with discrete latent variables have been obtained on some datasets that are smaller than Netﬂix, such as MovieLens and EachMovie. However, these datasets are much easier to predict, because there is less imbalance between the number of reviews performed by different users (in Netﬂix, some users have rated more than 10,000 movies, whereas others have rated less than 5).\n",
      "\n",
      "980\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "Figure 27.27 Visualization of a small relational dataset, where we have one relation, likes(user, movie), and features for movies (here, genre) and users (here, occupation). From Figure 5 of (Xu et al. 2008). Used with kind permission of Zhao Xu.\n",
      "\n",
      "μu\n",
      "\n",
      "Σu\n",
      "\n",
      "σ2\n",
      "\n",
      "ui\n",
      "\n",
      "μv\n",
      "\n",
      "vj\n",
      "\n",
      "Σv\n",
      "\n",
      "Rij\n",
      "\n",
      "T 2\n",
      "\n",
      "T 1\n",
      "\n",
      "2\n",
      "\n",
      "r o t c e V\n",
      "\n",
      "r o t c a F\n",
      "\n",
      "5 . 1\n",
      "\n",
      "0 . 1\n",
      "\n",
      "5 . 0\n",
      "\n",
      "0 . 0\n",
      "\n",
      "5 . 0 −\n",
      "\n",
      "0 . 1 −\n",
      "\n",
      "5 . 1 −\n",
      "\n",
      "Freddy G ot Fingered H alf B aked Freddy vs. Jason R oad Trip T he Longest Y ard T he F ast and the F urious Arm ageddon C atw o m an C oyote U gly\n",
      "\n",
      "T he R oyal T enenbau m s Julien D onkey− B oy P unch− Drunk Love I H eart H uckabees Lost in Translation B eing John M alkovich B elle de Jour Kill Bill: V ol. 1 N atural B orn Killers Citizen K ane S carface\n",
      "\n",
      "T he W izard of O z\n",
      "\n",
      "R una w ay Bride M aid in M anhattan\n",
      "\n",
      "Sister A ct Step m o m\n",
      "\n",
      "A nnie H all S ophie’s C hoice M oonstruck T he W ay W e W ere T he S ound of M usic T he W altons: S eason 1\n",
      "\n",
      "−1.5\n",
      "\n",
      "−1.0\n",
      "\n",
      "−0.5\n",
      "\n",
      "0.0\n",
      "\n",
      "0.5\n",
      "\n",
      "1.0\n",
      "\n",
      "Factor Vector 1\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.28 (a) A DGM for probabilistic matrix factorization. (b) Visualization of the ﬁrst two factors in the PMF model estimated from the Netﬂix challenge data. Each movie j is plotted at the location speciﬁed ˆvj. On the left we have low-brow humor and horror movies (Half Baked, Freddy vs Jason), and on the right we have more serious dramas (Sophie’s Choice, Moonstruck). On the top we have critically acclaimed independent movies (Punch-Drunk Love, I Heart Huckabees), and on the bottom we have mainstream Hollywood blockbusters (Armageddon, Runway Bride). The Wizard of Oz is right in the middle of these axes. From Figure 3 of (Koren et al. 2009). Used with kind permission of Yehuda Koren.\n",
      "\n",
      "This has been called probabilistic matrix factorization (PMF) (Salakhutdinov and Mnih 2008). See Figure 27.28(a) for the DGM. The intuition behind this method is that each user and each movie get embedded into the same low-dimensional continuous space (see Figure 27.28(b)). If a user is close to a movie in that space, they are likely to rate it highly. All of the best entries in the Netﬂix competition used this approach in one form or another.8\n",
      "\n",
      "PMF is closely related to the SVD. In particular, if there is no missing data, then computing the MLE for the ui’s and the vj’s is equivalent to ﬁnding a rank K approximation to R. However, as soon as we have missing data, the problem becomes non-convex, as shown in\n",
      "\n",
      "8. The winning entry was actually an ensemble of different methods, including PMF, nearest neighbor methods, etc.\n",
      "\n",
      "27.6. LVMs for relational data\n",
      "\n",
      "981\n",
      "\n",
      "0.97\n",
      "\n",
      "0.91\n",
      "\n",
      "40\n",
      "\n",
      "60\n",
      "\n",
      "Plain\n",
      "\n",
      "0.96\n",
      "\n",
      "0.95\n",
      "\n",
      "Netflix Baseline Score\n",
      "\n",
      "0.905\n",
      "\n",
      "0.9\n",
      "\n",
      "50\n",
      "\n",
      "90\n",
      "\n",
      "100\n",
      "\n",
      "128\n",
      "\n",
      "180\n",
      "\n",
      "200 w/Biases\n",
      "\n",
      "E S M R\n",
      "\n",
      "0.94\n",
      "\n",
      "0.93\n",
      "\n",
      "SVD\n",
      "\n",
      "E S M R\n",
      "\n",
      "0.895\n",
      "\n",
      "0.89\n",
      "\n",
      "50\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "w/Implicit feedback\n",
      "\n",
      "0.92\n",
      "\n",
      "0.885\n",
      "\n",
      "50\n",
      "\n",
      "0.91\n",
      "\n",
      "0.9 0\n",
      "\n",
      "Constrained PMF 5\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "20\n",
      "\n",
      "PMF\n",
      "\n",
      "25\n",
      "\n",
      "30 Epochs\n",
      "\n",
      "35\n",
      "\n",
      "40\n",
      "\n",
      "45\n",
      "\n",
      "50\n",
      "\n",
      "55\n",
      "\n",
      "60\n",
      "\n",
      "0.88\n",
      "\n",
      "0.875\n",
      "\n",
      "10\n",
      "\n",
      "100\n",
      "\n",
      "w/Temporal dynamics\n",
      "\n",
      "100\n",
      "\n",
      "200\n",
      "\n",
      "500 1000\n",
      "\n",
      "1500\n",
      "\n",
      "1000 Millions of Parameters\n",
      "\n",
      "10000\n",
      "\n",
      "100000\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.29 (a) RMSE on the validation set for different PMF variants vs number of passes through the data. “SVD” is the unregularized version, λU = λV = 0. “PMF1” corresponds to λU = 0.01 and λV = 0.001, while “PMF2” corresponds to λU = 0.001 and λV = 0.0001. “PMFA1” corresponds to a version where the mean and diagonal covariance of the Gaussian prior were learned from data. From Figure 2 of (Salakhutdinov and Mnih 2008). Used with kind permission of Ruslan Salakhutdinov. (b) RMSE on the test set (quiz portion) vs number of parameters for several different models. “Plain” is the baseline PMF with suitably chosen λU , λV . “With biases” adds fi and gj offset terms. “With implicit feedback” “With temporal dynamics” allows the offset terms to change over time. The Netﬂix baseline system achieves an RMSE of 0.9514, and the grand prize’s required accuracy is 0.8563 (which was obtained on 21 September 2009). Figure generated by netflixResultsPlot. From Figure 4 of (Koren et al. 2009). Used with kind permission of Yehuda Koren.\n",
      "\n",
      "(Srebro and Jaakkola 2003), and standard SVD methods cannot be applied. Netﬂix challenge, only about 1% of the matrix is observed.)\n",
      "\n",
      "(Recall that in the\n",
      "\n",
      "The most straightforward way to ﬁt the PMF model is to minimize the overall NLL: ⎞\n",
      "\n",
      "J(U, V) = − log p(R|U, V, O) = − log\n",
      "\n",
      "⎛\n",
      "\n",
      "⎝\n",
      "\n",
      "N(cid:20)\n",
      "\n",
      "M(cid:20)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "N (Rij|uT\n",
      "\n",
      "i vj, σ2)\n",
      "\n",
      "(cid:14)I(Oij =1)\n",
      "\n",
      "⎠ (27.87)\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "where Oij = 1 if user i has seen movie j. Since this is non-convex, we can just ﬁnd a locally optimal MLE. Since the Netﬂix data is so large (about 100 million observed entries), it is common to use stochastic gradient descent (Section 8.5.2) for this task. The gradient for ui is given by\n",
      "\n",
      "dJ dui\n",
      "\n",
      "=\n",
      "\n",
      "d dui\n",
      "\n",
      "1 2\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ij\n",
      "\n",
      "I(Oij = 1)(Rij − uT\n",
      "\n",
      "i vj)2 = −\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "j:Oij =1\n",
      "\n",
      "eijvj\n",
      "\n",
      "(27.88)\n",
      "\n",
      "where eij = Rij − uT i has watched, the update takes the following simple form:\n",
      "\n",
      "i vj is the error term. By stochastically sampling a single movie j that user\n",
      "\n",
      "ui = ui + ηeijvj\n",
      "\n",
      "(27.89)\n",
      "\n",
      "where η is the learning rate. The update for vj is similar.\n",
      "\n",
      "982\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "Of course, just maximizing the likelihood results in overﬁtting, as shown in Figure 27.29(a).\n",
      "\n",
      "We can regularize this by imposing Gaussian priors:\n",
      "\n",
      "p(U, V) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "N (ui|μu, Σu)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "N (vj|μv, Σv)\n",
      "\n",
      "(27.90)\n",
      "\n",
      "i\n",
      "\n",
      "j\n",
      "\n",
      "If we use μu = μv = 0, Σu = σ2\n",
      "\n",
      "U IK, and Σv = σ2\n",
      "\n",
      "V IK, the new objective becomes\n",
      "\n",
      "J(U, V) =− log p(R, U, V|O, θ)\n",
      "\n",
      "=\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "I(Oij = 1)(Rij − uT\n",
      "\n",
      "i vj)2\n",
      "\n",
      "(27.91)\n",
      "\n",
      "i\n",
      "\n",
      "+λU\n",
      "\n",
      "j (cid:4)\n",
      "\n",
      "||ui||2\n",
      "\n",
      "2 + λV\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "||vj||2\n",
      "\n",
      "2 + const\n",
      "\n",
      "(27.92)\n",
      "\n",
      "i\n",
      "\n",
      "j\n",
      "\n",
      "where we have deﬁned λU = σ2/σ2 U and λV = σ2/σ2 V . By varying the regularizers, we can reduce the effect of overﬁtting, as shown in Figure 27.29(a). We can ﬁnd MAP estimates using stochastic gradient descent. We can also compute approximate posteriors using variational Bayes (Ilin and Raiko 2010).\n",
      "\n",
      "If we use diagonal covariances for the priors, we can penalize each latent dimension by a different amount. Also, if we use non-zero means for the priors, we can account for offset terms. Optimizing the prior parameters (μu, Σu, μv, Σv) at the same time as the model parameters (U, V, σ2) is a way to create an adaptive prior. This avoids the need to search for the optimal values of λU and λV , and gives even better results, as shown in Figure 27.29(a).\n",
      "\n",
      "It turns out that much of the variation in the data can be explained by movie-speciﬁc or user-speciﬁc effects. For example, some movies are popular for all types of users. And some users give low scores for all types of movies. We can model this by allowing for user and movie speciﬁc offset or bias terms as follows: p(Rij = r|ui, vj, θ) = N (r|uT\n",
      "\n",
      "i vj + μ + fi + gj, σ2)\n",
      "\n",
      "(27.93)\n",
      "\n",
      "where μ is the overall mean, fi is the user bias, gj is the movie bias, and uT i vj is the interaction term. This is equivalent to applying PMF just to the residual matrix, and gives much better results, as shown in Figure 27.29(b). We can estimate the fi, gj and μ terms using stochastic gradient descent, just as we estimated U, V and θ.\n",
      "\n",
      "We can also allow the bias terms to evolve over time, to reﬂect the changing preferences of users (Koren 2009b). This is important since in the Netﬂix competition, the test data was more recent than the training data. Figure 27.29(b) shows that allowing for temporal dynamics can help a lot.\n",
      "\n",
      "In the Netﬂix competition, entrants knew which movies the user had rated in the test set, even though they did not know the values of these ratings. That is, they knew the value of the (dense) O matrix even on the If a user chooses to rate a movie, it is likely because they have seen it, which in test set. turns means they thought they would like it. Thus the very act of rating reveals information. Conversely, if a user chooses not rate a movie, it suggests they knew they would not like it. So the data is not missing at random (see e.g., (Marlin and Zemel 2009)). Exploiting this can improve performance, as shown in Figure 27.29(b). In real problems, information on the test set is not available. However, we often know which movies the user has watched or declined to\n",
      "\n",
      "Often we also have side information of various kinds.\n",
      "\n",
      "27.7. Restricted Boltzmann machines (RBMs)\n",
      "\n",
      "983\n",
      "\n",
      "watch, even if they did not rate them (this is called implicit feedback), and this can be used as useful side information.\n",
      "\n",
      "Another source of side information concerns the content of the movie, such as the movie genre, the list of the actors, or a synopsis of the plot. This can be denoted by xv, the features (In the case where we just have the id of the video, we can treat xv as a |V|- of the video. dimensional bit vector with just one bit turned on.) We may also know features about the user, which we can denote by xu. In some cases, we only know if the user clicked on the video or not, that is, we may not have a numerical rating. We can then modify the model as follows:\n",
      "\n",
      "p(R(u, v)|xu, xv, θ) = Ber(R(u, v)|(Uxu)T (Vxv))\n",
      "\n",
      "(27.94)\n",
      "\n",
      "where U is a |U| × K matrix, and V is a |V| × K matrix (we can incorporate an offset term by appending a 1 to xu and xv in the usual way). A method for computing the approximate posterior p(U, V|D) in an online fashion, using ADF and EP, was described in (Stern et al. 2009). This was implemented by Microsoft and has been deployed to predict click through rates on all the ads used by Bing.\n",
      "\n",
      "Unfortunately, ﬁtting this model just from positive binary data can result in an over prediction of links, since no negative examples are included. Better performance is obtained if one has access to the set of all videos shown to the user, of which at most one was picked; data of this form is known as an impression log. In this case, we can use a multinomial model instead of a binary model; in (Yang et al. 2011), this was shown to work much better than a binary model. To understand why, suppose some is presented with a choice of an action movie starring Arnold Schwarzenegger, an action movie starring Vin Diesel, and a comedy starring Hugh Grant. If the user picks Arnold Schwarzenegger, we learn not only that they like prefer action movies to comedies, but also that they prefer Schwarzenegger to Diesel. This is more informative than just knowing that they like Schwarzenegger and action movies.\n",
      "\n",
      "27.7\n",
      "\n",
      "Restricted Boltzmann machines (RBMs)\n",
      "\n",
      "So far, all the models we have proposed in this chapter have been representable by directed graphical models. But some models are better represented using undirected graphs. For example, the Boltzmann machine (Ackley et al. 1985) is a pairwise MRF with hidden nodes h and visible nodes v, as shown in Figure 27.30(a). The main problem with the Boltzmann machine is that exact inference is intractable, and even approximate inference, using e.g., Gibbs sampling, can be slow. However, suppose we restrict the architecture so that the nodes are arranged in layers, and so that there are no connections between nodes within the same layer (see Figure 27.30(b)). Then the model has the form\n",
      "\n",
      "p(h, v|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "R(cid:20)\n",
      "\n",
      "r=1\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "k=1\n",
      "\n",
      "ψrk(vr, hk)\n",
      "\n",
      "(27.95)\n",
      "\n",
      "where R is the number of visible (response) variables, K is the number of hidden variables, and v plays the role of y earlier in this chapter. This model is known as a restricted Boltzmann machine (RBM) (Hinton 2002), or a harmonium (Smolensky 1986).\n",
      "\n",
      "An RBM is a special case of a product of experts (PoE) (Hinton 1999), which is so-called because we are multiplying together a set of “experts” (here, potential functions on each edge)\n",
      "\n",
      "984\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "H\n",
      "\n",
      "V\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 27.30 (a) A general Boltzmann machine, with an arbitrary graph structure. The shaded (visible) nodes are partitioned into input and output, although the model is actually symmetric and deﬁnes a joint (b) A restricted Boltzmann machine with a bipartite structure. Note the lack of density on all the nodes. intra-layer connections.\n",
      "\n",
      "and then normalizing, whereas in a mixture of experts, we take a convex combination of normalized distributions. The intuitive reason why PoE models might work better than a mixture is that each expert can enforce a constraint (if the expert has a value which is (cid:30) 1 or (cid:22) 1) or a “don’t care” condition (if the expert has value 1). By multiplying these experts together in different ways we can create “sharp” distributions which predict data which satisﬁes the speciﬁed constraints (Hinton and Teh 2001). For example, consider a distributed model of text. A given document might have the topics “government”, “maﬁa” and “playboy”. If we “multiply” the predictions of each topic together, the model may give very high probability to the word “Berlusconi”9 (Salakhutdinov and Hinton 2010). By contrast, adding together experts can only make the distribution broader (see Figure 14.17).\n",
      "\n",
      "Typically the hidden nodes in an RBM are binary, so h speciﬁes which constraints are active. It is worth comparing this with the directed models we have discussed. In a mixture model, we have one hidden variable q ∈ {1, . . . , K}. We can represent this using a set of K bits, with the restriction that exactly one bit is on at a time. This is called a localist encoding, since only one hidden unit is used to generate the response vector. This is analogous to the hypothetical notion of grandmother cells in the brain, that are able to recognize only one kind of object. By contrast, an RBM uses a distributed encoding, where many units are involved in generating each output. Models that used vector-valued hidden variables, such as π ∈ SK, as in mPCA/ LDA, or z ∈ RK, as in ePCA also use distributed encodings.\n",
      "\n",
      "The main difference between an RBM and directed two-layer models is that the hidden\n",
      "\n",
      "variables are conditionally independent given the visible variables, so the posterior factorizes:\n",
      "\n",
      "p(h|v, θ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "p(hk|v, θ)\n",
      "\n",
      "(27.96)\n",
      "\n",
      "k\n",
      "\n",
      "This makes inference much simpler than in a directed model, since we can estimate each hk\n",
      "\n",
      "9. Silvio Berlusconi is the current (2011) prime minister of Italy.\n",
      "\n",
      "27.7. Restricted Boltzmann machines (RBMs)\n",
      "\n",
      "985\n",
      "\n",
      "Visible Binary Gaussian Categorical Multiple categorical Gaussian Binary\n",
      "\n",
      "Hidden Binary Binary Binary Binary Gaussian Gaussian\n",
      "\n",
      "Name Binary RBM Gaussian RBM Categorical RBM Replicated softmax/ undirected LDA Undirected PCA Undirected binary PCA\n",
      "\n",
      "Reference (Hinton 2002) (Welling and Sutton 2005) (Salakhutdinov et al. 2007) (Salakhutdinov and Hinton 2010) (Marks and Movellan 2001) (Welling and Sutton 2005)\n",
      "\n",
      "Table 27.2 Summary of different kinds of RBM.\n",
      "\n",
      "independently and in parallel, as in a feedforward neural network. The disadvantage is that training undirected models is much harder, as we discuss below.\n",
      "\n",
      "27.7.1\n",
      "\n",
      "Varieties of RBMs\n",
      "\n",
      "In this section, we describe various forms of RBMs, by deﬁning different pairwise potential functions. See Table 27.2 for a summary. All of these are special cases of the exponential family harmonium (Welling et al. 2004).\n",
      "\n",
      "27.7.1.1\n",
      "\n",
      "Binary RBMs\n",
      "\n",
      "The most common form of RBM has binary hidden nodes and binary visible nodes. The joint distribution then has the following form:\n",
      "\n",
      "E(v, h; θ) (cid:2) −\n",
      "\n",
      "p(v, h|θ) =\n",
      "\n",
      "1 Z(θ) R(cid:4)\n",
      "\n",
      "exp(−E(v, h; θ))\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "vrhkWrk −\n",
      "\n",
      "R(cid:4)\n",
      "\n",
      "vrbr −\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "hkck\n",
      "\n",
      "(27.97)\n",
      "\n",
      "(27.98)\n",
      "\n",
      "Z(θ) =\n",
      "\n",
      "r=1 = −(vT Wh + vT b + hT c) exp(−E(v, h; θ))\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "r=1\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "k=1\n",
      "\n",
      "(27.99)\n",
      "\n",
      "(27.100)\n",
      "\n",
      "v\n",
      "\n",
      "h\n",
      "\n",
      "where E is the energy function, W is a R × K weight matrix, b are the visible bias terms, c are the hidden bias terms, and θ = (W, b, c) are all the parameters. For notational simplicity, we will absorb the bias terms into the weight matrix by clamping dummy units v0 = 1 and h0 = 1 and setting w0,: = c and w:,0 = b. Note that naively computing Z(θ) takes O(2R2K) time but we can reduce this to O(min{R2K, K2R}) time (Exercise 27.1).\n",
      "\n",
      "When using a binary RBM, the posterior can be computed as follows:\n",
      "\n",
      "p(h|v, θ) =\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "p(hk|v, θ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Ber(hk|sigm(wT\n",
      "\n",
      ":,kv))\n",
      "\n",
      "(27.101)\n",
      "\n",
      "k=1\n",
      "\n",
      "k\n",
      "\n",
      "By symmetry, one can show that we can generate data given the hidden variables as follows:\n",
      "\n",
      "p(v|h, θ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "p(vr|h, θ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Ber(vr|sigm(wT\n",
      "\n",
      "r,:h))\n",
      "\n",
      "(27.102)\n",
      "\n",
      "r\n",
      "\n",
      "r\n",
      "\n",
      "986\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "We can write this in matrix-vetor notation as follows:\n",
      "\n",
      "E [h|vθ] = sigm(WT v) E [v|h, θ] = sigm(Wh)\n",
      "\n",
      "(27.103)\n",
      "\n",
      "(27.104)\n",
      "\n",
      "The weights in W are called the generative weights, since they are used to generate the observations, and the weights in WT are called the recognition weights, since they are used to recognize the input.\n",
      "\n",
      "From Equation 27.101, we see that we activate hidden node k in proportion to how much the input vector v “looks like” the weight vector w:,k (up to scaling factors). Thus each hidden node captures certain features of the input, as encoded in its weight vector, similar to a feedforward neural network.\n",
      "\n",
      "27.7.1.2\n",
      "\n",
      "Categorical RBM\n",
      "\n",
      "We can extend the binary RBM to categorical visible variables by using a 1-of-C encoding, where C is the number of states for each vir. We deﬁne a new energy function as follows (Salakhutdinov et al. 2007; Salakhutdinov and Hinton 2010):\n",
      "\n",
      "E(v, h; θ) (cid:2) −\n",
      "\n",
      "R(cid:4)\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "rhkW c vc\n",
      "\n",
      "rk −\n",
      "\n",
      "R(cid:4)\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "rbc vc\n",
      "\n",
      "r −\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "hkck\n",
      "\n",
      "(27.105)\n",
      "\n",
      "r=1\n",
      "\n",
      "k=1\n",
      "\n",
      "c=1\n",
      "\n",
      "r=1\n",
      "\n",
      "c=1\n",
      "\n",
      "k=1\n",
      "\n",
      "The full conditionals are given by\n",
      "\n",
      "p(vr|h, θ) = Cat(S({bc\n",
      "\n",
      "r +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "hkW c\n",
      "\n",
      "rk}C\n",
      "\n",
      "c=1))\n",
      "\n",
      "(27.106)\n",
      "\n",
      "p(hk = 1|c, θ) = sigm(ck +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "k (cid:4)\n",
      "\n",
      "rW c vc\n",
      "\n",
      "rk)\n",
      "\n",
      "(27.107)\n",
      "\n",
      "r\n",
      "\n",
      "c\n",
      "\n",
      "27.7.1.3\n",
      "\n",
      "Gaussian RBM\n",
      "\n",
      "We can generalize the model to handle real-valued data. In particular, a Gaussian RBM has the following energy function:\n",
      "\n",
      "E(v, h|θ) = −\n",
      "\n",
      "R(cid:4)\n",
      "\n",
      "r=1\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "Wrkhkvr − 1 2\n",
      "\n",
      "R(cid:4)\n",
      "\n",
      "(vr − br)2 −\n",
      "\n",
      "r=1\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "k=1\n",
      "\n",
      "akhk\n",
      "\n",
      "(27.108)\n",
      "\n",
      "The parameters of the model are θ = (wrk, ak, br). (We have assumed the data is standardized, so we ﬁx the variance to σ2 = 1.) Compare this to a Gaussian in information form:\n",
      "\n",
      "Nc(v|η, Λ) ∝ exp(ηT v − 1 2 where η = Λμ. We see that we have set Λ = I, and η = given by μ = Λ−1η =\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "k hkw:,k. Thus the mean is k hkw:,k. The full conditionals, which are needed for inference and\n",
      "\n",
      "vT Λv)\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "(27.109)\n",
      "\n",
      "27.7. Restricted Boltzmann machines (RBMs)\n",
      "\n",
      "987\n",
      "\n",
      "learning, are given by\n",
      "\n",
      "p(hk = 1|v, θ) = sigm\n",
      "\n",
      "p(vr|h, θ) =N (vr|br +\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "ck +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "k (cid:4)\n",
      "\n",
      "wrkhk, 1) (cid:11)\n",
      "\n",
      "wrkvr\n",
      "\n",
      "(27.110)\n",
      "\n",
      "(27.111)\n",
      "\n",
      "r\n",
      "\n",
      "We see that each visible unit has a Gaussian distribution whose mean is a function of the hidden bit vector. More powerful models, which make the (co)variance depend on the hidden state, can also be developed (Ranzato and Hinton 2010).\n",
      "\n",
      "27.7.1.4\n",
      "\n",
      "RBMs with Gaussian hidden units\n",
      "\n",
      "If we use Gaussian latent variables and Gaussian visible variables, we get an undirected version of factor analysis. However, it turns out that it is identical to the standard directed version (Marks and Movellan 2001).\n",
      "\n",
      "If we use Gaussian latent variables and categorical observed variables, we get an undirected version of categorical PCA (Section 27.2.2). In (Salakhutdinov et al. 2007), this was applied to the Netﬂix collaborative ﬁltering problem, but was found to be signiﬁcantly inferior to using binary latent variables, which have more expressive power.\n",
      "\n",
      "27.7.2\n",
      "\n",
      "Learning RBMs\n",
      "\n",
      "In this section, we discuss some ways to compute ML parameter estimates of RBMs, using gradient-based optimizers. It is common to use stochastic gradient descent, since RBMs often have many parameters and therefore need to be trained on very large datasets. In addition, it is standard to use (cid:2)2 regularization, a technique that is often called weight decay in this context. This requires a very small change to the objective and gradient, as discussed in Section 8.3.6.\n",
      "\n",
      "27.7.2.1\n",
      "\n",
      "Deriving the gradient using p(h, v|θ)\n",
      "\n",
      "To compute the gradient, we can modify the equations from Section 19.5.2, which show how to ﬁt a generic latent variable maxent model. In the context of the Boltzmann machine, we have one feature per edge, so the gradient is given by\n",
      "\n",
      "∂(cid:2) ∂wrk\n",
      "\n",
      "=\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "E [vrhk|vi, θ] − E [vrhk|θ]\n",
      "\n",
      "(27.112)\n",
      "\n",
      "We can write this in matrix-vector form as follows:\n",
      "\n",
      "∇w (cid:2) = Epemp(·|θ)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "vhT\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "− Ep(·|θ)\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "vhT\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(27.113)\n",
      "\n",
      "where pemp(v, h|θ) (cid:2) p(h|v, θ)pemp(v), and pemp(v) = 1 N distribution. hk = 1.)\n",
      "\n",
      "is sometimes called the clamped phase, and the second term, when v is free, is sometimes called the unclamped\n",
      "\n",
      "The ﬁrst term on the gradient, when v is ﬁxed to a data case,\n",
      "\n",
      "i=1 δvi (v) is the empirical (We can derive a similar expression for the bias terms by setting vr = 1 or\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "988\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "phase. When the model expectations match the empirical expectations, the two terms cancel out, the gradient becomes zero and learning stops. This algorithm was ﬁrst proposed in (Ackley et al. 1985). The main problem is efficiently computing the expectations. We discuss some ways to do this below.\n",
      "\n",
      "27.7.2.2\n",
      "\n",
      "Deriving the gradient using p(v|θ)\n",
      "\n",
      "We now present an alternative way to derive Equation 27.112, which also applies to other energy based models. First we marginalize out the hidden variables and write the RBM in the form p(v|θ) = 1\n",
      "\n",
      "F (v) (cid:2)\n",
      "\n",
      "Z(θ) exp(−F (v; θ)), whereF (v; θ) is the free energy: (cid:10)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "E(v, h) =\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "exp\n",
      "\n",
      "R(cid:4)\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "vrhkWrk\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(27.114)\n",
      "\n",
      "=\n",
      "\n",
      "h (cid:4)\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "h R(cid:4)\n",
      "\n",
      "r=1 (cid:11)\n",
      "\n",
      "k=1\n",
      "\n",
      "vrhkWrk\n",
      "\n",
      "(27.115)\n",
      "\n",
      "=\n",
      "\n",
      "h K(cid:20)\n",
      "\n",
      "k=1\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "r=1 (cid:10)\n",
      "\n",
      "R(cid:4)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "vrhrWrk\n",
      "\n",
      "(27.116)\n",
      "\n",
      "=\n",
      "\n",
      "k=1\n",
      "\n",
      "K(cid:20)\n",
      "\n",
      "hr∈{0,1} (cid:10)\n",
      "\n",
      "R(cid:4)\n",
      "\n",
      "1 + exp(\n",
      "\n",
      "r=1\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "vrWrk)\n",
      "\n",
      "(27.117)\n",
      "\n",
      "k=1\n",
      "\n",
      "r=1\n",
      "\n",
      "Next we write the (scaled) log-likelihood in the following form:\n",
      "\n",
      "Using the fact that Z(θ) =\n",
      "\n",
      "(cid:2)(θ) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "log p(vi|θ) = − 1 N (cid:7)\n",
      "\n",
      "v exp(−F (v; θ)) we have\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "F (vi|θ) − log Z(θ)\n",
      "\n",
      "(27.118)\n",
      "\n",
      "∇(cid:2)(θ) =− 1 N\n",
      "\n",
      "= − 1 N\n",
      "\n",
      "= − 1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1 N(cid:4)\n",
      "\n",
      "i=1 N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "∇F (vi) −\n",
      "\n",
      "∇Z Z\n",
      "\n",
      "∇F (vi) +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "∇F (v)\n",
      "\n",
      "v\n",
      "\n",
      "∇F (vi) +E [∇F (v)]\n",
      "\n",
      "exp(−F (v)) Z\n",
      "\n",
      "(27.119)\n",
      "\n",
      "(27.120)\n",
      "\n",
      "(27.121)\n",
      "\n",
      "Plugging in the free energy (Equation 27.117), one can show that\n",
      "\n",
      "∂ ∂wrk\n",
      "\n",
      "F (v) = −vrE [hk|v, θ] = −E [vrhk|v, θ]\n",
      "\n",
      "(27.122)\n",
      "\n",
      "Hence ∂ ∂wrk\n",
      "\n",
      "(cid:2)(θ) =\n",
      "\n",
      "1 N\n",
      "\n",
      "N(cid:4)\n",
      "\n",
      "i=1\n",
      "\n",
      "E [vrhk|v, θ] − E [vrhk|θ]\n",
      "\n",
      "(27.123)\n",
      "\n",
      "which matches Equation 27.112.\n",
      "\n",
      "27.7. Restricted Boltzmann machines (RBMs)\n",
      "\n",
      "989\n",
      "\n",
      "(cid:43)(cid:77)\n",
      "\n",
      "(cid:31)(cid:59)(cid:76)(cid:43)(cid:77)(cid:33) (cid:19)\n",
      "\n",
      "(cid:31)(cid:59)(cid:76)(cid:43)(cid:77)(cid:33) (cid:20)\n",
      "\n",
      "(cid:31)(cid:59)(cid:76)(cid:43)(cid:77)(cid:33)\n",
      "\n",
      "(cid:76)(cid:81)(cid:73)(cid:76)(cid:81)(cid:76)(cid:87)(cid:92)\n",
      "\n",
      "(cid:59)(cid:76)\n",
      "\n",
      "(cid:55)(cid:3)(cid:32)(cid:3)(cid:19)(cid:29)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68)\n",
      "\n",
      "(cid:55)(cid:3)(cid:32)(cid:3)(cid:20)(cid:29)(cid:3)(cid:20)(cid:16)(cid:86)(cid:87)(cid:72)(cid:83)(cid:3) (cid:85)(cid:72)(cid:70)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)\n",
      "\n",
      "(cid:55)(cid:3)(cid:32)(cid:3)(cid:76)(cid:81)(cid:73)(cid:76)(cid:81)(cid:76)(cid:87)(cid:92)(cid:29)(cid:3)(cid:72)(cid:84)(cid:88)(cid:76)(cid:79)(cid:76)(cid:69)(cid:85)(cid:76)(cid:88)(cid:80)(cid:3) (cid:86)(cid:68)(cid:80)(cid:83)(cid:79)(cid:72)(cid:86)\n",
      "\n",
      "Figure 27.31 Illustration of Gibbs sampling in an RBM. The visible nodes are initialized at a datavector, then we sample a hidden vector, then another visible vector, etc. Eventually (at “inﬁnity”) we will be producing samples from the joint distribution p(v, h|θ).\n",
      "\n",
      "27.7.2.3\n",
      "\n",
      "Approximating the expectations\n",
      "\n",
      "We can approximate the expectations needed to evaluate the gradient by performing block Gibbs sampling, using Equations 27.101 and 27.102. In more detail, we can sample from the initialize the chain at vv1 (e.g. by setting v1 = vi for joint distribution p(v, h|θ) as follows: some data vector), and then sample from h1 ∼ p(h|v1), then from v2 ∼ p(v|h1), then from h2 ∼ p(h|v2), etc. See Figure 27.31 for an illustration. Note, however, that we have to wait until the Markov chain reaches equilibrium (i.e., until it has “burned in”) before we can interpret the samples as coming from the joint distribution of interest, and this might take a long time.\n",
      "\n",
      "A faster alternative is to use mean ﬁeld, where we make the approximation E [vrhk] ≈ E [vr] E [hk]. However, since p(v, h) is typically multimodal, this is usually a very poor approx- imation, since it will average over different modes (see Section 21.2.2). Furthermore, there is a more subtle reason not to use mean ﬁeld: since the gradient has the form E [vrhk|v]−E [vrhk], we see that the negative sign in front means that the method will try to make the variational bound as loose as possible (Salakhutdinov and Hinton 2009). This explains why earlier attempts to use mean ﬁeld to learn Boltzmann machines (e.g., (Kappen and Rodriguez 1998)) did not work.\n",
      "\n",
      "27.7.2.4\n",
      "\n",
      "Contrastive divergence\n",
      "\n",
      "The problem with using Gibbs sampling to compute the gradient is that it is slow. We now present a faster method known as contrastive divergence or CD (Hinton 2002). CD was originally derived by approximating an objective function deﬁned as the difference of two KL divergences, rather than trying to maximize the likelihood itself. However, from an algorithmic point of view, it can be thought of as similar to stochastic gradient descent, except it approxi- mates the “unclamped” expectations with “brief” Gibbs sampling where we initialize each Markov chain at the data vectors. That is, we approximate the gradient for one datavector as follows:\n",
      "\n",
      "∇w (cid:2) ≈ E\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "vhT |vi\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "− Eq\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "vhT\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "(27.124)\n",
      "\n",
      "where q corresponds to the distribution generated by K up-down Gibbs sweeps, started at vi, In more detail, the procedure (for K = 1) is as as in Figure 27.31. This is known as CD-K.\n",
      "\n",
      "990\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "follows:\n",
      "\n",
      "hi ∼ p(h|vi, θ) v(cid:2) i ∼ p(v|hi, θ) i ∼ p(h|v(cid:2) h(cid:2) i, θ)\n",
      "\n",
      "(27.125)\n",
      "\n",
      "(27.126)\n",
      "\n",
      "(27.127)\n",
      "\n",
      "We then make the approximation\n",
      "\n",
      "Eq\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "vhT\n",
      "\n",
      "(cid:14)\n",
      "\n",
      "≈ vi(h(cid:2)\n",
      "\n",
      "i)T\n",
      "\n",
      "(27.128)\n",
      "\n",
      "Such samples are sometimes called fantasy data. We can think of v(cid:2) i as the model’s best attempt at reconstructing vi after being coded and then decoded by the model. This is similar to the way we train auto-encoders, which are models which try to “squeeze” the data through a restricted parametric “bottleneck” (see Section 28.3.2).\n",
      "\n",
      "i in the ﬁnal upwards pass, since this reduces the variance. However, it is not valid to use E [h|vi] instead of sampling hi ∼ p(h|vi) in the earlier upwards passes, because then each hidden unit would be able to pass more than 1 bit of information, so it would not act as much of a bottleneck.\n",
      "\n",
      "In practice, it is common to use E [h|v(cid:2)\n",
      "\n",
      "i] instead of a sampled value h(cid:2)\n",
      "\n",
      "The whole procedure is summarized in Algorithm 3. (Note that we follow the positive gradient since we are maximizing likelihood.) Various tricks can be used to speed this algorithm up, such as using a momentum term (Section 8.3.2), using mini-batches, averaging the updates, etc. Such details can be found in (Hinton 2010; Swersky et al. 2010).\n",
      "\n",
      "10\n",
      "\n",
      "12\n",
      "\n",
      "13\n",
      "\n",
      "11\n",
      "\n",
      "Algorithm 27.3: CD-1 training for an RBM with binary hidden and visible units 1 Initialize weights W ∈ RR×K randomly; 2 t := 0; 3 for each epoch do t := t + 1 ; 4 for each minibatch of size B do\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "Set minibatch gradient to zero, g := 0 ; for each case vi in the minibatch do Compute μi = E [h|vi, W]; Sample hi ∼ p(h|vi, W); Sample v(cid:2) i ∼ p(v|hi, W); Compute μ(cid:2) i = E [h|v(cid:2) Compute gradient ∇W = (vi)(μi)T − (v(cid:2) Accumulate g := g + ∇W;\n",
      "\n",
      "i, W];\n",
      "\n",
      "i)(μ(cid:2)\n",
      "\n",
      "i)T ;\n",
      "\n",
      "14\n",
      "\n",
      "Update parameters W := W + (αt/B)g\n",
      "\n",
      "27.7.2.5\n",
      "\n",
      "Persistent CD\n",
      "\n",
      "In Section 19.5.5, we presented a technique called stochastic maximum likelihood (SML) for ﬁtting maxent models. This avoids the need to run MCMC to convergence at each iteration,\n",
      "\n",
      "27.7. Restricted Boltzmann machines (RBMs)\n",
      "\n",
      "991\n",
      "\n",
      "by exploiting the fact that the parameters are changing slowly, so the Markov chains will not be pushed too far from equilibrium after each update (Younes 1989). In other words, there are two dynamical processes running at different time scales: the states change quickly, and the parameters change slowly. This algorithm was independently rediscovered in (Tieleman 2008), who called it persistent CD. See Algorithm 3 for the pseudocode.\n",
      "\n",
      "PCD often works better than CD (see e.g., (Tieleman 2008; Marlin et al. 2010; Swersky et al.\n",
      "\n",
      "2010)), although CD can be faster in the early stages of learning.\n",
      "\n",
      "Algorithm 27.4: Persistent CD for training an RBM with binary hidden and visible units 1 Initialize weights W ∈ RD×L randomly; 2 Initialize chains (vs, hs)S s=1 randomly ; 3 for t = 1, 2, . . . do 4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "// Mean ﬁeld updates ; for each case i = 1 : N do\n",
      "\n",
      "μik = sigm(vT // MCMC updates ; for each sample s = 1 : S do\n",
      "\n",
      "Generate (vs, hs) by brief Gibbs sampling from old (vs, hs)\n",
      "\n",
      "i w:,k)\n",
      "\n",
      "// Parameter updates ; (cid:7)N g = 1 11 N 12 W := W + αtg; Decrease αt 13\n",
      "\n",
      "10\n",
      "\n",
      "i=1 vi(μi)T − 1 S\n",
      "\n",
      "(cid:7)S\n",
      "\n",
      "s=1 vs(hs)T ;\n",
      "\n",
      "27.7.3\n",
      "\n",
      "Applications of RBMs\n",
      "\n",
      "The main application of RBMs is as a building block for deep generative models, which we discuss in Section 28.2. But they can also be used as substitutes for directed two-layer models. They are particularly useful in cases where inference of the hidden states at test time must be fast. We give some examples below.\n",
      "\n",
      "27.7.3.1\n",
      "\n",
      "Language modeling and document retrieval\n",
      "\n",
      "We can use a categorical RBM to deﬁne a generative model for bag-of-words, as an alternative to LDA. One subtlety is that the partition function in an undirected models depends on how big the graph is, and therefore on how long the document is. A solution to this was proposed in (Salakhutdinov and Hinton 2010): use a categorical RBM with tied weights, but multiply the hidden activation bias terms ck by the document length L to compensate form the fact that the observed word-count vector v is larger in magnitude:\n",
      "\n",
      "E(v, h; θ) (cid:2) −\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "vchkW c\n",
      "\n",
      "k −\n",
      "\n",
      "C(cid:4)\n",
      "\n",
      "vcbc\n",
      "\n",
      "r − L\n",
      "\n",
      "K(cid:4)\n",
      "\n",
      "hkck\n",
      "\n",
      "(27.129)\n",
      "\n",
      "k=1\n",
      "\n",
      "c=1\n",
      "\n",
      "c=1\n",
      "\n",
      "k=1\n",
      "\n",
      "992\n",
      "\n",
      "Chapter 27. Latent variable models for discrete data\n",
      "\n",
      "Data set\n",
      "\n",
      "Number of docs\n",
      "\n",
      "K\n",
      "\n",
      "¯D\n",
      "\n",
      "St. Dev.\n",
      "\n",
      "Avg. Test perplexity per word (in nats)\n",
      "\n",
      "Train\n",
      "\n",
      "Test\n",
      "\n",
      "LDA-50\n",
      "\n",
      "LDA-200 R. Soft-50 Unigram\n",
      "\n",
      "NIPS 20-news Reuters\n",
      "\n",
      "1,690 11,314 794,414\n",
      "\n",
      "50 7,531 10,000\n",
      "\n",
      "13,649 2,000 10,000\n",
      "\n",
      "98.0 51.8 94.6\n",
      "\n",
      "245.3 70.8 69.3\n",
      "\n",
      "3576 1091 1437\n",
      "\n",
      "3391 1058 1142\n",
      "\n",
      "3405 953 988\n",
      "\n",
      "4385 1335 2208\n",
      "\n",
      "Figure 27.32 Comparison of RBM (replicated softmax) and LDA on three corpora. K is the number of words in the vocabulary, D is the average document length, and St. Dev. is the standard deviation of the document length. Source: (Salakhutdinov and Hinton 2010) .\n",
      "\n",
      ")\n",
      "\n",
      "%\n",
      "\n",
      "( n o i s i c e r P\n",
      "\n",
      "60\n",
      "\n",
      "50\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "20-newsgroups\n",
      "\n",
      "Replicated Softmax 50−D\n",
      "\n",
      "LDA 50−D\n",
      "\n",
      ")\n",
      "\n",
      "%\n",
      "\n",
      "( n o s c e r P\n",
      "\n",
      "i\n",
      "\n",
      "i\n",
      "\n",
      "50\n",
      "\n",
      "40\n",
      "\n",
      "30\n",
      "\n",
      "20\n",
      "\n",
      "Reuters\n",
      "\n",
      "Replicated Softmax 50−D\n",
      "\n",
      "LDA 50−D\n",
      "\n",
      "10\n",
      "\n",
      "10\n",
      "\n",
      "0.02 0.1 0.4 1.6 6.4 25.6 100 Recall (%)\n",
      "\n",
      "0.001 0.006 0.051 0.4 1.6 6.4 25.6 100 Recall (%)\n",
      "\n",
      "Figure 27.33 Precision-recall curves for RBM (replicated softmax) and LDA on two corpora. From Figure 3 of (Salakhutdinov and Hinton 2010). Used with kind permission of Ruslan Salakhutdinov.\n",
      "\n",
      "where vc = l=1 I(yil = c). This is like having a single multinomial node (so we have dropped the r subscript) with C states, where C is the number of words in the vocabulary. This is called the replicated softmax model (Salakhutdinov and Hinton 2010), and is an undirected alternative to mPCA/ LDA.\n",
      "\n",
      "(cid:7)L\n",
      "\n",
      "We can compare the modeling power of RBMs vs LDA by measuring the perplexity on a test set. This can be approximated using annealing importance sampling (Section 24.6.2). The results are shown in Figure 27.32. We see that the LDA is signiﬁcantly better than a unigram model, but that an RBM is signiﬁcantly better than LDA.\n",
      "\n",
      "just a single matrix-vector Another advantage of the LDA is that inference is fast and exact: multiply followed by a sigmoid nonlinearity, as in Equation 27.107. In addition to being faster, the RBM is more accurate. This is illustrated in Figure 27.33, which shows precision-recall curves for RBMs and LDA on two different corpora. These curves were generated as follows: a query document from the test set is taken, its similarity to all the training documents is computed, where the similarity is deﬁned as the cosine of the angle between the two topic vectors, and then the top M documents are returned for varying M . A retrieved document is considered relevant if it has the same class label as that of the query’s (this is the only place where labels are used).\n",
      "\n",
      "27.7. Restricted Boltzmann machines (RBMs)\n",
      "\n",
      "993\n",
      "\n",
      "27.7.3.2\n",
      "\n",
      "RBMs for collaborative ﬁltering\n",
      "\n",
      "RBMs have been applied to the Netﬂix collaborative ﬁltering competition (Salakhutdinov et al. In fact, an RBM with binary hidden nodes and categorical visible nodes can slightly 2007). outperform SVD. By combining the two methods, performance can be further improved. (The winning entry in the challenge was an ensemble of many different types of model (Koren 2009a).)\n",
      "\n",
      "Exercises\n",
      "\n",
      "Exercise 27.1 Partition function for an RBM Show how to compute Z(θ) for an RBM with K binary hidden nodes and R binary observed nodes in O(R2K ) time, assuming K < R.\n",
      "\n",
      "28 Deep learning\n",
      "\n",
      "28.1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Many of the models we have looked at in this book have a simple two-layer architecture of the form z → y for unsupervised latent variable models, or x → y for supervised models. However, when we look at the brain, we seem many levels of processing. It is believed that each level is learning features or representations at increasing levels of abstraction. For example, the standard model of the visual cortex (Hubel and Wiesel 1962; Serre et al. 2005; Ranzato et al. 2007) suggests that (roughly speaking) the brain ﬁrst extracts edges, then patches, then surfaces, then objects, etc. (See e.g., (Palmer 1999; Kandel et al. 2000) for more information about how the brain might perform vision.)\n",
      "\n",
      "(see e.g., replicate this kind of architecture in a computer. problems as well, such as speech and language.)\n",
      "\n",
      "This observation has inspired a recent trend in machine learning known as deep learning (Bengio 2009), deeplearning.net, and the references therein), which attempts to (Note the idea can be applied to non-vision\n",
      "\n",
      "In this chapter, we give a brief overview of this new ﬁeld. However, we caution the reader that the topic of deep learning is currently evolving very quickly, so the material in this chapter may soon be outdated.\n",
      "\n",
      "28.2 Deep generative models\n",
      "\n",
      "Deep models often have millions of parameters. Acquiring enough labeled data to train such models is diffcult, despite crowd sourcing sites such as Mechanical Turk. In simple settings, such as hand-written character recognition, it is possible to generate lots of labeled data by making modiﬁed copies of a small manually labeled training set (see e.g., Figure 16.13), but it seems unlikely that this approach will scale to complex scenes.1\n",
      "\n",
      "To overcome the problem of needing labeled training data, we will focus on unsupervised learning. The most natural way to perform this is to use generative models. In this section, we discuss three different kinds of deep generative models: directed, undirected, and mixed.\n",
      "\n",
      "1. There have been some attempts to use computer graphics and video games to generate realistic-looking images of complex scenes, and then to use this as training data for computer vision systems. However, often graphics programs cut corners in order to make perceptually appealing images which are not reﬂective of the natural statistics of real-world images.\n",
      "\n",
      "996\n",
      "\n",
      "Chapter 28. Deep learning\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 28.1 Some deep multi-layer graphical models. Observed variables are at the bottom. (a) A directed model. (b) An undirected model (deep Boltzmann machine). (c) A mixed directed-undirected model (deep belief net).\n",
      "\n",
      "28.2.1\n",
      "\n",
      "Deep directed networks\n",
      "\n",
      "Perhaps the most natural way to build a deep generative model is to construct a deep directed graphical model, as shown in Figure 28.1(a). The bottom level contains the observed pixels (or whatever the data is), and the remaining layers are hidden. We have assumed just 3 layers for notational simplicity. The number and size of layers is usually chosen by hand, although one can also use non-parametric Bayesian methods (Adams et al. 2010) or boosting (Chen et al. 2010) to infer the model structure.\n",
      "\n",
      "binary, and all CPDs are logistic functions, this is called a sigmoid belief net (Neal 1992). this case, the model deﬁnes the following joint distribution: (cid:20)\n",
      "\n",
      "We shall call models of this form deep directed networks or DDNs.\n",
      "\n",
      "p(h1, h2, h3, v|θ) =\n",
      "\n",
      "Ber(vi|sigm(hT\n",
      "\n",
      "1 w0i))\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Ber(h1j|sigm(hT\n",
      "\n",
      "2 w1j))\n",
      "\n",
      "If all the nodes are In\n",
      "\n",
      "(28.1)\n",
      "\n",
      "i (cid:20)\n",
      "\n",
      "Ber(h2k|sigm(hT\n",
      "\n",
      "3 w2k))\n",
      "\n",
      "j\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Ber(h3l|w3l)\n",
      "\n",
      "(28.2)\n",
      "\n",
      "k\n",
      "\n",
      "l\n",
      "\n",
      "Unfortunately, inference in directed models such as these is intractable because the posterior on the hidden nodes is correlated due to explaining away. One can use fast mean ﬁeld approxi- mations (Jaakkola and Jordan 1996a; Saul and Jordan 2000), but these may not be very accurate, since they approximate the correlated posterior with a factorial posterior. One can also use MCMC inference (Neal 1992; Adams et al. 2010), but this can be quite slow because the variables are highly correlated. Slow inference also results in slow learning.\n",
      "\n",
      "28.2.2\n",
      "\n",
      "Deep Boltzmann machines\n",
      "\n",
      "A natural alternative to a directed model is to construct a deep undirected model. For example, we can stack a series of RBMs on top of each other, as shown in Figure 28.1(b). This is known as a deep Boltzmann machine or DBM (Salakhutdinov and Hinton 2009). If we have 3 hidden layers, the model is deﬁned as follows:\n",
      "\n",
      "⎛\n",
      "\n",
      "⎞\n",
      "\n",
      "p(h1, h2, h3, v|θ) =\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "exp\n",
      "\n",
      "⎝\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "ij\n",
      "\n",
      "vih1jW1ij +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "jk\n",
      "\n",
      "h1jh2jW2jk +\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "kl\n",
      "\n",
      "h2kh3lW3kl\n",
      "\n",
      "⎠(28.3)\n",
      "\n",
      "28.2. Deep generative models\n",
      "\n",
      "997\n",
      "\n",
      "where we are ignoring constant offset or bias terms.\n",
      "\n",
      "The main advantage over the directed model is that one can perform efficient block (layer- wise) Gibbs sampling, or block mean ﬁeld, since all the nodes in each layer are conditionally independent of each other given the layers above and below (Salakhutdinov and Larochelle 2010). The main disadvantage is that training undirected models is more difficult, because of the partition function. However, below we will see a greedy layer-wise strategy for learning deep undirected models.\n",
      "\n",
      "28.2.3\n",
      "\n",
      "Deep belief networks\n",
      "\n",
      "An interesting compromise is to use a model that is partially directed and partially undirected. In particular, suppose we construct a layered model which has directed arrows, except at the top, where there is an undirected bipartite graph, as shown in Figure 28.1(c). This model is known as a deep belief network (Hinton et al. 2006) or DBN.2 If we have 3 hidden layers, the model is deﬁned as follows:\n",
      "\n",
      "p(h1, h2, h3, v|θ) =\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Ber(vi|sigm(hT\n",
      "\n",
      "1 w1i)\n",
      "\n",
      "(cid:20)\n",
      "\n",
      "Ber(h1j|sigm(hT\n",
      "\n",
      "2 w2j)\n",
      "\n",
      "(28.4)\n",
      "\n",
      "i\n",
      "\n",
      "1 Z(θ)\n",
      "\n",
      "exp\n",
      "\n",
      "(cid:10)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "kl\n",
      "\n",
      "j\n",
      "\n",
      "h2kh3lW3kl\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(28.5)\n",
      "\n",
      "Essentially the top two layers act as an associative memory, and the remaining layers then generate the output.\n",
      "\n",
      "The advantage of this peculiar architecture is that we can infer the hidden states in a fast, bottom-up fashion. To see why, suppose we only have two hidden layers, and that W2 = WT 1 , so the second level weights are tied to the ﬁrst level weights (see Figure 28.2(a)). This deﬁnes a model of the form p(h1, h2, v|W1). One can show that the distribution Z(W1) exp(vT W1h1), p(h1, h2, v|W1) has the form p(h1, v|W1) = p(h1, v|W1) = which is equivalent to an RBM. Since the DBN is equivalent to the RBM as far as p(h1, v|W1) is concerned, we can infer the posterior p(h1|v, W1) in the DBN exactly as in the RBM. This posterior is exact, even though it is fully factorized.\n",
      "\n",
      "Now the only way to get a factored posterior is if the prior p(h1|W1) is a complementary prior. This is a prior which, when multiplied by the likelihood p(v|h1), results in a perfectly factored posterior. Thus we see that the top level RBM in a DBN acts as a complementary prior for the bottom level directed sigmoidal likelihood function.\n",
      "\n",
      "(cid:7)\n",
      "\n",
      "h2\n",
      "\n",
      "1\n",
      "\n",
      "If we have multiple hidden levels, and/or if the weights are not tied, the correspondence between the DBN and the RBM does not hold exactly any more, but we can still use the factored inference rule as a form of approximate bottom-up inference. Below we show that this is a valid variational lower bound. This bound also suggests a layer-wise training strategy, that we will explain in more detail later. Note, however, that top-down inference in a DBN is not tractable, so DBNs are usually only used in a feedforward manner.\n",
      "\n",
      "2. Unforuntately the acronym “DBN” also stands for “dynamic Bayes net” (Section 17.6.7). Geoff Hinton, who invented deep belief networks, has suggested the acronyms DeeBNs and DyBNs for these two different meanings. However, this terminology is non-standard.\n",
      "\n",
      "998\n",
      "\n",
      "Chapter 28. Deep learning\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "Figure 28.2 (a) A DBN with two hidden layers and tied weights that is equivalent to an RBM. Source: Figure 2.2 of (Salakhutdinov 2009). (c) The corresponding DBN. Source: Figure 2.3 of (Salakhutdinov 2009). Used with kind permission of Ruslan Salakhutdinov.\n",
      "\n",
      "(b) A stack of RBMs trained greedily.\n",
      "\n",
      "28.2.4\n",
      "\n",
      "Greedy layer-wise learning of DBNs\n",
      "\n",
      "The equivalence between DBNs and RBMs suggests the following strategy for learning a DBN.\n",
      "\n",
      "Fit an RBM to learn W1 using methods described in Section 27.7.2. • Unroll the RBM into a DBN with 2 hidden layers, as in Figure 28.2(a). Now “freeze” the directed weights W1 and let W2 be “untied” so it is no longer forced to be equal to WT 1 . We will now learn a better prior for p(h1|W2) by ﬁtting a second RBM. The input data to this new RBM is the activation of the hidden units E [h1|v, W1] which can be computed using a factorial approximation.\n",
      "\n",
      "Continue to add more hidden layers until some stopping criterion is satisiﬁed, e.g., you run out of time or memory, or you start to overﬁt the validation set. Construct the DBN from these RBMs, as illustrated in Figure 28.2(c).\n",
      "\n",
      "One can show (Hinton et al. 2006) that this procedure always increases a lower bound the observed data likelihood. Of course this procedure might result in overﬁtting, but that is a different matter.\n",
      "\n",
      "In practice, we want to be able to use any number of hidden units in each level. This means we will not be able to initialize the weights so that W(cid:8) = WT (cid:8)−1. This voids the theoretical guarantee. Nevertheless the method works well in practice, as we will see. The method can also be extended to train DBMs in a greedy way (Salakhutdinov and Larochelle 2010).\n",
      "\n",
      "After using the greedy layer-wise training strategy, it is standard to “ﬁne tune” the weights, using a technique called backﬁtting. This works as follows. Perform an upwards sampling pass to the top. Then perform brief Gibbs sampling in the top level RBM, and perform a CD update of the RBM parameters. Finally, perform a downwards ancestral sampling pass (which is an approximate sample from the posterior), and update the logistic CPD parameters using a small gradient step. This is called the up-down procedure (Hinton et al. 2006). Unfortunately this procedure is very slow.\n",
      "\n",
      "28.3. Deep neural networks\n",
      "\n",
      "999\n",
      "\n",
      "28.3 Deep neural networks\n",
      "\n",
      "Given that DBNs are often only used in a feed-forward, or bottom-up, mode, they are effectively acting like neural networks. In view of this, it is natural to dispense with the generative story and try to ﬁt deep neural networks directly, as we discuss below. The resulting training methods are often simpler to implement, and can be faster.\n",
      "\n",
      "Note, however, that performance with deep neural nets is sometimes not as good as with probabilistic models (Bengio et al. 2007). One reason for this is that probabilistic models support top-down inference as well as bottom-up inference. (DBNs do not support efficient top-down inference, but DBMs do, and this has been shown to help (Salakhutdinov and Larochelle 2010).) Top-down inference is useful when there is a lot of ambiguity about the correct interpretation of the signal.\n",
      "\n",
      "It is interesting to note that in the mammalian visual cortex, there are many more feedback connections than there are feedforward connections (see e.g., (Palmer 1999; Kandel et al. 2000)). The role of these feedback connections is not precisely understood, but they presumably provide contextual prior information (e.g., coming from the previous “frame” or retinal glance) which can be used to disambiguate the current bottom-up signals (Lee and Mumford 2003).\n",
      "\n",
      "Of course, we can simulate the effect of top-down inference using a neural network. However\n",
      "\n",
      "the models we discuss below do not do this.\n",
      "\n",
      "28.3.1\n",
      "\n",
      "Deep multi-layer perceptrons\n",
      "\n",
      "Many decision problems can be reduced to classiﬁcation, e.g., predict which object (if any) is present in an image patch, or predict which phoneme is present in a given acoustic feature vector. We can solve such problems by creating a deep feedforward neural network or multi- layer perceptron (MLP), as in Section 16.5, and then ﬁtting the parameters using gradient descent (aka back-propagation).\n",
      "\n",
      "Unfortunately, this method does not work very well. One problem is that the gradient becomes weaker the further we move away from the data; this is known as the “vanishing gradient” problem (Bengio and Frasconi 1995). A related problem is that there can be large plateaus in the error surface, which cause simple ﬁrst-order gadient-based methods to get stuck (Glorot and Bengio 2010).\n",
      "\n",
      "Consequently early attempts to learn deep neural networks proved unsuccesful. Recently there has been some progress, due to the adoption of GPUs (Ciresan et al. 2010) and second-order optimization algorithms (Martens 2010). Nevertheless, such models remain difficult to train.\n",
      "\n",
      "Below we discuss a way to initialize the parameters using unsupervised learning; this is called generative pre-training. The advantage of performing unsupervised learning ﬁrst is that the model is forced to model a high-dimensional response, namely the input feature vector, rather than just predicting a scalar response. This acts like a data-induced regularizer, and helps backpropagation ﬁnd local minima with good generalization properties (Erhan et al. 2010; Glorot and Bengio 2010).\n",
      "\n",
      "1000\n",
      "\n",
      "Chapter 28. Deep learning\n",
      "\n",
      "$(cid:14)(cid:30)(cid:21)(cid:31)(cid:14)(cid:13)\n",
      "\n",
      "(cid:23)(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:9)(cid:5)(cid:5)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:10)(cid:21)% (cid:6)(cid:7)(cid:8)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:10) (cid:3)\n",
      "\n",
      "(cid:10) (cid:2) (cid:2)(cid:1) (cid:3)\n",
      "\n",
      "(cid:28)\n",
      "\n",
      "(cid:4)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:4)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:9)(cid:5)(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:10) (cid:4)\n",
      "\n",
      "(cid:10) (cid:2) (cid:2)(cid:1) (cid:4)\n",
      "\n",
      "(cid:27)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:3)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:6)(cid:7)(cid:8)\n",
      "\n",
      "(cid:3)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:10) (cid:23)\n",
      "\n",
      "(cid:9)(cid:5)(cid:5)\n",
      "\n",
      "(cid:3)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:10) (cid:2) (cid:2)(cid:1) (cid:23)\n",
      "\n",
      "(cid:9)(cid:5)(cid:5)\n",
      "\n",
      "(cid:26)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:10) (cid:11)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:10) (cid:11)\n",
      "\n",
      "(cid:2)(cid:1)\n",
      "\n",
      "(cid:9)\n",
      "\n",
      "(cid:3)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:23)(cid:5)\n",
      "\n",
      "!(cid:21)(cid:31)(cid:14)\"(cid:22)(cid:16)#(cid:14)(cid:13)\n",
      "\n",
      "(cid:23)(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:4)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:6)(cid:7)(cid:8)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:9)(cid:5)(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:2) (cid:2)(cid:1) (cid:11)\n",
      "\n",
      "(cid:9)(cid:5)(cid:5)\n",
      "\n",
      "(cid:2) (cid:2)(cid:1) (cid:23)\n",
      "\n",
      "(cid:11)\n",
      "\n",
      "(cid:23)\n",
      "\n",
      "(cid:3)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:3)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:2) (cid:2)(cid:1) (cid:4)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(cid:4)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:4)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:4)(cid:5)(cid:5)(cid:5)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:2)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:2) (cid:2)(cid:1) (cid:3)\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "(cid:6)(cid:7)(cid:8)\n",
      "\n",
      "(cid:29)(cid:18)(cid:30)(cid:21)(cid:31)(cid:14)(cid:13)\n",
      "\n",
      "(cid:12)(cid:13)(cid:14)(cid:15)(cid:13)(cid:16)(cid:17)(cid:18)(cid:17)(cid:18)(cid:19)\n",
      "\n",
      "(cid:20)(cid:18)(cid:13)(cid:21)(cid:22)(cid:22)(cid:17)(cid:18)(cid:19)\n",
      "\n",
      "(cid:24)(cid:17)(cid:18)(cid:14)(cid:1)(cid:15)(cid:25)(cid:18)(cid:17)(cid:18)(cid:19)\n",
      "\n",
      "Figure 28.3 Training a deep autoencoder. (a) First we greedily train some RBMs. (b) Then we construct the auto-encoder by replicating the weights. (c) Finally we ﬁne-tune the weights using back-propagation. From Figure 1 of (Hinton and Salakhutdinov 2006). Used with kind permission of Ruslan Salakhutdinov.\n",
      "\n",
      "28.3.2\n",
      "\n",
      "Deep auto-encoders\n",
      "\n",
      "An auto-encoder is a kind of unsupervised neural network that is used for dimensionality reduction and feature discovery. More precisely, an auto-encoder is a feedforward neural network that is trained to predict the input itself. To prevent the system from learning the trivial identity mapping, the hidden layer in the middle is usually constrained to be a narrow bottleneck. The system can minimize the reconstruction error by ensuring the hidden units capture the most relevant aspects of the data.\n",
      "\n",
      "Suppose the system has one hidden layer, so the model has the form v → h → v. Further, In this case, one can show that the weights to the K suppose all the functions are linear. hidden units will span the same subspace as the ﬁrst K principal components of the data (Karhunen and Joutsensalo 1995; Japkowicz et al. 2000). In other words, linear auto-encoders are equivalent to PCA. However, by using nonlinear activation functions, one can discover nonlinear representations of the data.\n",
      "\n",
      "More powerful representations can be learned by using deep auto-encoders. Unfortunately training such models using back-propagation does not work well, because the gradient signal becomes too small as it passes back through multiple layers, and the learning algorithm often gets stuck in poor local minima.\n",
      "\n",
      "One solution to this problem is to greedily train a series of RBMs and to use these to initialize an auto-encoder, as illustrated in Figure 28.3. The whole system can then be ﬁne-tuned using backprop in the usual fashion. This approach, ﬁrst suggested in (Hinton and Salakhutdinov\n",
      "\n",
      "28.4. Applications of deep networks\n",
      "\n",
      "1001\n",
      "\n",
      "2000 top-level units\n",
      "\n",
      "10 label units\n",
      "\n",
      "500 units\n",
      "\n",
      "This could be the top level of another sensory pathway\n",
      "\n",
      "500 units\n",
      "\n",
      "28 x 28 pixel image\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 28.4 Used with kind permission of Geoff Hinton. test cases of MNIST. Above each image is the estimated label. Used with kind permission of Geoff Hinton. Compare to Figure 16.15.\n",
      "\n",
      "(a) A DBN architecture for classifying MNIST digits. Source: Figure 1 of (Hinton et al. 2006). (b) These are the 125 errors made by the DBN on the 10,000 Source: Figure 6 of (Hinton et al. 2006).\n",
      "\n",
      "2006), works much better than trying to ﬁt the deep auto-encoder directly starting with random weights.\n",
      "\n",
      "28.3.3\n",
      "\n",
      "Stacked denoising auto-encoders\n",
      "\n",
      "A standard way to train an auto-encoder is to ensure that the hidden layer is narrower than the visible layer. This prevents the model from learning the identity function. But there are other ways to prevent this trivial solution, which allow for the use of an over-complete representation. One approach is to impose sparsity constraints on the activation of the hidden units (Ranzato et al. 2006). Another approach is to add noise to the inputs; this is called a denoising auto- encoder (Vincent et al. 2010). For example, we can corrupt some of the inputs, for example by setting them to zero, so the model has to learn to predict the missing entries. This can be shown to be equivalent to a certain approximate form of maximum likelihood training (known as score matching) applied to an RBM (Vincent 2011).\n",
      "\n",
      "Of course, we can stack these models on top of each other to learn a deep stacked denoising auto-encoder, which can be discriminatively ﬁne-tuned just like a feedforward neural network, if desired.\n",
      "\n",
      "28.4\n",
      "\n",
      "Applications of deep networks\n",
      "\n",
      "In this section, we mention a few applications of the models we have been discussing.\n",
      "\n",
      "28.4.1\n",
      "\n",
      "Handwritten digit classiﬁcation using DBNs\n",
      "\n",
      "Figure 28.4(a) shows a DBN (from (Hinton et al. 2006)) consisting of 3 hidden layers. The visible layer corresponds to binary images of handwritten digits from the MNIST data set. In addition, the top RBM is connected to a softmax layer with 10 units, representing the class label.\n",
      "\n",
      "1002\n",
      "\n",
      "Chapter 28. Deep learning\n",
      "\n",
      "Interbank Markets\n",
      "\n",
      "European Community Monetary/Economic\n",
      "\n",
      "Energy Markets\n",
      "\n",
      "Disasters and Accidents\n",
      "\n",
      "Leading Economic Indicators\n",
      "\n",
      "Legal/Judicial\n",
      "\n",
      "Accounts/ Earnings\n",
      "\n",
      "Government Borrowings\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 28.5 2d visualization of some bag of words data from the Reuters RCV1-v2 corpus. (a) Results of using LSA. (b) results of using a deep auto-encoder. Source: Figure 4 of (Hinton and Salakhutdinov 2006). Used with kind permission of Ruslan Salakhutdinov.\n",
      "\n",
      "The ﬁrst 2 hidden layers were trained in a greedy unsupervised fashion from 50,000 MNIST digits, using 30 epochs (passes over the data) and stochastic gradient descent, with the CD heuristic. This process took “a few hours per layer” (Hinton et al. 2006, p1540). Then the top layer was trained using as input the activations of the lower hidden layer, as well as the class labels. The corresponding generative model had a test error of about 2.5%. The network weights were then carefully ﬁne-tuned on all 60,000 training images using the up-down procedure. This process took “about a week” (Hinton et al. 2006, p1540). The model can be used to classify by performing a deterministic bottom-up pass, and then computing the free energy for the top-level RBM for each possible class label. The ﬁnal error on the test set was about 1.25%. The misclassiﬁed examples are shown in Figure 28.4(b).\n",
      "\n",
      "This was the best error rate of any method on the permutation-invariant version of MNIST (By permutation-invariant, we mean a method that does not exploit the fact that at that time. the input is an image. Generic methods work just as well on permuted versions of the input (see Figure 1.5), and can therefore be applied to other kinds of datasets.) The only other method that comes close is an SVM with a degree 9 polynomial kernel, which has achieved an error rate of 1.4% (Decoste and Schoelkopf 2002). By way of comparison, 1-nearest neighbor (using all 60,000 examples) achieves 3.1% (see mnist1NNdemo). This is not as good, although 1-NN is much simpler.3\n",
      "\n",
      "28.4.2\n",
      "\n",
      "Data visualization and feature discovery using deep auto-encoders\n",
      "\n",
      "Deep autoencoders can learn informative features from raw data. Such features are often used as input to standard supervised learning methods.\n",
      "\n",
      "To illustrate this, consider ﬁtting a deep auto-encoder with a 2d hidden bottleneck to some\n",
      "\n",
      "3. One can get much improved performance on this task by exploiting the fact that the input is an image. One way to do this is to create distorted versions of the input, adding small shifts and translations (see Figure 16.13 for some examples). Applying this trick reduced the SVM error rate to 0.56%. Similar error rates can be achieved using convolutional neural networks (Section 16.5.1) trained on distorted images ((Simard et al. 2003) got 0.4%). However, the point of DBNs is that they offer a way to learn such prior knowledge, without it having to be hand-crafted.\n",
      "\n",
      "28.4. Applications of deep networks\n",
      "\n",
      "1003\n",
      "\n",
      "(cid:11)(cid:2)\n",
      "\n",
      "(cid:7)(cid:2)\n",
      "\n",
      "(cid:26)(cid:27)(cid:28)(cid:24)(cid:13)(cid:25)(cid:14)(cid:24)(cid:29)(cid:13)(cid:21)(cid:5)(cid:4)(cid:2)(cid:30) (cid:31)!(cid:26)(cid:5)(cid:4)(cid:2)(cid:30) (cid:31)!(cid:26)(cid:5)(cid:11)(cid:2)(cid:30) (cid:26)(cid:27)(cid:28)(cid:24)(cid:13)(cid:25)(cid:14)(cid:24)(cid:29)(cid:13)(cid:21)(cid:5)(cid:4)(cid:2)(cid:30) \"(cid:21)(cid:22)(cid:24)(cid:21)(cid:5)(cid:28)(cid:24)(cid:5)#(cid:22)(cid:25)(cid:13)(cid:1)(cid:28)(cid:27)(cid:25)(cid:22)(cid:25)$\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:17) (cid:5) (cid:25) (cid:24) (cid:23)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:22)\n",
      "\n",
      "(cid:14) (cid:13) (cid:21) (cid:20)\n",
      "\n",
      "(cid:10)(cid:2)\n",
      "\n",
      "(cid:6)(cid:2)\n",
      "\n",
      "(cid:4)(cid:2)\n",
      "\n",
      "(cid:2)(cid:3)(cid:4)(cid:5)\n",
      "\n",
      "(cid:2)(cid:3)(cid:6)(cid:5)\n",
      "\n",
      "(cid:2)(cid:3)(cid:7)(cid:5)\n",
      "\n",
      "(cid:2)(cid:3)(cid:8)(cid:5)\n",
      "\n",
      "(cid:4)(cid:3)(cid:9)(cid:5) (cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:16)(cid:5)(cid:17)(cid:18)(cid:19)(cid:5)\n",
      "\n",
      "(cid:10)(cid:3)(cid:6)(cid:5)\n",
      "\n",
      "(cid:9)(cid:3)(cid:7)(cid:5)\n",
      "\n",
      "(cid:4)(cid:6)(cid:3)(cid:8) (cid:6)(cid:11)(cid:3)(cid:9) (cid:11)(cid:4)(cid:3)(cid:6) (cid:4)(cid:2)(cid:2)(cid:5)\n",
      "\n",
      "Figure 28.6 Precision-recall curves for document retrieval in the Reuters RCV1-v2 corpus. Source: Figure 3.9 of (Salakhutdinov 2009). Used with kind permission of Ruslan Salakhutdinov.\n",
      "\n",
      "text data. The results are shown in Figure 28.5. On the left we show the 2d embedding produced by LSA (Section 27.2.2), and on the right, the 2d embedding produced by the auto-encoder. It is clear that the low-dimensional representation created by the auto-encoder has captured a lot of the meaning of the documents, even though class labels were not used.4\n",
      "\n",
      "Note that various other ways of learning low-dimensional continuous embeddings of words\n",
      "\n",
      "have been proposed. See e.g., (Turian et al. 2010) for details.\n",
      "\n",
      "28.4.3\n",
      "\n",
      "Information retrieval using deep auto-encoders (semantic hashing)\n",
      "\n",
      "In view of the sucess of RBMs for information retrieval discussed in Section 27.7.3.1, it is natural to wonder if deep models can do even better. In fact they can, as is shown in Figure 28.6.\n",
      "\n",
      "More interestingly, we can use a binary low-dimensional representation in the middle layer of the deep auto-encoder, rather than a continuous representation as we used above. This enables very fast retrieval of related documents. For example, if we use a 20-bit code, we can precompute the binary representation for all the documents, and then create a hash-table mapping codewords to documents. This approach is known as semantic hashing, since the binary representation of semantically similar documents will be close in Hamming distance.\n",
      "\n",
      "For the 402,207 test documents in Reuters RCV1-v2, this results in about 0.4 documents per entry in the table. At test time, we compute the codeword for the query, and then simply retrieve the relevant documents in constant time by looking up the contents of the relevant address in memory. To ﬁnd other other related documents, we can compute all the codewords within a\n",
      "\n",
      "4. Some details. Salakhutdinov and Hinton used the Reuters RCV1-v2 data set, which consists of 804,414 newswire articles, manually classiﬁed into 103 topics. They represent each document by counting how many times each of the top 2000 most frequent words occurs. They trained a deep auto-encoder with 2000-500-250-125-2 layers on half of the data. The 2000 visible units use a replicated softmax distribution, the 2 hidden units in the middle layer have a Gaussian distribution, and the remaining units have the usual Bernoulli-logistic distribution. When ﬁne tuning the auto-encoder, a cross-entropy loss function (equivalent to maximum likelihood under a multinoulli distribution) was used. See (Hinton and Salakhutdinov 2006) for further details.\n",
      "\n",
      "1004\n",
      "\n",
      "Chapter 28. Deep learning\n",
      "\n",
      "h1 1\n",
      "\n",
      "w1\n",
      "\n",
      "h1 2\n",
      "\n",
      "h1 3 w1\n",
      "\n",
      "w1\n",
      "\n",
      "h2 1\n",
      "\n",
      "w2\n",
      "\n",
      "h2 2\n",
      "\n",
      "w2\n",
      "\n",
      "h2 3 w2\n",
      "\n",
      "x1\n",
      "\n",
      "x2\n",
      "\n",
      "x3\n",
      "\n",
      "x4\n",
      "\n",
      "Figure 28.7 A small 1d convolutional RBM with two groups of hidden units, each associated with a ﬁlter of size 2. h1 1 are two different “views” of the data in the ﬁrst window, (x1, x2). The ﬁrst view is computed using the ﬁlter w1, the second view using ﬁlter w2. Similarly, h1 2 are the views of the data in the second window, (x2, x3), computed using w1 and w2 respectively.\n",
      "\n",
      "1 and h2\n",
      "\n",
      "2 and h2\n",
      "\n",
      "Hamming distance of, say, 4. This results in retrieving about 6196 × 0.4 ≈ 2500 documents5. The key point is that the total time is independent of the size of the corpus.\n",
      "\n",
      "Of course, there are other techniques for fast document retrieval, such as inverted indices. These rely on the fact that individual words are quite informative, so we can simply intersect all the documents that contain each word. However, when performing image retrieval, it is clear that we do not want to work at the pixel level. Recently (Krizhevsky and Hinton 2010) showed that a deep autoencoder could learn a good semantic hashing function that outperformed previous It techniques (Torralba et al. 2008; Weiss et al. 2008) on the 80 million tiny images dataset. is hard to apply inverted indexing techniques to real-valued data (although one could imagine vector quantizing image patches).\n",
      "\n",
      "28.4.4\n",
      "\n",
      "Learning audio features using 1d convolutional DBNs\n",
      "\n",
      "To apply DBNs to time series of unbounded length, it is necessary to use some form of parameter tying. One way to do this is to use convolutional DBNs (Lee et al. 2009; Desjardins and Bengio 2008), which use convolutional RBMs as their basic unit. These models are a generative version of convolutional neural nets discussed in Section 16.5.1. The basic idea is illustrated in Figure 28.7. The hidden activation vector for each group is computed by convolving the input vector with that group’s ﬁlter (weight vector or matrix). In other words, each node within a hidden group is a weighted combination of a subset of the inputs. We compute the activaton of all the hidden nodes by “sliding” this weight vector over the input. This allows us to model translation invariance, since we use the same weights no matter where in the input vector the pattern occurs.6 Each group has its own ﬁlter, corresponding to its own pattern detector.\n",
      "\n",
      "5. Note that 6196 = 6. It is often said that the goal of deep learnng is to discover invariant features, e.g., a representation of an object that does not change even as nuisance variables, such as the lighting, do change. However, sometimes these so-called “nuisance variables” may be the variables of interest. For example if the task is to determine if a photograph was taken in the morning or the evening, then lighting is one of the more salient features, and object identity may be less relevant. As always, one task’s “signal” is another task’s “noise”, so it unwise to “throw away” apparently irrelevant information\n",
      "\n",
      "(cid:2)4\n",
      "\n",
      "k=0\n",
      "\n",
      "(cid:12)20 k\n",
      "\n",
      "(cid:13)\n",
      "\n",
      "is the number of bit vectors that are up to a Hamming distance of 4 away.\n",
      "\n",
      "28.5. Discussion\n",
      "\n",
      "1005\n",
      "\n",
      "More formally, for binary 1d signals, we can deﬁne the full conditionals in a convolutional\n",
      "\n",
      "RBM as follows (Lee et al. 2009):\n",
      "\n",
      "p(hk p(vs = 1|h) = sigm(\n",
      "\n",
      "t = 1|v) = sigm((wk ⊗ v)t + bt)\n",
      "\n",
      "(cid:4)\n",
      "\n",
      "(wk ⊗ hk)s + cs)\n",
      "\n",
      "(28.6)\n",
      "\n",
      "(28.7)\n",
      "\n",
      "k\n",
      "\n",
      "where wk is the weight vector for group k, bt and cs are bias terms, and a ⊗ b represents the convolution of vectors a and b.\n",
      "\n",
      "It is common to add a max pooling layer as well as a convolutional layer, which computes a local maximum over the ﬁltered response. This allows for a small amount of translation invariance. It also reduces the size of the higher levels, which speeds up computation consider- ably. Deﬁning this for a neural network is simple, but deﬁning this in a way which allows for information ﬂow backwards as well as forwards is a bit more involved. The basic idea is similar to a noisy-OR CPD (Section 10.2.3), where we deﬁne a probabilistic relationship between the max node and the parts it is maxing over. See (Lee et al. 2009) for details. Note, however, that the top-down generative process will be difficult, since the max pooling operation throws away so much information.\n",
      "\n",
      "(Lee et al. 2009) applies 1d convolutional DBNs of depth 2 to auditory data. When the input consists of speech signals, the method recovers a representation that is similar to phonemes. When applied to music classiﬁcation and speaker identiﬁcation, their method outperforms tech- niques using standard features such as MFCC. (All features were fed into the same discriminative classiﬁer.)\n",
      "\n",
      "In (Seide et al. 2011), a deep neural net was used in place of a GMM inside a conventional HMM. The use of DNNs signiﬁcantly improved performance on conversational speech recogni- tion. In an interview, the tech lead of this project said “historically, there have been very few individual technologies in speech recognition that have led to improvements of this magnitude”.7\n",
      "\n",
      "28.4.5\n",
      "\n",
      "Learning image features using 2d convolutional DBNs\n",
      "\n",
      "We can extend a convolutional DBN from 1d to 2d in a straightforward way (Lee et al. 2009), as illustrated in Figure 28.8. The results of a 3 layer system trained on four classes of visual objects (cars, motorbikes, faces and airplanes) from the Caltech 101 dataset are shown in Figure 28.9. We only show the results for layers 2 and 3, because layer 1 learns Gabor-like ﬁlters that are very similar to those learned by sparse coding, shown in Figure 13.21(b). We see that layer 2 has learned some generic visual parts that are shared amongst object classes, and layer 3 seems to have learned ﬁlters that look like grandmother cells, that are speciﬁc to individual object classes, and in some cases, to individual objects.\n",
      "\n",
      "28.5 Discussion\n",
      "\n",
      "So far, we have been discussing models inspired by low-level processing in the brain. These models have produced useful features for simple classiﬁcation tasks. But can this pure bottom-up\n",
      "\n",
      "too early. 7. Source: http://research.microsoft.com/en-us/news/features/speechrecognition-082911.aspx.\n",
      "\n",
      "1006\n",
      "\n",
      "Chapter 28. Deep learning\n",
      "\n",
      "Figure 28.8 A 2d convolutional RBM with max-pooling layers. The input signal is a stack of 2d images (e.g., color planes). Each input layer is passed through a different set of ﬁlters. Each hidden unit is obtained by convolving with the appropriate ﬁlter, and then summing over the input planes. The ﬁnal layer is obtained by computing the local maximum within a small window. Source: Figure 1 of (Chen et al. 2010) . Used with kind permission of Bo Chen.\n",
      "\n",
      "faces, cars, airplanes, motorbikes\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 28.9 Visualization of the ﬁlters learned by a convolutional DBN in layers two and three. Source: Figure 3 of (Lee et al. 2009). Used with kind permission of Honglak Lee.\n",
      "\n",
      "approach scale to more challenging problems, such as scene interpretation or natural language understanding?\n",
      "\n",
      "To put the problem in perspective, consider the DBN for handwritten digit classiﬁcation in Figure 28.4(a). This has about 1.6M free parameters (28 × 28 × 500 + 500 × 500 + 510 × 2000 = 1, 662, 000). Although this is a lot, it is tiny compared to the number of neurons in the brain. As Hinton says,\n",
      "\n",
      "This is about as many parameters as 0.002 cubic millimetres of mouse cortex, and several hundred networks of this complexity could ﬁt within a single voxel of a high-resolution fMRI scan. This suggests that much bigger networks may be required to compete with human shape recognition abilities. — (Hinton et al. 2006, p1547).\n",
      "\n",
      "To scale up to more challenging problems, various groups are using GPUs (see e.g., (Raina et al. 2009)) and/or parallel computing. But perhaps a more efficient approach is to work at a higher level of abstraction, where inference is done in the space of objects or their parts, rather\n",
      "\n",
      "28.5. Discussion\n",
      "\n",
      "1007\n",
      "\n",
      "than in the space of bits and pixels. That is, we want to bridge the signal-to-symbol divide, where by “symbol” we mean something atomic, that can be combined with other symbols in a compositional way.\n",
      "\n",
      "The question of how to convert low level signals into a more structured/ “semantic” represen- tation is known as the symbol grounding problem (Harnard 1990). Traditionally such symbols are associated with words in natural language, but it seems unlikely we can jump directly from low-level signals to high-level semantic concepts. Instead, what we need is an intermediate level of symbolic or atomic parts.\n",
      "\n",
      "A very simple way to create such parts from real-valued signals, such as images, is to apply vector quantization. This generates a set of visual words. These can then be modelled using some of the techniques from Chapter 27 for modeling bags of words. Such models, however, are still quite “shallow”.\n",
      "\n",
      "It is possible to deﬁne, and learn, deep models which use discrete latent parts. Here we just mention a few recent approaches, to give a ﬂavor of the possibilites. (Salakhutdinov et al. 2011) combine RBMs with hierarchical latent Dirichlet allocation methods, trained in an unsupervised way. (Zhu et al. 2010) use latent and-or graphs, trained in a manner similar to a latent structural SVM. A similar approach, based on grammars, is described in (Girshick et al. 2011). What is interesting about these techniques is that they apply data-driven machine learning methods to rich structured/symbolic “AI-style” models. This seems like a promising future direction for machine learning.\n",
      "\n",
      "Notation\n",
      "\n",
      "Introduction\n",
      "\n",
      "It is very difficult to come up with a single, consistent notation to cover the wide variety of data, models and algorithms that we discuss. Furthermore, conventions differ between machine learning and statistics, and between different books and papers. Nevertheless, we have tried to be as consistent as possible. Below we summarize most of the notation used in this book, although individual sections may introduce new notation. Note also that the same symbol may have different meanings depending on the context, although we try to avoid this where possible.\n",
      "\n",
      "General math notation\n",
      "\n",
      "Symbol (cid:28)x(cid:29) %x& x ⊗ y x $ y a ∧ b a ∨ b ¬a I(x) ∞ → ∝ |x| |S| n! ∇ ∇2 (cid:2) O(·) R 1 :n ≈ argmaxx f (x)\n",
      "\n",
      "Meaning Floor of x, i.e., round down to nearest integer Ceiling of x, i.e., round up to nearest integer Convolution of x and y Hadamard (elementwise) product of x and y logical AND logical OR logical NOT Indicator function, I(x) = 1 if x is true, else I(x) = 0 Inﬁnity Tends towards, e.g., n → ∞ Proportional to, so y = ax can be written as y ∝ x Absolute value Size (cardinality) of a set Factorial function Vector of ﬁrst derivatives Hessian matrix of second derivatives Deﬁned as Big-O: roughly means order of magnitude The real numbers Range (Matlab convention): 1 :n = {1, 2, . . . , n} Approximately equal to Argmax: the value x that maximizes f\n",
      "\n",
      "1010\n",
      "\n",
      "Notation\n",
      "\n",
      "B(a, b) B(α) (cid:23) (cid:22) n k δ(x) δij δx(y) exp(x) Γ(x) Ψ(x) X\n",
      "\n",
      "Beta function, B(a, b) = k Γ(αk) (cid:2) Multivariate beta function, k αk) n choose k, equal to n!/(k!(n − k)!) Dirac delta function, δ(x) =∞ if x = 0, else δ(x) = 0 Kronecker delta, equals 1 if i = j, otherwise equals 0 Kronecker delta, equals 1 if x = y, otherwise equals 0 Exponential function ex (cid:21) ∞ Gamma function, Γ(x) = 0 Digamma function, Ψ(x) = d A set from which values are drawn (e.g., X = RD)\n",
      "\n",
      "Γ(a)Γ(b) Γ(a+b)\n",
      "\n",
      "Γ(\n",
      "\n",
      "(cid:3)\n",
      "\n",
      "dx log Γ(x)\n",
      "\n",
      "ux−1e−udu\n",
      "\n",
      "Linear algebra notation\n",
      "\n",
      "We use boldface lowercase to denote vectors, such as a, and boldface uppercase to denote matrices, such as A. Vectors are assumed to be column vectors, unless noted otherwise.\n",
      "\n",
      "Symbol A ’ 0 tr(A) det(A) |A| A−1 A† AT aT diag(a) diag(A) I or Id 1 or 1d 0 or 0d ||x|| = ||x||2 ||x||1 A:,j Ai,: Aij x ⊗ y\n",
      "\n",
      "Meaning A is a positive deﬁnite matrix Trace of a matrix Determinant of matrix A Determinant of matrix A Inverse of a matrix Pseudo-inverse of a matrix Transpose of a matrix Transpose of a vector Diagonal matrix made from vector a Diagonal vector extracted from matrix A Identity matrix of size d × d (ones on diagonal, zeros off) Vector of ones (of length d) Vector of zeros (of length d) $(cid:7)d Euclidean or (cid:2)2 norm (cid:7)d (cid:2)1 norm j’th column of matrix transpose of i’th row of matrix (a column vector) Element (i, j) of matrix A Tensor product of x and y\n",
      "\n",
      "j=1 |xj|\n",
      "\n",
      "j=1 x2 j\n",
      "\n",
      "Probability notation\n",
      "\n",
      "We denote random and ﬁxed scalars by lower case, random and ﬁxed vectors by bold lower case, and random and ﬁxed matrices by bold upper case. Occastionally we use non-bold upper case to denote scalar random variables. Also, we use p() for both discrete and continuous random variables.\n",
      "\n",
      "Notation\n",
      "\n",
      "1011\n",
      "\n",
      "Symbol X ⊥ Y X (cid:8)⊥ Y X ⊥ Y |Z X (cid:8)⊥ Y |Z X ∼ p α cov [x] E [X] Eq [X] H (X) or H (p) I (X; Y ) KL (p||q) (cid:2)(θ) L(θ, a) λ Λ mode [X] μ μ p(x) p(x|y) Φ φ π ρ sigm(x) σ2 Σ var [x] ν Z\n",
      "\n",
      "Meaning X is independent of Y X is not independent of Y X is conditionally independent of Y given Z X is not conditionally independent of Y given Z X is distributed according to distribution p Parameters of a Beta or Dirichlet distribution Covariance of x Expected value of X Expected value of X wrt distribution q Entropy of distribution p(X) Mutual information between X and Y KL divergence from distribution p to q Log-likelihood function Loss function for taking action a when true state of nature is θ Precision (inverse variance) λ = 1/σ2 Precision matrix Λ = Σ−1 Most probable value of X Mean of a scalar distribution Mean of a multivariate distribution Probability density or mass function Conditional probability density of x given y cdf of standard normal pdf of standard normal multinomial parameter vector, Stationary distribution of Markov chain Correlation coefficient Sigmoid (logistic) function, Variance Covariance matrix Variance of x Degrees of freedom parameter Normalization constant of a probability distribution\n",
      "\n",
      "1 1+e−x\n",
      "\n",
      "Machine learning/statistics notation\n",
      "\n",
      "In general, we use upper case letters to denote constants, such as C, D, K, N , S, T , etc. We use lower case letters as dummy indexes of the appropriate range, such as c = 1 :C to index classes, j = 1 :D to index input features, k = 1 :K to index states or clusters, s = 1 :S to index samples, t = 1 :T to index time, etc. To index data cases, we use the notation i = 1 :N , although the notation n = 1 : N is also widely used.\n",
      "\n",
      "We use x to represent an observed data vector. In a supervised problem, we use y or y to represent the desired output label. We use z to represent a hidden variable. Sometimes we also use q to represent a hidden discrete variable.\n",
      "\n",
      "1012\n",
      "\n",
      "Notation\n",
      "\n",
      "Symbol C D R D Dtest J(θ) K κ(x, y) K λ N Nc φ(x) Φ q() Q(θ, θold) S T T (D) T θ θ(s) ˆθ ˆθM L ˆθM AP θ w W xij xi X x ˜x x∗ y zij\n",
      "\n",
      "Meaning Number of classes Dimensionality of data vector (number of features) Number of outputs (response variables) Training data D = {xi|i = 1 :N } or D = {(xi, yi)|i = 1 :N } Test data Cost function Number of states or dimensions of a variable (often latent) Kernel function Kernel matrix Strength of (cid:2)2 or (cid:2)1 regularizer Number of data cases Number of examples of class c, Nc = Basis function expansion of feature vector x Basis function expansion of design matrix X Approximate or proposal distribution Auxiliary function in EM Number of samples Length of a sequence Test statistic for data Transition matrix of Markov chain Parameter vector s’th sample of parameter vector Estimate (usually MLE or MAP) of θ Maximum likelihood estimate of θ MAP estimate of θ Estimate (usually posterior mean) of θ Vector of regression weights (called β in statistics) Matrix of regression weights Component (i.e., feature) j of data case i, fori = 1 :N ,j = 1 : D Training case, i = 1 :N Design matrix of size N × D (cid:7)N Empirical mean x = 1 N Future test case Future test case Vector of all training labels y = (y1, . . . , yN ) Latent component j for case i\n",
      "\n",
      "i=1 xi\n",
      "\n",
      "(cid:7)N\n",
      "\n",
      "n=1 I(yn = c)\n",
      "\n",
      "Graphical model notation\n",
      "\n",
      "In graphical models, we index nodes by s, t, u ∈ V, and states by i, j, k ∈ X .\n",
      "\n",
      "Notation\n",
      "\n",
      "1013\n",
      "\n",
      "Symbol Meaning s ∼ t bel C chj descj G E mbt nbdt pat predt ψc(xc) S θsjk V\n",
      "\n",
      "Node s is connected to node t Belief function Cliques of a graph Child of node j in a DAG Descendants of node j in a DAG A graph Edges of a graph Markov blanket of node t Neighborhood of node t Parents of node t in a DAG Predecessors of node t in a DAG wrt some ordering Potential function for clique c Separators of a graph prob. node s is in state k given its parents are in states j Nodes of a graph\n",
      "\n",
      "1014\n",
      "\n",
      "Notation\n",
      "\n",
      "List of commonly used abbreviations\n",
      "\n",
      "Abbreviation Meaning cdf CPD CPT CRF DAG DGM EB EM EP GLM GMM HMM iid iff KL LDS LHS MAP MCMC MH MLE MPM MRF MSE NLL OLS pd pdf pmf RBPF RHS RJMCMC RSS SLDS SSE UGM VB wrt\n",
      "\n",
      "Cumulative distribution function Conditional probability distribution Conditional probability table Conditional random ﬁeld Directed acyclic graphic Directed graphical model Empirical Bayes Expectation maximization algorithm Expectation propagation Generalized linear model Gaussian mixture model Hidden Markov model Independent and identically distributed If and only if Kullback Leibler divergence Linear dynamical system Left hand side (of an equation) Maximum A Posterior estimate Markov chain Monte Carlo Metropolis Hastings Maximum likelihood estimate Maximum of Posterior Marginals Markov random ﬁeld Mean squared error Negative log likelihood Ordinary least squares Positive deﬁnite (matrix) Probability density function Probability mass function Rao-Blackwellised particle ﬁlter Right hand side (of an equation) Reversible jump MCMC Residual sum of squares Switching linear dynamical system Sum of squared errors Undirected graphical model Variational Bayes With respect to\n",
      "\n",
      "Bibliography\n",
      "\n",
      "Aji, S. M. and R.\n",
      "\n",
      "J. McEliece (2000, March). The generalized distribu- tive law. Info. The- IEEE Trans. ory 46(2), 325–343.\n",
      "\n",
      "Alag, S. and A. Agogino (1996).\n",
      "\n",
      "In- ference using message propoga- tion and topology transformation in vector Gaussian continuous net- works. In UAI.\n",
      "\n",
      "Albers, C., M. Leisink, and H. Kap- pen (2006). The Cluster Variation Method for Efficient Linkage Anal- ysis on Extended Pedigrees. BMC Bioinformatics 7.\n",
      "\n",
      "Andrieu, C., N. de Freitas,\n",
      "\n",
      "and A. Doucet Sequential (2000). Bayesian estimation and model se- lection for dynamic kernel ma- chines. Technical report, Cam- bridge Univ.\n",
      "\n",
      "Andrieu, C., N. de Freitas,\n",
      "\n",
      "and A. Doucet Robust Full Bayesian Learning for Radial Ba- Neural Computa- sis Networks. tion 13(10), 2359–2407.\n",
      "\n",
      "(2001).\n",
      "\n",
      "Andrieu, C., N. de Freitas, A. Doucet, and M. Jordan (2003). An introduc- tion to MCMC for machine learn- ing. Machine Learning 50, 5–43.\n",
      "\n",
      "Abend, K., T. J. Harley, and L. N. Kanal (1965). Classiﬁcation of Binary Ran- dom Patterns. IEEE Transactions on Information Theory 11(4), 538–544.\n",
      "\n",
      "Ackley, D., G. Hinton, and T. Sejnowski (1985). A learning algorithm for boltzmann machines. Cognitive Science 9, 147–169.\n",
      "\n",
      "Albert, J. and S. Chib (1993). Bayesian analysis of binary and polychoto- mous response data. J. of the Am. Stat. Assoc. 88(422), 669–679.\n",
      "\n",
      "Allwein, E., R. Schapire, and Y. Singer (2000). Reducing multiclass to bi- nary: A unifying approach for mar- gin classiﬁers. J. of Machine Learn- ing Research, 113–141.\n",
      "\n",
      "Andrieu, C., A. Doucet, and V. Tadic (2005). Online EM for parameter estimation in nonlinear-non Gaus- sian state-space models. In Proc. IEEE CDC.\n",
      "\n",
      "Andrieu, C. and J. Thoms (2008). A tutorial on adaptive MCMC. Statis- tical Computing 18, 343–373.\n",
      "\n",
      "Aoki, M. (1987). State space modeling of\n",
      "\n",
      "time series. Springer.\n",
      "\n",
      "Adams,\n",
      "\n",
      "and R. P., H. Wallach, Z. Ghahramani (2010). Learning the structure of deep sparse graphical models. In AI/Statistics.\n",
      "\n",
      "Aggarwal, D. and S. Merugu (2007). Predictive discrete latent factor models for large scale dyadic data. In Proc. of the Int’l Conf. on Knowl- edge Discovery and Data Mining.\n",
      "\n",
      "Ahmed, A.\n",
      "\n",
      "(2007). On tight approximate inference of the logistic-normal topic admixture model. In AI/Statistics.\n",
      "\n",
      "and E. Xing\n",
      "\n",
      "Ahn, J.-H. and J.-H. Oh (2003). A Con- strained EM Algorithm for Princi- pal Component Analysis. Neural Computation 15, 57–65.\n",
      "\n",
      "Ahn, S., A. Korattikara, and M. Welling (2012). Bayesian Posterior Sam- pling via Stochastic Gradient Fisher Scoring. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Airoldi, E., D. Blei, S. Fienberg, and E. Xing (2008). Mixed-membership J. of Ma- stochastic blockmodels. chine Learning Research 9, 1981– 2014.\n",
      "\n",
      "Aitchison,\n",
      "\n",
      "The statistical analysis of compositional data. J. of Royal Stat. Soc. Series B 44(2), 139– 177.\n",
      "\n",
      "J.\n",
      "\n",
      "(1982).\n",
      "\n",
      "Aloise, D., A. Deshpande, P. Hansen, and P. Popat (2009). NP-hardness of Euclidean sum-of-squares clus- tering. Machine Learning 75, 245– 249.\n",
      "\n",
      "Alpaydin, E.\n",
      "\n",
      "(2004). machine learning. MIT Press.\n",
      "\n",
      "Introduction to\n",
      "\n",
      "Altun, Y., T. Hofmann, and I. Tsochan- taridis (2006). Large Margin Meth- ods for Structured and Interde- In pendent Output Variables. G. Bakir, T. Hofmann, B. Scholkopf, A. Smola, B. Taskar, and S. Vish- wanathan (Eds.), Machine Learning with Structured Outputs. MIT Press.\n",
      "\n",
      "Amir, E.\n",
      "\n",
      "(2010). Approximation Al- gorithms for Treewidth. Algorith- mica 56(4), 448.\n",
      "\n",
      "Amir, E. and S. McIlraith (2005). Partition-based reason- ing for ﬁrst-order and propo- sitional Artiﬁcial Intelligence 162(1), 49–88.\n",
      "\n",
      "theories.\n",
      "\n",
      "logical\n",
      "\n",
      "Ando, R. and T. Zhang (2005).\n",
      "\n",
      "A framework for learning predictive structures from multiple tasks and unlabeled data. J. of Machine Learning Research 6, 1817–1853.\n",
      "\n",
      "Andrews, D. and C. Mallows (1974). Scale mixtures of Normal distribu- J. of Royal Stat. Soc. Series tions. B 36, 99–102.\n",
      "\n",
      "Archambeau, C. and F. Bach (2008). Sparse probabilistic projections. In NIPS.\n",
      "\n",
      "Argyriou, A., T. Evgeniou, and M. Pon- til (2008). Convex multi-task fea- Machine Learn- ture learning. ing 73(3), 243–272.\n",
      "\n",
      "Armagan, A., D. Dunson, and J. Lee (2011). Generalized double pareto shrinkage. Technical report, Duke.\n",
      "\n",
      "Armstrong, H.\n",
      "\n",
      "(2005). Bayesian esti- mation of decomposable Gaussian graphical models. thesis, UNSW.\n",
      "\n",
      "Ph.D.\n",
      "\n",
      "Armstrong, H., C. Carter, K. Wong, and R. Kohn (2008). Bayesian Co- variance Matrix Estimation using a Mixture of Decomposable Graphi- cal Models. Statistics and Comput- ing, 1573–1375.\n",
      "\n",
      "Arnborg,\n",
      "\n",
      "S., D. G. Corneil, and A. Proskurowski (1987). Complex- ity of ﬁnding embeddings in a k- tree. SIAM J. on Algebraic and Dis- crete Methods 8, 277–284.\n",
      "\n",
      "Arora, S. and B. Barak (2009). Com- plexity Theory: A Modern Approach. Cambridge.\n",
      "\n",
      "Arthur, D. and S. Vassilvitskii (2007). k- means++: the advantages of careful seeding. In Proc. 18th ACM-SIAM symp. on Discrete algorithms, pp. 1027â ˘A ¸S1035.\n",
      "\n",
      "1016\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Arulampalam, M., S. Maskell, N. Gor- don, and T. Clapp (2002, Febru- ary). A Tutorial on Particle Fil- for Online Nonlinear/Non- ters Gaussian Bayesian Tracking. IEEE Trans. on Signal Processing 50(2), 174–189.\n",
      "\n",
      "Asavathiratham, C. (2000). The Inﬂu- ence Model: A Tractable Representa- tion for the Dynamics of Networked Markov Chains. Ph.D. thesis, MIT, Dept. EECS.\n",
      "\n",
      "Atay-Kayis, A. and H. Massam (2005). A Monte Carlo method for comput- ing the marginal likelihood in non- decomposable Gaussian graphical models. Biometrika 92, 317–335.\n",
      "\n",
      "Attenberg, J., K. Weinberger, A. Smola, A. Dasgupta, and M. Zinkevich (2009). Collaborative spam ﬁlter- ing with the hashing trick. In Virus Bulletin.\n",
      "\n",
      "Attias, H. (1999).\n",
      "\n",
      "Independent factor analysis. Neural Computation 11, 803–851.\n",
      "\n",
      "Attias, H.\n",
      "\n",
      "A variational Bayesian framework for graphical models. In NIPS-12.\n",
      "\n",
      "(2000).\n",
      "\n",
      "Bach, F. (2008). Bolasso: Model Con- sistent Lasso Estimation through the Bootstrap. In Intl. Conf. on Ma- chine Learning.\n",
      "\n",
      "Bach, F. and M. Jordan (2001). Thin\n",
      "\n",
      "junction trees. In NIPS.\n",
      "\n",
      "Bach, F. and M. Jordan (2005). A prob- abilistic interpretation of canonical correlation analysis. Technical Re- port 688, U. C. Berkeley.\n",
      "\n",
      "Bach, F. and E. Moulines (2011). Non- asymptotic analysis of stochastic approximation algorithms for ma- chine learning. In NIPS.\n",
      "\n",
      "Bahmani, B., B. Moseley, A. Vattani, R. Kumar, and S. Vassilvitskii (2012). Scalable k-Means++. In VLDB.\n",
      "\n",
      "Bakker, B. and T. Heskes (2003). Task Clustering and Gating for Bayesian Multitask Learning. J. of Machine Learning Research 4, 83–99.\n",
      "\n",
      "Baldi,\n",
      "\n",
      "P. and Y. Chauvin (1994). Smooth online learning algorithms for hidden Markov models. Neural Computation 6, 305–316.\n",
      "\n",
      "Balding, D. (2006). A tutorial on sta- tistical methods for population as- sociation studies. Nature Reviews Genetics 7, 81–91.\n",
      "\n",
      "Banerjee, O.,\n",
      "\n",
      "and A. d’Aspremont (2008). Model se- lection through sparse maximum likelihood estimation for multivari- ate gaussian or binary data. J. of Machine Learning Research 9, 485– 516.\n",
      "\n",
      "L. E. Ghaoui,\n",
      "\n",
      "Bar-Shalom, Y. and T. Fortmann (1988). Tracking and data associa- tion. Academic Press.\n",
      "\n",
      "Bar-Shalom, Y. and X. Li (1993). Es- timation and Tracking: Principles, Techniques and Software. Artech House.\n",
      "\n",
      "Barash, Y. and N. Friedman (2002). Context-speciﬁc Bayesian cluster- ing for gene expression data. J. Comp. Bio. 9, 169–191.\n",
      "\n",
      "Barber, D.\n",
      "\n",
      "(2006). Expectation Cor- rection for Smoothed Inference in Switching Linear Dynamical Sys- tems. J. of Machine Learning Re- search 7, 2515–2540.\n",
      "\n",
      "Barber, D. and C. Bishop (1998). Ensemble Learning in Bayesian Neural Networks. In C. Bishop (Ed.), Neural Networks and Machine Learning, pp. 215–237. Springer.\n",
      "\n",
      "Barber, D. and S. Chiappa (2007). Uniﬁed inference for variational bayesian linear gaussian state space models. In NIPS.\n",
      "\n",
      "Barbieri, M. and J. Berger (2004). Op- timal predictive model selection. Annals of Statistics 32, 870–897.\n",
      "\n",
      "Bartlett, P., M. Jordan, and J. McAuliffe (2006). Convexity, Classiﬁcation, and Risk Bounds. J. of the Am. Stat. Assoc. 101(473), 138–156.\n",
      "\n",
      "Baruniak, R. (2007). Compressive sens- IEEE Signal Processing Maga-\n",
      "\n",
      "ing. zine.\n",
      "\n",
      "technique occuring in the statisti- cal analysis of probabalistic func- tions in markov chains. The Annals of Mathematical Statistics 41, 164– 171.\n",
      "\n",
      "Beal, M. (2003). Variational Algorithms for Approximate Bayesian Inference. Ph.D. thesis, Gatsby Unit.\n",
      "\n",
      "Beal, M. and Z. Ghahramani\n",
      "\n",
      "(2006). Variational Bayesian Learning of Directed Graphical Models with Hidden Variables. Bayesian Anal- ysis 1(4).\n",
      "\n",
      "Beal, M. J., Z. Ghahramani, and C. E. Rasmussen (2002). The inﬁnite hid- den Markov model. In NIPS-14.\n",
      "\n",
      "Beck, A. and M. Teboulle (2009). A fast iterative shrinkage-thresholding al- gorothm for linear inverse prob- SIAM J. on Imaging Sci- lems. ences 2(1), 183–202.\n",
      "\n",
      "Beinlich, I., H. Suermondt, R. Chavez, and G. Cooper (1989). The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. In Proc. of the Second European Conf. on AI in Medicine, pp. 247–256.\n",
      "\n",
      "Bekkerman, R., M. Bilenko,\n",
      "\n",
      "and J. Langford (Eds.) (2011). Scaling Up Machine Learning. Cambridge.\n",
      "\n",
      "Bell, A. J. and T. J. Sejnowski (1995). An information maximisation ap- proach to blind separation and blind deconvolution. Neural Com- putation 7 (6), 1129–1159.\n",
      "\n",
      "Bengio, Y. (2009). Learning deep ar- chitectures for AI. Foundations and Trends in Machine Learning 2(1), 1– 127.\n",
      "\n",
      "Bengio, Y. and S. Bengio (2000). Modeling high-dimensional dis- crete data with multi-layer neural networks. In NIPS.\n",
      "\n",
      "Barzilai, J. and J. Borwein (1988). Two point step size gradient methods. IMA J. of Numerical Analysis 8, 141– 148.\n",
      "\n",
      "Basu, S., T. Choudhury, B. Clarkson, Learn- and A. Pentland (2001). interactions with ing the inﬂuence model. Techni- cal Report 539, MIT Media Lab. ftp://whitechapel.media.mit.edu/pub/tech- reports/TR-539-ABSTRACT.html.\n",
      "\n",
      "Baum, L. E., T. Petrie, G. Soules, and N. Weiss (1970). A maximization\n",
      "\n",
      "human\n",
      "\n",
      "Bengio, Y., O. Delalleau, N. Roux, and J. Paiement, M. Ouimet (2004). Learning eigen- functions links spectral embedding and kernel PCA. Neural Computa- tion 16, 2197–2219.\n",
      "\n",
      "Bengio, Y. and P. Frasconi (1995). Dif- fusion of context and credit infor- mation in markovian models. J. of AI Research 3, 249–270.\n",
      "\n",
      "Bengio, Y. and P. Frasconi\n",
      "\n",
      "(1996). Input/output HMMs for sequence IEEE Trans. on Neural processing. Networks 7 (5), 1231–1249.\n",
      "\n",
      "P. Vincent,\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1017\n",
      "\n",
      "Bengio, Y., P. Lamblin, D. Popovici, and H. Larochelle (2007). Greedy layer-wise training of deep net- works. In NIPS.\n",
      "\n",
      "Berchtold, A. (1999). The double chain markov model. Comm. Stat. Theor. Methods 28, 2569–2589.\n",
      "\n",
      "Berger, J. (1985). Bayesian salesman- ship. In P. K. Goel and A. Zellner (Eds.), Bayesian Inference and De- cision Techniques with Applications: Essays in Honor of Bruno deFinetti. North-Holland.\n",
      "\n",
      "Berger, J. and R. Wolpert (1988). The Likelihood Principle. The Institute of Mathematical Statistics. 2nd edi- tion.\n",
      "\n",
      "Berkhin, P.\n",
      "\n",
      "A survey of clustering datamining techniques. In J. Kogan, C. Nicholas, and M. Teboulle (Eds.), Grouping Multi- dimensional Data: Recent Advances in Clustering, pp. 25–71. Springer.\n",
      "\n",
      "(2006).\n",
      "\n",
      "Bernardo,\n",
      "\n",
      "Bayesian Theory. John Wiley.\n",
      "\n",
      "J. and A. Smith (1994).\n",
      "\n",
      "Berrou, C., A. Glavieux, and P. Thiti- majashima (1993). Near Shannon limit error-correcting coding and decoding: Turbo codes. Proc. IEEE Intl. Comm. Conf..\n",
      "\n",
      "Berry, D. and Y. Hochberg (1999). Bayesian perspectives on multiple comparisons. J. Statist. Planning and Inference 82, 215–227.\n",
      "\n",
      "Bhattacharya, A. and D. B. Dunson (2011). Simplex factor models for multivariate unordered categorical J. of the Am. Stat. Assoc.. To data. appear.\n",
      "\n",
      "Bickel, P. and E. Levina (2004). Some theory for Fisher’s linear discrimi- nant function, \"Naive Bayes\", and some alternatives when there are many more variables than obser- vations. Bernoulli 10, 989–1010.\n",
      "\n",
      "Bickson, D.\n",
      "\n",
      "(2009). Gaussian Belief Propagation: Theory and Applica- tion. Ph.D. thesis, Hebrew Univer- sity of Jerusalem.\n",
      "\n",
      "Bilmes,\n",
      "\n",
      "multinets. In UAI.\n",
      "\n",
      "J.\n",
      "\n",
      "(2000). Dynamic Bayesian\n",
      "\n",
      "Bilmes, J. A. (2001). Graphical models and automatic speech recognition. Technical Report UWEETR-2001- 0005, Univ. Washington, Dept. of Elec. Eng.\n",
      "\n",
      "Binder, J., D. Koller, S. J. Russell, and K. Kanazawa (1997). Adaptive prob- abilistic networks with hidden vari- ables. Machine Learning 29, 213– 244.\n",
      "\n",
      "Binder, J., K. Murphy, and S. Russell (1997). Space-efficient inference in dynamic probabilistic networks. In Intl. Joint Conf. on AI.\n",
      "\n",
      "Birnbaum, A. (1962). On the founda- J. of\n",
      "\n",
      "tions of statistical infernece. the Am. Stat. Assoc. 57, 269–326.\n",
      "\n",
      "Bishop, C. (1999). Bayesian PCA.\n",
      "\n",
      "NIPS.\n",
      "\n",
      "In\n",
      "\n",
      "Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Clarendon Press.\n",
      "\n",
      "Bishop, Y., S. Fienberg, and P. Holland (1975). Discrete Multivariate Analy- sis: Theory and Practice. MIT Press.\n",
      "\n",
      "Bistarelli,\n",
      "\n",
      "and Semiring-based F. Rossi constraint satisfaction and opti- mization. J. of the ACM 44(2), 201– 236.\n",
      "\n",
      "S., U. Montanari,\n",
      "\n",
      "(1997).\n",
      "\n",
      "Blake, A., P. Kohli, and C. Rother (Eds.) (2011). Advances in Markov Random Fields for Vision and Image Process- ing. MIT Press.\n",
      "\n",
      "Blei, D. and J. Lafferty (2006a). Corre-\n",
      "\n",
      "lated topic models. In NIPS.\n",
      "\n",
      "Blei, D. and J. Lafferty (2006b). Dy- In Intl. Conf. namic topic models. on Machine Learning, pp. 113–120.\n",
      "\n",
      "Blei, D. and J. Lafferty (2007). A Corre- lated Topic Model of \"Science\". An- nals of Applied Stat. 1(1), 17–35.\n",
      "\n",
      "Blei, D. and J. McAuliffe (2010, March). Supervised topic models. Technical report, Princeton.\n",
      "\n",
      "Blei, D., A. Ng, and M. Jordan (2003). Latent dirichlet allocation. J. of Machine Learning Research 3, 993– 1022.\n",
      "\n",
      "Blumensath, T. and M. Davies (2007). On the difference between Orthog- onal Matching Pursuit and Orthog- onal Least Squares. Technical re- port, U. Edinburgh.\n",
      "\n",
      "Bertele, U. and F. Brioschi (1972). Non- serial Dynamic Programming. Aca- demic Press.\n",
      "\n",
      "Bertsekas, D. (1997). Parallel and Dis- tribution Computation: Numerical Methods. Athena Scientiﬁc.\n",
      "\n",
      "Bertsekas, D.\n",
      "\n",
      "gramming (Second ed.). Scientiﬁc.\n",
      "\n",
      "(1999). Nonlinear Pro- Athena\n",
      "\n",
      "Bertsekas, D. and J. Tsitsiklis (2008). Introduction to Probability. Athena Scientiﬁc. 2nd Edition.\n",
      "\n",
      "Besag, J. (1975). Statistical analysis of non-lattice data. The Statistician 24, 179–196.\n",
      "\n",
      "Bishop, C. (2006a). Pattern recognition and machine learning. Springer.\n",
      "\n",
      "Bishop, C. (2006b). Pattern recognition and machine learning. Springer.\n",
      "\n",
      "Bishop, C. and G. James (1993). Analy- sis of multiphase ﬂows using dual- energy densitometry and neural networks. Nuclear Instruments and Methods in Physics Research A327, 580–593.\n",
      "\n",
      "Bishop, C. and M. Svensén (2003). Bayesian hierarchical mixtures of experts. In UAI.\n",
      "\n",
      "Bishop, C. and M. Tipping (2000). Variational relevance vector ma- chines. In UAI.\n",
      "\n",
      "Bo, L., C. Sminchisescu, A. Kanaujia, and D. Metaxas (2008). Fast Algo- rithms for Large Scale Conditional 3D Prediction. In CVPR.\n",
      "\n",
      "Bohning, D. (1992). Multinomial logis- tic regression algorithm. Annals of the Inst. of Statistical Math. 44, 197– 200.\n",
      "\n",
      "Bollen, K. (1989). Structural Equation John\n",
      "\n",
      "Models with Latent Variables. Wiley & Sons.\n",
      "\n",
      "Bordes, A., L. Bottou, and P. Galli- nari Sgd-qn: Care- July). ful quasi-newton stochastic gradi- ent descent. J. of Machine Learning Research 10, 1737–1754.\n",
      "\n",
      "(2009,\n",
      "\n",
      "Bhatnagar, N., C. Bogdanov,\n",
      "\n",
      "and E. Mossel The compu- tational complexity of estimating Technical re- convergence time. port, .\n",
      "\n",
      "(2010).\n",
      "\n",
      "Bishop, C. M. (1994). Mixture density networks. Technical Report NCRG 4288, Neural Computing Research Group, Department of Computer Science, Aston University.\n",
      "\n",
      "Bordes, A., L. Bottou, P. Gallinari, J. Chang, and S. A. Smith (2010). Er- ratum: SGDQN is Less Careful than J. of Machine Learning Expected. Research 11, 2229–2240.\n",
      "\n",
      "1018\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Boser, B. E., I. M. Guyon, and V. N. Vapnik (1992). A training algorithm for optimal margin classiﬁers. In Proc. of the Workshop on Computa- tional Learning Theory.\n",
      "\n",
      "Brand, M.\n",
      "\n",
      "(1999). Structure learning in conditional probability models via an entropic prior and param- eter extinction. Neural Computa- tion 11, 1155–1182.\n",
      "\n",
      "Brown, P., M. Vannucci, and T. Fearn (1998). Multivariate Bayesian vari- able selection and prediction. J. of the Royal Statistical Society B 60(3), 627–641.\n",
      "\n",
      "Bottcher, S. G. and C. Dethlefsen (2003). deal: A package for learning bayesian networks. J. of Statistical Software 8(20).\n",
      "\n",
      "Braun, M. and J. McAuliffe (2010). Vari- ational Inference for Large-Scale Models of Discrete Choice. J. of the Am. Stat. Assoc. 105(489), 324–335.\n",
      "\n",
      "Bruckstein, A., D. Donoho,\n",
      "\n",
      "and M. Elad (2009). From sparse so- lutions of systems of equations to sparse modeling of signals and im- ages. SIAM Review 51(1), 34–81.\n",
      "\n",
      "Bottolo, L. and S. Richardson (2010). search.\n",
      "\n",
      "Evolutionary Bayesian Analysis 5(3), 583–618.\n",
      "\n",
      "stochastic\n",
      "\n",
      "Bottou, L.\n",
      "\n",
      "(1998). Online algorithms and stochastic approximations. In D. Saad (Ed.), Online Learning and Neural Networks. Cambridge.\n",
      "\n",
      "Bottou, L. (2007). Learning with large\n",
      "\n",
      "datasets (nips tutorial).\n",
      "\n",
      "Bottou, L., O. Chapelle, D. DeCoste, and J. Weston (Eds.) (2007). Large Scale Kernel Machines. MIT Press.\n",
      "\n",
      "Bouchard, G. (2007). Efficient bounds for the softmax and applications to approximate inference in hybrid models. In NIPS 2007 Workshop on Approximate Inference in Hybrid Models.\n",
      "\n",
      "Bouchard-Cote, A. and M.\n",
      "\n",
      "Jordan (2009). Optimization of structured mean ﬁeld objectives. In UAI.\n",
      "\n",
      "Bowman, A. and A. Azzalini (1997). Ap- plied Smoothing Techniques for Data Analysis. Oxford.\n",
      "\n",
      "Box, G. and N. Draper (1987). Empir- ical Model-Building and Response Surfaces. Wiley.\n",
      "\n",
      "Box, G. and G. Tiao (1973). Bayesian in statistical analysis.\n",
      "\n",
      "inference Addison-Wesley.\n",
      "\n",
      "Boyd, S. and L. Vandenberghe (2004). Convex optimization. Cambridge.\n",
      "\n",
      "Boyen, X.\n",
      "\n",
      "(1998). and D. Koller Tractable inference for complex stochastic processes. In UAI.\n",
      "\n",
      "Boykov, Y., O. Veksler, and R. Zabih Fast approximate energy (2001). minimization via graph cuts. IEEE Trans. on Pattern Analysis and Ma- chine Intelligence 23(11).\n",
      "\n",
      "Brand, M.\n",
      "\n",
      "Coupled hidden Markov models for modeling inter- acting processes. Technical Report 405, MIT Lab for Perceptual Com- puting.\n",
      "\n",
      "(1996).\n",
      "\n",
      "Breiman, L. (1996). Bagging predictors. Machine Learning 24, 123–140.\n",
      "\n",
      "Breiman, L. (1998). Arcing classiﬁers. Annals of Statistics 26, 801–849.\n",
      "\n",
      "Breiman, L. (2001a). Random forests. Machine Learning 45(1), 5–32.\n",
      "\n",
      "Breiman, L. (2001b). Statistical mod- eling: the two cultures. Statistical Science 16(3), 199–231.\n",
      "\n",
      "Breiman, L.,\n",
      "\n",
      "J. Friedman, and R. Ol- shen (1984). Classiﬁcation and re- gression trees. Wadsworth.\n",
      "\n",
      "Breslow, N. E. and D. G. Clayton (1993). Approximate inference in general- ized linear mixed models. J. of the Am. Stat. Assoc. 88(421), 9–25.\n",
      "\n",
      "Briers, M., A. Doucet, and S. Maskel Smoothing algorithms for (2010). state-space models. Annals of the Institute of Statistical Mathemat- ics 62(1), 61–89.\n",
      "\n",
      "Brochu, E., M. Cora, and N. de Fre- itas (2009, November). A tutorial on Bayesian optimization of expen- sive cost functions, with applica- tion to active user modeling and hierarchical learn- ing. Technical Report TR-2009-23, Department of Computer Science, University of British Columbia.\n",
      "\n",
      "reinforcement\n",
      "\n",
      "Brooks, S. and G. Roberts\n",
      "\n",
      "(1998). Assessing convergence of Markov Chain Monte Carlo algorithms. Statistics and Computing 8, 319– 335.\n",
      "\n",
      "Brown, L., T. Cai, and A. DasGupta (2001). Interval estimation for a bi- nomial proportion. Statistical Sci- ence 16(2), 101–133.\n",
      "\n",
      "Brown, M. P., R. Hughey, A. Krogh, I. S. Mian, K. Sjölander, and D. Haus- sler (1993). Using dirichlet mixtures priors to derive hidden Markov models for protein families. In Intl. Conf. on Intelligent Systems for Molecular Biology, pp. 47–55.\n",
      "\n",
      "Bryson, A. and Y.-C. Ho (1969). Applied optimal control: optimization, esti- mation, and control. Blaisdell Pub- lishing Company.\n",
      "\n",
      "Buhlmann, P. and T. Hothorn (2007). Boosting Algorithms: Regulariza- tion, Prediction and Model Fitting. Statistical Science 22(4), 477–505.\n",
      "\n",
      "de and Buhlmann, Geer Statistics for High- Dimensional Data: Methodology, Theory and Applications. Springer.\n",
      "\n",
      "P. (2011).\n",
      "\n",
      "S.\n",
      "\n",
      "van\n",
      "\n",
      "Buhlmann, P. and B. Yu (2003). Boost- ing with the L2 loss: Regression and classiﬁcation. J. of the Am. Stat. Assoc. 98(462), 324–339.\n",
      "\n",
      "Buhlmann, P. and B. Yu (2006). Sparse J. of Machine Learning\n",
      "\n",
      "boosting. Research 7, 1001–1024.\n",
      "\n",
      "Bui, H., S. Venkatesh, and G. West (2002). Policy Recognition in the Abstract Hidden Markov Model. J. of AI Research 17, 451–499.\n",
      "\n",
      "Buntine, W. (2002). Variational Exten- sions to EM and Multinomial PCA. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Buntine, W. and A. Jakulin (2004). Ap- plying Discrete PCA in Data Analy- sis. In UAI.\n",
      "\n",
      "Buntine, W. and A. Jakulin (2006). Dis- crete Component Analysis. In Sub- space, Latent Structure and Feature Selection: Statistical and Optimiza- tion Perspectives Workshop.\n",
      "\n",
      "Buntine, W. and A. Weigend (1991). Bayesian backpropagation. Com- plex Systems 5, 603–643.\n",
      "\n",
      "Burges, C. J., T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender (2005). Learning to rank using gradient descent. In Intl. Conf. on Machine Learning, pp. 89–96.\n",
      "\n",
      "Burkard, R., M. Dell’Amico,\n",
      "\n",
      "S. Martello (2009). Problems. SIAM.\n",
      "\n",
      "and Assignment\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1019\n",
      "\n",
      "Byran, K. and T. Leise (2006). The 25,000,000,000 Eigenvector: The Linear Algebra behind Google. SIAM Review 48(3).\n",
      "\n",
      "Calvetti, D. and E. Somersalo (2007). Introduction to Bayesian Scientiﬁc Computing. Springer.\n",
      "\n",
      "Candes, E.,\n",
      "\n",
      "J. Romberg, and T. Tao (2006). Robust uncertainty prin- ciples: Exact signal reconstruction from highly incomplete frequency information. IEEE. Trans. Inform. Theory 52(2), 489–509.\n",
      "\n",
      "Candes, E. and M. Wakin (2008, March). An introduction to com- pressive sampling. IEEE Signal Pro- cessing Magazine 21.\n",
      "\n",
      "Candes, E., M. Wakin, and S. Boyd Enhancing sparsity by (2008). reweighted l1 minimization. J. of Fourier Analysis and Applications 1, 877–905.\n",
      "\n",
      "Caron,\n",
      "\n",
      "(2008). Sparse Bayesian nonparametric re- gression. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "F. and A. Doucet\n",
      "\n",
      "Carreira-Perpinan, M. and C. Williams (2003). An isotropic gaussian mix- ture can have more modes than components. Technical Report EDI-INF-RR-0185, School of Infor- matics, U. Edinburgh.\n",
      "\n",
      "Carter, C. and R. Kohn (1994). On Gibbs sampling for state space models. Biometrika 81(3), 541–553.\n",
      "\n",
      "Carterette, B., P. Bennett, D. Chicker- ing, and S. Dumais (2008). Here or There: Preference Judgments for Relevance. In Proc. ECIR.\n",
      "\n",
      "Caruana, R. (1998). A dozen tricks with In G. Orr and multitask learning. K.-R. Mueller (Eds.), Neural Net- works: Tricks of the Trade. Springer- Verlag.\n",
      "\n",
      "Cesa-Bianchi, N. and G. Lugosi (2006). learning, and games.\n",
      "\n",
      "Prediction, Cambridge University Press.\n",
      "\n",
      "Cevher, V. (2009). Learning with com-\n",
      "\n",
      "pressible priors. In NIPS.\n",
      "\n",
      "Chai, K. M. A. (2010). Multi-task learn- ing with Gaussian processes. Ph.D. thesis, U. Edinburgh.\n",
      "\n",
      "Chang, H., Y. Weiss, and W. Freeman (2009). Informative Sensing. Tech- nical report, Hebrew U. Submitted to IEEE Transactions on Info. The- ory.\n",
      "\n",
      "Chang, J. and D. Blei (2010). Hierar- chical relational models for docu- ment networks. The Annals of Ap- plied Statistics 4(1), 124–150.\n",
      "\n",
      "Chang,\n",
      "\n",
      "J. Boyd-Graber, S. Gerrish, C. Wang, and D. Blei (2009). Read- ing tea leaves: How humans inter- pret topic models. In NIPS.\n",
      "\n",
      "J.,\n",
      "\n",
      "Cannings, C., E. A. Thompson, and Probabil- M. H. Skolnick (1978). ity functions in complex pedigrees. Advances in Applied Probability 10, 26–61.\n",
      "\n",
      "Canny, J. (2004). Gap: a factor model In Proc. An- Intl. ACM SIGIR Conference,\n",
      "\n",
      "for discrete data. nual pp. 122–129.\n",
      "\n",
      "Cao, Z., T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li (2007). Learning to rank: From pairwise approach to listwise approach. In Intl. Conf. on Machine Learning, pp. 129â ˘A ¸S136.\n",
      "\n",
      "Cappe, O. (2010). Online Expectation Maximisation. In K. Mengersen, M. Titterington, and C. Robert (Eds.), Mixtures.\n",
      "\n",
      "Cappe, O. and E. Mouline (2009, June). Online EM Algorithm for Latent Data Models. J. of Royal Stat. Soc. Series B 71(3), 593–613.\n",
      "\n",
      "Cappe, O., E. Moulines, and T. Ryden (2005). Inference in Hidden Markov Models. Springer.\n",
      "\n",
      "Carbonetto, P.\n",
      "\n",
      "(2003). Unsupervised statistical models for general object recognition. Master’s thesis, Uni- versity of British Columbia.\n",
      "\n",
      "Carlin, B. P. and T. A. Louis (1996). Bayes and Empirical Bayes Methods for Data Analysis. Chapman and Hall.\n",
      "\n",
      "Caruana, R. and A. Niculescu-Mizil (2006). An empirical comparison of supervised learning algorithms. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Carvahlo, C., N. Polson, and J. Scott (2010). The horseshoe estimator for sparse signals. Biometrika 97 (2), 465.\n",
      "\n",
      "Carvahlo, L. and C. Lawrence (2007). Centroid estimation in discrete high-dimensional spaces with ap- plications in biology. Proc. of the National Academy of Science, USA 105(4).\n",
      "\n",
      "Carvalho, C. M. and M. West (2007). Dynamic matrix-variate graphical models. Bayesian Analysis 2(1), 69– 98.\n",
      "\n",
      "Casella, G. and R. Berger (2002). Statis- tical inference. Duxbury. 2nd edi- tion.\n",
      "\n",
      "Castro, M., M. Coates, and R. D. Nowak (2004). Likelihood based hi- erarchical clustering. IEEE Trans. in Signal Processing 52(8), 230.\n",
      "\n",
      "Celeux, G. and J. Diebolt\n",
      "\n",
      "(1985). The SEM algorithm: A probabilis- tic teacher derive from the EM algorithm for the mixture prob- lem. Computational Statistics Quar- terly 2, 73–82.\n",
      "\n",
      "Cemgil, A. T. (2001). A technique for painless derivation of kalman ﬁlter- ing recursions. Technical report, U. Nijmegen.\n",
      "\n",
      "Chapelle, O. and L. Li (2011). An empir- ical evaluation of Thompson sam- pling. In NIPS.\n",
      "\n",
      "Chartrand, R. and W. Yin (2008).\n",
      "\n",
      "It- eratively reweighted algorithms for compressive sensing. In Intl. Conf. on Acoustics, Speech and Signal Proc.\n",
      "\n",
      "Chechik, G., A. G. N. Tishby, and Information bot- Y. Weiss (2005). tleneck for gaussian variables. J. of Machine Learning Research 6, 165â ˘A ¸S188.\n",
      "\n",
      "Cheeseman, P., J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman (1988). Autoclass: A Bayesian classiﬁcation system. In Proc. of the Fifth Intl. Workshop on Machine Learning.\n",
      "\n",
      "Cheeseman, P. and J. Stutz (1996). Bayesian classiﬁcation (autoclass): In Fayyad, Theory and results. and Pratetsky-Shapiro, in Uthurasamy (Eds.), Advances Knowledge Discovery and Data Min- ing. MIT Press.\n",
      "\n",
      "Smyth,\n",
      "\n",
      "Chen, B., K. Swersky, B. Marlin, and N. de Freitas (2010). Sparsity priors and boosting for learning localized distributed feature representations. Technical report, UBC.\n",
      "\n",
      "Chen, B.,\n",
      "\n",
      "J.-A. Ting, B. Marlin, and N. de Freitas (2010). Deep learning of fea- invariant spatio-temporal tures from video. In NIPS Workshop on Deep Learning.\n",
      "\n",
      "1020\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Chen, M., D. Carlson, A. Zaas, C. Woods, G. Ginsburg, A. Hero, J. Lucas, and L. Carin (2011, The Bayesian Elastic March). Net: Classifying Multi-Task Gene- Expression Data. IEEE Trans. Biomed. Eng. 58(3), 468–79.\n",
      "\n",
      "Chen, R. and S. Liu (2000). Mixture Kalman ﬁlters. J. Royal Stat. Soc. B.\n",
      "\n",
      "Chen, S. and J. Goodman (1996). An empirical study of smoothing tech- niques for language modeling. In Proc. 34th ACL, pp. 310–318.\n",
      "\n",
      "Chen, S. and J. Goodman (1998). An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Dept. Comp. Sci., Harvard.\n",
      "\n",
      "Chen, S. and J. Wigger (1995, July). Fast orthogonal least squares algorithm for efficient subset model selection. IEEE Trans. Signal Processing 3(7), 1713–1715.\n",
      "\n",
      "Chen, S. S., D. L. Donoho, and M. A. Saunders (1998). Atomic decompo- sition by basis pursuit. SIAM Jour- nal on Scientiﬁc Computing 20(1), 33–61.\n",
      "\n",
      "Chen, X., S. Kim, Q. Lin,\n",
      "\n",
      "J. G. Carbonell, and E. P. Xing (2010). Graph-Structured Multi-task Re- gression and an Efficient Optimiza- tion Method for General Fused Lasso. Technical report, CMU.\n",
      "\n",
      "Chipman, H., E. George, and R. Mc- Culloch (2006). Bayesian Ensemble Learning. In NIPS.\n",
      "\n",
      "Chipman, H., E. George, and R. Mc- Culloch (2010). BART: Bayesian ad- ditive regression trees. Ann. Appl. Stat. 4(1), 266–298.\n",
      "\n",
      "Choi, M., V. Tan, A. Anandkumar, and A. Willsky (2011). Learning latent tree graphical models. J. of Ma- chine Learning Research.\n",
      "\n",
      "J. Trees and Be- Choi, M. Exploiting and Improving yond: Tree-Structured Graphical Models. Ph.D. thesis, MIT.\n",
      "\n",
      "(2011).\n",
      "\n",
      "Choset, H. and K. Nagatani\n",
      "\n",
      "(2001). Topological simultaneous localiza- tion and mapping (SLAM): toward exact localization without explicit localization. IEEE Trans. Robotics and Automation 17 (2).\n",
      "\n",
      "Chow, C. K. and C. N. Liu (1968). Approximating discrete probabil- ity distributions with dependence trees. IEEE Trans. on Info. Theory 14, 462–67.\n",
      "\n",
      "Christensen, O., G. Roberts,\n",
      "\n",
      "and M. SkÃ˝uld (2006). Robust Markov chain Monte Carlo methods for spatial generalized linear mixed models. J. of Computational and Graphical Statistics 15, 1–17.\n",
      "\n",
      "Cleveland, W. and S. Devlin (1988). Locally-weighted regression: An approach to regression analysis by J. of the Am. Stat. As- local ﬁtting. soc. 83(403), 596–610.\n",
      "\n",
      "Collins, M.\n",
      "\n",
      "Discrimina- tive Training Methods for Hidden Markov Models: Theory and Exper- iments with Perceptron Algorithms. In EMNLP.\n",
      "\n",
      "(2002).\n",
      "\n",
      "Collins, M., S. Dasgupta, and R. E. Schapire (2002). A generalization of principal components analysis to the exponential family. In NIPS- 14.\n",
      "\n",
      "Collins, M. and N. Duffy (2002). Con- volution kernels for natural lan- guage. In NIPS.\n",
      "\n",
      "Collobert, R. and J. Weston (2008). A Uniﬁed Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Combettes, P. and V. Wajs (2005). Sig- nal recovery by proximal forward- backward splitting. SIAM J. Multi- scale Model. Simul. 4(4), 1168–1200.\n",
      "\n",
      "Cook,\n",
      "\n",
      "Exact Calculation of Beta Inequalities. Technical re- port, M. D. Anderson Cancer Cen- ter, Dept. Biostatistics.\n",
      "\n",
      "J.\n",
      "\n",
      "(2005).\n",
      "\n",
      "Chib, S.\n",
      "\n",
      "(1995). Marginal from the Gibbs output. Am. Stat. Assoc. 90, 1313–1321.\n",
      "\n",
      "likelihood J. of the\n",
      "\n",
      "Chickering, D.\n",
      "\n",
      "Learning (1996). Bayesian networks is NP-Complete. In AI/Stats V.\n",
      "\n",
      "Chickering, D. and D. Heckerman (1997). Efficient approximations for the marginal likelihood of incom- plete data given a Bayesian net- work. Machine Learning 29, 181– 212.\n",
      "\n",
      "Chickering, D. M.\n",
      "\n",
      "(2002). Optimal structure identiﬁcation with greedy search. Journal of Machine Learn- ing Research 3, 507–554.\n",
      "\n",
      "Chung, F. (1997). Spectral Graph The-\n",
      "\n",
      "ory. AMS.\n",
      "\n",
      "Cimiano, P., A. Schultz, S. Sizov, P. Sorg, and S. Staab (2009). Ex- plicit versus latent concept models for cross-language information re- trieval. In Intl. Joint Conf. on AI.\n",
      "\n",
      "Cipra, B. (2000). The Ising Model Is NP-Complete. SIAM News 33(6).\n",
      "\n",
      "Ciresan, D. C., U. Meier, L. M. Gambardella, and J. Schmidhuber (2010). Deep big simple neural nets for handwritten digit recognition. Neural Computation 22(12), 3207– 3220.\n",
      "\n",
      "Cooper, G. and E. Herskovits (1992). A Bayesian method for the induc- tion of probabilistic networks from data. Machine Learning 9, 309–347.\n",
      "\n",
      "Cooper, G. and C. Yoo (1999). Causal discovery from a mixture of exper- imental and observational data. In UAI.\n",
      "\n",
      "Cover, T. and P. Hart (1967).\n",
      "\n",
      "Near- est neighbor pattern classiﬁcation. IEEE Trans. Inform. Theory 13(1), 21– 27.\n",
      "\n",
      "Cover, T. M. and J. A. Thomas (1991). Elements of Information The- ory. John Wiley.\n",
      "\n",
      "Chipman, H., E. George, and R. Mc- Bayesian CART J. of the Am. Stat.\n",
      "\n",
      "Culloch (1998). model search. Assoc. 93, 935–960.\n",
      "\n",
      "Clarke, B.\n",
      "\n",
      "Bayes model av- eraging and stacking when model approximation error cannot be ig- nored. J. of Machine Learning Re- search, 683–712.\n",
      "\n",
      "(2003).\n",
      "\n",
      "Cover, T. M. and J. A. Thomas Information Elements of (2006). Theory. John Wiley. 2nd edition.\n",
      "\n",
      "Chipman, H., E. George, and R. Mc- Culloch (2001). The practical imple- mentation of Bayesian Model Se- IMS Lec- lection. Model Selection. ture Notes.\n",
      "\n",
      "Clarke, B., E. Fokoue, and H. H. Zhang (2009). Principles and Theory for Data Mining and Machine Learn- ing. Springer.\n",
      "\n",
      "Cowles, M. and B. Carlin (1996). Markov chain monte carlo conver- gence diagnostics: A comparative J. of the Am. Stat. Assoc. 91, review. 883–904.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1021\n",
      "\n",
      "Crisan, D., P. D. Moral, and T. Lyons (1999). Discrete ﬁltering using branching and interacting particle systems. Markov Processes and Re- lated Fields 5(3), 293–318.\n",
      "\n",
      "Dawid, A. P. and S. L. Lauritzen (1993). Hyper-markov laws in the statistical analysis of decompos- able graphical models. The Annals of Statistics 3, 1272–1317.\n",
      "\n",
      "Dempster, A. P., N. M. Laird, and D. B. Rubin (1977). Maximum likelihood from incomplete data via the EM J. of the Royal Statistical algorithm. Society, Series B 34, 1–38.\n",
      "\n",
      "Cui, Y., X. Z. Fern, and J. G. Dy (2010). Learning multiple nonre- dundant clusterings. ACM Transac- tions on Knowledge Discovery from Data 4(3).\n",
      "\n",
      "de Freitas, N., R. Dearden, F. Hut- ter, R. Morales-Menendez, J. Mutch, and D. Poole (2004). Diagnosis by a waiter and a mars explorer. Proc. IEEE 92(3).\n",
      "\n",
      "Denison, D., C. Holmes, B. Mallick, and A. Smith (2002). Bayesian methods for nonlinear classiﬁcation and regression. Wiley.\n",
      "\n",
      "Cukier, K. (2010, February). Data, data\n",
      "\n",
      "everywhere.\n",
      "\n",
      "Dagum, P. and M. Luby (1993). Ap- proximating probabilistic inference in Bayesian belief networks is NP- hard. Artiﬁcial Intelligence 60, 141– 153.\n",
      "\n",
      "Dahl, J., L. Vandenberghe, and V. Roy- chowdhury (2008, August). Co- variance selection for non-chordal graphs via chordal embedding. Optimization Methods and Soft- ware 23(4), 501–502.\n",
      "\n",
      "Dahlhaus, R. and M. Eichler (2000). Causality and graphical models for time series. In P. Green, N. Hjort, and S. Richardson (Eds.), Highly structured stochastic systems. Ox- ford University Press.\n",
      "\n",
      "Dallal, S. and W. Hall (1983). Approxi- mating priors by mixtures of natu- ral conjugate priors. J. of Royal Stat. Soc. Series B 45, 278–286.\n",
      "\n",
      "Darwiche, A.\n",
      "\n",
      "(2009). Modeling and Reasoning with Bayesian Networks. Cambridge.\n",
      "\n",
      "Daume, H.\n",
      "\n",
      "Fast search for Dirichlet process mixture models. In AI/Statistics.\n",
      "\n",
      "(2007a).\n",
      "\n",
      "Daume, H. (2007b). Frustratingly easy domain adaptation. In Proc. the As- soc. for Comp. Ling.\n",
      "\n",
      "Dawid, A. P. (1992). Applications of a general propagation algorithm for probabilistic expert systems. Statis- tics and Computing 2, 25–36.\n",
      "\n",
      "Dawid, A. P.\n",
      "\n",
      "Inﬂuence dia- grams for causal modelling and in- ference. Intl. Stat. Review 70, 161– 189. Corrections p437.\n",
      "\n",
      "(2002).\n",
      "\n",
      "Dawid, A. P. (2010). Beware of the DAG! J. of Machine Learning Research 6, 59–86.\n",
      "\n",
      "de Freitas, N., M. Niranjan, and A. Gee (2000). Hierarchical Bayesian mod- els for regularisation in sequential learning. Neural Computation 12(4), 955–993.\n",
      "\n",
      "Dechter, R. (1996). Bucket elimination: a unifying framework for proba- bilistic inference. In UAI.\n",
      "\n",
      "Dechter, R. (2003). Constraint Process-\n",
      "\n",
      "ing. Morgan Kaufmann.\n",
      "\n",
      "Decoste, D. and B. Schoelkopf (2002). Training invariant support vector machines. Machine learnng 41, 161– 190.\n",
      "\n",
      "Deerwester, S., S. Dumais, G. Fur- nas, T. Landauer, and R. Harshman (1990). Indexing by latent semantic analysis. J. of the American Society for Information Science 41(6), 391– 407.\n",
      "\n",
      "DeGroot, M. (1970). Optimal Statistical\n",
      "\n",
      "Decisions. McGraw-Hill.\n",
      "\n",
      "Deisenroth, M., C. Rasmussen, and J. Peters (2009). Gaussian Process Dynamic Programming. Neurocom- puting 72(7), 1508–1524.\n",
      "\n",
      "Dellaportas,\n",
      "\n",
      "and G. Roberts (2003). Bayesian infer- ence for nondecomposable graphi- cal gaussian models. Sankhya, Ser. A 65, 43–55.\n",
      "\n",
      "P.,\n",
      "\n",
      "P. Giudici,\n",
      "\n",
      "Dellaportas, P. and A. F. M. Smith (1993). Bayesian Inference for Gen- eralized Linear and Proportional Hazards Models via Gibbs Sam- pling. the Royal Statisti- cal Society. Series C (Applied Statis- tics) 42(3), 443–459.\n",
      "\n",
      "J. of\n",
      "\n",
      "Delyon,\n",
      "\n",
      "and E. Moulines (1999). Convergence of a stochastic approximation version of the EM algorithm. Annals of Statistics 27 (1), 94–128.\n",
      "\n",
      "B., M.\n",
      "\n",
      "Lavielle,\n",
      "\n",
      "Dempster, A. (1972). Covariance selec-\n",
      "\n",
      "tion. Biometrics 28(1).\n",
      "\n",
      "Denison, D., B. Mallick, and A. Smith (1998). A Bayesian CART algorithm. Biometrika 85, 363–377.\n",
      "\n",
      "Desjardins, G. and Y. Bengio (2008). Empirical evaluation of convolu- tional RBMs for vision. Technical Report 1327, U. Montreal.\n",
      "\n",
      "Dey, D., S. Ghosh, and B. Mallick (Eds.) (2000). Generalized Linear Models: A Bayesian Perspective. Chapman & Hall/CRC Biostatistics Series.\n",
      "\n",
      "Diaconis, P., S. Holmes, and R. Mont- gomery (2007). Dynamical Bias in the Coin Toss. SIAM Review 49(2), 211–235.\n",
      "\n",
      "Diaconis, P. and D. Ylvisaker (1985). In\n",
      "\n",
      "Quantifying prior opinion. Bayesian Statistics 2.\n",
      "\n",
      "Dietterich, T. G. and G. Bakiri (1995). Solving multiclass learning prob- lems via ECOCs. J. of AI Research 2, 263–286.\n",
      "\n",
      "Diggle, P. and P. Ribeiro (2007). Model-\n",
      "\n",
      "based Geostatistics. Springer.\n",
      "\n",
      "Ding, Y. and R. Harrison (2010). A sparse multinomial probit model for classiﬁcation. Pattern Analysis and Applications, 1–9.\n",
      "\n",
      "Dobra, A.\n",
      "\n",
      "(2009). Dependency net- works for genome-wide data. Tech- nical report, U. Washington.\n",
      "\n",
      "Dobra, A. and H. Massam (2010). The mode oriented stochastic search (MOSS) algorithm for log-linear models with conjugate priors. Sta- tistical Methodology 7, 240–253.\n",
      "\n",
      "Domingos, P. and D. Lowd (2009). Markov Logic: An Interface Layer for AI. Morgan & Claypool.\n",
      "\n",
      "Domingos, P. and M. Pazzani (1997). On the optimality of the simple bayesian classiﬁer under zero-one loss. Machine Learning 29, 103– 130.\n",
      "\n",
      "1022\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Domke, J., A. Karapurkar, and Y. Aloi- monos (2008). Who killed the di- rected model? In CVPR.\n",
      "\n",
      "Doucet, A., N. de Freitas, and N. J. Gor- don (2001). Sequential Monte Carlo Methods in Practice. Springer Ver- lag.\n",
      "\n",
      "Doucet, A., N. Gordon, and V. Krish- namurthy (2001). Particle Filters for State Estimation of Jump Markov Linear Systems. IEEE Trans. on Sig- nal Processing 49(3), 613–624.\n",
      "\n",
      "Dow, J. and J. Endersby (2004). Multi- nomial probit and multinomial logit: a comparison of choice mod- els for voting research. Electoral Studies 23(1), 107–122.\n",
      "\n",
      "Drineas, P., A. Frieze, R. Kannan, S. Vempala, and V. Vinay (2004). Clustering large graphs via the sin- gular value decomposition. Ma- chine Learning 56, 9–33.\n",
      "\n",
      "Drugowitsch, J. (2008). Bayesian lin- ear regression. Technical report, U. Rochester.\n",
      "\n",
      "Druilhet, P. and J.-M. Marin (2007). In- variant HPD credible sets and MAP estimators. Bayesian Analysis 2(4), 681–692.\n",
      "\n",
      "Duane, S., A. Kennedy, B. Pendle- ton, and D. Roweth (1987). Hy- brid Monte Carlo. Physics Letters B 195(2), 216–222.\n",
      "\n",
      "Duchi,\n",
      "\n",
      "J., S. Gould, and D. Koller (2008). Projected subgradient methods for learning sparse gaus- sians. In UAI.\n",
      "\n",
      "Duchi,\n",
      "\n",
      "J., E. Hazan, and Y. Singer (2010). Adaptive Subgradient Meth- and ods for Online Learning Stochastic Optimization. In Proc. of the Workshop on Computational Learning Theory.\n",
      "\n",
      "Duchi, J., S. Shalev-Shwartz, Y. Singer, and T. Chandra (2008). Efficient for projections onto the L1-ball learning in high dimensions. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Duchi, J. and Y. Singer (2009). Boost- ing with structural sparsity. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Duda, R. O., P. E. Hart, and D. G. Stork (2001). Pattern Classiﬁcation. Wiley Interscience. 2nd edition.\n",
      "\n",
      "Dumais, S. and T. Landauer (1997). A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and repre- sentation of knowledge. Psycholog- ical Review 104, 211–240.\n",
      "\n",
      "Dunson, D., J. Palomo, and K. Bollen (2005). Bayesian Structural Equa- tion Modeling. Technical Report 2005-5, SAMSI.\n",
      "\n",
      "Durbin,\n",
      "\n",
      "J. and S. J. Koopman (2001). Time Series Analysis by State Space Methods. Oxford University Press.\n",
      "\n",
      "Durbin, R., S. Eddy, A. Krogh, and G. Mitchison (1998). Biological Se- quence Analysis: Probabilistic Mod- els of Proteins and Nucleic Acids. Cambridge: Cambridge University Press.\n",
      "\n",
      "Earl, D. and M. Deem (2005). Paral- lel tempering: Theory, applications, and new perspectives. Phys. Chem. Chem. Phys. 7, 3910.\n",
      "\n",
      "Eaton, D. and K. Murphy (2007). Exact Bayesian structure learning from In AI/S- uncertain interventions. tatistics.\n",
      "\n",
      "Edakunni, N., S. Schaal, and S. Vi- jayakumar (2010). Probabilistic in- cremental locally weighted learn- ing using randomly varying coeffi- cient model. Technical report, USC.\n",
      "\n",
      "Edwards, D., G. de Abreu,\n",
      "\n",
      "and R. Labouriau (2010). Selecting high- dimensional mixed graphical mod- els using minimal AIC or BIC forests. BMC Bioinformatics 11(18).\n",
      "\n",
      "Efron, B. (1986). Why Isn’t Everyone a Bayesian? The American Statisti- cian 40(1).\n",
      "\n",
      "Efron, B.\n",
      "\n",
      "Large-Scale Infer- ence: Empirical Bayes Methods for Estimation, Testing, and Prediction. Cambridge.\n",
      "\n",
      "(2010).\n",
      "\n",
      "Efron, B., I. Johnstone, T. Hastie, and R. Tibshirani (2004). Least angle re- gression. Annals of Statistics 32(2), 407–499.\n",
      "\n",
      "Elad, M. and I. Yavnch (2009). A plu- rality of sparse representations is better than the sparsest one alone. IEEE Trans. on Info. Theory 55(10), 4701–4714.\n",
      "\n",
      "Elidan, G. and S. Gould (2008). Learn- ing Bounded Treewidth Bayesian Networks. J. of Machine Learning Research, 2699–2731.\n",
      "\n",
      "Elidan, G., N. Lotner, N. Friedman, and D. Koller (2000). Discovering hid- den variables: A structure-based approach. In NIPS.\n",
      "\n",
      "Elidan, G., I. McGraw, and D. Koller Residual belief propa- (2006). gation: Informed scheduling for asynchronous message passing. In UAI.\n",
      "\n",
      "Elkan, C. (2003). Using the triangle in- equality to accelerate k-means. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Elkan, C. (2005). Deriving TF-IDF as a Fisher kernel. In Proc. Intl. Symp. on String Processing and Informa- tion Retrieval (SPIRE), pp. 296–301.\n",
      "\n",
      "Elkan, C.\n",
      "\n",
      "Clustering docu- ments with an exponential fmaily the Dirichlet approximation of In compoind multinomial model. Intl. Conf. on Machine Learning.\n",
      "\n",
      "(2006).\n",
      "\n",
      "Ellis, B. and W. H. Wong (2008). Learn- ing causal bayesian network struc- tures from experimental data. J. of the Am. Stat. Assoc. 103(482), 778– 789.\n",
      "\n",
      "Engel, Y., S. Mannor, and R. Meir Reinforcement Learning In Intl.\n",
      "\n",
      "(2005). with Gaussian Processes. Conf. on Machine Learning.\n",
      "\n",
      "Erhan, D., Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Ben- gio (2010). Why Does Unsupervised Pre-training Help Deep Learning? J. of Machine Learning Research 11, 625–660.\n",
      "\n",
      "Erosheva, and C. Describing disability through individual-level mixture models for multivariate bi- nary data. Annals of Applied Statis- tics.\n",
      "\n",
      "Joutard (2007).\n",
      "\n",
      "E.,\n",
      "\n",
      "S.\n",
      "\n",
      "Fienberg,\n",
      "\n",
      "Duchi,\n",
      "\n",
      "J., D. Tarlow, G. Elidan, and D. Koller (2007). Using combi- natorial optimization within max- In product belief propagation. NIPS.\n",
      "\n",
      "Efron, B. and C. Morris (1975). Data analysis using stein’s estimator and its generalizations. J. of the Am. Stat. Assoc. 70(350), 311–319.\n",
      "\n",
      "Erosheva, E., S. Fienberg, and J. Laf- ferty (2004). Mixed-membership models of scientiﬁc publications. Proc. of the National Academy of Science, USA 101, 5220–2227.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1023\n",
      "\n",
      "Escobar, M. D. and M. West (1995). Bayesian density estimation and inference using mixtures. J. of the Am. Stat. Assoc. 90(430), 577–588.\n",
      "\n",
      "Ewens, W.\n",
      "\n",
      "(1990). Population genet- ics theory - the past and the fu- ture. In S.Lessard (Ed.), Mathemeti- cal and Statistica Developments of Evolutionary Theory, pp. 177–227. Reidel.\n",
      "\n",
      "Fan, J. and R. Z. Li (2001). Variable se- lection via non-concave penalized likelihood and its oracle properties. J. of the Am. Stat. Assoc. 96(456), 1348–1360.\n",
      "\n",
      "Finkel, J. and C. Manning (2009). Hier- archical bayesian domain adapta- tion. In Proc. NAACL, pp. 602–610.\n",
      "\n",
      "Fischer, B. and J. Schumann (2003). Autobayes: A system for generating data analysis programs from sta- tistical models. J. Functional Pro- gramming 13(3), 483–508.\n",
      "\n",
      "Fishelson, M. and D. Geiger (2002). Exact genetic linkage computations for general pedigrees. BMC Bioin- formatics 18.\n",
      "\n",
      "Fletcher, R.\n",
      "\n",
      "Borwein Method. mization 96, 235–256.\n",
      "\n",
      "(2005). On the Barzilai- Applied Opti-\n",
      "\n",
      "Frey, B.\n",
      "\n",
      "Extending factor graphs so as to unify directed and undirected graphical models. In UAI.\n",
      "\n",
      "(2003).\n",
      "\n",
      "Frey, B. and D. Dueck (2007, Febru- ary). Clustering by Passing Mes- sages Between Data Points. Sci- ence 315, 972â ˘A ¸S976.\n",
      "\n",
      "Friedman,\n",
      "\n",
      "Multivariate adaptive regression splines. Ann. Statist. 19, 1–67.\n",
      "\n",
      "J.\n",
      "\n",
      "(1991).\n",
      "\n",
      "Friedman, J. (1997a). On bias, variance, 0-1 loss and the curse of dimen- sionality. J. Data Mining and Knowl- edge Discovery 1, 55–77.\n",
      "\n",
      "Fearnhead, P. (2004). Exact bayesian curve ﬁtting and signal segmen- tation. IEEE Trans. Signal Process- ing 53, 2160–2166.\n",
      "\n",
      "Fokoue, E. (2005). Mixtures of factor analyzers: an extension with co- variates. J. Multivariate Analysis 95, 370–384.\n",
      "\n",
      "Friedman, J. (2001). Greedy function approximation: a gradient boost- ing machine. Annals of Statistics 29, 1189–1232.\n",
      "\n",
      "Felzenszwalb, P. and D. Huttenlocher (2006). Efficient belief propagation for early vision. Intl. J. Computer Vision 70(1), 41–54.\n",
      "\n",
      "Forbes, J., T. Huang, K. Kanazawa, and S. Russell (1995). The BATmobile: Towards a Bayesian automated taxi. In Intl. Joint Conf. on AI.\n",
      "\n",
      "Friedman, J., T. Hastie, and R. Tibshi- rani (2000). Additive logistic regres- sion: a statistical view of boosting. Annals of statistics 28(2), 337–374.\n",
      "\n",
      "Ferrucci, D., E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. Kalyanpur, A. Lally, J. W. Murdock, E. N. amd J. Prager, N. Schlaefter, and C. Welty (2010). Building Wat- son: An Overview of the DeepQA Project. AI Magazine, 59–79.\n",
      "\n",
      "Fienberg, S. (1970). An iterative pro- cedure for estimation in contin- gency tables. Annals of Mathemat- ical Statistics 41(3), 907â ˘A ¸S917.\n",
      "\n",
      "Figueiredo, M.\n",
      "\n",
      "Adaptive (2003). sparseness for supervised learn- ing. IEEE Trans. on Pattern Anal- ysis and Machine Intelligence 25(9), 1150–1159.\n",
      "\n",
      "R. Nowak, and Figueiredo, M., S. Wright (2007). Gradient pro- jection for sparse reconstruction: application to compressed sensing and other inverse problems. IEEE. J. on Selected Topics in Signal Pro- cessing.\n",
      "\n",
      "Figueiredo, M. A. T. and A. K.\n",
      "\n",
      "Jain (2002). Unsupervised learning of ﬁ- nite mixture models. IEEE Trans. on Pattern Analysis and Machine Intel- ligence 24(3), 381–396. Matlab code at http://www.lx.it.pt/ mtf/mixture- code.zip.\n",
      "\n",
      "Forsyth, D. and J. Ponce (2002). Com- puter vision: a modern approach. Prentice Hall.\n",
      "\n",
      "Fraley, C. and A. Raftery (2002). Model-based clustering, discrimi- nant analysis, and density estima- tion. J. of the Am. Stat. Assoc. (97), 611–631.\n",
      "\n",
      "Fraley, C. and A. Raftery (2007). Bayesian Regularization for Normal Mixture Estimation and Model- Based Clustering. J. of Classiﬁca- tion 24, 155–181.\n",
      "\n",
      "Franc, V., A. Zien, and B. Schoelkopf (2011). Support vector machines as In Intl. Conf. probabilistic models. on Machine Learning.\n",
      "\n",
      "Frank,\n",
      "\n",
      "I. and J. Friedman (1993). A statistical view of some chemomet- rics regression tools. Technomet- rics 35(2), 109–135.\n",
      "\n",
      "Fraser, A. (2008). Hidden Markov Mod- els and Dynamical Systems. SIAM Press.\n",
      "\n",
      "Freund, Y. and R. R. Schapire (1996). Experiments with a new boosting algorithm. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Friedman,\n",
      "\n",
      "J., T. Hastie, and R. Tib- shirani (2008). Sparse inverse co- variance estimation the graphical lasso. Biostatistics 9(3), 432–441.\n",
      "\n",
      "Friedman, J., T. Hastie, and R. Tibshi- rani (2010, Februrary). Regulariza- tion Paths for Generalized Linear Models via Coordinate Descent. J. of Statistical Software 33(1).\n",
      "\n",
      "Friedman, N.\n",
      "\n",
      "(1997b). Learning Bayesian networks in the presence of missing values and hidden vari- ables. In UAI.\n",
      "\n",
      "Friedman, N., D. Geiger, and M. Gold- szmidt (1997). Bayesian network classiﬁers. Machine Learning J. 29, 131–163.\n",
      "\n",
      "Friedman, N., D. Geiger, and N. Lot- ner (2000). Likelihood computation with value abstraction. In UAI.\n",
      "\n",
      "Friedman, N. and D. Koller (2003). Be- ing Bayesian about Network Struc- ture: A Bayesian Approach to Structure Discovery in Bayesian Networks. Machine Learning 50, 95–126.\n",
      "\n",
      "Friedman, N., M. Ninion, I. Pe’er, and T. Pupko (2002). A Structural EM Algorithm for Phylogenetic Infer- ence. J. Comp. Bio. 9, 331–353.\n",
      "\n",
      "Fine, S., Y. Singer, and N. Tishby (1998). The hierarchical Hidden Markov Model: Analysis and appli- cations. Machine Learning 32, 41.\n",
      "\n",
      "Frey, B.\n",
      "\n",
      "(1998). Graphical Models for Machine Learning and Digital Com- munication. MIT Press.\n",
      "\n",
      "Friedman, N. and Y. Singer (1999). Ef- ﬁcient Bayesian parameter estima- In tion in large discrete domains. NIPS-11.\n",
      "\n",
      "1024\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Fruhwirth-Schnatter, S.\n",
      "\n",
      "Fi- nite Mixture and Markov Switching Models. Springer.\n",
      "\n",
      "(2007).\n",
      "\n",
      "Fruhwirth-Schnatter, S. and R. Fruh- wirth (2010). Data Augmentation and MCMC for Binary and Multi- nomial Logit Models. In T. Kneib and G. Tutz (Eds.), Statistical Mod- elling and Regression Structures, pp. 111–132. Springer.\n",
      "\n",
      "Fu, W. (1998). Penalized regressions: the bridge verus the lasso. J. Com- putational and graphical statistics.\n",
      "\n",
      "Gelman, A.,\n",
      "\n",
      "J. Carlin, H. Stern, and D. Rubin (2004). Bayesian data analysis. Chapman and Hall. 2nd edition.\n",
      "\n",
      "Gelman, A. and J. Hill\n",
      "\n",
      "(2007). Data analysis using regression and mul- tilevel/ hierarchical models. Cam- bridge.\n",
      "\n",
      "Gelman, A. and X.-L. Meng (1998). Simulating normalizing constants: from importance to bridge sampling to path sampling. Statisical Science 13, 163–185.\n",
      "\n",
      "sampling\n",
      "\n",
      "Ghahramani, Z. and G. Hinton (1996b). Parameter estimation for linear dy- namical systems. Technical Re- port CRG-TR-96-2, Dept. Comp. Sci., Univ. Toronto.\n",
      "\n",
      "Ghahramani, Z. and M. Jordan (1997). Factorial hidden Markov models. Machine Learning 29, 245–273.\n",
      "\n",
      "Gilks, W. and C. Berzuini\n",
      "\n",
      "(2001). Following target – a moving Monte Carlo infernece for dynamic Bayesian models. J. of Royal Stat. Soc. Series B 63, 127–146.\n",
      "\n",
      "Fukushima, K.\n",
      "\n",
      "(1975). Cognitron: a self-organizing multilayered neu- ral network. Biological Cybernet- ics 20(6), 121–136.\n",
      "\n",
      "Gelman, A. and T. Raghunathan (2001). Using conditional distributions for missing-data imputation. Statistical Science.\n",
      "\n",
      "Gilks, W., N. Best, and K. Tan (1995). Adaptive rejection Metropolis sam- pling. Applied Statistics 44, 455–472.\n",
      "\n",
      "Fung, R. and K. Chang (1989). Weight- ing and integrating evidence for stochastic simulation in Bayesian networks. In UAI.\n",
      "\n",
      "Gabow, H., Z. Galil, and T. Spencer (1984). implementation of graph algorithms using contrac- In IEEE Symposium on the tion. Foundations of Computer Science.\n",
      "\n",
      "Efficient\n",
      "\n",
      "Gales, M.\n",
      "\n",
      "(2002). Maximum like- lihood multiple subspace projec- tions for hidden Markov models. IEEE. Trans. on Speech and Audio Processing 10(2), 37–47.\n",
      "\n",
      "Gales, M. J. F. (1999). Semi-tied covari- ance matrices for hidden Markov models. IEEE Trans. on Speech and Audio Processing 7 (3), 272–281.\n",
      "\n",
      "Gamerman, D. (1997). Efficient sam- pling from the posterior distribu- tion in generalized linear mixed models. Statistics and Computing 7, 57–68.\n",
      "\n",
      "Geiger, D. and D. Heckerman (1994). Learning Gaussian networks. In UAI, Volume 10, pp. 235–243.\n",
      "\n",
      "Geiger, D. and D. Heckerman (1997). A characterization of Dirchlet dis- tributions through local and global independence. Annals of Statis- tics 25, 1344–1368.\n",
      "\n",
      "Gelfand, A. (1996). Model determina- tion using sampling-based meth- ods. In Gilks, Richardson, and Spiegelhalter (Eds.), Markov Chain Monte Carlo in Practice. Chapman & Hall.\n",
      "\n",
      "Gelfand, A. and A. Smith (1990). Sampling-based approaches to cal- culating marginal densities. J. of the Am. Stat. Assoc. 85, 385–409.\n",
      "\n",
      "Gelman, A. and D. Rubin (1992). Infer- ence from iterative simulation us- ing multiple sequences. Statistical Science 7, 457–511.\n",
      "\n",
      "Geman,\n",
      "\n",
      "and R. Doursat (1992). Neural networks and the bias-variance dilemma. Neural Computing 4, 1–58.\n",
      "\n",
      "S.,\n",
      "\n",
      "E. Bienenstock,\n",
      "\n",
      "Geman, S. and D. Geman (1984). Stochastic relaxation, Gibbs distri- butions, and the Bayesian restora- tion of images. IEEE Trans. on Pat- tern Analysis and Machine Intelli- gence 6(6).\n",
      "\n",
      "Geoffrion, A.\n",
      "\n",
      "Lagrangian relaxation for integer program- ming. Mathematical Programming Study 2, 82–114.\n",
      "\n",
      "(1974).\n",
      "\n",
      "George, E. and D. Foster (2000). Cal- ibration and empirical bayes vari- able selection. Biometrika 87 (4), 731–747.\n",
      "\n",
      "Getoor, L. and B. Taskar (Eds.) (2007). Introduction to Relational Statistical Learning. MIT Press.\n",
      "\n",
      "Geyer, C.\n",
      "\n",
      "Practical markov chain monte carlo. Statistical Sci- ence 7, 473–483.\n",
      "\n",
      "(1992).\n",
      "\n",
      "Ghahramani, Z. and M. Beal\n",
      "\n",
      "Variational mixtures of factor analysers. NIPS-12.\n",
      "\n",
      "(2000). inference for Bayesian In\n",
      "\n",
      "Ghahramani, Z. and M. Beal\n",
      "\n",
      "(2001). Propagation algorithms for varia- tional Bayesian learning. In NIPS- 13.\n",
      "\n",
      "Ghahramani, Z. and G. Hinton (1996a). The EM algorithm for mixtures of factor analyzers. Technical report, Dept. of Comp. Sci., Uni. Toronto.\n",
      "\n",
      "Gilks, W. and P. Wild (1992). Adaptive rejection sampling for Gibbs sam- pling. Applied Statistics 41, 337–348.\n",
      "\n",
      "Girolami, M., B. Calderhead,\n",
      "\n",
      "and S. Chin (2010). Riemannian Man- ifold Hamiltonian Monte Carlo. J. of Royal Stat. Soc. Series B. To ap- pear.\n",
      "\n",
      "Girolami, M. and S. Rogers (2005). Hi- erarchic bayesian models for kernel learning. In Intl. Conf. on Machine Learning, pp. 241–248.\n",
      "\n",
      "Girolami, M. and S. Rogers (2006). Variational Bayesian multinomial probit regression with Gaussian process priors. Neural Comptua- tion 18(8), 1790 – 1817.\n",
      "\n",
      "Girshick, R., P. Felzenszwalb, and D. McAllester (2011). Object de- tection with grammar models. In NIPS.\n",
      "\n",
      "Gittins, J. (1989). Multi-armed Bandit\n",
      "\n",
      "Allocation Indices. Wiley.\n",
      "\n",
      "Giudici,\n",
      "\n",
      "P. Decomposable sian model Biometrika 86(4), 785–801.\n",
      "\n",
      "and P. Green (1999). gaus- graphical determination.\n",
      "\n",
      "Givoni, I. E. and B. J. Frey (2009, June). A binary variable model for affin- ity propagation. Neural Computa- tion 21(6), 1589–1600.\n",
      "\n",
      "Globerson, A. and T. Jaakkola (2008). Fixing max-product: Convergent for message passing algorithms MAP LP-relaxations. In NIPS.\n",
      "\n",
      "Glorot, X. and Y. Bengio (2010, May). Understanding the difficulty of training deep feedforward neural networks. In AI/Statistics, Volume 9, pp. 249–256.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1025\n",
      "\n",
      "Gogate, V., W. A. Webb, and P. Domin- Learning efficient\n",
      "\n",
      "gos Markov networks. In NIPS.\n",
      "\n",
      "(2010).\n",
      "\n",
      "Goldenberg, A., A. X. Zheng, S. E. Fien- berg, and E. M. Airoldi (2009). A Survey of Statistical Network Mod- els. Foundations and Trends in Ma- chine Learning, 129–233.\n",
      "\n",
      "Golub, G. and C. F. van Loan (1996). Johns Hop-\n",
      "\n",
      "Matrix computations. kins University Press.\n",
      "\n",
      "Gonen, M., W. Johnson, Y. Lu, and The P. Westfall (2005, August). Bayesian Two-Sample t Test. The American Statistician 59(3), 252– 257.\n",
      "\n",
      "Gonzales, T.\n",
      "\n",
      "Clustering to minimize the maximum interclus- ter distance. Theor. Comp. Sci. 38, 293–306.\n",
      "\n",
      "(1985).\n",
      "\n",
      "Gorder, P. F.\n",
      "\n",
      "(2006, Nov/Dec). Neu- ral networks show new promise for machine vision. Computing in sci- ence & engineering 8(6), 4–8.\n",
      "\n",
      "Gordon, N.\n",
      "\n",
      "ap- proach to nonlinear/non-Gaussian Bayesian state estimation. IEE Pro- ceedings (F) 140(2), 107–113.\n",
      "\n",
      "(1993).\n",
      "\n",
      "Novel\n",
      "\n",
      "Graepel, T.,\n",
      "\n",
      "J. Quinonero-Candela, T. Borchert, and R. Herbrich (2010). Web-Scale Bayesian Click- Through Rate Prediction for Spon- sored Search Advertising in Mi- crosoftâ ˘A ´Zs Bing Search Engine. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Grauman, K. and T. Darrell\n",
      "\n",
      "(2007, April). The Pyramid Match Kernel: Efficient Learning with Sets of Fea- tures. J. of Machine Learning Re- search 8, 725–760.\n",
      "\n",
      "Green, P.\n",
      "\n",
      "Reversible jump Markov chain Monte Carlo compu- tation and Bayesian model deter- mination. Biometrika 82, 711–732.\n",
      "\n",
      "(1998).\n",
      "\n",
      "Green, P.\n",
      "\n",
      "(2003). Tutorial on trans- dimensional MCMC. In P. Green, N. Hjort, and S. Richardson (Eds.), Highly Structured Stochastic Systems. OUP.\n",
      "\n",
      "Greenshtein, E. and J. Park (2009). Ap- plication of Non Parametric Empir- ical Bayes Estimation to High Di- J. of Ma- mensional Classiﬁcation. chine Learning Research 10, 1687– 1704.\n",
      "\n",
      "Greig, D., B. Porteous, and A. Seheult (1989). Exact maximum a posteriori J. of estimation for binary images. Royal Stat. Soc. Series B 51(2), 271– 279.\n",
      "\n",
      "Griffin,\n",
      "\n",
      "and P. Brown (2007). Bayesian adaptive lassos with non- convex penalization. Technical re- port, U. Kent.\n",
      "\n",
      "J.\n",
      "\n",
      "Griffin,\n",
      "\n",
      "J. and P. Brown (2010). In- ference with normal-gamma prior distributions in regression prob- lems. Bayesian Analysis 5(1), 171– 188.\n",
      "\n",
      "Griffiths, T. . and J. Tenenbaum (2009). Theory-Based Causal Induction. Psychological Review 116(4), 661– 716.\n",
      "\n",
      "Griffiths, T. and M. Steyvers (2004). Finding scientiﬁc topics. Proc. of the National Academy of Science, USA 101, 5228–5235.\n",
      "\n",
      "Griffiths, T., M. Steyvers, D. Blei, and Integrating\n",
      "\n",
      "J. Tenenbaum (2004). topics and syntax. In NIPS.\n",
      "\n",
      "Griffiths, T. and J. Tenenbaum (2001). Using vocabulary knowledge in bayesian multinomial estimation. In NIPS, pp. 1385–1392.\n",
      "\n",
      "Griffiths, T. and J. Tenenbaum (2005). Structure and strength in causal induction. Cognitive Psychology 51, 334–384.\n",
      "\n",
      "Grimmett, G. and D. Stirzaker (1992). Probability and Random Processes. Oxford.\n",
      "\n",
      "Guan, Y., J. Dy, D. Niu, and Z. Ghahra- mani (2010). Variational Inference for Nonparametric Multiple Clus- tering. In 1st Intl. Workshop on Discovering, Summarizing and Us- ing Multiple Clustering (MultiClust).\n",
      "\n",
      "Guedon, Y. (2003). Estimating hidden semi-markov chains from discrete sequences. J. of Computational and Graphical Statistics 12, 604–639.\n",
      "\n",
      "Gustafsson, M.\n",
      "\n",
      "(2001). bilistic derivation of least-squares algorithm. Chemical ing 41, 288–294.\n",
      "\n",
      "A proba- the partial Journal of Information and Model-\n",
      "\n",
      "Guyon, I., S. Gunn, M. Nikravesh, and L. Zadeh (Eds.) (2006). Feature Ex- traction: Foundations and Applica- tions. Springer.\n",
      "\n",
      "Hacker,\n",
      "\n",
      "J. and P. Pierson (2010). Winner-Take-All How Washington Made the Rich Richer– and Turned Its Back on the Middle Class. Simon & Schuster.\n",
      "\n",
      "Politics:\n",
      "\n",
      "Halevy, A., P. Norvig, and F. Pereira (2009). The unreasonable effective- ness of data. IEEE Intelligent Sys- tems 24(2), 8–12.\n",
      "\n",
      "Hall, P., J. T. Ormerod, and M. P. Wand (2011). Theory of Gaussian Varia- tional Approximation for a Gener- alised Linear Mixed Model. Statis- tica Sinica 21, 269–389.\n",
      "\n",
      "Hamilton, J. (1990). Analysis of time series subject to changes in regime. J. Econometrics 45, 39–70.\n",
      "\n",
      "Hans, C.\n",
      "\n",
      "(2009). Bayesian Lasso re- gression. Biometrika 96(4), 835– 845.\n",
      "\n",
      "Hansen, M. and B. Yu (2001). Model selection and the principle of min- imum description length. J. of the Am. Stat. Assoc..\n",
      "\n",
      "Hara, H. and A. Takimura (2008). A Localization Approach to Im- prove Iterative Proportional Scal- ing in Gaussian Graphical Models. Communications in Statistics - The- ory and Method. to appear.\n",
      "\n",
      "Hardin, J. and J. Hilbe (2003). Gener- alized Estimating Equations. Chap- man and Hall/CRC.\n",
      "\n",
      "Harmeling, S. and C. K. I. Williams (2011). Greedy learning of binary latent trees. IEEE Trans. on Pat- tern Analysis and Machine Intelli- gence 33(6), 1087–1097.\n",
      "\n",
      "Harnard,\n",
      "\n",
      "The symbol grounding problem. Physica D 42, 335–346.\n",
      "\n",
      "S.\n",
      "\n",
      "(1990).\n",
      "\n",
      "Green, P. and B. Silverman (1994). Non- parametric regression and general- ized linear models. Chapman and Hall.\n",
      "\n",
      "Guo, Y. (2009). Supervised exponential family principal component anal- In ysis via convex optimization. NIPS.\n",
      "\n",
      "Harvey, A. C. (1990). Forecasting, Struc- tural Time Series Models, and the Kalman Filter. Cambridge Univer- ity Press.\n",
      "\n",
      "1026\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Hastie, T., S. Rosset, R. Tibshirani, and J. Zhu (2004). The entire regular- ization path for the support vector J. of Machine Learning machine. Research 5, 1391–1415.\n",
      "\n",
      "and Heckerman, D., G. Cooper A Bayesian approach to causal dis- covery. Technical Report MSR-TR- 97-05, Microsoft Research.\n",
      "\n",
      "(1997, February).\n",
      "\n",
      "C. Meek,\n",
      "\n",
      "Hinton, G. E., P. Dayan, and M. Revow (1997). Modeling the manifolds of images of handwritten digits. IEEE Trans. on Neural Networks 8, 65–74.\n",
      "\n",
      "Hastie, T. and R. Tibshirani\n",
      "\n",
      "(1990). Generalized additive models. Chap- man and Hall.\n",
      "\n",
      "Hastie, T., R. Tibshirani, and J. Fried- man (2001). The Elements of Statis- tical Learning. Springer.\n",
      "\n",
      "Hastie, T., R. Tibshirani, and J. Fried- man (2009). The Elements of Statisti- cal Learning. Springer. 2nd edition.\n",
      "\n",
      "Hastings, W.\n",
      "\n",
      "(1970). Monte carlo sampling methods using markov chains applications. their Biometrika 57 (1), 97–109.\n",
      "\n",
      "and\n",
      "\n",
      "Heckerman, D., C. Meek, and D. Koller Probabilistic models for (2004). relational data. Technical Re- port MSR-TR-2004-30, Microsoft Research.\n",
      "\n",
      "Heller, K. and Z. Ghahramani (2005). Bayesian Hierarchical Clustering. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Henrion, M.\n",
      "\n",
      "Propagation of uncertainty by logic sampling in Bayes’ networks. In UAI, pp. 149– 164.\n",
      "\n",
      "(1988).\n",
      "\n",
      "Herbrich, R., T. Minka, and T. Graepel (2007). TrueSkill: A Bayesian skill rating system. In NIPS.\n",
      "\n",
      "Hinton, G. E. and Y. Teh (2001). Discovering multiple constraints that are frequently approximately satisï ˇn ˛Aed. In UAI.\n",
      "\n",
      "Hjort, N., C. Holmes, P. Muller, and Bayesian\n",
      "\n",
      "S. Walker (Eds.) Nonparametrics. Cambridge.\n",
      "\n",
      "(2010).\n",
      "\n",
      "Hoeﬂing, H. (2010). A Path Algorithm for the Fused Lasso Signal Approx- imator. Technical report, Stanford.\n",
      "\n",
      "Tibshirani R. Hoeﬂing, H. (2009). Estimation of Sparse Bi- nary Pairwise Markov Networks us- J. of Ma- ing Pseudo-likelihoods. chine Learning Research 10.\n",
      "\n",
      "and\n",
      "\n",
      "Haykin, S. (1998). Neural Networks: A Comprehensive Foundation. Pren- tice Hall. 2nd Edition.\n",
      "\n",
      "Haykin, S. (Ed.) (2001). Kalman Filter- ing and Neural Networks. Wiley.\n",
      "\n",
      "Hazan, T. and A. Shashua (2008). Convergent message-passing algo- rithms for inference over general graphs with convex free energy. In UAI.\n",
      "\n",
      "Hazan, T. and A. Shashua (2010). Norm-product belief propagation: primal-dual message passing for approximate inference. IEEE Trans. on Info. Theory 56(12), 6294–6316.\n",
      "\n",
      "He, Y.-B. and Z. Geng (2009). Active learning of causal networks with intervention experiments and opti- mal designs. J. of Machine Learning Research 10, 2523–2547.\n",
      "\n",
      "Heaton, M.\n",
      "\n",
      "(2009). Bayesian computation and the lin- ear model. Technical report, Duke.\n",
      "\n",
      "and J.\n",
      "\n",
      "Scott\n",
      "\n",
      "Heckerman,\n",
      "\n",
      "Chickering, D., and C. Meek, R. Rounthwaite, C. Kadie (2000). Dependency networks for density estimation, collaborative ﬁltering, and data vi- sualization. J. of Machine Learning Research 1, 49–75.\n",
      "\n",
      "D.\n",
      "\n",
      "Heckerman, D., D. Geiger,\n",
      "\n",
      "and Learning M. Chickering (1995). Bayesian networks: the combina- tion of knowledge and statistical data. Machine Learning 20(3), 197– 243.\n",
      "\n",
      "Hertz, J., A. Krogh, and R. G. Palmer (1991). An Introduction to the Theory of Neural Comptuation. Addison- Wesley.\n",
      "\n",
      "Hillar, C.,\n",
      "\n",
      "and K. Koepsell (2012, April). Efficient and optimal binary hopﬁeld asso- ciative memory storage using min- imum probability ﬂow. Technical report.\n",
      "\n",
      "J.\n",
      "\n",
      "Sohl-Dickstein,\n",
      "\n",
      "Hinton, G. (1999). Products of experts. In Proc. 9th Intl. Conf. on Artif. Neu- ral Networks (ICANN), Volume 1, pp. 1–6.\n",
      "\n",
      "Hinton, G. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation 14, 1771–1800.\n",
      "\n",
      "Hinton, G. (2010). A Practical Guide to Training Restricted Boltzmann Machines. Technical report, U. Toronto.\n",
      "\n",
      "Hinton, G. and D. V. Camp (1993). Keeping neural networks simple by minimizing the description length In in Proc. of the of the weights. 6th Ann. ACM Conf. on Computa- tional Learning Theory, pp. 5–13. ACM Press.\n",
      "\n",
      "Hinton, G., S. Osindero, and Y. Teh (2006). A fast learning algorithm for deep belief nets. Neural Com- putation 18, 1527–1554.\n",
      "\n",
      "Hinton, G. and R. Salakhutdinov (2006, July). Reducing the dimensionality of data with neural networks. Sci- ence 313(5786), 504–507.\n",
      "\n",
      "Hoeting,\n",
      "\n",
      "J., D. Madigan, A. Raftery, and C. Volinsky (1999). Bayesian model averaging: A tutorial. Statis- tical Science 4(4).\n",
      "\n",
      "Hoff, P. D.\n",
      "\n",
      "A First Course in Bayesian Statistical Meth- ods. Springer.\n",
      "\n",
      "(2009,\n",
      "\n",
      "July).\n",
      "\n",
      "Hoffman, M., D. Blei, and F. Bach (2010). Online learning for latent dirichlet allocation. In NIPS.\n",
      "\n",
      "Hoffman, M. and A. Gelman (2011). The no-U-turn sampler: Adaptively setting path lengths in Hamilto- nian Monte Carlo. Technical report, Columbia U.\n",
      "\n",
      "Hofmann, T. (1999). Probabilistic la- tent semantic indexing. Research and Development in Information Retrieval, 50–57.\n",
      "\n",
      "Holmes, C. and L. Held (2006). Bayesian auxiliary variable models for binary and multinomial regres- sion. Bayesian Analysis 1(1), 145– 168.\n",
      "\n",
      "Honkela, A. and H. Valpola (2004). Variational Learning and Bits-Back Coding: An Information-Theoretic View to Bayesian Learning. IEEE. Trans. on Neural Networks 15(4).\n",
      "\n",
      "Honkela,\n",
      "\n",
      "and A., J. Karhunen (2003). Accelerat- ing Cyclic Update Algorithms for Parameter Estimation by Pattern Searches. Neural Processing Let- ters 17, 191–203.\n",
      "\n",
      "H. Valpola,\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1027\n",
      "\n",
      "Hopﬁeld,\n",
      "\n",
      "Neu- ral networks and physical systems with emergent collective computa- tional abilities. Proc. of the National Academy of Science, USA 79(8), 2554â ˘A ¸S2558.\n",
      "\n",
      "J.\n",
      "\n",
      "J.\n",
      "\n",
      "(1982, April).\n",
      "\n",
      "Hunter, D. and R. Li (2005). Variable selection using MM algorithms. Annals of Statistics 33, 1617–1642.\n",
      "\n",
      "Hunter, D. R. and K. Lange (2004). A Tutorial on MM Algorithms. The American Statistician 58, 30–37.\n",
      "\n",
      "Jacob, L., F. Bach, and J.-P. Vert (2008). Clustered Multi-Task Learning: a Convex Formulation. In NIPS.\n",
      "\n",
      "Jain, A. and R. Dubes (1988). Algo- rithms for Clustering Data. Prentice Hall.\n",
      "\n",
      "Hornik, K. (1991). Approximation ca- pabilities of multilayer feedforward networks. Neural Networks 4(2), 251â ˘A ¸S257.\n",
      "\n",
      "Hyaﬁl, L. and R. Rivest (1976). Con- structing Optimal Binary Decision Trees is NP-complete. Information Processing Letters 5(1), 15–17.\n",
      "\n",
      "James, G. and T. Hastie (1998). The error coding method and PICTS. J. of Computational and Graphical Statistics 7 (3), 377–387.\n",
      "\n",
      "Horvitz, E., J. Apacible, R. Sarin, and L. Liao (2005). Prediction, Expecta- tion, and Surprise: Methods, De- signs, and Study of a Deployed Traffic Forecasting Service. In UAI.\n",
      "\n",
      "Hyvarinen, A., J. Hurri, and P. Hoyer (2009). Natural Image Statistics: a probabilistic approach to early com- putational vision. Springer.\n",
      "\n",
      "Japkowicz, N.,\n",
      "\n",
      "and M. Gluck (2000). Nonlinear autoas- sociation is not equivalent to PCA. Neural Computation 12, 531–545.\n",
      "\n",
      "S. Hanson,\n",
      "\n",
      "Howard, R. and J. Matheson (1981). In- ﬂuence diagrams. In R. Howard and J. Matheson (Eds.), Readings on the Principles and Applications of Decision Analysis, volume II. Strate- gic Decisions Group.\n",
      "\n",
      "Hoyer, P. (2004). Non-negative matrix factorizaton with sparseness con- straints. J. of Machine Learning Re- search 5, 1457–1469.\n",
      "\n",
      "Hsu, C.-W., C.-C. Chang, and C.-J. Lin (2009). A practical guide to sup- port vector classiﬁcation. Technical report, Dept. Comp. Sci., National Taiwan University.\n",
      "\n",
      "Hu, D., L. van der Maaten, Y. Cho, L. Saul, and S. Lerner (2010). Latent Variable Models for Predicting File Dependencies in Large-Scale Soft- ware Development. In NIPS.\n",
      "\n",
      "Hyvarinen, A. and E. Oja (2000).\n",
      "\n",
      "In- dependent component analysis: al- gorithms and applications. Neural Networks 13, 411–430.\n",
      "\n",
      "Ilin, A. and T. Raiko (2010). Practi- cal Approaches to Principal Com- ponent Analysis in the Presence of Missing Values. J. of Machine Learning Research 11, 1957–2000.\n",
      "\n",
      "Insua, D. R. and F. Ruggeri\n",
      "\n",
      "(Eds.) (2000). Robust Bayesian Analysis. Springer.\n",
      "\n",
      "Isard, M. (2003). PAMPAS: Real-Valued Graphical Models for Computer Vi- sion. In CVPR, Volume 1, pp. 613.\n",
      "\n",
      "Isard, M. and A. Blake (1998). CON- DENSATION - conditional density propagation for visual tracking. Intl. J. of Computer Vision 29(1), 5– 18.\n",
      "\n",
      "Jaynes, E. T.\n",
      "\n",
      "(2003). Probability the- ory: the logic of science. Cambridge university press.\n",
      "\n",
      "Jebara, T., R. Kondor, and A. Howard (2004). Probability product kernels. J. of Machine Learning Research 5, 819–844.\n",
      "\n",
      "Jeffreys, H. (1961). Theory of Probability.\n",
      "\n",
      "Oxford.\n",
      "\n",
      "Jelinek, F. (1997). Statistical methods for speech recognition. MIT Press.\n",
      "\n",
      "Jensen, C. S., A. Kong, and U. Kjaerulff Blocking-gibbs sampling (1995). in very large probabilistic expert systems. Intl. J. Human-Computer Studies, 647–666.\n",
      "\n",
      "Jermyn, I. (2005).\n",
      "\n",
      "Invariant bayesian estimation on manifolds. Annals of Statistics 33(2), 583–605.\n",
      "\n",
      "Hu, M., C. Ingram, M.Sirski, C. Pal, S. Swamy, and C. Patten (2000). A Hierarchical HMM Implementa- tion for Vertebrate Gene Splice Site Prediction. Technical report, Dept. Computer Science, Univ. Waterloo.\n",
      "\n",
      "Jaakkola, T. (2001). Tutorial on varia- tional approximation methods. In M. Opper and D. Saad (Eds.), Ad- vanced mean ﬁeld methods. MIT Press.\n",
      "\n",
      "Jerrum, M. and A. Sinclair\n",
      "\n",
      "(1993). Polynomial-time approximation al- gorithms for the Ising model. SIAM J. on Computing 22, 1087–1116.\n",
      "\n",
      "Huang,\n",
      "\n",
      "J., Q. Morris, and B. Frey (2007). Bayesian inference of Mi- croRNA targets from sequence and expression data. J. Comp. Bio..\n",
      "\n",
      "Hubel, D. and T. Wiesel (1962). Recep- tive ﬁelds, binocular itneraction, and functional architecture in the cat’s visual cortex. J. Physiology 160, 106–154.\n",
      "\n",
      "Huber, P.\n",
      "\n",
      "(1964). Robust estimation of a location parameter. Annals of Statistics 53, 73â ˘A ¸S101.\n",
      "\n",
      "Hubert, L. and P. Arabie (1985). Com- J. of Classiﬁca-\n",
      "\n",
      "paring partitions. tion 2, 193–218.\n",
      "\n",
      "Jaakkola, T. and D. Haussler (1998). Ex- ploiting generative models in dis- criminative classiﬁers. In NIPS, pp. 487–493.\n",
      "\n",
      "Jaakkola, T. and M.\n",
      "\n",
      "Computing and bounds on likelihoods tractable networks. In UAI.\n",
      "\n",
      "upper\n",
      "\n",
      "Jordan (1996a). lower in in-\n",
      "\n",
      "Jaakkola, T. and M.\n",
      "\n",
      "Jordan (1996b). A variational approach to Bayesian logistic regression problems and their extensions. In AI + Statistics.\n",
      "\n",
      "Jaakkola, T. S. and M. I. Jordan (2000). Bayesian parameter estimation via variational methods. Statistics and Computing 10, 25–37.\n",
      "\n",
      "Jerrum, M. and A. Sinclair\n",
      "\n",
      "(1996). The markov chain monte carlo method: an approach to approxi- mate counting and integration. In D. S. Hochbaum (Ed.), Approxima- tion Algorithms for NP-hard prob- lems. PWS Publishing.\n",
      "\n",
      "Jerrum, M., A. Sinclair, and E. Vigoda (2004). A polynomial-time approx- imation algorithm for the perma- nent of a matrix with non-negative entries. Journal of the ACM , 671– 697.\n",
      "\n",
      "Ji,\n",
      "\n",
      "and L. Carin S., D. Dunson, (2009). Multi-task compressive sensing. IEEE Trans. Signal Process- ing 57 (1).\n",
      "\n",
      "1028\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Ji, S., L. Tang, S. Yu, and J. Ye (2010). A shared-subspace learning frame- work for multi-label classiﬁcation. ACM Trans. on Knowledge Discovery from Data 4(2).\n",
      "\n",
      "Jirousek, R. and S. Preucil (1995). On the effective implementation of the iterative proportional ﬁtting proce- Computational Statistics & dure. Data Analysis 19, 177–189.\n",
      "\n",
      "Joachims, T.\n",
      "\n",
      "(2006). Training Linear SVMs in Linear Time. In Proc. of the Int’l Conf. on Knowledge Discov- ery and Data Mining.\n",
      "\n",
      "Joachims, T., T. Finley, and C.-N. Yu (2009). Cutting-Plane Training of Structural SVMs. Machine Learn- ing 77 (1), 27–59.\n",
      "\n",
      "Johnson,\n",
      "\n",
      "J. K., D. M. Malioutov, and A. S. Willsky (2006). Walk-sum in- terpretation and analysis of gaus- sian belief propagation. In NIPS, pp. 579–586.\n",
      "\n",
      "Johnson, M.\n",
      "\n",
      "Capacity and complexity of HMM duration mod- eling techniques. Signal Processing Letters 12(5), 407–410.\n",
      "\n",
      "(2005).\n",
      "\n",
      "Johnson, N.\n",
      "\n",
      "(2009). A study of the NIPS feature selection challenge. Technical report, Stanford.\n",
      "\n",
      "Johnson, V. and J. Albert (1999). Ordi-\n",
      "\n",
      "nal data modeling. Springer.\n",
      "\n",
      "Jones, B., A. Dobra, C. Carvalho, C. Hans, C. Carter, and M. West (2005). Experiments in stochastic computation for high-dimensional graphical models. Statistical Sci- ence 20, 388–400.\n",
      "\n",
      "Jordan, M. I. (2007). An introduction to probabilistic graphical models. In preparation.\n",
      "\n",
      "Jordan, M. I. data. pp. 1–3.\n",
      "\n",
      "(2011). The era of big In ISBA Bulletin, Volume 18,\n",
      "\n",
      "Jordan, M.\n",
      "\n",
      "I., Z. Ghahramani, T. S. Jaakkola, and L. K. Saul (1998). An introduction to variational meth- ods for graphical models. In M. Jor- dan (Ed.), Learning in Graphical Models. MIT Press.\n",
      "\n",
      "Jordan, M. I. and R. A. Jacobs (1994). Hierarchical mixtures of experts Neural and the EM algorithm. Computation 6, 181–214.\n",
      "\n",
      "Journee, M., Y. Nesterov, P. Richtarik, and R. Sepulchre (2010). General- ized power method for sparse prin- J. of cipal components analysis. Machine Learning Research 11, 517– 553.\n",
      "\n",
      "Julier, S. and J. Uhlmann (1997). A new extension of the Kalman ﬁl- In Proc. ter to nonlinear systems. of AeroSense: The 11th Intl. Symp. on Aerospace/Defence Sensing, Simula- tion and Controls.\n",
      "\n",
      "Jurafsky, D. and J. H. Martin (2000). Speech and language processing: An Introduction to Natural Lan- guage Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall.\n",
      "\n",
      "Jurafsky, D. and J. H. Martin (2008). Speech and language processing: An Introduction to Natural Lan- guage Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall. 2nd edition.\n",
      "\n",
      "Kaariainen, M. and J. Langford (2005). A Comparison of Tight Generaliza- tion Bounds. In Intl. Conf. on Ma- chine Learning.\n",
      "\n",
      "Kaelbling,\n",
      "\n",
      "L., M. A. Moore (1996). learning: A survey. search 4, 237–285.\n",
      "\n",
      "and Littman, Reinforcement J. of AI Re-\n",
      "\n",
      "Kaelbling, L. P., M. Littman, and Planning A. Cassandra (1998). and acting in partially observable stochastic domains. Artiﬁcial Intel- ligence 101.\n",
      "\n",
      "Kaiser, H. (1958). The varimax crite- rion for analytic rotation in factor analysis. Psychometrika 23(3).\n",
      "\n",
      "Kakade, S., Y. W. Teh, and S. Roweis (2002). An alternate objective func- tion for markovian ﬁelds. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Kanazawa, K., D. Koller, and S. Rus- sell (1995). Stochastic simulation al- gorithms for dynamic probabilistic networks. In UAI.\n",
      "\n",
      "Kandel, E., J. Schwarts, and T. Jessell (2000). Principles of Neural Science. McGraw-Hill.\n",
      "\n",
      "Kappen, H. and F. Rodriguez (1998). Boltzmann machine learning using mean ﬁeld theory and linear re- sponse correction. In NIPS.\n",
      "\n",
      "Karhunen,\n",
      "\n",
      "and J. Joutsensalo (1995). Generalizations of princi- pal component analysis, optimiza- tion problems, and neural net- works. Neural Networks 8(4), 549– 562.\n",
      "\n",
      "J.\n",
      "\n",
      "Kass, R. and L. Wasserman (1995). A reference bayesian test for nested hypotheses and its relationship to the schwarz criterio. J. of the Am. Stat. Assoc. 90(431), 928–934.\n",
      "\n",
      "Katayama, T. (2005). Subspace Methods for Systems Identiﬁcation. Springer Verlag.\n",
      "\n",
      "Kaufman, L. and P. Rousseeuw (1990). Finding Groups in Data: An Intro- duction to Cluster Analysis. Wiley.\n",
      "\n",
      "Kawakatsu, H. and A. Largey (2009). EM algorithms for ordered probit models with endogenous regres- sors. The Econometrics Journal 12(1), 164–186.\n",
      "\n",
      "Kearns, M. J. and U. V. Vazirani (1994). An Introduction to Computational Learning Theory. MIT Press.\n",
      "\n",
      "Kelley, J. E. (1960). The cutting-plane method for solving convex pro- grams. J. of the Soc. for Industrial and Applied Math. 8, 703–712.\n",
      "\n",
      "Kemp, C., J. Tenenbaum, S. Niyogi, and T. Griffiths (2010). A probabilistic model of theory formation. Cogni- tion 114, 165–196.\n",
      "\n",
      "Kemp, C., J. Tenenbaum, T. Y. T. Grif- ﬁths and, and N. Ueda (2006). Learning systems of concepts with an inﬁnite relational model. In AAAI.\n",
      "\n",
      "Kersting, K., S. Natarajan, and D. Poole Statistical Relational AI: (2011). Logic, Probability and Computa- tion. Technical report, UBC.\n",
      "\n",
      "Khan, M. E., B. Marlin, G. Bouchard, and K. P. Murphy (2010). Varia- tional bounds for mixed-data fac- tor analysis. In NIPS.\n",
      "\n",
      "Khan, Z., T. Balch, and F. Dellaert (2006). MCMC Data Association and Sparse Factorization Updating for Real Time Multitarget Tracking with Merged and Multiple Mea- surements. IEEE Trans. on Pat- tern Analysis and Machine Intelli- gence 28(12).\n",
      "\n",
      "Kirkpatrick, S., C. G. Jr., and M. Vecchi (1983). Optimization by simulated annealing. Science 220, 671–680.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1029\n",
      "\n",
      "Kitagawa, G. (2004). The two-ﬁlter for- mula for smoothing and an im- plementation of the Gaussian-sum smoother. Annals of the Institute of Statistical Mathematics 46(4), 605– 623.\n",
      "\n",
      "Kjaerulff, U.\n",
      "\n",
      "(1990). Triangulation of graphs – algorithms giving small total state space. Technical Report R-90-09, Dept. of Math. and Comp. Sci., Aalborg Univ., Denmark.\n",
      "\n",
      "Kjaerulff, U. and A. Madsen (2008). Bayesian Networks and Inﬂuence Diagrams: A Guide to Construction and Analysis. Springer.\n",
      "\n",
      "Klaassen, C. and J. A. Wellner (1997). Efficient estimation in the bivari- ate noramal copula model: Nor- mal margins are least favorable. Bernoulli 3(1), 55–77.\n",
      "\n",
      "Klami, A. and S. Kaski (2008). Proba- bilistic approach to detecting de- pendencies between data sets. Neurocomputing 72, 39–46.\n",
      "\n",
      "Klami, A., S. Virtanen, and S. Kaski (2010). Bayesian exponential fam- ily projections for coupled data sources. In UAI.\n",
      "\n",
      "Koller, D. and U. Lerner (2001). Sam- pling in Factored Dynamic Sys- tems. In A. Doucet, N. de Fre- itas, and N. Gordon (Eds.), Sequen- tial Monte Carlo Methods in Prac- tice. Springer.\n",
      "\n",
      "Kolmogorov, V. (2006, October). Con- vergent Tree-reweighted Message Passing for Energy Minimization. IEEE Trans. on Pattern Analysis and Machine Intelligence 28(10), 1568– 1583.\n",
      "\n",
      "Kolmogorov, V. and M. Wainwright (2005). On optimality properties of tree-reweighted message pass- ing. In UAI, pp. 316–322.\n",
      "\n",
      "Kolmogorov, V. and R. Zabin (2004). What energy functions can be min- imized via graph cuts? IEEE Trans. on Pattern Analysis and Machine Intelligence 26(2), 147–159.\n",
      "\n",
      "Komodakis, N., N. Paragios,\n",
      "\n",
      "and G. Tziritas (2011). MRF Energy Mini- mization and Beyond via Dual De- composition. IEEE Trans. on Pat- tern Analysis and Machine Intelli- gence 33(3), 531–552.\n",
      "\n",
      "Kschischang, F., B. Frey, and H.-A. Loeliger (2001, February). Factor graphs and the sum-product algo- rithm. IEEE Trans Info. Theory.\n",
      "\n",
      "Kuan, P., G. Pan,\n",
      "\n",
      "J. A. Thomson, R. Stewart, and S. Keles (2009). A hierarchical semi-Markov model for detecting enrichment with ap- plication to ChIP-Seq experiments. Technical report, U. Wisconsin.\n",
      "\n",
      "Kulesza, A. and B. Taskar (2011). Learn- ing Determinantal Point Processes. In UAI.\n",
      "\n",
      "Kumar, N. and A. Andreo (1998). Het- eroscedastic discriminant analysis and reduced rank HMMs for im- proved speech recognition. Speech Communication 26, 283–297.\n",
      "\n",
      "Kumar, S. and M. Hebert (2003). Dis- criminative random ﬁelds: A dis- criminative framework for contex- tual interaction in classiﬁcation. In Intl. Conf. on Computer Vision.\n",
      "\n",
      "Kuo, L. and B. Mallick (1998). Vari- able selection for regression mod- els. Sankhya Series B 60, 65–81.\n",
      "\n",
      "Kleiner, A., A. Talwalkar, P. Sarkar, and M. I. Jordan (2011). A scalable boot- strap for massive data. Technical report, UC Berkeley.\n",
      "\n",
      "Kneser, R. and H. Ney (1995).\n",
      "\n",
      "Im- proved backing-off for n-gram lan- guage modeling. In Intl. Conf. on Acoustics, Speech and Signal Proc., Volume 1, pp. 181–184.\n",
      "\n",
      "Ko,\n",
      "\n",
      "GP- BayesFilters: Bayesian Filtering Us- ing Gaussian Process Prediction and Observation Models. Au- tonomous Robots Journal.\n",
      "\n",
      "J. and D. Fox (2009).\n",
      "\n",
      "Kohn, R., M. Smith, and D. Chan (2001). Nonparametric regression using linear combinations of basis functions. Statistical Computing 11, 313–322.\n",
      "\n",
      "Koivisto, M. (2006). Advances in ex- act Bayesian structure discovery in Bayesian networks. In UAI.\n",
      "\n",
      "Koivisto, M. and K. Sood (2004). Ex- act Bayesian structure discovery in Bayesian networks. J. of Machine Learning Research 5, 549–573.\n",
      "\n",
      "Koo, T., A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag (2010). Dual Decomposition for Parsing with Non-Projective Head Au- tomata. In Proc. EMNLP, pp. 1288â ˘A ¸S1298.\n",
      "\n",
      "Koren, Y. (2009a). The bellkor solution to the netﬂix grand prize. Techni- cal report, Yahoo! Research.\n",
      "\n",
      "Koren, Y.\n",
      "\n",
      "(2009b). Collaborative ﬁl- tering with temporal dynamics. In Proc. of the Int’l Conf. on Knowledge Discovery and Data Mining.\n",
      "\n",
      "Koren, Y., R. Bell, and C. Volinsky (2009). Matrix factorization tech- niques for recommender systems. IEEE Computer 42(8), 30–37.\n",
      "\n",
      "Krishnapuram,\n",
      "\n",
      "Carin, B., M. Figueiredo, and A. Hartemink (2005). Learning sparse bayesian classiﬁers: multi-class formulation, fast algorithms, and generalization IEEE Transaction on bounds. Pattern Analysis and Machine Intelligence.\n",
      "\n",
      "L.\n",
      "\n",
      "Kurihara, K., M. Welling, and N. Vlas- sis (2006). Accelerated variational DP mixture models. In NIPS.\n",
      "\n",
      "Kushner, H.\n",
      "\n",
      "and G. Yin (2003). Stochastic approximation and recur- sive algorithms and applications. Springer.\n",
      "\n",
      "Kuss and C. Rasmussen (2005). As- sessing approximate inference for binary gaussian process classiﬁca- tion. J. of Machine Learning Re- search 6, 1679–1704.\n",
      "\n",
      "Kwon, J. and K. Murphy (2000). Mod- eling freeway traffic with coupled HMMs. Technical report, Univ. Cal- ifornia, Berkeley.\n",
      "\n",
      "Kyung, M.,\n",
      "\n",
      "J. Gill, M. Ghosh, and G. Casella (2010). Penalized Regres- sion, Standard Errors and Bayesian Lassos. Bayesian Analysis 5(2), 369– 412.\n",
      "\n",
      "Lacoste-Julien,\n",
      "\n",
      "and Z. Ghahramani (2011). Approximate inference for the loss-calibrated Bayesian. In AI/Statistics.\n",
      "\n",
      "S., F. Huszar,\n",
      "\n",
      "Koller, D. and N. Friedman (2009). Probabilistic Graphical Models: Principles and Techniques. MIT Press.\n",
      "\n",
      "Krizhevsky, A. and G. Hinton (2010). Using Very Deep Autoencoders for Content-Based Image Retrieval. Submitted.\n",
      "\n",
      "Lacoste-Julien, S., F. Sha, and M. I. Jor- dan (2009). DiscLDA: Discrimina- tive learning for dimensionality re- duction and classiﬁcation. In NIPS.\n",
      "\n",
      "1030\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Lafferty,\n",
      "\n",
      "and F. Pereira (2001). Conditional ran- dom ﬁelds: Probabilistic models for segmenting and labeling se- quence data. In Intl. Conf. on Ma- chine Learning.\n",
      "\n",
      "J.,\n",
      "\n",
      "A. McCallum,\n",
      "\n",
      "Lange, K., R. Little, and J. Taylor (1989). Robust statistical modeling using the t disribution. J. of the Am. Stat. Assoc. 84(408), 881–896.\n",
      "\n",
      "Lauritzen, S. L. and D. J. Spiegelhal- ter (1988). Local computations with probabilities on graphical struc- tures and their applications to ex- pert systems. J. R. Stat. Soc. B B(50), 127–224.\n",
      "\n",
      "Law, E., B. Settles, and T. Mitchell (2010). Learning to tag from open In Proc. Euro- vocabulary labels. pean Conf. on Machine Learning.\n",
      "\n",
      "Lee, A., F. Caron, A. Doucet, and C. Holmes (2010). A hierarchical bayesian framework for construct- ing sparsity-inducing priors. Tech- nical report, U. Oxford.\n",
      "\n",
      "Lee, A., F. Caron, A. Doucet, and C. Holmes (2011). Bayesian Sparsity- Path-Analysis of Genetic Associ- ation Signal using Generalized t Prior. Technical report, U. Oxford.\n",
      "\n",
      "Langville, A. and C. Meyer (2006). Updating Markov chains with an eye on Google’s PageRank. SIAM J. on Matrix Analysis and Applica- tions 27 (4), 968–987.\n",
      "\n",
      "Larranaga, P., C. M. H. Kuijpers, M. Poza, and R. H. Murga (1997). Decomposing bayesian networks: triangulation of the moral graph with genetic algorithms. Statistics and Computing (UK) 7 (1), 19–34.\n",
      "\n",
      "Lashkari, D. and P. Golland (2007). Convex clustering with examplar- based models. In NIPS.\n",
      "\n",
      "Lasserre, J., C. Bishop, and T. Minka (2006). Principled hybrids of gen- erative and discriminative models. In CVPR.\n",
      "\n",
      "Lau,\n",
      "\n",
      "(2006). Bayesian model-based clustering procedures. Journal of Computa- tional and Graphical Statistics 12, 351–357.\n",
      "\n",
      "J.\n",
      "\n",
      "and\n",
      "\n",
      "P. Green\n",
      "\n",
      "Lauritzen, S. (1996). Graphical Models.\n",
      "\n",
      "OUP.\n",
      "\n",
      "Lauritzen, S.\n",
      "\n",
      "infer- (2000). ence from graphical models. In D. R. C. O. E. Barndoff-Nielsen and C. Klueppelberg (Eds.), Com- plex stochastic systems. Chapman and Hall.\n",
      "\n",
      "Causal\n",
      "\n",
      "Lauritzen, S. and D. Nilsson (2001). Representing and solving decision problems with limited information. Management Science 47, 1238–1251.\n",
      "\n",
      "Lauritzen, S. L.\n",
      "\n",
      "(1992, December). Propagation of probabilities, means and variances in mixed graphical J. of the Am. association models. Stat. Assoc. 87 (420), 1098–1108.\n",
      "\n",
      "Law, M., M. Figueiredo, and A. Jain (2004). Simultaneous Feature Se- lection and Clustering Using Mix- ture Models. IEEE Trans. on Pat- tern Analysis and Machine Intelli- gence 26(4).\n",
      "\n",
      "Lawrence, N. D.\n",
      "\n",
      "Probabilis- (2005). tic non-linear principal component analysis with gaussian process la- tent variable models. J. of Machine Learning Research 6, 1783–1816.\n",
      "\n",
      "Lawrence, N. D.\n",
      "\n",
      "(2012). A unifying probabilistic perspective for spec- in- tral dimensionality reduction: J. of Ma- sights and new models. chine Learning Research 13, 1609– 1638.\n",
      "\n",
      "Learned-Miller, E. (2004). Hyperspac- ings and the estimation of infor- mation theoretic quantities. Tech- nical Report 04-104, U. Mass. Amherst Comp. Sci. Dept.\n",
      "\n",
      "J. S. Denker, LeCun, Y., B. Boser, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel (1989, Winter). Backpropagation applied to handwritten zip code recogni- tion. Neural Computation 1(4), 541– 551.\n",
      "\n",
      "LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner (1998, November). Gradient-based learning applied to document recognition. Proceedings of the IEEE 86(11), 2278–2324.\n",
      "\n",
      "LeCun, Y., S. Chopra, R. Hadsell, F.-J. Huang, and M.-A. Ranzato (2006). A tutorial on energy-based learn- ing. (Ed.), Predicting Structured Outputs. MIT press.\n",
      "\n",
      "In B. et al.\n",
      "\n",
      "Ledoit, O. and M. Wolf (2004a). Honey, I Shrunk the Sample Covariance J. of Portfolio Manage- Matrix. ment 31(1).\n",
      "\n",
      "Lee, D. and S. Seung (2001). Algo- rithms for non-negative matrix fac- torization. In NIPS.\n",
      "\n",
      "Lee, H., R. Grosse, R. Ranganath, and A. Ng (2009). Convolutional deep belief networks for scalable un- supervised learning of hierarchical representations. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Lee, H., Y. Largman, P. Pham, and A. Ng (2009). Unsupervised feature learning for audio classiﬁcation us- ing convolutional deep belief net- works. In NIPS.\n",
      "\n",
      "Lee, S.-I., V. Ganapathi, and D. Koller (2006). Efficient structure learn- ing of Markov networks using L1- regularization. In NIPS.\n",
      "\n",
      "Lee, T. S. and D. Mumford (2003). Hi- erarchical Bayesian inference in the visual cortex. J. of Optical Society of America A 20(7), 1434–1448.\n",
      "\n",
      "Lenk, P., W. S. DeSarbo, P. Green, and M. Young (1996). Hierarchi- cal Bayes Conjoint Analysis: Re- covery of Partworth Heterogeneity from Reduced Experimental De- signs. Marketing Science 15(2), 173– 191.\n",
      "\n",
      "Lenkoski, A. and A. Dobra (2008). Bayesian structural learning and estimation in Gaussian graphical models. Technical Report 545, De- partment of Statistics, University of Washington.\n",
      "\n",
      "Lepar, V. and P. P. Shenoy (1998). A Lauritzen- Spiegelhalter, Hugin and Shenoy- Shafer Architectures for Computing Marginals of Probability Distribu- In G. Cooper and S. Moral tions. (Eds.), UAI, pp. 328–337. Morgan Kaufmann.\n",
      "\n",
      "Comparison\n",
      "\n",
      "of\n",
      "\n",
      "Lauritzen, S. L.\n",
      "\n",
      "(1995). The EM al- gorithm for graphical association models with missing data. Com- putational Statistics and Data Anal- ysis 19, 191–201.\n",
      "\n",
      "Ledoit, O. and M. Wolf (2004b). A well- conditioned estimator large- dimensional covariance matrices. J. of Multivariate Analysis 88(2), 365– 411.\n",
      "\n",
      "for\n",
      "\n",
      "Lerner, U. and R. Parr (2001).\n",
      "\n",
      "Infer- ence in hybrid networks: Theoreti- cal limits and practical algorithms. In UAI.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1031\n",
      "\n",
      "Leslie, C., E. Eskin, A. Cohen, J. We- ston, and W. Noble (2003). Mis- match string kernels for discrimi- native protein classiﬁcation. Bioin- formatics 1, 1–10.\n",
      "\n",
      "Levy, S.\n",
      "\n",
      "In The Plex: How Google Thinks, Works, and Shapes Our Lives. Simon & Schuster.\n",
      "\n",
      "(2011).\n",
      "\n",
      "Li, L., W. Chu,\n",
      "\n",
      "J. Langford, and X. Wang (2011). Unbiased offline evaluation of contextual-bandit- based news article recommenda- tion algorithms. In WSDM.\n",
      "\n",
      "Liang, F., S. Mukherjee, and M. West (2007). Understanding the use of unlabelled data in predictive mod- elling. Statistical Science 22, 189– 205.\n",
      "\n",
      "Liang,\n",
      "\n",
      "R. Paulo, G. Molina, M. Clyde, and J. Berger (2008). Mix- tures of g-priors for Bayesian Vari- able Selection. J. of the Am. Stat. Assoc. 103(481), 410–423.\n",
      "\n",
      "F.,\n",
      "\n",
      "Liang, P. and M.\n",
      "\n",
      "Jordan (2008). I. An asymptotic analysis of gen- erative, discriminative, and pseu- dolikelihood estimators. In In- ternational Conference on Machine Learning (ICML).\n",
      "\n",
      "Liang, P. and D. Klein. Online EM In Proc.\n",
      "\n",
      "for Unsupervised Models. NAACL Conference.\n",
      "\n",
      "Liao, L., D. J. Patterson, D. Fox, and H. Kautz (2007). Learning and Inferring Transportation Routines. Artiﬁcial Intelligence 171(5), 311–331.\n",
      "\n",
      "Lindley, D. (1982). Scoring rules and the inevetability of probability. ISI Review 50, 1–26.\n",
      "\n",
      "Lindley, D. V. (1972). Bayesian Statistics:\n",
      "\n",
      "A Review. SIAM.\n",
      "\n",
      "Lindley, D. V. and L. D. Phillips (1976). Inference for a Bernoulli Process (A Bayesian View). The American Statistician 30(3), 112–119.\n",
      "\n",
      "Lindsay, B.\n",
      "\n",
      "lihood methods. Mathematics 80(1), 221–239.\n",
      "\n",
      "(1988). Composite like- Contemporary\n",
      "\n",
      "Lipton, R. J. and R. E. Tarjan (1979). theorem for planar SIAM Journal of Applied\n",
      "\n",
      "A separator graphs. Math 36, 177–189.\n",
      "\n",
      "Little., R. J. and D. B. Rubin (1987). Sta- tistical Analysis with Missing Data. New York: Wiley and Son.\n",
      "\n",
      "Liu, C. and D. Rubin (1995). ML Esti- mation of the T distribution using EM and its extensions, ECM and ECME. Statistica Sinica 5, 19–39.\n",
      "\n",
      "Liu, H., J. Lafferty, and L. Wasserman (2009). The nonparanormal: Semi- parametric estimation of high di- mensional undirected graphs. J. of Machine Learning Research 10, 2295–2328.\n",
      "\n",
      "Liu, J. (2001). Monte Carlo Strategies in Scientiﬁc Computation. Springer.\n",
      "\n",
      "Liu, J. S., W. H. Wong, and A. Kong (1994). Covariance structure of the gibbs sampler with applications to the comparisons of estima- tors and augmentation schemes. Biometrika 81(1), 27–40.\n",
      "\n",
      "Liu, T.-Y.\n",
      "\n",
      "Learning to rank for information retrieval. Founda- tions and Trends in Information Re- trieval 3(3), 225–331.\n",
      "\n",
      "(2009).\n",
      "\n",
      "Lizotte, D. (2008). Practical Bayesian optimization. Ph.D. thesis, U. Al- berta.\n",
      "\n",
      "Ljung, L. (1987). System Identiﬁciation: Theory for the User. Prentice Hall.\n",
      "\n",
      "Lo, C. H.\n",
      "\n",
      "Statistical methods for high throughput genomics. Ph.D. thesis, UBC.\n",
      "\n",
      "(2009).\n",
      "\n",
      "Lo, K., F. Hahne, R. Brinkman, R. Ryan, and R. Gottardo (2009, May). ﬂow- clust: a bioconductor package for automated gating of ﬂow cytome- try data. BMC Bioinformatics 10, 145+.\n",
      "\n",
      "Lopes, H.\n",
      "\n",
      "(2004). and M. West Bayesian model assessment in fac- tor analysis. Statisica Sinica 14, 41– 67.\n",
      "\n",
      "Lowe, D. G. (1999). Object recognition from local scale-invariant features. In Proc. of the International Con- ference on Computer Vision ICCV, Corfu, pp. 1150–1157.\n",
      "\n",
      "Luce, R. (1959).\n",
      "\n",
      "Individual choice be- havior: A theoretical analysis. Wi- ley.\n",
      "\n",
      "Lunn, D., N. Best, and J. Whit- taker (2009). Generic reversible jump MCMC using graphical mod- els. Statistics and Computing 19(4), 395–408.\n",
      "\n",
      "Lunn, D., A. Thomas, N. Best, and D. Spiegelhalter (2000). WinBUGS – a Bayesian modelling framework: concepts, structure, and extensibil- ity. Statistics and Computing 10, 325–337.\n",
      "\n",
      "Ma, H., H. Yang, M. Lyu, and I. King (2008). SoRec: Social recommenda- tion using probabilistic matrix fac- torization. In Proc. of 17th Conf. on Information and Knowledge Man- agement.\n",
      "\n",
      "Ma, S., C. Ji, and J. Farmer (1997). An efficient EM-based training algo- rithm for feedforward neural net- works. Neural Networks 10(2), 243– 256.\n",
      "\n",
      "Maathuis, M., D. Colombo, M. Kalisch, and P. BÃijhlmann (2010). Pre- dicting causal effects in large-scale systems from observational data. Nature Methods 7, 247–248.\n",
      "\n",
      "Maathuis, M., M. Kalisch,\n",
      "\n",
      "and P. BÃijhlmann (2009). Estimating intervention ef- high-dimensional fects from observational data. An- nals of Statistics 37, 3133–3164.\n",
      "\n",
      "MacKay, D. (1992). Bayesian interpo- lation. Neural Computation 4, 415– 447.\n",
      "\n",
      "MacKay, D. (1995a). Developments in probabilistic modeling with neural networks — ensemble learning. In Proc. 3rd Ann. Symp. Neural Net- works.\n",
      "\n",
      "MacKay, D.\n",
      "\n",
      "Probable net- works and plausible predictions — a review of practical Bayesian methods for supervised neural net- works. Network.\n",
      "\n",
      "(1995b).\n",
      "\n",
      "MacKay, D. (1997). Ensemble learning for Hidden Markov Models. Tech- nical report, U. Cambridge.\n",
      "\n",
      "MacKay, D.\n",
      "\n",
      "(1999). Comparision of approximate methods for handling hyperparameters. Neural Computa- tion 11(5), 1035–1068.\n",
      "\n",
      "MacKay, D. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.\n",
      "\n",
      "Macnaughton-Smith,\n",
      "\n",
      "T. Williams, M. B. Dale, and G. Mock- ett (1964). Dissimilarity analysis: a new technique of hierarchical sub-division. Nature 202, 1034 – 1035.\n",
      "\n",
      "P., W.\n",
      "\n",
      "1032\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Madeira, S. C. and A. L. Oliveira (2004). Biclustering algorithms for biological data analysis: A survey. IEEE/ACM Transactions on Compu- tational Biology and Bioinformat- ics 1(1), 24–45.\n",
      "\n",
      "Madigan, D. and A. Raftery (1994). Model selection and accounting for model uncertainty in graphical models using Occam’s window. J. of the Am. Stat. Assoc. 89, 1535–1546.\n",
      "\n",
      "Madsen, R., D. Kauchak, and C. Elkan (2005). Modeling word burstiness using the Dirichlet distribution. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Mairal,\n",
      "\n",
      "J. Ponce, and G. Sapiro (2010). Online learning for matrix factorization and sparse J. of Machine Learning Re- coding. search 11, 19–60.\n",
      "\n",
      "J., F. Bach,\n",
      "\n",
      "Mairal,\n",
      "\n",
      "J., M. Elad, and G. Sapiro (2008). Sparse representation for color image restoration. IEEE Trans. on Image Processing 17 (1), 53–69.\n",
      "\n",
      "Mansinghka, V., P. Shafto, E.\n",
      "\n",
      "Jonas, C. Petschulat, and J. Tenenbaum (2011). A Nonparametric Bayesian Method for Modeling Heterogeneous, High Dimensional Data. Technical re- port, MIT.\n",
      "\n",
      "Cross-Categorization:\n",
      "\n",
      "Margolin, A., I. Nemenman, K. Basso, C. Wiggins, G. Stolovitzky, and R. F. abd A. Califano (2006). ARACNE: An Algorithm for the Reconstruc- tion of Gene Regulatory Networks in a Mammalian Cellular Context. BMC Bionformatics 7.\n",
      "\n",
      "Marin,\n",
      "\n",
      "(2007). Bayesian Core: a practical approach to computational Bayesian statistics. Springer.\n",
      "\n",
      "J.-M. and C. Robert\n",
      "\n",
      "Marks, T. K. and J. R. Movellan (2001). Diffusion networks, products of ex- perts, and factor analysis. Techni- cal report, University of California San Diego.\n",
      "\n",
      "Marlin, B. (2003). Modeling user rat- ing proﬁles for collaborative ﬁlter- ing. In NIPS.\n",
      "\n",
      "Matthews, R. (1998). Bayesian Critique of Statistics in Health: The Great Health Hoax.\n",
      "\n",
      "Maybeck, P. (1979). Stochastic models, estimation, and control. Academic Press.\n",
      "\n",
      "Mazumder, R. and T. Hastie (2012). The Graphical Lasso: New Insights and Alternatives. Technical report.\n",
      "\n",
      "McAuliffe,\n",
      "\n",
      "J., D. Blei, and M. Jordan (2006). Nonparametric empirical bayes for the dirichlet process mix- ture model. Statistics and Comput- ing 16(1), 5–14.\n",
      "\n",
      "McCallum, A. (2003). Efficiently induc- ing features of conditional random ﬁelds. In UAI.\n",
      "\n",
      "McCallum,\n",
      "\n",
      "and F. Pereira (2000). Maximum En- tropy Markov Models for Informa- tion Extraction and Segmentation. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "A., D.\n",
      "\n",
      "Freitag,\n",
      "\n",
      "Malioutov, D., J. Johnson, and A. Will- sky (2006). Walk-sums and belief propagation in gaussian graphical models. J. of Machine Learning Re- search 7, 2003–2030.\n",
      "\n",
      "Mallat, S., G. Davis, and Z. Zhang time- (1994, Adaptive frequency decompositions. SPIE Journal of Optical Engineering 33, 2183–2919.\n",
      "\n",
      "July).\n",
      "\n",
      "Mallat, S. and Z. Zhang (1993). Match- ing pursuits with time-frequency dictionaries. IEEE Transactions on Signal Processing 41(12), 3397–3415.\n",
      "\n",
      "Malouf, R.\n",
      "\n",
      "(2002). A comparison of algorithms for maximum entropy parameter estimation. In Proc. Sixth Conference on Natural Lan- guage Learning (CoNLL-2002), pp. 49–55.\n",
      "\n",
      "Manning, C.,\n",
      "\n",
      "and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press.\n",
      "\n",
      "P. Raghavan,\n",
      "\n",
      "Manning, C. and H. Schuetze (1999). statistical natural\n",
      "\n",
      "Foundations of language processing. MIT Press.\n",
      "\n",
      "Mansinghka, V., D. Roy, R. Rifkin, and J. Tenenbaum (2007). AClass: An online algorithm for generative classiﬁcation. In AI/Statistics.\n",
      "\n",
      "Marlin, B. (2008). Missing Data Prob- lems in Machine Learning. Ph.D. thesis, U. Toronto.\n",
      "\n",
      "Marlin, B., E. Khan, and K. Murphy (2011). Piecewise Bounds for Es- timating Bernoulli-Logistic Latent Gaussian Models. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Marlin, B. and R. Zemel (2009). Collab- orative prediction and ranking with non-random missing data. In Proc. of the 3rd ACM Conference on Rec- ommender Systems.\n",
      "\n",
      "Marlin, B. M., K. Swersky, B. Chen, and N. de Freitas (2010). Inductive prin- ciples for restricted boltzmann ma- chine learning. In AI/Statistics.\n",
      "\n",
      "Marroquin,\n",
      "\n",
      "J., S. Mitter, and T. Pog- gio (1987). Probabilistic solution of ill-posed problems in computa- J. of the Am. Stat. As- tional vision. soc. 82(297), 76–89.\n",
      "\n",
      "Martens, J. (2010). Deep learning via In Intl.\n",
      "\n",
      "hessian-free optimization. Conf. on Machine Learning.\n",
      "\n",
      "Maruyama, Y. and E. George (2008). A g-prior extension for p > n. Tech- nical report, U. Tokyo.\n",
      "\n",
      "Mason, L.,\n",
      "\n",
      "M. Frean (2000). rithms as gradient descent. NIPS, Volume 12, pp. 512–518.\n",
      "\n",
      "J. Baxter, P. Bartlett, and Boosting algo- In\n",
      "\n",
      "McCallum, A. and K. Nigam (1998). A comparison of event models for naive Bayes text classiﬁcation. In AAAI/ICML workshop on Learning for Text Categorization.\n",
      "\n",
      "McCray, A.\n",
      "\n",
      "(2003). An upper level ontology for the biomedical do- main. Comparative and Functional Genomics 4, 80–84.\n",
      "\n",
      "McCullagh, P. and J. Nelder (1989). Generalized linear models. Chap- man and Hall. 2nd edition.\n",
      "\n",
      "McCullich, W. and W. Pitts (1943). A logical calculus of the ideas imma- nent in nervous activity. Bulletin of Mathematical Biophysics 5, 115–137.\n",
      "\n",
      "McDonald,\n",
      "\n",
      "J. and W. Newey (1988). Partially Adaptive Estimation of Re- gression Models via the General- ized t Distribution. Econometric Theory 4(3), 428–445.\n",
      "\n",
      "McEliece, R. J., D. J. C. MacKay, and J. F. Cheng (1998). Turbo decod- ing as an instance of Pearl’s ’belief propagation’ algorithm. IEEE J. on Selected Areas in Comm. 16(2), 140– 152.\n",
      "\n",
      "McFadden, D. (1974). Conditional logit analysis of qualitative choice be- havior. In P. Zarembka (Ed.), Fron- tiers in econometrics, pp. 105–142. Academic Press.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1033\n",
      "\n",
      "McGrayne, S. B.\n",
      "\n",
      "The the- ory that would not die: how Bayes’ rule cracked the enigma code, hunted down Russian submarines, and emerged triumphant from two centuries of controversy. Yale Uni- versity Press.\n",
      "\n",
      "(2011).\n",
      "\n",
      "McKay, B. D., F. E. Oggier, G. F. Royle, N. J. A. Sloane, I. M. Wanless, and H. S. Wilf (2004). Acyclic digraphs and eigenvalues of (0,1)-matrices. J. Integer Sequences 7 (04.3.3).\n",
      "\n",
      "McKay, D. and L. C. B. Peto (1995). language A hierarchical dirichlet model. Natural Language Engineer- ing 1(3), 289–307.\n",
      "\n",
      "McLachlan, G.\n",
      "\n",
      "J. and T. Krishnan (1997). The EM Algorithm and Ex- tensions. Wiley.\n",
      "\n",
      "Meek, C. and D. Heckerman (1997). learn- independence and In UAI,\n",
      "\n",
      "Structure and parameter ing for causal causal interaction models. pp. 366–375.\n",
      "\n",
      "Meltzer, T., C. Yanover, and Y. Weiss (2005). solu- tions for energy minimization in stereo vision using reweighted be- lief propagation. In ICCV, pp. 428– 435.\n",
      "\n",
      "Globally optimal\n",
      "\n",
      "Meng, X. L. and D. van Dyk (1997). The EM algorithm — an old folk song sung to a fast new tune (with Discussion). J. Royal Stat. Soc. B 59, 511–567.\n",
      "\n",
      "Mesot, B. and D. Barber (2009). A Sim- ple Alternative Derivation of the Expectation Correction Algorithm. IEEE Signal Processing Letters 16(1), 121–124.\n",
      "\n",
      "Metropolis,\n",
      "\n",
      "Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller (1953). Equation of state computing calculations by fast machines. J. of Chemical Physics 21, 1087–1092.\n",
      "\n",
      "N.,\n",
      "\n",
      "A.\n",
      "\n",
      "Metz, C. (2010). Google behavioral ad targeter is a Smart Ass. The Regis- ter.\n",
      "\n",
      "Minka, T. (2001b). Empirical Risk Min- imization is an incomplete induc- tive principle. Technical report, MIT.\n",
      "\n",
      "Minka, T. (2001c). Expectation propa- gation for approximate Bayesian in- ference. In UAI.\n",
      "\n",
      "Minka, T.\n",
      "\n",
      "(2001d). A family of algo- rithms for approximate Bayesian in- ference. Ph.D. thesis, MIT.\n",
      "\n",
      "Minka, T.\n",
      "\n",
      "Statistical ap- proaches to learning and discovery 10-602: Homework assignment 2, question 5. Technical report, CMU.\n",
      "\n",
      "(2001e).\n",
      "\n",
      "Minka, T. (2003). A comparison of nu- merical optimizers for logistic re- gression. Technical report, MSR.\n",
      "\n",
      "Minka, T. (2005). Divergence measures and message passing. Technical re- port, MSR Cambridge.\n",
      "\n",
      "Minka, T. and Y. Qi\n",
      "\n",
      "Tree- structured approximations by ex- pectation propagation. In NIPS.\n",
      "\n",
      "(2003).\n",
      "\n",
      "Meek, C., B. Thiesson, and D. Hecker- man (2002). Staged mixture mod- elling and boosting. In UAI, San Francisco, CA, pp. 335–343. Mor- gan Kaufmann.\n",
      "\n",
      "Meila, M. (2001). A random walks view In AI/S-\n",
      "\n",
      "of spectral segmentation. tatistics.\n",
      "\n",
      "Meila, M. (2005). Comparing cluster- In Intl.\n",
      "\n",
      "ings: an axiomatic view. Conf. on Machine Learning.\n",
      "\n",
      "Meila, M. and T.\n",
      "\n",
      "Jaakkola (2006). Tractable Bayesian learning of tree belief networks. Statistics and Com- puting 16, 77–92.\n",
      "\n",
      "Meila, M. and M.\n",
      "\n",
      "Jordan (2000). Learning with mixtures of trees. J. of Machine Learning Research 1, 1– 48.\n",
      "\n",
      "I.\n",
      "\n",
      "Meinshausen, N. (2005). A note on the lasso for gaussian graphical model selection. Technical report, ETH Seminar fur Statistik.\n",
      "\n",
      "Meinshausen, N. and P. Buhlmann (2006). High dimensional graphs and variable selection with the lasso. The Annals of Statistics 34, 1436–1462.\n",
      "\n",
      "Meinshausen, N. and P. BÃijhlmann (2010). Stability selection. J. of Royal Stat. Soc. Series B 72, 417–473.\n",
      "\n",
      "Miller, A. (2002). Subset selection in re- gression. Chapman and Hall. 2nd edition.\n",
      "\n",
      "Mimno, D. and A. McCallum (2008). Topic models conditioned on ar- bitrary features with dirichlet- multinomial regression. In UAI.\n",
      "\n",
      "Minka, T. (1999). Pathologies of ortho- dox statisics. Technical report, MIT Media Lab.\n",
      "\n",
      "Minka, T. (2000a). Automatical choice of dimensionality for PCA. Techni- cal report, MIT.\n",
      "\n",
      "Minka, T. (2000b). Bayesian linear re- gression. Technical report, MIT.\n",
      "\n",
      "Minka, T. (2000c). Bayesian model av- eraging is not model combination. Technical report, MIT Media Lab.\n",
      "\n",
      "Minka, T. (2000d). Empirical risk min- imization is an incomplete induc- tive principle. Technical report, MIT.\n",
      "\n",
      "Minka, T. (2000e). Estimating a Dirich- let distribution. Technical report, MIT.\n",
      "\n",
      "Minka, T. (2000f). Inferring a Gaussian distribution. Technical report, MIT.\n",
      "\n",
      "Minka, T. (2001a). Bayesian inference of a uniform distribution. Techni- cal report, MIT.\n",
      "\n",
      "Minka, T.,\n",
      "\n",
      "J. Winn, D. Knowles (2010). Microsoft http://research.microsoft.com/infernet.\n",
      "\n",
      "J. Guiver, and Infer.NET 2.4. Research Cambridge.\n",
      "\n",
      "Minsky, M. and S. Papert (1969). Per-\n",
      "\n",
      "ceptrons. MIT Press.\n",
      "\n",
      "Mitchell, T. (1997). Machine Learning.\n",
      "\n",
      "McGraw Hill.\n",
      "\n",
      "Mitchell, T. and J. Beauchamp (1988). Bayesian Variable Selection in Lin- ear Regression. J. of the Am. Stat. Assoc. 83, 1023–1036.\n",
      "\n",
      "Mobahi, H., R. Collobert, and J. We- ston (2009). Deep learning from In temporal coherence in video. Intl. Conf. on Machine Learning.\n",
      "\n",
      "Mockus,\n",
      "\n",
      "J., W. Eddy, A. Mockus, L. Mockus, and G. Reklaitis (1996). Bayesian Heuristic Approach to Dis- crete and Global Optimization: Al- gorithms, Visualization, Software, and Applications. Kluwer.\n",
      "\n",
      "Moghaddam, B., A. Gruber, Y. Weiss, and S. Avidan (2008). Sparse re- gression as a sparse eigenvalue In Information Theory & problem. Applications Workshop (ITA’08).\n",
      "\n",
      "Moghaddam, B., B. Marlin, E. Khan, and K. Murphy (2009). Accel- erating bayesian structural infer- ence for non-decomposable gaus- sian graphical models. In NIPS.\n",
      "\n",
      "1034\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Moghaddam, B. and A. Pentland (1995). Probabilistic visual learning for object detection. In Intl. Conf. on Computer Vision.\n",
      "\n",
      "Mohamed,\n",
      "\n",
      "Z. Ghahramani Exponential Family PCA. In NIPS.\n",
      "\n",
      "S.,\n",
      "\n",
      "K. Heller, (2008).\n",
      "\n",
      "and Bayesian\n",
      "\n",
      "Moler, C. (2004). Numerical Computing\n",
      "\n",
      "with MATLAB. SIAM.\n",
      "\n",
      "Morris, R. D., X. Descombes, and J. Zerubia (1996). The Ising/Potts model is not well suited to seg- mentation tasks. In IEEE DSP Work- shop.\n",
      "\n",
      "Mosterman, P. J. and G. Biswas (1999). Diagnosis of continuous valued systems in transient operating re- gions. IEEE Trans. on Systems, Man, and Cybernetics, Part A 29(6), 554– 565.\n",
      "\n",
      "Moulines, E., J.-F. Cardoso, and E. Gas- siat (1997). Maximum likelihood for blind separation and deconvo- lution of noisy signals using mix- IEEE Int. ture models. Conf. on Acoustics, Speech and Sig- nal Processing (ICASSP’97), Munich, Germany, pp. 3617–3620.\n",
      "\n",
      "In Proc.\n",
      "\n",
      "Muller, P., G. Parmigiani, C. Robert, and J. Rousseau (2004). Optimal sample size for multiple testing: the case of gene expression mi- croarrays. J. of the Am. Stat. As- soc. 99, 990–1001.\n",
      "\n",
      "Mumford, D. (1994). Neuronal archi- tectures for pattern-theoretic prob- lems. In C. Koch and J. Davis (Eds.), Large Scale Neuronal Theories of the Brain. MIT Press.\n",
      "\n",
      "Murphy, K.\n",
      "\n",
      "Bayesian map learning in dynamic environments. In NIPS, Volume 12.\n",
      "\n",
      "(2000).\n",
      "\n",
      "Murphy, K. and M. Paskin (2001). Lin- ear time inference in hierarchical HMMs. In NIPS.\n",
      "\n",
      "Murphy, K., Y. Weiss, and M. Jordan (1999). Loopy belief propagation for approximate inference: an empiri- cal study. In UAI.\n",
      "\n",
      "Murphy, K. P.\n",
      "\n",
      "Filtering and smoothing in linear dynamical sys- tems using the junction tree algo- rithm. Technical report, U.C. Berke- ley, Dept. Comp. Sci.\n",
      "\n",
      "(1998).\n",
      "\n",
      "Murray, I. and Z. Ghahramani (2005). A note on the evidence and bayesian occam’s razor. Technical report, Gatsby.\n",
      "\n",
      "Musso, C., N. Oudjane, and F. LeGland Improving regularized par- (2001). ticle ﬁlters. In A. Doucet, J. F. G. de Freitas, and N. Gordon (Eds.), Sequential Monte Carlo Methods in Practice. Springer.\n",
      "\n",
      "Nabney, I. (2001). NETLAB: algorithms for pattern recognition. Springer.\n",
      "\n",
      "Neal, R. (1992). Connectionist learning of belief networks. Artiﬁcial Intelli- gence 56, 71–113.\n",
      "\n",
      "Neal, R. (1993). Probabilistic Inference Using Markov Chain Monte Carlo Methods. Technical report, Univ. Toronto.\n",
      "\n",
      "Neal, R. (1996). Bayesian learning for\n",
      "\n",
      "neural networks. Springer.\n",
      "\n",
      "Neal, R.\n",
      "\n",
      "(1997). Monte Carlo Im- plementation of Gaussian Process Models for Bayesian Regression and Classiﬁcation. Technical Re- port 9702, U. Toronto.\n",
      "\n",
      "Neal, R.\n",
      "\n",
      "Erroneous Results in ’Marginal Likelihood from the Gibbs Output’. Technical report, U. Toronto.\n",
      "\n",
      "(1998).\n",
      "\n",
      "Neal, R. (2000). Markov Chain Sam- pling Methods for Dirichlet Process J. of Computa- Mixture Models. tional and Graphical Statistics 9(2), 249–265.\n",
      "\n",
      "Neal, R. (2003a). Slice sampling. An- nals of Statistics 31(3), 7–5–767.\n",
      "\n",
      "Neal, R. (2010). MCMC using Hamil- In S. Brooks, tonian Dynamics. A. Gelman, G. Jones, and X.-L. Meng (Eds.), Handbook of Markov Chain Monte Carlo. Chapman & Hall.\n",
      "\n",
      "Neal, R. and D. MacKay (1998). Likelihood-based boosting. Techni- cal report, U. Toronto.\n",
      "\n",
      "Neal, R. and J. Zhang (2006). High dimensional classiﬁcation Bayesian neural networks and Dirichlet dif- fusion trees. In I. Guyon, S. Gunn, M. Nikravesh, and L. Zadeh (Eds.), Feature Extraction. Springer.\n",
      "\n",
      "Neal, R. M. (2001). Annealed impor- tance sampling. Statistics and Com- puting 11, 125–139.\n",
      "\n",
      "Neal, R. M. (2003b). Density Model- ing and Clustering using Dirichlet Diffusion Trees. In J. M. Bernardo et al. (Eds.), Bayesian Statistics 7, pp. 619–629. Oxford University Press.\n",
      "\n",
      "Neal, R. M. and G. E. Hinton (1998). A new view of the EM algorithm that justiﬁes incremental and other variants. In M. Jordan (Ed.), Learn- ing in Graphical Models. MIT Press.\n",
      "\n",
      "Neapolitan, R.\n",
      "\n",
      "(2003).\n",
      "\n",
      "Learning\n",
      "\n",
      "Bayesian Networks. Prentice Hall.\n",
      "\n",
      "Neﬁan, A., L. Liang, X. Pi, X. Liu, and K. Murphy (2002). Dynamic for Audio- Bayesian Networks Visual Speech Recognition. J. Ap- plied Signal Processing.\n",
      "\n",
      "Nemirovski, A. and D. Yudin (1978). On Cezari’s convergence of the steep- est descent method for approxi- mating saddle points of convex- concave functions. Soviet Math. Dokl. 19.\n",
      "\n",
      "Nesterov, Y. (2004).\n",
      "\n",
      "Introductory Lec- tures on Convex Optimization. A basic course. Kluwer.\n",
      "\n",
      "Newton, M., D. Noueiry, D. Sarkar, and P. Ahlquist (2004). Detecting differential gene expression with a semiparametric hierarchical mix- ture method. Biostatistics 5, 155– 176.\n",
      "\n",
      "Newton, M. and A. Raftery (1994). Ap- proximate Bayesian Inference with the Weighted Likelihood Bootstrap. J. of Royal Stat. Soc. Series B 56(1), 3–48.\n",
      "\n",
      "Ng, A., M. Jordan, and Y. Weiss (2001). On Spectral Clustering: Analysis and an algorithm. In NIPS.\n",
      "\n",
      "Ng, A. Y. and M. I. Jordan (2002). On discriminative vs. generative classi- ﬁers: A comparison of logistic re- gression and naive bayes. In NIPS- 14.\n",
      "\n",
      "Nickisch, H. and C. Rasmussen (2008). Approximations for binary gaus- sian process classiﬁcation. J. of Ma- chine Learning Research 9, 2035– 2078.\n",
      "\n",
      "Nilsson, D. (1998). An efficient algo- rithm for ﬁnding the M most prob- able conﬁgurations in a probabilis- tic expert system. Statistics and Computing 8, 159–173.\n",
      "\n",
      "Nilsson, D. and J. Goldberger (2001). Sequentially ﬁnding the N-Best List in Hidden Markov Models. In Intl. Joint Conf. on AI, pp. 1280–1285.\n",
      "\n",
      "Nocedal, J. and S. Wright (2006). Nu- merical Optimization. Springer.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1035\n",
      "\n",
      "Nowicki, K. and T. A. B. Snijders (2001). Estimation and prediction for stochastic blockstructures. Jour- nal of the American Statistical Asso- ciation 96(455), 1077–??\n",
      "\n",
      "Nowlan, S. and G. Hinton (1992). Sim- plifying neural networks by soft weight sharing. Neural Computa- tion 4(4), 473–493.\n",
      "\n",
      "Nummiaro, K., E. Koller-Meier, and An adaptive (2003). L. V. Gool color-based particle ﬁlter. Image and Vision Computing 21(1), 99–110.\n",
      "\n",
      "Obozinski, G., B. Taskar, and M. I. Jor- dan (2007). Joint covariate selection for grouped classiﬁcation. Techni- cal report, UC Berkeley.\n",
      "\n",
      "Oh, M.-S. and J. Berger (1992). Adap- tive importance sampling in Monte Carlo integration. J. of Statistical Computation and Simulation 41(3), 143 – 168.\n",
      "\n",
      "Osborne, M. R., B. Presnell, and B. A. Turlach (2000b). On the lasso and its dual. J. Computational and graphical statistics 9, 319–337.\n",
      "\n",
      "Ostendorf, M., V. Digalakis,\n",
      "\n",
      "and O. Kimball (1996). From HMMs to segment models: a uniﬁed view of stochastic modeling for speech IEEE Trans. on Speech recognition. and Audio Processing 4(5), 360– 378.\n",
      "\n",
      "Overschee, P. V. and B. D. Moor (1996). Subspace Identiﬁcation for Linear Systems: Theory, Implemen- tation, Applications. Kluwer Aca- demic Publishers.\n",
      "\n",
      "Paatero, P. and U. Tapper (1994). Pos- itive matrix factorization: A non- negative factor model with opti- mal utilization of error estimates of data values. Environmetrics 5, 111– 126.\n",
      "\n",
      "Pearl, J. and T. Verma (1991). A theory of inferred causation. In Knowledge Representation, pp. 441–452.\n",
      "\n",
      "Pe’er, D. (2005, April). Bayesian net- work analysis of signaling net- works: a primer. Science STKE 281, 14.\n",
      "\n",
      "Peng, F., R. (1996).\n",
      "\n",
      "Jacobs, and M. Tan- Bayesian Inference ner in Mixtures-of-Experts and Hier- archical Mixtures-of-Experts Mod- els With an Application to Speech Recognition. J. of the Am. Stat. As- soc. 91(435), 953–960.\n",
      "\n",
      "Petris, G., S. Petrone, and P. Campag- noli (2009). Dynamic linear models with R. Springer.\n",
      "\n",
      "Pham, D.-T. and P. Garrat (1997). Blind separation of mixture of inde- pendent sources through a quasi- approach. maximum likelihood IEEE Trans. on Signal Process- ing 45(7), 1712–1725.\n",
      "\n",
      "Oh, S., S. Russell, and S. Sastry (2009). Markov Chain Monte Carlo Data Association for Multi-Target Track- ing. IEEE Trans. on Automatic Con- trol 54(3), 481–497.\n",
      "\n",
      "O’Hagan, A. (1978). Curve ﬁtting and optimal design for prediction. J. of Royal Stat. Soc. Series B 40, 1–42.\n",
      "\n",
      "O’Hara, R. and M. Sillanpaa (2009). A Review of Bayesian Variable Se- lection Methods: What, How and Which. Bayesian Analysis 4(1), 85– 118.\n",
      "\n",
      "Olshausen, B. A. and D. J. Field (1996). Emergence of simple cell recep- tive ﬁeld properties by learning a sparse code for natural images. Na- ture 381, 607–609.\n",
      "\n",
      "Opper, M. (1998). A Bayesian approach to online learning. In D. Saad (Ed.), On-line learning in neural networks. Cambridge.\n",
      "\n",
      "Opper, M. and C. Archambeau (2009). The variational Gaussian approxi- mation revisited. Neural Computa- tion 21(3), 786–792.\n",
      "\n",
      "Opper, M. and D. Saad (Eds.) (2001). Advanced mean ﬁeld methods: the- ory and practice. MIT Press.\n",
      "\n",
      "Osborne, M. R., B. Presnell, and B. A. Turlach (2000a). A new approach to variable selection in least squares IMA Journal of Numeri- problems. cal Analysis 20(3), 389–403.\n",
      "\n",
      "Padadimitriou, C. and K. Steiglitz (1982). Combinatorial optimization: Algorithms and Complexity. Pren- tice Hall.\n",
      "\n",
      "Paisley, J. and L. Carin (2009). Non- parametric factor analysis with beta process priors. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Palmer, S. (1999). Vision Science: Pho- tons to Phenomenology. MIT Press.\n",
      "\n",
      "Parise, S. and M. Welling (2005). Random in Markov Learning Fields: An Empirical Study. In Joint Statistical Meeting.\n",
      "\n",
      "Park, T. and G. Casella (2008). The J. of the Am. Stat.\n",
      "\n",
      "Bayesian Lasso. Assoc. 103(482), 681–686.\n",
      "\n",
      "Parviainen, P. and M. Koivisto (2011). Ancestor relations in the presence of unobserved variables. In Proc. European Conf. on Machine Learn- ing.\n",
      "\n",
      "Paskin, M. (2003). Thin junction tree ﬁlters for simultaneous localization and mapping. In Intl. Joint Conf. on AI.\n",
      "\n",
      "Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kauf- mann.\n",
      "\n",
      "Pearl, J. (2000). Causality: Models, Rea- soning and Inference. Cambridge Univ. Press.\n",
      "\n",
      "Pietra, S. D., V. D. Pietra, and J. Laf- ferty (1997). Inducing features of IEEE Trans. on Pat- random ﬁelds. tern Analysis and Machine Intelli- gence 19(4).\n",
      "\n",
      "Plackett, R. (1975). The analysis of per- mutations. Applied Stat. 24, 193– 202.\n",
      "\n",
      "Platt,\n",
      "\n",
      "(1998). Using analytic QP and sparseness to speed training of support vector machines. In NIPS.\n",
      "\n",
      "J.\n",
      "\n",
      "Platt, J. (2000). Probabilities for sv ma- In A. Smola, P. Bartlett, chines. B. Schoelkopf, and D. Schuurmans (Eds.), Advances in Large Margin Classiﬁers. MIT Press.\n",
      "\n",
      "Platt, J., N. Cristianini, and J. Shawe- Taylor (2000). Large margin DAGs In for multiclass classiﬁcation. NIPS, Volume 12, pp. 547–553.\n",
      "\n",
      "Plummer, M. (2003).\n",
      "\n",
      "JAGS: A Program for Analysis of Bayesian Graphi- cal Models Using Gibbs Sampling. In Proc. 3rd Intl. Workshop on Dis- tributed Statistical Computing.\n",
      "\n",
      "Polson, N. and S. Scott (2011). Data augmentation for support vector machines. Bayesian Analysis 6(1), 1–124.\n",
      "\n",
      "Pontil, M., S. Mukherjee, and F. Girosi (1998). On the Noise Model of Sup- port Vector Machine Regression. Technical report, MIT AI Lab.\n",
      "\n",
      "Poon, H. and P. Domingos (2011). Sum- product networks: A new deep ar- chitecture. In UAI.\n",
      "\n",
      "1036\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Pourahmadi, M. (2004). Simultaneous Modelling of Covariance Matrices: GLM, Bayesian and Nonparamet- ric Perspectives. Technical report, Northern Illinois University.\n",
      "\n",
      "Prado, R. and M. West (2010). Time Series: Modelling, Computation and Inference. CRC Press.\n",
      "\n",
      "Raiffa, H. (1968). Decision Analysis. Ad-\n",
      "\n",
      "dison Wesley.\n",
      "\n",
      "Raina, R., A. Madhavan, and A. Ng (2009). Large-scale deep unsuper- vised learning using graphics pro- cessors. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Rao, A. and K. Rose (2001, February). Deterministically Annealed Design of Hidden Markov Model Speech Recognizers. IEEE Trans. on Speech and Audio Proc. 9(2), 111–126.\n",
      "\n",
      "Rasmussen, C.\n",
      "\n",
      "(2000). gaussian mixture model. In NIPS.\n",
      "\n",
      "The inﬁnite\n",
      "\n",
      "Press, S.\n",
      "\n",
      "Applied mul- tivariate analysis, using Bayesian and frequentist methods of infer- ence. Dover. Second edition.\n",
      "\n",
      "J.\n",
      "\n",
      "(2005).\n",
      "\n",
      "Press, W., W. Vetterling, S. Teukolosky, and B. Flannery (1988). Numeri- cal Recipes in C: The Art of Scien- tiﬁc Computing (Second ed.). Cam- bridge University Press.\n",
      "\n",
      "Prince, S.\n",
      "\n",
      "Computer Vision: Models, Learning and Inference. Cambridge.\n",
      "\n",
      "(2012).\n",
      "\n",
      "Pritchard,\n",
      "\n",
      "J., M. M. Stephens, and P. Donnelly (2000). Inference of population structure using multi- locus genotype data. Genetics 155, 945–959.\n",
      "\n",
      "Qi, Y. and T. Jaakkola (2008). Param- eter Expanded Variational Bayesian Methods. In NIPS.\n",
      "\n",
      "Qi, Y., M. Szummer, and T. Minka (2005). Bayesian Conditional Ran- dom Fields. In 10th Intl. Workshop on AI/Statistics.\n",
      "\n",
      "Raina, R., A. Ng, and D. Koller (2005). Transfer learning by constructing informative priors. In NIPS.\n",
      "\n",
      "Rajaraman, A. and J. Ullman (2010). Self-\n",
      "\n",
      "Mining of massive datasets. published.\n",
      "\n",
      "Rajaraman, A. and J. Ullman (2011). Mining of massive datasets. Cam- bridge.\n",
      "\n",
      "Rakotomamonjy, A., F. Bach, S. Canu, and Y. Grandvalet Sim- pleMKL. J. of Machine Learning Re- search 9, 2491–2521.\n",
      "\n",
      "(2008).\n",
      "\n",
      "Ramage, D., D. Hall, R. Nallapati, and C. Manning (2009). Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora. In EMNLP.\n",
      "\n",
      "Ramage, D., C. Manning, and S. Du- mais (2011). Partially Labeled Topic Models for Interpretable Text Min- ing. In Proc. of the Int’l Conf. on Knowledge Discovery and Data Min- ing.\n",
      "\n",
      "Rasmussen, C. E. and J. Quiñonero- Candela (2005). Healing the rele- vance vector machine by augmen- tation. In Intl. Conf. on Machine Learning, pp. 689–696.\n",
      "\n",
      "Rasmussen, C. E. and C. K. I. Williams (2006). Gaussian Processes for Ma- chine Learning. MIT Press.\n",
      "\n",
      "Ratsch, G., T. Onoda, and K. Muller (2001). Soft margins for adaboost. Machine Learning 42, 287–320.\n",
      "\n",
      "Rattray, M., O. Stegle, K. Sharp, and J. Winn (2009). Inference algo- rithms and learning theory for Bayesian sparse factor analysis. In Proc. Intl. Workshop on Statistical- Mechanical Informatics.\n",
      "\n",
      "Rauch, H. E., F. Tung, and C. T. Striebel (1965). Maximum likelihood esti- mates of linear dynamic systems. AIAA Journal 3(8), 1445–1450.\n",
      "\n",
      "Ravikumar, P., J. Lafferty, H. Liu, and L. Wasserman (2009). Sparse Ad- ditive Models. J. of Royal Stat. Soc. Series B 71(5), 1009–1030.\n",
      "\n",
      "Quinlan, J. (1990). Learning logical def- initions from relations. Machine Learning 5, 239–266.\n",
      "\n",
      "Quinlan, J. R. (1986). Induction of de- cision trees. Machine Learning 1, 81–106.\n",
      "\n",
      "Quinlan, J. R. (1993). C4.5 Programs for Machine Learning. Morgan Kauff- man.\n",
      "\n",
      "Quinonero-Candela, J., C. Rasmussen, and C. Williams (2007). Approxi- mation methods for gaussian pro- cess regression. In L. Bottou, O. Chapelle, D. DeCoste, and J. We- ston (Eds.), Large Scale Kernel Ma- chines, pp. 203–223. MIT Press.\n",
      "\n",
      "Rabiner, L. R. (1989). A tutorial on Hid- den Markov Models and selected applications in speech recognition. Proc. of the IEEE 77 (2), 257–286.\n",
      "\n",
      "Rai, P. and H. Daume (2009). Multi- label prediction via sparse inﬁnite CCA. In NIPS.\n",
      "\n",
      "Ramaswamy, S., P. Tamayo, R. Rifkin, S. Mukherjee, C. Yeang, M. Angelo, C. Ladd, M. Reich, E. Latulippe, J. Mesirov, T. Poggio, W. Gerald, M. Loda, E. Lander, and T. Golub (2001). Multiclass cancer diagno- sis using tumor gene expression Proc. of the National signature. Academy of Science, USA 98, 15149– 15154.\n",
      "\n",
      "Ranzato, M. and G. Hinton (2010). Modeling pixel means and covari- ances using factored third-order Boltzmann machines. In CVPR.\n",
      "\n",
      "Ranzato, M.,\n",
      "\n",
      "Y.-L. Boureau, and Y. LeCun (2007). Un- supervised Learning of Invariant Feature Hierarchies with Applica- In tions to Object Recognition. CVPR.\n",
      "\n",
      "F.-J. Huang,\n",
      "\n",
      "Ranzato, M., C. Poultney, S. Chopra, and Y.LeCun (2006). Efficient learning of sparse representations In with an energy-based model. NIPS.\n",
      "\n",
      "Raydan, M.\n",
      "\n",
      "(1997). The barzilai and borwein gradient method for the large scale unconstrained mini- mization problem. SIAM J. on Opti- mization 7 (1), 26–33.\n",
      "\n",
      "Rennie, J. (2004). Why sums are bad.\n",
      "\n",
      "Technical report, MIT.\n",
      "\n",
      "Rennie,\n",
      "\n",
      "J. Teevan, and D. Karger (2003). Tackling the poor assumptions of naive Bayes text classiﬁers. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "J., L. Shih,\n",
      "\n",
      "Reshed, D., Y. Reshef, H. Finucane, S. Grossman, G. McVean, P. Turn- baugh, E. Lander, M. Mitzen- macher, and P. Sabeti (2011, De- cember). Detecting novel associa- tions in large data sets. Science 334, 1518–1524.\n",
      "\n",
      "Resnick, S.\n",
      "\n",
      "Stochastic Processes. Birkhauser.\n",
      "\n",
      "I.\n",
      "\n",
      "(1992). Adventures in\n",
      "\n",
      "Rice, J. (1995). Mathematical statistics and data analysis. Duxbury. 2nd edition.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1037\n",
      "\n",
      "Richardson, S. and P. Green (1997). On Bayesian Analysis of Mixtures With an Unknown Number of Compo- J. of Royal Stat. Soc. Series nents. B 59, 731–758.\n",
      "\n",
      "Riesenhuber, M.\n",
      "\n",
      "Poggio (1999). Hierarchical models of ob- ject recognition in cortex. Nature Neuroscience 2, 1019–1025.\n",
      "\n",
      "and T.\n",
      "\n",
      "Rish,\n",
      "\n",
      "I., G. Grabarnik, G. Cec- chi, F. Pereira, and G. Gordon (2008). Closed-form supervised di- mensionality reduction with gener- In Intl. Conf. alized linear models. on Machine Learning.\n",
      "\n",
      "Ristic, B., S. Arulampalam, and N. Gor- don (2004). Beyond the Kalman Fil- ter: Particle Filters for Tracking Ap- plications. Artech House Radar Li- brary.\n",
      "\n",
      "Robert, C. (1995). Simulation of trun- cated normal distributions. Statis- tics and computing 5, 121–125.\n",
      "\n",
      "Robert, C. and G. Casella (2004). Monte Carlo Statisical Methods. Springer. 2nd edition.\n",
      "\n",
      "Rosenblatt, F.\n",
      "\n",
      "The percep- (1958). tron: A probabilistic model for information storage and organiza- tion in the brain. Psychological Re- view 65(6), 386â ˘A ¸S408.\n",
      "\n",
      "Ross, S. (1989).\n",
      "\n",
      "bility Models. Academic Press.\n",
      "\n",
      "Introduction to Proba-\n",
      "\n",
      "Rosset, S., J. Zhu, and T. Hastie (2004). Boosting as a regularized path to a maximum margin classiﬁer. J. of Machine Learning Research 5, 941– 973.\n",
      "\n",
      "Rossi, P., G. Allenby, and R. McCulloch (2006). Bayesian Statistics and Mar- keting. Wiley.\n",
      "\n",
      "Roth, D. (1996, Apr). On the hardness of approximate reasoning. Artiﬁcial Intelligence 82(1-2), 273–302.\n",
      "\n",
      "Rother, C., P. Kohli, W. Feng, and J. Jia (2009). Minimizing sparse higher order energy functions of discrete variables. In CVPR, pp. 1382–1389.\n",
      "\n",
      "Rouder, J., P. Speckman, D. Sun, and R. Morey (2009). Bayesian t tests for accepting and rejecting the null hypothesis. Pyschonomic Bulletin & Review 16(2), 225–237.\n",
      "\n",
      "Ruppert, D., M. Wand, and R. Carroll (2003). Semiparametric Regression. Cambridge University Press.\n",
      "\n",
      "Rush, A. M. and M. Collins (2012). A tutorial on Lagrangian relaxation and dual decomposition for NLP. Technical report, Columbia U.\n",
      "\n",
      "Russell, S.,\n",
      "\n",
      "J. Binder, D. Koller, and K. Kanazawa (1995). Local learning in probabilistic networks with hid- den variables. In Intl. Joint Conf. on AI.\n",
      "\n",
      "Russell, S. and P. Norvig (1995). Ar- tiﬁcial Intelligence: A Modern Ap- proach. Englewood Cliffs, NJ: Pren- tice Hall.\n",
      "\n",
      "Russell, S. and P. Norvig (2002). Ar- tiﬁcial Intelligence: A Modern Ap- proach. Prentice Hall. 2nd edition.\n",
      "\n",
      "Russell, S. and P. Norvig (2010). Ar- tiﬁcial Intelligence: A Modern Ap- proach. Prentice Hall. 3rd edition.\n",
      "\n",
      "S. and M. Black (2009, April). Fields J. Computer Vi- Intl.\n",
      "\n",
      "of experts. sion 82(2), 205–229.\n",
      "\n",
      "Roberts, G. and J. Rosenthal\n",
      "\n",
      "Optimal Metropolis-Hastings Statistical Science 16, 351–367.\n",
      "\n",
      "scaling\n",
      "\n",
      "for\n",
      "\n",
      "(2001). various algorithms.\n",
      "\n",
      "Roberts, G. O. and S. K. Sahu (1997). Updating schemes, corre- lation structure, blocking and pa- rameterization for the gibbs sam- pler. J. of Royal Stat. Soc. Series B 59(2), 291–317.\n",
      "\n",
      "Robinson, R. W. (1973). Counting la- beled acyclic digraphs. In F. Harary (Ed.), New Directions in the Theory of Graphs, pp. 239–273. Academic Press.\n",
      "\n",
      "Roch,\n",
      "\n",
      "A short proof that phylogenetic tree reconstru- tion by maximum likelihood is hard. IEEE/ACM Trans. Comp. Bio. Bioinformatics 31(1).\n",
      "\n",
      "S.\n",
      "\n",
      "(2006).\n",
      "\n",
      "and Rodriguez, Modeling (2011). through nested data models. Biometrika. To appear.\n",
      "\n",
      "A.\n",
      "\n",
      "K.\n",
      "\n",
      "Ghosh relational partition\n",
      "\n",
      "Rose, K. (1998, November). Determin- istic annealing for clustering, com- pression, classiﬁcation, regression, and related optimization problems. Proc. IEEE 80, 2210–2239.\n",
      "\n",
      "Roverato, A.\n",
      "\n",
      "in- verse Wishart distribution for non- decomposable graphs and its ap- plication to Bayesian inference for Gaussian graphical models. Scand. J. Statistics 29, 391–411.\n",
      "\n",
      "(2002).\n",
      "\n",
      "Hyper\n",
      "\n",
      "Roweis, S. (1997). EM algorithms for\n",
      "\n",
      "PCA and SPCA. In NIPS.\n",
      "\n",
      "Rubin, D. (1998). Using the SIR algo- rithm to simulate posterior distri- butions. In Bayesian Statistics 3.\n",
      "\n",
      "Rue, H. and L. Held (2005). Gaus- sian Markov Random Fields: The- ory and Applications, Volume 104 of Monographs on Statistics and Ap- plied Probability. London: Chap- man & Hall.\n",
      "\n",
      "Rue, H., S. Martino, and N. Chopin (2009). Approximate Bayesian In- ference for Latent Gaussian Models Using Integrated Nested Laplace Approximations. J. of Royal Stat. Soc. Series B 71, 319–392.\n",
      "\n",
      "Rumelhart, D., G. Hinton,\n",
      "\n",
      "and R. Williams (1986). Learning inter- nal representations by error propa- gation. In D. Rumelhart, J. McClel- land, and the PDD Research Group (Eds.), Parallel Distributed Process- ing: Explorations in the Microstruc- ture of Cognition. MIT Press.\n",
      "\n",
      "Sachs,\n",
      "\n",
      "Pe’er, D. Lauffenburger, and G. Nolan (2005). Causal protein-signaling networks derived from multipa- rameter Sci- single-cell data. ence 308.\n",
      "\n",
      "K., O.\n",
      "\n",
      "Perez, D.\n",
      "\n",
      "Sahami, M. and T. Heilman (2006). A Web-based Kernel Function for Measuring the Similarity of Short Text Snippets. In WWW conferenec.\n",
      "\n",
      "Salakhutdinov, R. erative Models. Toronto.\n",
      "\n",
      "(2009). Deep Gen- thesis, U.\n",
      "\n",
      "Ph.D.\n",
      "\n",
      "Salakhutdinov, R. and G. Hinton (2009). Deep Boltzmann machines. In AI/Statistics, Volume 5, pp. 448– 455.\n",
      "\n",
      "Salakhutdinov, R. and G. Hinton an\n",
      "\n",
      "(2010). Undirected Topic Model. In NIPS.\n",
      "\n",
      "Replicated Softmax:\n",
      "\n",
      "Salakhutdinov, R. and H. Larochelle (2010). Efficient Learning of Deep Boltzmann Machines. In AI/Statis- tics.\n",
      "\n",
      "Salakhutdinov, R. and A. Mnih (2008). Probabilistic matrix factorization. In NIPS, Volume 20.\n",
      "\n",
      "1038\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Salakhutdinov, R.\n",
      "\n",
      "and S. Roweis (2003). Adaptive overrelaxed bound optimization methods. In Proceed- ings of the International Conference on Machine Learning, Volume 20, pp. 664–671.\n",
      "\n",
      "Schaefer,\n",
      "\n",
      "J. and K. Strimmer (2005). A shrinkage approach to large- scale covariance matrix estimation and implications for functional ge- nomics. Statist. Appl. Genet. Mol. Biol 4(32).\n",
      "\n",
      "Schniter, P., L. C. Potter, and J. Ziniel (2008). Fast Bayesian Matching Pursuit: Model Uncertainty and Parameter Estimation for Sparse Linear Models. Technical report, U. Ohio. Submitted to IEEE Trans. on Signal Processing.\n",
      "\n",
      "Salakhutdinov, R., J. Tenenbaum, and A. Torralba (2011). Learning To Learn with Compound HD Models. In NIPS.\n",
      "\n",
      "Salakhutdinov, R. R., A. Mnih, and G. E. Hinton (2007). Restricted boltzmann machines for collabo- rative ﬁltering. In Intl. Conf. on Machine Learning, Volume 24, pp. 791–798.\n",
      "\n",
      "Salojarvi,\n",
      "\n",
      "and J., K. Puolamaki, S. Klaski (2005). On discriminative joint density modeling. In Proc. Eu- ropean Conf. on Machine Learning.\n",
      "\n",
      "Sampson, F. (1968). A Novitiate in a Period of Change: An Experimental and Case Study of Social Relation- ships. Ph.D. thesis, Cornell.\n",
      "\n",
      "Santner, T., B. Williams, and W. Notz (2003). The Design and Analysis of Computer Experiments. Springer.\n",
      "\n",
      "Sarkar,\n",
      "\n",
      "(1991). One-armed bandit problems with covariates. The An- nals of Statistics 19(4), 1978–2002.\n",
      "\n",
      "J.\n",
      "\n",
      "Sato, M. and S. Ishii (2000). On-line EM algorithm for the normalized Gaussian network. Neural Compu- tation 12, 407–432.\n",
      "\n",
      "Saul, L., T.\n",
      "\n",
      "Jordan Jaakkola, and M. (1996). Mean Field Theory for Sig- moid Belief Networks. J. of AI Re- search 4, 61–76.\n",
      "\n",
      "Saul, L. and M. Jordan (1995). Exploit- ing tractable substructures in in- tractable networks. In NIPS, Vol- ume 8.\n",
      "\n",
      "Saul, L. and M. Jordan (2000). Attrac- tor dynamics in feedforward neural networks. Neural Computation 12, 1313–1335.\n",
      "\n",
      "Saunders, C.,\n",
      "\n",
      "J. Shawe-Taylor, and A. Vinokourov (2003). String Ker- nels, Fisher Kernels and Finite State Automata. In NIPS.\n",
      "\n",
      "Savage, R., K. Heller, Y. Xi, Z. Ghahra- mani, W. Truman, M. Grant, K. Denby, and D. Wild (2009). R/BHC: fast Bayesian hierarchi- cal clustering for microarray data. BMC Bioinformatics 10(242).\n",
      "\n",
      "Schapire, R.\n",
      "\n",
      "(1990). The strength of weak learnability. Machine Learn- ing 5, 197–227.\n",
      "\n",
      "Schapire, R. and Y. Freund (2012). Foundations and Algo-\n",
      "\n",
      "Boosting: rithms. MIT Press.\n",
      "\n",
      "Schapire, R., Y. Freund, P. Bartlett, and W. Lee (1998). Boosting the mar- gin: a new explanation for the ef- fectiveness of voting methods. An- nals of Statistics 5, 1651–1686.\n",
      "\n",
      "Scharstein, D. and R. Szeliski (2002). A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. J. Computer Vi- sion 47 (1), 7–42.\n",
      "\n",
      "Intl.\n",
      "\n",
      "Schaul, T., S. Zhang, and Y. LeCun (2012). No more pesky learning rates. Technical report, Courant In- stite of Mathematical Sciences.\n",
      "\n",
      "Schmee, J. and G. Hahn (1979). A sim- ple method for regresssion analy- sis with censored data. Technomet- rics 21, 417–432.\n",
      "\n",
      "Schmidt, M. (2010). Graphical model structure learning with L1 regular- ization. Ph.D. thesis, UBC.\n",
      "\n",
      "Schmidt, M., G. Fung, and R. Rosales (2009). Optimization methods for (cid:7) − 1 regularization. Technical re- port, U. British Columbia.\n",
      "\n",
      "Schmidt, M. and K. Murphy (2009). Modeling Discrete Interventional Data using Directed Cyclic Graphi- cal Models. In UAI.\n",
      "\n",
      "Schmidt, M., K. Murphy, G. Fung, and R. Rosales (2008). Structure Learn- ing in Random Fields for Heart In Motion Abnormality Detection. CVPR.\n",
      "\n",
      "Schmidt, M., A. Niculescu-Mizil, and K. Murphy (2007). Learning Graph- ical Model Structure using L1- Regularization Paths. In AAAI.\n",
      "\n",
      "Schmidt, M.,\n",
      "\n",
      "van den Berg, M. Friedlander, and K. Murphy (2009). Optimizing Costly Func- tions with Simple Constraints: A Limited-Memory Projected Quasi- In AI & Statis- Newton Algorithm. tics.\n",
      "\n",
      "E.\n",
      "\n",
      "Schnitzspan, P., S. Roth, and B. Schiele (2010). Automatic discovery of meaningful object parts with latent CRFs. In CVPR.\n",
      "\n",
      "Schoelkopf, B. and A. Smola (2002). Learning with Kernels: Support Vec- tor Machines, Regularization, Opti- mization, and Beyond. MIT Press.\n",
      "\n",
      "Schoelkopf, B., A. Smola, and K.-R. Mueller (1998). Nonlinear compo- nent analysis as a kernel eigen- value problem. Neural Computa- tion 10, 1299 – 1319.\n",
      "\n",
      "Schraudolph, N. N., J. Yu, and S. Gün- ter (2007). A Stochastic Quasi- Newton Method for Online Convex Optimization. In AI/Statistics, pp. 436–443.\n",
      "\n",
      "Schwarz, G. (1978). Estimating the di- mension of a model. Annals of Statistics 6(2), 461â ˘A ¸S464.\n",
      "\n",
      "Schwarz, R. and Y. Chow (1990). The n-best algorithm: an efficient and exact procedure for ﬁnding the n most In Intl. Conf. on Acoustics, Speech and Sig- nal Proc.\n",
      "\n",
      "likely hypotheses.\n",
      "\n",
      "Schweikerta, G., A. Zien, G. Zeller, J. Behr, C. Dieterich, C. Ong, P. Philips, F. D. Bona, L. Hartmann, A. Bohlen, N. KrÃijger, S. Son- nenburg, and G. RÃd’tsch (2009). mGene: Accurate SVM-based Gene Finding with an Application to Ne- Genome Re- matode Genomes. search, 19, 2133–2143.\n",
      "\n",
      "(1979). Scott, D. and data-based Biometrika 66(3), 605–610.\n",
      "\n",
      "On optimal histograms.\n",
      "\n",
      "Scott, J. G. and C. M. Carvalho (2008). Feature-inclusion stochastic search for gaussian graphical models. J. of Computational and Graphical Statistics 17 (4), 790–808.\n",
      "\n",
      "Scott, S. Data augmenta- frequentist estimation, and tion, the bayesian analysis of multino- mial logit models. Statistical Papers.\n",
      "\n",
      "(2009).\n",
      "\n",
      "Scott, S.\n",
      "\n",
      "(2010). A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Busi- ness and Industry 26, 639–658.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1039\n",
      "\n",
      "Sedgewick, R. and K. Wayne (2011). Al-\n",
      "\n",
      "gorithms. Addison Wesley.\n",
      "\n",
      "Seeger, M. (2008). Bayesian Inference and Optimal Design in the Sparse Linear Model. J. of Machine Learn- ing Research 9, 759–813.\n",
      "\n",
      "Seeger, M. and H. Nickish (2008). Compressed sensing and bayesian experimental design. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Segal, D. (2011, 12 February). The dirty little secrets of search. New York Times.\n",
      "\n",
      "Seide, F., G. Li, and D. Yu (2011). Con- versational Speech Transcription Using Context-Dependent Deep Neural Networks. In Interspeech.\n",
      "\n",
      "Sejnowski, T. and C. Rosenberg (1987). Parallel networks that learn to pro- nounce english text. Complex Sys- tems 1, 145–168.\n",
      "\n",
      "Sellke, T., M. J. Bayarri, and J. Berger Calibration of p Values (2001). for Testing Precise Null Hypothe- ses. The American Statistician 55(1), 62–71.\n",
      "\n",
      "Serre, T., L. Wolf, and T. Poggio (2005). recognition with features Object inspired by visual cortex. In CVPR, pp. 994–1000.\n",
      "\n",
      "Shachter, R.\n",
      "\n",
      "(1998). Bayes-ball: The rational pastime (for determining irrelevance and requisite informa- tion in belief networks and inﬂu- ence diagrams). In UAI.\n",
      "\n",
      "R. and C. R. Kenley Shachter, (1989). Gaussian inﬂuence dia- grams. Managment Science 35(5), 527–550.\n",
      "\n",
      "Shachter, R. D. and M. A. Peot (1989). Simulation approaches to general probabilistic inference on belief networks. In UAI, Volume 5.\n",
      "\n",
      "Shafer, G. R. and P. P. Shenoy (1990). Probability propagation. Annals of Mathematics and AI 2, 327–352.\n",
      "\n",
      "Shafto, P., C. Kemp, V. Mansinghka, M. Gordon, and J. B. Tenenbaum (2006). Learning cross-cutting sys- tems of categories. In Cognitive Sci- ence Conference.\n",
      "\n",
      "Shahaf, D.,\n",
      "\n",
      "and C. Guestrin (2009). Learning Thin In Junction Trees via Graph Cuts. AISTATS.\n",
      "\n",
      "A. Chechetka,\n",
      "\n",
      "Shalev-Shwartz, S., Y. Singer, and N. Srebro (2007). Pegasos: pri- mal estimated sub-gradient solver In Intl. Conf. on Machine for svm. Learning.\n",
      "\n",
      "Shalizi, C. (2009). Cs 36-350 lecture 10: Principal components: mathe- matics, example, interpretation.\n",
      "\n",
      "Shan, H. and A. Banerjee (2010). Resid- ual Bayesian co-clustering for ma- trix approximation. In SIAM Intl. Conf. on Data Mining.\n",
      "\n",
      "Shawe-Taylor,\n",
      "\n",
      "J. and N. Cristianini (2004). Kernel Methods for Pattern Analysis. Cambridge.\n",
      "\n",
      "Sheng, Q., Y. Moreau, and B. D. Moor (2003). Biclustering Microarray data by Gibbs sampling. Bioinformat- ics 19, ii196–ii205.\n",
      "\n",
      "Shi,\n",
      "\n",
      "J. and J. Malik (2000). Normal- ized cuts and image segmentation. IEEE Trans. on Pattern Analysis and Machine Intelligence.\n",
      "\n",
      "Shoham, Y. and K. Leyton-Brown (2009). Multiagent Systems: Algo- rithmic, Game- Theoretic, and Log- ical Foundations. Cambridge Uni- versity Press.\n",
      "\n",
      "Shotton,\n",
      "\n",
      "J., A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake (2011). Real-time human pose recognition in parts from a single depth image. In CVPR.\n",
      "\n",
      "Shwe, M., B. Middleton, D. Heck- erman, M. Henrion, E. Horvitz, H. Lehmann, and G. Cooper (1991). Probabilistic diagnosis using a re- formulation of the internist-1/qmr knowledge base. Inf. Med 30(4), 241–255.\n",
      "\n",
      "Methods.\n",
      "\n",
      "Siddiqi, S., B. Boots, and G. Gordon (2007). A constraint generation ap- proach to learning stable linear dy- namical systems. In NIPS.\n",
      "\n",
      "Siepel, A. and D. Haussler\n",
      "\n",
      "(2003). Combining phylogenetic and hid- den markov models in biosequence analysis. In Proc. 7th Intl. Conf. on Computational Molecular Biol- ogy (RECOMB).\n",
      "\n",
      "Silander, T., P. Kontkanen, and P. Myl- lymÃd’ki (2007). On Sensitivity of the MAP Bayesian Network Struc- ture to the Equivalent Sample Size Parameter. In UAI, pp. 360–367.\n",
      "\n",
      "Silander, T. and P. Myllmaki\n",
      "\n",
      "(2006). A simple approach for ﬁnding the globally optimal Bayesian network structure. In UAI.\n",
      "\n",
      "Sill,\n",
      "\n",
      "J., G. Takacs, L. Mackey, and D. Lin (2009). Feature-weighted lin- ear stacking. Technical report, .\n",
      "\n",
      "Silverman, B. W.\n",
      "\n",
      "Spline smoothing: the equivalent variable kernel method. Annals of Statis- tics 12(3), 898–916.\n",
      "\n",
      "(1984).\n",
      "\n",
      "Simard, P., D. Steinkraus, and J. Platt (2003). Best practices for convolu- tional neural networks applied to visual document analysis. In Intl. Conf. on Document Analysis and Recognition (ICDAR).\n",
      "\n",
      "Simon, D.\n",
      "\n",
      "(2006). Optimal State Es- timation: Kalman, H Inﬁnity, and Nonlinear Approaches. Wiley.\n",
      "\n",
      "Singliar, T. and M. Hauskrecht (2006). Noisy-OR Component Analysis and its Application to Link Analysis. J. of Machine Learning Research 7.\n",
      "\n",
      "Smidl, V. and A. Quinn (2005). The Variational Bayes Method in Signal Processing. Springer.\n",
      "\n",
      "Smith, A. F. M. and A. E. Gelfand (1992). Bayesian statistics with- out tears: A sampling-resampling perspective. The American Statisti- cian 46(2), 84–88.\n",
      "\n",
      "Smith, R. and P. Cheeseman (1986). On the representation and estima- Intl. J. tion of spatial uncertainty. Robotics Research 5(4), 56–68.\n",
      "\n",
      "Smith,\n",
      "\n",
      "Smulders, A. Hartemink, and E. Jarvis (2006). Computational Inference of Neural Information Flow Networks. PLOS 1436– Computational Biology 2, 1439.\n",
      "\n",
      "V.,\n",
      "\n",
      "J. Yu,\n",
      "\n",
      "T.\n",
      "\n",
      "Smolensky, P.\n",
      "\n",
      "(1986). Information processing in dynamical systems: foundations of harmony theory. In D. Rumehart and J. McClel- land (Eds.), Parallel Distributed Pro- cessing: Explorations in the Mi- crostructure of Cognition. Volume 1. McGraw-Hill.\n",
      "\n",
      "Smyth, P., D. Heckerman, and M. I. Jor- dan (1997). Probabilistic indepen- dence networks for hidden Markov probability models. Neural Compu- tation 9(2), 227–269.\n",
      "\n",
      "Sohl-Dickstein,\n",
      "\n",
      "M. DeWeese (2011). on Machine Learning.\n",
      "\n",
      "J., P. Battaglino, and In Intl. Conf.\n",
      "\n",
      "1040\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Sollich, P.\n",
      "\n",
      "(2002). Bayesian methods for support vector machines: evi- dence and predictive class proba- bilities. Machine Learning 46, 21– 52.\n",
      "\n",
      "Stephens, M.\n",
      "\n",
      "(2000). Dealing with label-switching in mixture mod- els. J. Royal Statistical Society, Series B 62, 795–809.\n",
      "\n",
      "Sun, L., S. Ji, S. Yu, and J. Ye (2009). On the equivalence between canonical correlation analysis and orthonor- In malized partial Intl. Joint Conf. on AI.\n",
      "\n",
      "least squares.\n",
      "\n",
      "Sontag, D.,\n",
      "\n",
      "and T. Jaakkola (2011). Introduction to dual decomposition for inference. J. In S. Sra, S. Nowozin, and S. Wright (Eds.), Optimization for Ma- chine Learning. MIT Press.\n",
      "\n",
      "A. Globerson,\n",
      "\n",
      "Sorenson, H. and D. Alspach (1971). Recursive Bayesian estimation us- ing Gaussian sums. Automatica 7, 465â ˘A ¸S 479.\n",
      "\n",
      "Soussen, C.,\n",
      "\n",
      "Iier, D. Brie, and J. Duan (2010). From Bernoulli- Gaussian deconvolution to sparse signal restoration. Technical report, Centre de Recherche en Automa- tique de Nancy.\n",
      "\n",
      "J.\n",
      "\n",
      "Spaan, M. and N. Vlassis\n",
      "\n",
      "(2005). Perseus: Randomized Point-based Value Iteration for POMDPs. J. of AI Research 24, 195–220.\n",
      "\n",
      "Spall, J. (2003). Introduction to Stochas- tic Search and Optimization: Es- timation, Simulation, and Control. Wiley.\n",
      "\n",
      "Speed, T.\n",
      "\n",
      "(2011, December). A cor- relation for the 21st century. Sci- ence 334, 152–1503.\n",
      "\n",
      "Speed, T. and H. Kiiveri (1986). Gaus- sian Markov distributions over ﬁ- nite graphs. Annals of Statis- tics 14(1), 138–150.\n",
      "\n",
      "Spiegelhalter, D. J. and S. L. Lauritzen (1990). Sequential updating of con- ditional probabilities on directed graphical structures. Networks 20.\n",
      "\n",
      "and Spirtes, R. Causa- tion, Prediction, and Search. MIT Press. 2nd edition.\n",
      "\n",
      "P., Scheines\n",
      "\n",
      "C. Glymour,\n",
      "\n",
      "(2000).\n",
      "\n",
      "Srebro, N. (2001). Maximum Likelihood Bounded Tree-Width Markov Net- works. In UAI.\n",
      "\n",
      "Srebro, N. and T.\n",
      "\n",
      "Weighted tions. Learning.\n",
      "\n",
      "Jaakkola (2003). approxima- In Intl. Conf. on Machine\n",
      "\n",
      "low-rank\n",
      "\n",
      "Steinbach, M., G. Karypis, and V. Ku- mar (2000). A comparison of doc- In ument clustering techniques. KDD Workshop on Text Mining.\n",
      "\n",
      "Stern, D., R. Herbrich, and T. Grae- pel (2009). Matchbox: Large Scale Bayesian Recommendations. In Proc. 18th. Intl. World Wide Web Conference.\n",
      "\n",
      "Steyvers, M. and T. Griffiths (2007). Probabilistic topic models. In T. Landauer, D. McNamara, S. Den- nis, and W. Kintsch (Eds.), Latent Semantic Analysis: A Road to Mean- ing. Laurence Erlbaum.\n",
      "\n",
      "Stigler, S. (1986). The history of statis- tics. Harvard University press.\n",
      "\n",
      "Stolcke, A. and S. M. Omohundro (1992). Hidden Markov Model In- duction by Bayesian Model Merg- ing. In NIPS-5.\n",
      "\n",
      "Stoyanov, V., A. Ropson, and J. Eis- ner (2011). Empirical risk minimiza- tion of graphical model parameters given approximate inference, de- coding, and model structure. In AI/Statistics.\n",
      "\n",
      "Sudderth, E. (2006). Graphical Models for Visual Object Recognition and Tracking. Ph.D. thesis, MIT.\n",
      "\n",
      "Sudderth, E. and W. Freeman (2008, March). Signal and Image Process- ing with Belief Propagation. IEEE Signal Processing Magazine.\n",
      "\n",
      "Sudderth, E., A. Ihler, W. Freeman, and A. Willsky (2003). Nonparametric Belief Propagation. In CVPR.\n",
      "\n",
      "Sudderth, E., A.\n",
      "\n",
      "Isard, W. Freeman, and A. Willsky (2010). Nonparametric Belief Propagation. Comm. of the ACM 53(10).\n",
      "\n",
      "Ihler, M.\n",
      "\n",
      "Sudderth, E. and M.\n",
      "\n",
      "Jordan (2008). Shared Segmentation of Natural Scenes Using Dependent Pitman- Yor Processes. In NIPS.\n",
      "\n",
      "Sudderth, E., M. Wainwright, and A. Willsky (2008). Loop series and bethe variational bounds for attrac- tive graphical models. In NIPS.\n",
      "\n",
      "Sun,\n",
      "\n",
      "J., N. Zheng, and H. Shum (2003). Stereo matching using be- lief propagation. IEEE Trans. on Pat- tern Analysis and Machine Intelli- gence 25(7), 787–800.\n",
      "\n",
      "Sunehag, P., J. Trumpf, S. V. N. Vish- wanathan, and N. N. Schraudolph (2009). Variable Metric Stochastic Approximation Theory. In AI/Statis- tics, pp. 560–566.\n",
      "\n",
      "Sutton, C. and A. McCallum (2007). Improved Dynamic Schedules for Belief Propagation. In UAI.\n",
      "\n",
      "Sutton, R. and A. Barto (1998). Rein- forcment Learning: An Introduction. MIT Press.\n",
      "\n",
      "Swendsen, R. and J.-S. Wang (1987). Nonuniversal critical dynamics in Monte Carlo simulations. Physical Review Letters 58, 86–88.\n",
      "\n",
      "Swersky, K., B. Chen, B. Marlin, and N. de Freitas (2010). A Tuto- rial on Stochastic Approximation Algorithms for Training Restricted Boltzmann Machines and Deep Be- lief Nets. In Information Theory and Applications (ITA) Workshop.\n",
      "\n",
      "Szeliski, R.\n",
      "\n",
      "Computer Vi- sion: Algorithms and Applications. Springer.\n",
      "\n",
      "(2010).\n",
      "\n",
      "Szeliski, R., R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agar- wala, M. Tappen, and C. Rother (2008). A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors. IEEE Trans. on Pattern Analysis and Ma- chine Intelligence 30(6), 1068–1080.\n",
      "\n",
      "Szepesvari, C.\n",
      "\n",
      "(2010). Algorithms for Reinforcement Learning. Morgan Claypool.\n",
      "\n",
      "Taleb, N.\n",
      "\n",
      "The Black Swan: The Impact of the Highly Improba- ble. Random House.\n",
      "\n",
      "(2007).\n",
      "\n",
      "Talhouk, A., K. Murphy, and A. Doucet (2011). Efficient Bayesian Inference for Multivariate Probit Models with Sparse Inverse Correlation Matri- ces. J. Comp. Graph. Statist..\n",
      "\n",
      "Tanner, M. (1996). Tools for statistical\n",
      "\n",
      "inference. Springer.\n",
      "\n",
      "Tanner, M. and W. Wong (1987). The calculation of posterior distribu- tions by data augmentation. J. of the Am. Stat. Assoc. 82(398), 528– 540.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1041\n",
      "\n",
      "Tarlow, D.,\n",
      "\n",
      "I. Givoni, and R. Zemel (2010). Hop-map: efficient message passing with high order potentials. In AI/Statistics.\n",
      "\n",
      "Tibshirani, R.\n",
      "\n",
      "Regression (1996). shrinkage and selection via the lasso. J. Royal. Statist. Soc B 58(1), 267–288.\n",
      "\n",
      "Tseng, P. (2008). On Accelerated Proxi- mal Gradient Methods for Convex- Concave Optimization. Technical report, U. Washington.\n",
      "\n",
      "Taskar, B., C. Guestrin, and D. Koller (2003). Max-margin markov net- works. In NIPS.\n",
      "\n",
      "Taskar, B., D. Klein, M. Collins, D. Koller, and C. Manning (2004). Max-margin parsing. In Proc. Em- pirical Methods in Natural Lan- guage Processing.\n",
      "\n",
      "Teh, Y. W.\n",
      "\n",
      "A hierarchical Bayesian language model based on Pitman-Yor processes. In Proc. of the Assoc. for Computational Lin- guistics, pp. 985=992.\n",
      "\n",
      "(2006).\n",
      "\n",
      "Teh, Y.-W., M.\n",
      "\n",
      "Jordan, M. Beal, and D. Blei (2006). Hierarchical Dirich- let processes. J. of the Am. Stat. As- soc. 101(476), 1566–1581.\n",
      "\n",
      "Tenenbaum,\n",
      "\n",
      "framework for Ph.D. thesis, MIT.\n",
      "\n",
      "J.\n",
      "\n",
      "(1999).\n",
      "\n",
      "concept\n",
      "\n",
      "A Bayesian learning.\n",
      "\n",
      "Tenenbaum,\n",
      "\n",
      "J. B. and F. Xu (2000). Word learning as bayesian infer- ence. In Proc. 22nd Annual Conf.of the Cognitive Science Society.\n",
      "\n",
      "Theocharous, G., K. Murphy, and L. Kaelbling (2004). Representing hierarchical POMDPs as DBNs for multi-scale robot localization. In IEEE Intl. Conf. on Robotics and Au- tomation.\n",
      "\n",
      "Thiesson, B., C. Meek, D. Chickering, and D. Heckerman (1998). Learning mixtures of DAG models. In UAI.\n",
      "\n",
      "Thomas, A. and P. Green (2009). the decomposable Enumerating neighbours of a decomposable graph under a simple perturbation scheme. Comp. Statistics and Data Analysis 53, 1232–1238.\n",
      "\n",
      "Thrun, S., W. Burgard, and D. Fox (2006). Probabilistic Robotics. MIT Press.\n",
      "\n",
      "Thrun, S., M. Montemerlo, D. Koller, B. Wegbreit, J. Nieto, and E. Nebot (2004). Fastslam: An efficient so- lution to the simultaneous local- ization and mapping problem with unknown data association. J. of Machine Learning Research 2004.\n",
      "\n",
      "Thrun, S. and L. Pratt (Eds.) Learning to learn. Kluwer.\n",
      "\n",
      "(1997).\n",
      "\n",
      "Tibshirani,\n",
      "\n",
      "and T. Hastie (2001). Estimating the number of clusters in a dataset via the gap statistic. J. of Royal Stat. Soc. Series B 32(2), 411–423.\n",
      "\n",
      "R., G. Walther,\n",
      "\n",
      "Tieleman, T.\n",
      "\n",
      "Training re- stricted Boltzmann machines us- ing approximations to the likeli- hood gradient. In Proceedings of the 25th international conference on Machine learning, pp. 1064–1071. ACM New York, NY, USA.\n",
      "\n",
      "(2008).\n",
      "\n",
      "Ting,\n",
      "\n",
      "J., A. D’Souza, S. Vijayakumar, and S. Schaal Efficient (2010). learning and feature selection in high-dimensional regression. Neu- ral Computation 22(4), 831–886.\n",
      "\n",
      "Tipping, M. (1998). Probabilistic visual- ization of high-dimensional binary data. In NIPS.\n",
      "\n",
      "Tipping, M.\n",
      "\n",
      "Sparse bayesian learning and the relevance vector machine. J. of Machine Learning Research 1, 211–244.\n",
      "\n",
      "(2001).\n",
      "\n",
      "Tipping, M. and C. Bishop (1999). Probabilistic principal component analysis. J. of Royal Stat. Soc. Series B 21(3), 611–622.\n",
      "\n",
      "Tipping, M. and A. Faul (2003). Fast likelihood maximisation marginal for sparse bayesian models. In AI/S- tats.\n",
      "\n",
      "Tishby, N., F. Pereira, and W. Biale The information bottle- (1999). neck method. In The 37th an- nual Allerton Conf. on Communica- tion, Control, and Computing, pp. 368â ˘A ¸S377.\n",
      "\n",
      "Tomas, M., D. Anoop, K. Stefan, B. Lukas, and C. Jan (2011). Empir- ical evaluation and combination of advanced language modeling tech- niques. In Proc. 12th Annual Conf. of the Intl. Speech Communication Association (INTERSPEECH).\n",
      "\n",
      "Torralba, A., R. Fergus, and Y. Weiss (2008). Small codes and large im- age databases for recognition. In CVPR.\n",
      "\n",
      "Train, K. (2009). Discrete choice meth- Cambridge\n",
      "\n",
      "ods with simulation. University Press. Second edition.\n",
      "\n",
      "Tsochantaridis, I., T. Joachims, T. Hof- mann, and Y. Altun (2005, Septem- ber). Large margin methods for structured and interdependent J. of Machine output variables. Learning Research 6, 1453–1484.\n",
      "\n",
      "Tu, Z. and S. Zhu (2002).\n",
      "\n",
      "Image Seg- mentation by Data-Driven Markov Chain Monte Carlo. IEEE Trans. on Pattern Analysis and Machine Intel- ligence 24(5), 657–673.\n",
      "\n",
      "Turian,\n",
      "\n",
      "J., L. Ratinov, and Y. Ben- gio (2010). Word representations: a simple and general method for semi-supervised learning. In Proc. ACL.\n",
      "\n",
      "Turlach, B., W. Venables, and S. Wright (2005). Simultaneous variable se- lection. Technometrics 47 (3), 349– 363.\n",
      "\n",
      "Turner, R., P. Berkes, M. Sahani, and D. Mackay (2008). Counterexam- ples to variational free energy com- pactness folk theorems. Technical report, U. Cambridge.\n",
      "\n",
      "Ueda, N. and R. Nakano (1998). Deter- ministic annealing EM algorithm. Neural Networks 11, 271–282.\n",
      "\n",
      "Usunier, N., D. Buffoni, and P. Galli- nari (2009). Ranking with ordered weighted pairwise classiﬁcation.\n",
      "\n",
      "Vaithyanathan, S. and B. Dom (1999). Model selection in unsupervised learning with applications to doc- ument clustering. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "van der Merwe, R., A. Doucet, N. de Freitas, and E. Wan (2000). The unscented particle ﬁlter. In NIPS-13.\n",
      "\n",
      "van Dyk, D. and X.-L. Meng (2001). The Art of Data Augmentation. J. Computational and Graphical Statistics 10(1), 1–50.\n",
      "\n",
      "Vandenberghe, L. (2006). Applied nu- merical computing: Lecture notes.\n",
      "\n",
      "Vandenberghe, L. (2011). Ee236c - op- timization methods for large-scale systems.\n",
      "\n",
      "Vanhatalo, J. (2010). Speeding up the inference in Gaussian process mod- Ph.D. thesis, Helsinki Univ. els. Technology.\n",
      "\n",
      "1042\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Vanhatalo,\n",
      "\n",
      "J., V. PietilÃd’inen, and A. Vehtari (2010). Approximate in- ference for disease mapping with sparse gaussian processes. Statis- tics in Medicine 29(15), 1580–1607.\n",
      "\n",
      "Vapnik, V. (1998). Statistical Learning\n",
      "\n",
      "Theory. Wiley.\n",
      "\n",
      "Viterbi, A.\n",
      "\n",
      "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Trans. on Informa- tion Theory 13(2), 260â ˘A ¸S269.\n",
      "\n",
      "(1967).\n",
      "\n",
      "von Luxburg, U. (2007). A tutorial on Statistics and\n",
      "\n",
      "spectral clustering. Computing 17 (4), 395–416.\n",
      "\n",
      "Wand, M.\n",
      "\n",
      "Semiparametric regression and graphical models. Aust. N. Z. J. Stat. 51(1), 9–41.\n",
      "\n",
      "(2009).\n",
      "\n",
      "Wand, M. P.,\n",
      "\n",
      "J. T. Ormerod, S. A. Padoan, and R. Fruhrwirth (2011). Mean Field Variational Bayes for Elaborate Distributions. Bayesian Analysis 6(4), 847 – 900.\n",
      "\n",
      "Vapnik, V., S. Golowich, and A. Smola (1997). Support vector method for function approximation, regression estimation, and signal processing. In NIPS.\n",
      "\n",
      "Varian, H. (2011). Structural time series in R: a Tutorial. Technical report, Google.\n",
      "\n",
      "Verma, T. and J. Pearl (1990). Equiva- lence and synthesis of causal mod- els. In UAI.\n",
      "\n",
      "Wagenmakers,\n",
      "\n",
      "R. Wetzels, E.-J., D. Borsboom, and H. van der Maas (2011). Why Psychologists Must Change the Way They Ana- lyze Their Data: The Case of Psi. Journal of Personality and Social Psychology.\n",
      "\n",
      "Wagner, D. and F. Wagner (1993). Be- tween min cut and graph bisec- tion. In Proc. 18th Intl. Symp. on Math. Found. of Comp. Sci., pp. 744– 750.\n",
      "\n",
      "Wang, C. (2007). Variational Bayesian Approach to Canonical Correlation Analysis. IEEE Trans. on Neural Net- works 18(3), 905–910.\n",
      "\n",
      "Wasserman, L. (2004). All of statistics. A concise course in statistical infer- ence. Springer.\n",
      "\n",
      "Wei, G. and M. Tanner (1990). A Monte Carlo implementation of the EM al- gorithm and the poor man’s data J. of the augmentation algorithms. Am. Stat. Assoc. 85(411), 699–704.\n",
      "\n",
      "Viinikanoja, J., A. Klami, and S. Kaski (2010). Variational Bayesian Mixture of Robust CCA Models. In Proc. Eu- ropean Conf. on Machine Learning.\n",
      "\n",
      "Vincent, P. (2011). A Connection be- tween Score Matching and Denois- ing Autoencoders. Neural Compu- tation 23(7), 1661–1674.\n",
      "\n",
      "Vincent, P., H. Larochelle,\n",
      "\n",
      "I. La- joie, Y. Bengio, and P.-A. Manzagol (2010). Stacked Denoising Autoen- coders: Learning Useful Represen- tations in a Deep Network with a Local Denoising Criterion. J. of Ma- chine Learning Research 11, 3371– 3408.\n",
      "\n",
      "Vinh, N., J. Epps, and J. Bailey (2009). Information theoretic measures for Is a cor- clusterings comparison: rection for chance necessary? In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Vinyals, M., J. Cerquides, J. Rodriguez- Aguilar, and A. Farinelli (2010). Worst-case bounds on the quality of max-product ﬁxed-points. In NIPS.\n",
      "\n",
      "Viola, P. and M. Jones (2001). Rapid object detection using a boosted cascade of simple classiﬁers. In CVPR.\n",
      "\n",
      "Virtanen, S.\n",
      "\n",
      "Bayesian expo- nential family projections. Master’s thesis, Aalto University.\n",
      "\n",
      "(2010).\n",
      "\n",
      "Vishwanathan, S. V. N. and A. Smola (2003). Fast kernels for string and tree matching. In NIPS.\n",
      "\n",
      "Wainwright, M., T.\n",
      "\n",
      "and A. Willsky (2001). Tree-based repa- rameterization for approximate es- timation on loopy graphs. In NIPS- 14.\n",
      "\n",
      "Jaakkola,\n",
      "\n",
      "and Wainwright, M., T. A. Willsky (2005). A new class of upper bounds on the log parti- tion function. IEEE Trans. Info. The- ory 51(7), 2313–2335.\n",
      "\n",
      "Jaakkola,\n",
      "\n",
      "Wainwright, M., P. Ravikumar, and J. Lafferty (2006). Inferring graph- ical model structure using (cid:7) − 1- regularized pseudo-likelihood. In NIPS.\n",
      "\n",
      "Wainwright, M. J., T. S. Jaakkola, and A. S. Willsky (2003). Tree-based reparameterization framework for analysis of sum-product and re- lated algorithms. IEEE Trans. on In- formation Theory 49(5), 1120–1146.\n",
      "\n",
      "Wainwright, M.\n",
      "\n",
      "Jordan (2008a). Graphical models, expo- nential families, and variational in- ference. Foundations and Trends in Machine Learning 1–2, 1–305.\n",
      "\n",
      "J. and M.\n",
      "\n",
      "I.\n",
      "\n",
      "Wainwright, M.\n",
      "\n",
      "Jordan (2008b). Graphical models, expo- nential families, and variational in- ference. Foundations and Trends in Machine Learning 1–2, 1–305.\n",
      "\n",
      "J. and M.\n",
      "\n",
      "I.\n",
      "\n",
      "Wallach, H., I. Murray, R. Salakhutdi- nov, and D. Mimno (2009). Evalua- tion methods for topic models. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Wan, E. A. and R. V. der Merwe (2001). The Unscented Kalman Filter. In S. Haykin (Ed.), Kalman Filtering and Neural Networks. Wiley.\n",
      "\n",
      "Weinberger, K., A. Dasgupta, J. Atten- berg, J. Langford, and A. Smola (2009). Feature hashing for large scale multitask learning. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Weiss, D., B. Sapp, and B. Taskar (2010). Sidestepping intractable in- ference with structured ensemble cascades. In NIPS.\n",
      "\n",
      "Weiss, Y. (2000). Correctness of local probability propagation in graph- ical models with loops. Neural Computation 12, 1–41.\n",
      "\n",
      "Weiss, Y. (2001). Comparing the mean ﬁeld method and belief propaga- tion for approximate inference in MRFs. In Saad and Opper (Eds.), Advanced Mean Field Methods. MIT Press.\n",
      "\n",
      "Weiss, Y. and W. T. Freeman (1999). Correctness of belief propagation in Gaussian graphical models of ar- bitrary topology. In NIPS-12.\n",
      "\n",
      "Weiss, Y. and W. T. Freeman (2001a). Correctness of belief propagation in Gaussian graphical models of ar- bitrary topology. Neural Computa- tion 13(10), 2173–2200.\n",
      "\n",
      "Weiss, Y. and W. T. Freeman (2001b). On the optimality of solutions of the max-product belief propaga- tion algorithm in arbitrary graphs. IEEE Trans. Information Theory, Special Issue on Codes on Graphs and Iterative Algorithms 47 (2), 723– 735.\n",
      "\n",
      "Weiss, Y., A. Torralba, and R. Fergus (2008). Spectral hashing. In NIPS.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1043\n",
      "\n",
      "Welling, M., C. Chemudugunta, and N. Sutter (2008). Deterministic la- tent variable models and their pit- falls. In Intl. Conf. on Data Mining.\n",
      "\n",
      "Welling, M., T. Minka, and Y. W. Teh (2005). Structured region graphs: Morphing EP into GBP. In UAI.\n",
      "\n",
      "Welling, M., M. Rosen-Zvi, and G. Hin- ton (2004). Exponential family har- moniums with an application to information retrieval. In NIPS-14.\n",
      "\n",
      "Welling, M. and C. Sutton (2005). Learning in Markov random ﬁelds In with contrastive free energies. Tenth International Workshop on Artiﬁcial Intelligence and Statistics (AISTATS).\n",
      "\n",
      "Welling, M. and Y.-W. Teh (2001). Belief optimization for binary networks: a stable alternative to loopy belief propagation. In UAI.\n",
      "\n",
      "Werbos, P.\n",
      "\n",
      "(1974). Beyond regression: New Tools for Prediction and Analy- sis in the Behavioral Sciences. Ph.D. thesis, Harvard.\n",
      "\n",
      "West, M. tures Biometrika 74, 646–648.\n",
      "\n",
      "(1987). of\n",
      "\n",
      "normal\n",
      "\n",
      "On scale mix- distributions.\n",
      "\n",
      "West, M. (2003). Bayesian Factor Re- gression Models in the \"Large p, Small n\" Paradigm. Bayesian Statis- tics 7 .\n",
      "\n",
      "West, M. and J. Harrison (1997). Bayesian forecasting and dynamic models. Springer.\n",
      "\n",
      "Williams, C. (1998). Computation with inﬁnite networks. Neural Computa- tion 10(5), 1203–1216.\n",
      "\n",
      "Williams, C.\n",
      "\n",
      "A MCMC ap- proach to Hierarchical Mixture Modelling . In S. A. Solla, T. K. Leen, and K.-R. Müller (Eds.), NIPS. MIT Press.\n",
      "\n",
      "(2000).\n",
      "\n",
      "Williams, C.\n",
      "\n",
      "(2002). On a Connec- tion between Kernel PCA and Met- ric Multidimensional Scaling. Ma- chine Learning J. 46(1).\n",
      "\n",
      "Williams, O. and A. Fitzgibbon (2006). Gaussian process implicit surfaces. In Gaussian processes in practice.\n",
      "\n",
      "Williamson, S. and Z. Ghahramani (2008). Probabilistic models for data combination in recommender systems. In NIPS Workshop on Learning from Multiple Sources.\n",
      "\n",
      "Winn, J. and C. Bishop (2005). Varia- tional message passing. J. of Ma- chine Learning Research 6, 661– 694.\n",
      "\n",
      "Wipf, D. and S. Nagarajan (2007). A new view of automatic relevancy determination. In NIPS.\n",
      "\n",
      "Wipf, D. and S. Nagarajan (2010, April). Iterative Reweighted (cid:7)−1 and (cid:7)−2 Methods for Finding Sparse Solu- tions. J. of Selected Topics in Signal Processing (Special Issue on Com- pressive Sensing) 4(2).\n",
      "\n",
      "Wood, F., C. Archambeau, J. Gasthaus, L. James, and Y. W. Teh (2009). A stochastic memoizer for sequence In Intl. Conf. on Machine data. Learning.\n",
      "\n",
      "Wright,\n",
      "\n",
      "and M. Figueiredo (2009). Sparse reconstruction by separable ap- proximation. IEEE Trans. on Signal Processing 57 (7), 2479–2493.\n",
      "\n",
      "S.,\n",
      "\n",
      "R.\n",
      "\n",
      "Nowak,\n",
      "\n",
      "Wu, T. T. and K. Lange (2008). Coordi- nate descent algorithms for lasso penalized regression. Ann. Appl. Stat 2(1), 224–244.\n",
      "\n",
      "Wu, Y., H. Tjelmeland, and M. West (2007). Bayesian CART: Prior struc- ture and MCMC computations. J. of Computational and Graphical Statistics 16(1), 44–66.\n",
      "\n",
      "Xu, F. and J. Tenenbaum (2007). Word learning as Bayesian inference. Psy- chological Review 114(2).\n",
      "\n",
      "Xu, Z., V. Tresp, A. Rettinger, and K. Kersting (2008). Social net- work mining with nonparametric relational models. In ACM Work- shop on Social Network Mining and Analysis (SNA-KDD 2008).\n",
      "\n",
      "Xu, Z., V. Tresp, K. Yu, and H.-P. Kriegel (2006). Inﬁnite hidden rela- tional models. In UAI.\n",
      "\n",
      "Xu, Z., V. Tresp, S. Yu, K. Yu, and H.-P. Kriegel (2007). Fast inference in in- ﬁnite hidden relational models. In Workshop on Mining and Learning with Graphs.\n",
      "\n",
      "Weston, J., S. Bengio, and N. Usunier (2010). Large Scale Image Annota- tion: Learning to Rank with Joint Word-Image Embeddings. In Proc. European Conf. on Machine Learn- ing.\n",
      "\n",
      "Wipf, D., B. Rao, and S. Nagarajan Latent variable bayesian (2010). models for promoting sparsity. IEEE Transactions on Information Theory.\n",
      "\n",
      "Xue, Y., X. Liao, L. Carin, and B. Krish- napuram (2007). Multi-task learn- ing for classiﬁcation with dirichlet process priors. J. of Machine Learn- ing Research 8, 2007.\n",
      "\n",
      "Weston, J., F. Ratle, and R. Collobert (2008). Deep Learning via Semi- Supervised Embedding. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Weston,\n",
      "\n",
      "Multi-lcass chines. In ESANN.\n",
      "\n",
      "J. and C. Watkins (1999). support vector ma-\n",
      "\n",
      "Wiering, M. and M. van Otterlo (Eds.) (2012). Reinforcement learn- ing: State-of-the-art. Springer.\n",
      "\n",
      "Wilkinson, D. and S. Yeung (2002). Conditional simulation from highly structured gaussian systems with application to blocking-mcmc for the bayesian analysis of very large linear models. Statistics and Com- puting 12, 287–300.\n",
      "\n",
      "Witten, D., R. Tibshirani, and T. Hastie (2009). A penalized matrix de- composition, with applications to sparse principal components and canonical correlation analysis. Bio- statistics 10(3), 515–534.\n",
      "\n",
      "Wolpert, D. (1992). Stacked generaliza- tion. Neural Networks 5(2), 241–259.\n",
      "\n",
      "Wolpert, D. (1996). The lack of a priori distinctions between learning algo- rithms. Neural Computation 8(7), 1341–1390.\n",
      "\n",
      "Wong, F., C. Carter, and R. Kohn Efficient estimation of models.\n",
      "\n",
      "(2003). covariance Biometrika 90(4), 809–830.\n",
      "\n",
      "selection\n",
      "\n",
      "Yadollahpour,\n",
      "\n",
      "and G. Shakhnarovich (2011). Diverse M- best Solutions in MRFs. In NIPS workshop on Disrete Optimization in Machine Learning.\n",
      "\n",
      "P., D. Batra,\n",
      "\n",
      "Yan, D., L. Huang, and M. I. Jordan (2009). Fast approximate spectral clustering. In 15th ACM Conf. on Knowledge Discovery and Data Min- ing.\n",
      "\n",
      "Yang, A., A. Ganesh, S. Sastry, and Y. Ma (2010, l1- minimization algorithms and an application in robust face recog- nition: A review. Technical Re- port UCB/EECS-2010-13, EECS De- partment, University of California, Berkeley.\n",
      "\n",
      "Feb).\n",
      "\n",
      "Fast\n",
      "\n",
      "1044\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "Yang, C., R. Duraiswami, and L. David (2005). Efficient kernel machines using the improved fast Gauss transform. In NIPS.\n",
      "\n",
      "Yang, S., B. Long, A. Smola, H. Zha, and Z. Zheng (2011). Collaborative competitive ﬁltering: learning rec- ommender using context of user In Proc. Annual Intl. ACM choice. SIGIR Conference.\n",
      "\n",
      "Yanover, C., O. Schueler-Furman, and Y. Weiss (2007). Minimizing and Learning Energy Functions for Side-Chain Prediction. In Recomb.\n",
      "\n",
      "Yaun, G.-X., K.-W. Chang, C.-J. Hsieh, and C.-J. Lin (2010). A Comparison of Optimization Methods and Soft- ware for Large-scale L1-regularized Linear Classiﬁcation. J. of Machine Learning Research 11, 3183–3234.\n",
      "\n",
      "Yedidia,\n",
      "\n",
      "and J., W. T. Freeman, Y. Weiss (2001). Understanding be- lief propagation and its generaliza- tions. In Intl. Joint Conf. on AI.\n",
      "\n",
      "Yoshida, R. and M. West\n",
      "\n",
      "(2010). Bayesian learning in sparse graphi- cal factor models via annealed en- tropy. J. of Machine Learning Re- search 11, 1771–1798.\n",
      "\n",
      "Yuille, A.\n",
      "\n",
      "CCCP algorithms to minimze the Bethe and Kikuchi free energies: convergent alterna- tives to belief propagation. Neural Computation 14, 1691–1722.\n",
      "\n",
      "(2001).\n",
      "\n",
      "Yuille, A. and A. Rangarajan (2003). concave-convex procedure.\n",
      "\n",
      "The Neural Computation 15, 915.\n",
      "\n",
      "Yuille, A. and S. Zheng (2009). Com- positional noisy-logical learning. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Yuille, A. L. and X. He (2011). Proba- bilistic models of vision and max- margin methods. Frontiers of Electrical and Electronic Engineer- ing 7 (1).\n",
      "\n",
      "Zellner, A. (1986). On assessing prior distributions and bayesian regres- sion analysis with g-prior distri- butions. In Bayesian inference and decision techniques, Studies of Bayesian and Econometrics and Statistics volume 6. North Holland.\n",
      "\n",
      "Zhai, C. and J. Lafferty (2004).\n",
      "\n",
      "A study of smoothing methods for language models applied to infor- mation retrieval. ACM Trans. on In- formation Systems 22(2), 179–214.\n",
      "\n",
      "Zhao, P. and B. Yu (2007). Stagewise J. of Machine Learning Re-\n",
      "\n",
      "Lasso. search 8, 2701–2726.\n",
      "\n",
      "Zhou, H., D. Karakos, S. Khudanpur, A. Andreou, and C. Priebe (2009). On Projections of Gaussian Dis- tributions using Maximum Likeli- hood Criteria. In Proc. of the Work- shop on Information Theory and its Applications.\n",
      "\n",
      "Zhou, M., H. Chen, J. Paisley, L. Ren, G. Sapiro, and L. Carin (2009). Non-parametric Bayesian Dictio- nary Learning for Sparse Image Representations. In NIPS.\n",
      "\n",
      "Zhou, X. and X. Liu (2008).\n",
      "\n",
      "The EM algorithm for the extended ﬁ- nite mixture of the factor analyz- ers model. Computational Statistics and Data Analysis 52, 3939–3953.\n",
      "\n",
      "Zhu, C. S., N. Y. Wu, and D. Mum- ford (1997, November). Minimax entropy principle and its applica- tion to texture modeling. Neural Computation 9(8).\n",
      "\n",
      "Zhu, J. and E. Xing (2010). Conditional In Intl. Conf.\n",
      "\n",
      "topic random ﬁelds. on Machine Learning.\n",
      "\n",
      "Younes, L. (1989). Parameter estima- tion for imperfectly observed Gibb- sian ﬁelds. Probab. Theory and Re- lated Fields 82, 625–645.\n",
      "\n",
      "Yu, C. and T. Joachims (2009). Learn- ing structural SVMs with latent variables. In Intl. Conf. on Machine Learning.\n",
      "\n",
      "Yu, S., K. Yu, V. Tresp, K. H-P., and M. Wu (2006). Supervised proba- bilistic principal component anal- In Proc. of the Int’l Conf. on ysis. Knowledge Discovery and Data Min- ing.\n",
      "\n",
      "Zhang, N. (2004). Hierarchical latnet class models for cluster analysis. J. of Machine Learning Research, 301– 308.\n",
      "\n",
      "Zhang, N. and D. Poole (1996). Ex- independence in J. of\n",
      "\n",
      "ploiting causal Bayesian network inference. AI Research, 301–328.\n",
      "\n",
      "Zhang, T. (2008). Adaptive Forward- Backward Greedy Algorithm for Sparse Learning with Linear Mod- els. In NIPS.\n",
      "\n",
      "Zhu, L., Y. Chen, A.Yuille, and W. Free- man (2010). Latent hierarchical structure learning for object detec- tion. In CVPR.\n",
      "\n",
      "Zhu, M. and A. Ghodsi (2006). Au- tomatic dimensionality selection from the scree plot via the use of proﬁle likelihood. Computational Statistics & Data Analysis 51, 918– 930.\n",
      "\n",
      "Zhu, M. and A. Lu (2004). The counter- intuitive non-informative prior for the bernoulli family. J. Statistics Ed- ucation.\n",
      "\n",
      "Yu, S.-Z. and H. Kobayashi\n",
      "\n",
      "(2006). Practical implementation of an ef- ﬁcient forward-backward algorithm for an explicit-duration hidden Markov model. IEEE Trans. on Sig- nal Processing 54(5), 1947– 1951.\n",
      "\n",
      "Yuan, M. and Y. Lin (2006). Model selection and estimation in re- gression with grouped variables. J. Royal Statistical Society, Series B 68(1), 49–67.\n",
      "\n",
      "(2007). Yuan, M. Model selection and estimation in the gaussian graphical model. Biometrika 94(1), 19–35.\n",
      "\n",
      "and\n",
      "\n",
      "Y.\n",
      "\n",
      "Lin\n",
      "\n",
      "Zhang, X., T. Graepel, and R. Herbrich (2010). Bayesian Online Learning for Multi-label and Multi-variate Performance Measures. In AI/Statis- tics.\n",
      "\n",
      "Zhao,\n",
      "\n",
      "J.-H. and P. L. H. Yu (2008, November). Fast ML Estimation for the Mixture of Factor Analyzers via an ECM Algorithm. IEEE. Trans. on Neural Networks 19(11).\n",
      "\n",
      "Zhao, P., G. Rocha, and B. Yu (2005). Grouped and Hierarchical Model Selection through Composite Abso- lute Penalties. Technical report, UC Berkeley.\n",
      "\n",
      "Zinkevich, M.\n",
      "\n",
      "(2003). Online con- vex programming and generalized In inﬁnitesimal gradient ascent. Intl. Conf. on Machine Learning, pp. 928â ˘A ¸S936.\n",
      "\n",
      "Zobay, O. (2009). Mean ﬁeld inference for the Dirichlet process mixture model. Electronic J. of Statistics 3, 507–545.\n",
      "\n",
      "Zoeter, O. (2007). Bayesian generalized linear models in a terabyte world. In Proc. 5th International Sympo- sium on image and Signal Process- ing and Analysis.\n",
      "\n",
      "BIBLIOGRAPHY\n",
      "\n",
      "1045\n",
      "\n",
      "Zou, H.\n",
      "\n",
      "and its oracle properties. Am. Stat. Assoc., 1418–1429.\n",
      "\n",
      "(2006). The adaptive Lasso J. of the\n",
      "\n",
      "nent analysis. J. of Computational and Graphical Statistics 15(2), 262– 286.\n",
      "\n",
      "cave penalized likelihood models. Annals of Statistics 36(4), 1509–1533.\n",
      "\n",
      "Zou, H. and T. Hastie (2005). Regular- ization and variable selection via the elastic net. J. of Royal Stat. Soc. Series B 67 (2), 301–320.\n",
      "\n",
      "Zou, H., T. Hastie, and R. Tibshirani Sparse principal compo-\n",
      "\n",
      "(2006).\n",
      "\n",
      "Zou, H., T. Hastie, and R. Tibshirani (2007). On the \"Degrees of Free- dom\" of the Lasso. Annals of Statis- tics 35(5), 2173–2192.\n",
      "\n",
      "Zou, H. and R. Li\n",
      "\n",
      "One- step sparse estimates in noncon-\n",
      "\n",
      "(2008).\n",
      "\n",
      "Zweig, G.\n",
      "\n",
      "and M. Padmanabhan (2000). Exact alpha-beta computa- tion in logarithmic space with ap- plication to map word graph con- struction. In Proc. Intl. Conf. Spoken Lang.\n",
      "\n",
      "Index to code\n",
      "\n",
      "agglomDemo, 894 amazonSellerDemo, 155 arsDemo, 819 arsEnvelope, 819\n",
      "\n",
      "bayesChangeOfVar, 151 bayesLinRegDemo2d, 233 bayesTtestDemo, 138 beliefPropagation, 768 bernoulliEntropyFig, 57 besselk, 477 betaBinomPostPredDemo, 79 betaCredibleInt, 153 betaHPD, 153, 154 betaPlotDemo, 43 biasVarModelComplexity3, 204 bimodalDemo, 150 binaryFaDemoTipping, 403 binomDistPlot, 35 binomialBetaPosteriorDemo, 75 bleiLDAperplexityPlot, 955 bolassoDemo, 440 boostingDemo, 555, 558 bootstrapDemoBer, 192\n",
      "\n",
      "cancerHighDimClassifDemo, 110 cancerRatesEb, 172 casinoDemo, 606, 607 centralLimitDemo, 52 changeOfVarsDemo1d, 53 chowliuTreeDemo, 913 coinsModelSelDemo, 164 contoursSSEdemo, 219 convexFnHand, 222 curseDimensionality, 18\n",
      "\n",
      "demard, 580 depnetFit, 909 dirichlet3dPlot, 48 dirichletHistogramDemo, 48 discreteProbDistFig, 28 discrimAnalysisDboundariesDemo, 103, 105 discrimAnalysisFit, 106 discrimAnalysisHeightWeightDemo, 145 discrimAnalysisPredict, 106 dpmGauss2dDemo, 888 dpmSampleDemo, 881 dtﬁt, 545 dtreeDemoIris, 549, 550\n",
      "\n",
      "elasticDistortionsDemo, 567 emLogLikelihoodMax, 365\n",
      "\n",
      "faBiplotDemo, 383 ﬁsherDiscrimVowelDemo, 274 ﬁsheririsDemo, 6 ﬁsherLDAdemo, 272 fmGibbs, 843\n",
      "\n",
      "gaussInferParamsMean1d, 121 gaussInferParamsMean2d, 123 gaussInterpDemo, 113 gaussInterpNoisyDemo, 125 gaussMissingFitEm, 374 gaussMissingFitGibbs, 840 gaussPlot2d, 142 gaussPlot2Ddemo, 47 gaussPlotDemo, 19 gaussSeqUpdateSigma1D, 131 generativeVsDiscrim, 269 geomRidge, 229 ggmFitDemo, 939 ggmFitHtf, 939 ggmFitMinfunc, 939 ggmLassoDemo, 13, 940 ggmLassoHtf, 940 gibbsDemoIsing, 670, 873 gibbsGaussDemo, 848 giniDemo, 548 gpcDemo2d, 529 gpnnDemo, 536 gprDemoArd, 520 gprDemoChangeHparams, 519 gprDemoMarglik, 522 gprDemoNoiseFree, 517 gpSpatialDemoLaplace, 532 groupLassoDemo, 451\n",
      "\n",
      "hclustYeastDemo, 894, 896 hingeLossPlot, 211, 556 hmmFilter, 609 hmmFwdBack, 611 hmmLillypadDemo, 604 hmmSelfLoopDist, 623 hopﬁeldDemo, 670 huberLossDemo, 223, 497\n",
      "\n",
      "icaBasisDemo, 471 icaDemo, 408 icaDemoUniform, 409 IPFdemo2x2, 683 isingImageDenoiseDemo, 739, 839\n",
      "\n",
      "kalmanFilter, 641 kalmanTrackingDemo, 632 kernelBinaryClassifDemo, 489 kernelRegrDemo, 490, 491 kernelRegressionDemo, 510 KLfwdReverseMixGauss, 734 KLpqGauss, 734 kmeansHeightWeight, 10 kmeansModelSel1d, 371 kmeansYeastDemo, 341 knnClassifyDemo, 17, 23–25 knnVoronoi, 16 kpcaDemo2, 495 kpcaScholkopf, 493\n",
      "\n",
      "gammaPlotDemo, 41, 150 gammaRainfallDemo, 41 gampdf, 41 gaussCondition2Ddemo2, 112 gaussHeightWeight, 102 gaussImputationDemo, 115, 375\n",
      "\n",
      "lassoPathProstate, 437, 438 LassoShooting, 441 leastSquaresProjection, 221 linregAllsubsetsGraycodeDemo, 423 linregBayesCaterpillar, 237, 238 linregCensoredSchmeeHahnDemo, 379\n",
      "\n",
      "1048\n",
      "\n",
      "INDEXES\n",
      "\n",
      "linregDemo1, 241 linregEbModelSelVsN, 158, 159, 749 linregFitL1Test, 447 linregOnlineDemoKalman, 636 linregPolyLassoDemo, 436 linregPolyVsDegree, 9, 20, 436 linregPolyVsN, 231 linregPolyVsRegDemo, 208, 225, 226, 239 linregPostPredDemo, 235 linregRbfDemo, 487 linregRobustDemoCombined, 223 linregWedgeDemo2, 19 LMSdemo, 265 logregFit, 254 logregLaplaceGirolamiDemo, 257, 258 logregMultinomKernelDemo, 269 logregSATdemo, 21 logregSATdemoBayes, 259 logregSatMhDemo, 852 logregXorDemo, 486 logsumexp, 86 lossFunctionFig, 179 lsiCode, 419\n",
      "\n",
      "marsDemo, 554 mcAccuracyDemo, 55 mcEstimatePi, 54 mcmcGmmDemo, 851, 860, 861 mcQuantileDemo, 153 mcStatDist, 598 miMixedDemo, 59 mixBerMnistEM, 341 mixBetaDemo, 170 mixexpDemo, 343 mixexpDemoOneToMany, 344 mixGaussDemoFaithful, 353 mixGaussLikSurfaceDemo, 346 mixGaussMLvsMAP, 356 mixGaussOverRelaxedEmDemo, 369 mixGaussPlotDemo, 339 mixGaussSingularity, 356 mixGaussVbDemoFaithful, 753, 755 mixPpcaDemoNetlab, 386 mixStudentBankruptcyDemo, 361 mlpPriorsDemo, 574 mlpRegEvidenceDemo, 579 mlpRegHmcDemo, 579 mnist1NNdemo, 25, 1002 multilevelLinregDemo, 844 mutualInfoAllPairsMixed, 59\n",
      "\n",
      "naiveBayesBowDemo, 84, 88 naiveBayesFit, 83, 277 naiveBayesPredict, 86, 277 netﬂixResultsPlot, 981 newsgroupsVisualize, 5 newtonsMethodMinQuad, 250 newtonsMethodNonConvex, 250 ngramPlot, 592 NIXdemo2, 135 normalGammaPenaltyPlotDemo, 460 normalGammaThresholdPlotDemo, 461 numbersGame, 69–71\n",
      "\n",
      "pagerankDemo, 600, 603 pagerankDemoPmtk, 602 paretoPlot, 44 parzenWindowDemo2, 509 pcaDemo2d, 388 pcaDemo3d, 11\n",
      "\n",
      "pcaDemoHeightWeight, 389 pcaEmStepByStep, 397 pcaImageDemo, 12, 389 pcaOverﬁtDemo, 400–402 pcaPmtk, 393 pfColorTrackerDemo, 830 poissonPlotDemo, 37 postDensityIntervals, 154 ppcaDemo2d, 388 PRhand, 182 probitPlot, 259 probitRegDemo, 259, 294, 363 prostateComparison, 436 prostateSubsets, 427\n",
      "\n",
      "quantileDemo, 33\n",
      "\n",
      "randomWalk0to20Demo, 856 rbpfManeuverDemo, 834, 835 rbpfSlamDemo, 835 rdaFit, 108 regtreeSurfaceDemo, 545 rejectionSamplingDemo, 818 relevanceNetworkNewsgroupDemo, 908 residualsDemo, 219 ridgePathProstate, 437 riskFnGauss, 198 robustDemo, 40 robustPriorDemo, 168\n",
      "\n",
      "saDemoPeaks, 869, 870 sampleCdf, 816 samplingDistGaussShrinkage, 203 sensorFusion2d, 123 sensorFusionUnknownPrec, 141 seqlogoDemo, 36 shrinkageDemoBaseball, 175 shrinkcov, 130 shrinkcovDemo, 129 shrunkenCentroidsFit, 109 shrunkenCentroidsSRBCTdemo, 109, 110 shuffledDigitsDemo, 7, 25 sigmoidLowerBounds, 761 sigmoidPlot, 21 sigmoidplot2D, 246 simpsonsParadoxGraph, 933 sliceSamplingDemo1d, 865 sliceSamplingDemo2d, 865 smoothingKernelPlot, 507 softmaxDemo2, 103 SpaRSA, 445 sparseDictDemo, 471 sparseNnetDemo, 575 sparsePostPlot, 459 sparseSensingDemo, 438 spectralClusteringDemo, 893 splineBasisDemo, 125 ssmTimeSeriesSimple, 638, 639 steepestDescentDemo, 247, 248 stickBreakingDemo, 883 studentLaplacePdfPlot, 40 subgradientPlot, 432 subSuperGaussPlot, 412 surfaceFitDemo, 218 svdImageDemo, 394 svmCgammaDemo, 504\n",
      "\n",
      "tanhPlot, 570 trueskillDemo, 798\n",
      "\n",
      "INDEX TO CODE\n",
      "\n",
      "1049\n",
      "\n",
      "trueskillPlot, 797\n",
      "\n",
      "unigaussVbDemo, 745\n",
      "\n",
      "varEMbound, 368 variableElimination, 717 visDirichletGui, 48 visualizeAlarmNetwork, 314 vqDemo, 354\n",
      "\n",
      "wiPlotDemo, 127\n",
      "\n",
      "Index to keywords\n",
      "\n",
      "#P-hard, 727 0-1 loss, 177 3-SAT, 727\n",
      "\n",
      "A star search, 887 absorbing state, 598 accept, 848 action, 176 action nodes, 328 action space, 176 actions, 176 activation, 563 active learning, 230, 234, 938 Active set, 441 active set, 442 Activity recognition, 605 Adaboost.M1, 559 adagrad, 263 adaline, 569 adaptive basis-function model, 543 adaptive importance sampling, 821 adaptive lasso, 460 adaptive MCMC, 853 adaptive rejection Metropolis sampling, 820 adaptive rejection sampling, 820 add-one smoothing, 77, 593 ADF, 653, 983 adjacency matrix, 309, 970 adjust for, 934 adjusted Rand index, 878 admissible, 197 admixture mixture, 950 AdSense, 928 AdWords, 928 affinity propagation, 887 agglomerative clustering, 893 agglomerative hierarchical clustering, 927 aha, 68 AI, 1007 AIC, 162, 557 Akaike information criterion, 162 alarm network, 313 alignment, 701 all pairs, 503 alleles, 317 alpha divergence, 735 alpha expansion, 803 alpha-beta swap, 804 alternative hypothesis, 163 analysis view, 390 analysis-synthesis, 470 ancestors, 309 ancestral graph, 664 ancestral sampling, 822 and-or graphs, 1007 annealed importance sampling, 871, 923 annealing, 853 annealing importance sampling, 992 ANOVA, 553 anti-ferromagnets, 668 aperiodic, 598 approximate inference, 727 approximation error, 230 ARD, 238, 463, 520, 580 ARD kernel, 480 area under the curve, 181\n",
      "\n",
      "ARMA, 639, 674 array CGH, 454 association rules, 15 associative, 931 associative Markov network, 668 associative memory, 568, 669, 997 associative MRF, 802 assumed density ﬁlter, 267 assumed density ﬁltering, 653, 787 asymptotically normal, 194 asymptotically optimal, 201 asynchronous updates, 774 atom, 469 atomic bomb, 52 attractive MRF, 802 attributes, 2, 3 AUC, 181 audio-visual speech recognition, 628 augmented DAG, 932 auto-encoder, 1000 auto-encoders, 990 auto-regressive HMM, 626 autoclass, 11 autocorrelation function, 862 automatic relevance determination, 463 automatic relevancy determination, 238, 398, 580, 747 Automatic speech recognition, 605 automatic speech recognition, 624 auxiliary function, 350 auxiliary variables, 863, 868 average link clustering, 897 average precision, 303 average precision at K, 183 axis aligned, 47 axis parallel splits, 544\n",
      "\n",
      "back-propagation, 999 backdoor path, 934 backﬁtting, 552, 563, 998 background knowledge, 68 backoff smoothing, 594 backpropagation, 570, 970 backpropagation algorithm, 569 backslash operator, 228 Backwards selection, 428 bag of words, 5, 81, 945 bag-of-characters, 483 bag-of-words, 483 bagging, 551 bandwidth, 480, 507 barren node removal, 334, 714 BART, 551, 586 Barzilai-Borwein, 445 base distribution, 338 base learner, 554 base measure, 882 base rate fallacy, 30 basic feasible solution, 468 basis function expansion, 20, 217 basis functions, 421 basis pursuit denoising, 430 batch, 261 Baum-Welch, 618 Bayes ball algorithm, 324 Bayes decision rule, 177, 195 Bayes estimator, 177, 195\n",
      "\n",
      "INDEX TO KEYWORDS\n",
      "\n",
      "1051\n",
      "\n",
      "Bayes factor, 137, 163, 921 Bayes model averaging, 71, 581 Bayes point, 257 Bayes risk, 195 Bayes rule, 29, 340 Bayes Theorem, 29 Bayesian, xxvii, 27 Bayesian adaptive regression trees, 551 Bayesian factor regression, 405 Bayesian hierarchical clustering, 899 Bayesian information criterion, 161 Bayesian IPF, 683 Bayesian lasso, 448 Bayesian model selection, 156 Bayesian network structure learning, 914 Bayesian networks, 310 Bayesian Occam’s razor, 156 Bayesian statistics, 149, 191 BDe, 917 BDeu, 918 beam search, 428, 887 belief networks, 310 belief propagation, 611, 707, 767 belief state, 71, 332, 607, 609 belief state MDP, 332 belief updating, 709 bell curve, 20, 38 Berkson’s paradox, 326 Bernoulli, 21, 34 Bernoulli product model, 88 Bernoulli-Gaussian, 426 Bessel function, 483 beta distribution, 42, 74 beta function, 42 beta process, 470 beta-binomial, 78 Bethe, 781 Bethe energy functional, 781 Bethe free energy, 781 BFGS, 251 Bhattacharya distance, 828 bi-directed graph, 674 bias, 20, 200, 457 bias term, 669 bias-variance tradeoff, 202 BIC, 161, 256, 557, 920 biclustering, 903 big data, 1 bigram model, 591 binary classiﬁcation, 3, 65 binary entropy function, 57 binary independence model, 88 binary mask, 426, 470 binary tree, 895 Bing, 302, 799, 983 binomial, 34 binomial coefficient, 34 binomial distribution, 74 binomial regression, 292 BinomialBoost, 561 BIO, 687 biosequence analysis, 36, 170 bipartite graph, 313 biplot, 383 birth moves, 370 bisecting K-means, 898 bits, 56 bits-back, 733 black swan paradox, 77, 84 black-box, 340, 585\n",
      "\n",
      "Blackwell-MacQueen, 884 blank slate, 165 blind signal separation, 407 blind source separation, 407 blocked Gibbs sampling, 848 blocking Gibbs sampling, 848 bloodtype, 317 BN2O, 315 bolasso, 439 Boltzmann distribution, 104, 869 Boltzmann machine, 568, 669, 983 bond variables, 866 Boosting, 554 boosting, 553, 742 bootstrap, 192 bootstrap ﬁlter, 827 bootstrap lasso, 439 bootstrap resampling, 439 borrow statistical strength, 171, 231, 296, 845 bottleneck, 205, 337, 1000 bottleneck layer, 970 bound optimization, 369 box constraints, 444 Box-Muller, 817 boxcar kernel, 508, 508 Boyen-Koller, 654 BP, 707 BPDN, 430 Bradley Terry, 795 branch and bound, 811 branching factor, 954 bridge regression, 458 Brownian motion, 483 bucket elimination, 715 BUGS, 756, 847 Buried Markov models, 627 burn-in phase, 856 burned in, 838 burstiness, 88 bursty, 480\n",
      "\n",
      "C4.5, 545 calculus of variations, 289 calibration, 724 Candidate method, 872 Canonical correlation analysis, 407 canonical form, 282 canonical link function, 291 canonical parameters, 115, 282 Cardinality constraints, 810 CART, 544, 545 Cartesian, 51 cascade, 776 case analysis, 260 categorical, 2, 35 categorical PCA, 402, 947, 961 categorical variables, 876 Cauchy, 40 causal Markov assumption, 931 Causal models, 931 causal MRF, 661 causal networks, 310 causal sufficiency, 931 causality, 919, 929 CCA, 407 CCCP, 702 CD, 989 cdf, 32, 38 Censored regression, 379 censored regression, 380\n",
      "\n",
      "1052\n",
      "\n",
      "INDEXES\n",
      "\n",
      "centering matrix, 494 central composite design, 523 central interval, 152 central limit theorem, 38, 51, 255 central moment, 413 central-limit theorem, 55 centroid, 341 centroids, 486 certainty factors, 675 chain graph, 671 chain rule, 29, 307 chance nodes, 328 change of variables, 50 channel coding, 56 Chapman-Kolmogorov, 590 characteristic length scale, 480 Cheeseman-Stutz approximation, 923 Chi-squared distribution, 42 chi-squared statistic, 163, 213 children, 309, 310 Chinese restaurant process, 884 chip-Seq, 622 Cholesky decomposition, 227, 817 Chomsky normal form, 689 chordal, 665 chordal graph, 720 Chow-Liu algorithm, 312, 912 CI, 308 circuit complexity, 944 city block distance, 876 clamped phase, 987 clamped term, 677 clamping, 319 class imbalance, 503 class-conditional density, 30, 65 classical, 149 classical statistics, 191 classiﬁcation, 2, 3 Classiﬁcation and regression trees, 544 clausal form, 675 clause, 727 click-through rate, 4 clique, 310 cliques, 719, 722 closing the loop, 635 closure, 662 cluster variational method, 783 Clustering, 875 clustering, 10, 340 clusters, 487 clutter problem, 788 co-clustering, 979 co-occurrence matrix, 5 co-parents, 327 coarse-to-ﬁne grid, 775 cocktail party problem, 407 coclustering, 903 codebook, 354 collaborative ﬁltering, 14, 300, 387, 903, 979 collapsed Gibbs sampler, 841 collapsed Gibbs sampling, 956 collapsed particles, 831 collect evidence, 707 collect-to-root, 723 collider, 324 COLT, 210 committee method, 580 commutative semi-ring, 717 commutative semiring, 726 compactness, 897\n",
      "\n",
      "compelled edges, 915 complementary prior, 997 complete, 322 complete data, 270, 349 complete data assumption, 914 complete data log likelihood, 348, 350 complete link clustering, 897 completing the square, 143 composite likelihood, 678 compressed sensing, 472 compressive sensing, 472 computation tree, 772 computational learning theory, 210 computationalism, 569 concave, 222, 286 concave-convex procedure, 702 concentration matrix, 46 concentration parameter, 882 concept, 65 concept learning, 65 condensation, 827 conditional entropy, 59 conditional Gamma Poisson, 949 conditional Gaussian, 920 conditional independence, 308 conditional likelihood, 620 conditional logit model, 252 conditional probability, 29 conditional probability distribution, 308 conditional probability tables, 308 conditional random ﬁeld, 684 conditional random ﬁelds, 606, 661 conditional topic random ﬁeld, 969 conditionally conjugate, 132 conditionally independent, 31, 82 conditioning, 319 conditioning case, 322 conductance, 858 conﬁdence interval, 212 conﬁdence intervals, 153 confounder, 674 confounders, 931 confounding variable, 934 confusion matrix, 181 conjoint analysis, 297 conjugate gradients, 249, 524 conjugate prior, 74 conjugate priors, 281, 287 conjunctive normal form, 675 connectionism, 569 consensus sequence, 36, 606 conservation of probability mass, 157 consistent, 200 consistent estimator, 233 consistent estimators, 70 constant symbols, 676 constraint satisfaction problems, 717, 726 constraint-based approach, 924 content addressable memory, 669 context free grammar, 689 context speciﬁc independence, 321 context-speciﬁc independence, 944 contextual bandit, 184, 254 contingency table, 682 continuation method, 442, 869 contrastive divergence, 569, 989 contrastive term, 677 control signal, 625, 631 converge, 857 convex, 58, 221, 247, 285, 677\n",
      "\n",
      "INDEX TO KEYWORDS\n",
      "\n",
      "1053\n",
      "\n",
      "convex belief propagation, 785, 943 convex combination, 76, 130, 338 convex hull, 777 convolutional DBNs, 1004 convolutional neural nets, 1004 convolutional neural network, 565 cooling schedule, 870 corpus, 953 correlated topic model, 757, 961 correlation coefficient, 45, 876 correlation matrix, 45 correspondence, 658 cosine similarity, 480 cost-beneﬁt analysis, 186 coupled HMM, 628 covariance, 44 covariance graph, 674, 908 covariance matrix, 45, 46 covariance selection, 938 covariates, 2 CPD, 308 CPTs, 308 Cramer-Rao inequality, 201 Cramer-Rao lower bound, 201 credible interval, 137, 152, 212 CRF, 661, 684 critical temperature, 868 critical value, 671 cross entropy, 57, 571 cross over rate, 181 cross validation, 24, 206 cross-entropy, 246, 953 cross-language information retrieval, 963 crosscat, 904 crowd sourcing, 10, 995 CRP, 884 CTR, 4 cubic spline, 537 cumulant function, 282, 284 cumulants, 284 cumulative distribution function, 32, 38 curse of dimensionality, 18, 487 curved exponential family, 282 cutting plane, 698 CV, 24 cycle, 310 cyclic permutation property, 99\n",
      "\n",
      "d-prime, 106 d-separated, 324 DACE, 518 DAG, 310 damped updates, 739 damping, 773 Dasher, 591 data association, 658, 810 data augmentation, 362, 847 data compression, 56 data fragmentation, 546 data fusion, 404 data overwhelms the prior, 69 data-driven MCMC, 853 data-driven proposals, 828 DBM, 996 DBN, 628, 997 DCM, 89 DCT, 469 death moves, 370 debiasing, 439 decision, 176\n",
      "\n",
      "decision boundary, 22 decision diagram, 328 decision nodes, 328 decision problem, 176 decision procedure, 177 decision rule, 22 decision trees, 544 decoding, 693 decomposable, 665, 722, 941 decomposable graphs, 682 decomposes, 322, 917 DeeBN, 628 DeeBNs, 997 deep, 929 deep auto-encoders, 1000 deep belief network, 997 deep Boltzmann machine, 996 deep directed networks, 996 deep learning, 479, 995 deep networks, 569 defender’s fallacy, 61 deﬂated matrix, 418 degeneracy problem, 825 degenerate, 532, 535 degree, 310 degrees of freedom, 39, 161, 206, 229, 534 deleted interpolation, 593 delta rule, 265 dendrogram, 895 denoising auto-encoder, 1001 dense stereo reconstruction, 690 density estimation, 9 dependency network, 909 dependency networks, 679 derivative free ﬁlter, 651 descendants, 309 descriptive, 2 design matrix, 3, 875 detailed balance, 854 detailed balance equations, 599 determinism, 944 deterministic annealing, 367, 620 deviance, 547 DGM, 310 diagonal, 46 diagonal covariance LDA, 107 diagonal LDA, 108 diameter, 710, 897 dictionary, 469 digamma, 361, 752, 958 digital cameras, 8 dimensionality reduction, 11, 1000 Dirac delta function, 39 Dirac measure, 37, 68 Dirchlet process, 903 direct posterior probability approach, 184 directed, 309 directed acyclic graph, 310 directed graphical model, 310 directed local Markov property, 327 directed mixed graph, 929 directed mixed graphical model, 674 Dirichlet, 79 Dirichlet Compound Multinomial, 89 Dirichlet distribution, 47 Dirichlet multinomial regression LDA, 969 Dirichlet process, 596, 879, 882, 973, 976 Dirichlet process mixture models, 508, 755 discontinuity preserving, 691 discounted cumulative gain, 303\n",
      "\n",
      "1054\n",
      "\n",
      "INDEXES\n",
      "\n",
      "discrete, 35 discrete AdaBoost, 559 discrete choice modeling, 296 discrete random variable, 28 discrete with probability one, 884 discretize, 59, 691 discriminability, 106 discriminant analysis, 101 discriminant function, 500 discriminative, 245 discriminative classiﬁer, 30 discriminative LDA, 968 discriminative random ﬁeld, 684 disease mapping, 531 disease transmission, 970 disparity, 691 dispersion parameter, 290 dissimilarity analysis, 898 dissimilarity matrix, 875 distance matrix, 875 distance transform, 775 distorted, 566 distortion, 354 distribute evidence, 707 distribute-from-root, 724 distributed encoding, 984 distributed representation, 569, 627 distributional particles, 831 distributive law, 717 divisive clustering, 893 DNA sequences, 36 do calculus, 932 Document classiﬁcation, 87 document classiﬁcation, 5 Domain adaptation, 297 domain adaptation, 297 dominates, 197 double loop algorithms, 773 double Pareto distribution, 461 double sided exponential, 41 dRUM, 294 dual decomposition, 808 dual variables, 492, 499 dummy encoding, 35 dyadic, 976 DyBN, 628 DyBNs, 997 dynamic Bayes net, 653 dynamic Bayesian network, 628 dynamic linear model, 636 dynamic programming, 331, 920 dynamic topic model, 962\n",
      "\n",
      "E step, 350 e-commerce, 11 early stopping, 263, 557, 572 EB, 173 ECM, 369, 387 ECME, 369 ECOC, 581 econometric forecasting, 660 economy sized SVD, 392 edge appearance probability, 786 edges, 309 edit distance, 479 EER, 181 effective sample size, 75, 825, 862 efficient IPF, 683 efficiently PAC-learnable, 210 eigendecomposition, 98\n",
      "\n",
      "eigenfaces, 12 eigengap, 857 eigenvalue spectrum, 130 EKF, 648 elastic net, 438, 456, 936 elimination order, 718 EM, 271, 349, 618, 749 email spam ﬁltering, 5 embedding, 575 empirical Bayes, 157, 162, 173, 300, 746 empirical distribution, 37, 205 empirical measure, 37 empirical risk, 205, 697 empirical risk minimization, 205, 261 end effector, 344 energy based models, 666 energy function, 255 energy functional, 732, 778 ensemble, 980 Ensemble learning, 580 ensemble learning, 742 entanglement, 629 entanglement problem, 635, 653 Entropy, 547 entropy, 56 EP, 983 Epanechnikov kernel, 508 ePCA, 947 epigraph, 222 epistemological uncertainty, 973 epoch, 264, 566 epsilon insensitive loss function, 497 EPSR, 859 equal error rate, 181 equilibrium distribution, 597 equivalence class, 915 equivalent kernel, 512, 533 equivalent sample size, 76, 917 erf, 38 ergodic, 599 Erlang distribution, 42 ERM, 205, 261 error bar, 76 error correcting codes, 768 error correction, 56 error function, 38 error signal, 265 error-correcting output codes, 503, 581 ESS, 862 essential graph, 915 estimated potential scale reduction, 859 estimator, 191 Euclidean distance, 18 evidence, 156, 173 evidence procedure, 173, 238, 746 evolutionary MCMC, 429 exchangeable, 321, 963 exclusive or, 486 expectation correction, 658 expectation maximization, 349 expectation proagation, 735 Expectation propagation, 787 expectation propagation, 525 expected complete data log likelihood, 350, 351 expected proﬁt, 330 expected sufficient statistics, 350, 359, 619 expected value, 33 explaining away, 326 explicit duration HMM, 622 exploration-exploitation, 184\n",
      "\n",
      "INDEX TO KEYWORDS\n",
      "\n",
      "1055\n",
      "\n",
      "exploratory data analysis, 7 exponential cooling schedule, 870 Exponential distribution, 42 exponential family, 115, 253, 281, 282, 290, 347 exponential family harmonium, 985 exponential family PCA, 947 exponential loss, 556 exponential power distribution, 458 extended Kalman ﬁlter, 648 extension, 67 external ﬁeld, 668\n",
      "\n",
      "F score, 183 F1 score, 183, 699 FA, 381 face detection, 8 face detector, 555 face recognition, 8 Facebook, 974 factor, 665 factor analysis, 381, 402, 931, 947 factor analysis distance, 520 factor graph, 769, 769, 771, 888 factor loading matrix, 381 factorial HMM, 628 factorial prior, 463 factors, 382 faithful, 936 false alarm, 30, 180 false alarm rate, 181 false discovery rate, 184 false negative, 180 false positive, 30, 180 false positive rate, 181 family, 309 family marginal, 359 fan-in, 313 fantasy data, 990 farthest point clustering, 355 fast Fourier transform, 717, 775 fast Gauss transform, 524 fast ICA, 411 fast iterative shrinkage thesholding algorithm, 446 FastSLAM, 635, 835 fat hand, 933 fault diagnosis, 659 feature construction, 564 feature extraction, 6, 564 feature function, 667 feature induction, 680 feature maps, 565 feature matrix, 875 feature selection, 86 feature-based clustering, 875 features, 2, 3,412 feedback loops, 929 feedforward neural network, 563 ferro-magnets, 668 FFT, 775 ﬁelds of experts, 473 ﬁll-in edges, 719 Filtering, 607 ﬁltering, 87 ﬁnite difference matrix, 113 ﬁnite mixture model, 879 ﬁrst-order logic, 674 Fisher information, 166 Fisher information matrix, 152, 193, 293 Fisher kernel, 485 Fisher scoring method, 293\n",
      "\n",
      "Fisher’s linear discriminant analysis, 271 FISTA, 446 ﬁt-predict cycle, 206 ﬁxed effect, 298 Fixed lag smoothing, 608 ﬁxed point, 139 ﬂat clustering, 875 FLDA, 271 ﬂow cytometry, 936 folds, 24 forest, 310, 912 forward stagewise additive modeling, 557 forward stagewise linear regression, 562 forwards KL, 733 forwards model, 345 forwards selection, 428 forwards-backwards, 644, 688, 707, 720 forwards-backwards algorithm, 428, 611 founder model, 317 founder variables, 385 Fourier basis, 472 fraction of variance explained, 400 free energy, 988 free-form optimization, 737 frequent itemset mining, 15 frequentist, 27, 149 frequentist statistics, 191 Frobenius norm, 388 frustrated, 868 frustrated system, 668 full, 46 full conditional, 328, 838 function approximation, 3 functional data analysis, 124 functional gradient descent, 561 furthest neighbor clustering, 897 fused lasso, 454 fuzzy clustering, 973 fuzzy set theory, 65\n",
      "\n",
      "g-prior, 236, 425 game against nature, 176 game theory, 176 Gamma, 623 gamma distribution, 41 gamma function, 42 GaP, 949 gap statistic, 372 gating function, 342 Gauss-Seidel, 710 Gaussian, 20, 38 Gaussian approximation, 255, 731 Gaussian Bayes net, 318 Gaussian copulas, 942 Gaussian graphical models, 725 Gaussian kernel, 480, 507, 517 Gaussian mixture model, 339 Gaussian MRF, 672 Gaussian process, 483, 505, 509, 512, 882 Gaussian processes, 515 Gaussian random ﬁelds, 938 Gaussian RBM, 986 Gaussian scale mixture, 359, 447, 505 Gaussian sum ﬁlter, 656 GDA, 101 GEE, 300 GEM, 369 Gene ﬁnding, 606 gene ﬁnding, 622 gene knockout experiment, 931\n",
      "\n",
      "1056\n",
      "\n",
      "INDEXES\n",
      "\n",
      "gene microarrays, 421 generalization, 3 generalization error, 23, 180 generalization gradient, 66 generalize, 3 generalized additive model, 552 generalized belief propagation, 785 generalized cross validation, 207 generalized eigenvalue, 274 generalized EM, 361, 369 generalized estimating equations, 300 generalized linear mixed effects model, 298 generalized linear model, 281, 290 generalized linear models, 281 generalized pseudo Bayes ﬁlter, 657 generalized t distribution, 461 generate and test, 853 generative approach, 245 generative classiﬁer, 30 generative pre-training, 999 generative weights, 410, 986 genetic algorithms, 348, 720, 921 genetic linkage analysis, 315, 318 genome, 318 genotype, 317 geometric distribution, 622 Gibbs distribution, 290, 666 Gibbs sampler, 672 Gibbs sampling, 328, 669, 736, 838 Gini index, 548 gist, 963 Gittins Indices, 184 Glasso, 940 Glauber dynamics, 838 GLM, 290, 654 GLMM, 298 glmnet, 442 global balance equations, 597 global convergence, 248 global localization, 828 global Markov property, 661 global minimum, 222 global prior parameter independence, 916 globally normalized, 686 GM, 308 GMM, 339 GP-LVM, 540 GPs, 515 GPUs, 1006 gradient boosting, 560 gradient descent, 247, 445 Gram matrix, 481 grammars, 689, 1007 grandmother cells, 984, 1005 graph, 309 graph cuts, 890 graph Laplacian, 891 graph surgery, 932 graph-guided fused lasso, 454 graphcuts, 801 graphical lasso, 940 graphical model, 308, 311 graphical models, xxviii, 13, 31, 32, 308, 337, 909 Gray code, 422 greatest common divisor, 598 greedy equivalence search, 936 ground network, 676 ground states, 668 group lasso, 450, 579, 942 grouping effect, 456\n",
      "\n",
      "Gumbel, 295\n",
      "\n",
      "Hadamard product, 609 Haldane prior, 166 ham, 5 Hamiltonian MCMC, 868 Hammersley-Clifford, 666 hamming distance, 876 handwriting recognition, 7 haplotype, 317 hard clustering, 340 hard EM, 352 hard thresholding, 434, 435 harmonic mean, 183 harmonium, 983 Hastings correction, 849 hat matrix, 221 HDI, 154 heat bath, 838 heavy ball method, 249 heavy tails, 43, 223 Hellinger distance, 735 Helmholtz free energy, 733 Hessian, 193, 852 heteroscedastic LDA, 275 heuristics, 727 hidden, 10, 349 hidden layer, 563 hidden Markov model, 312, 603, 963 hidden nodes, 313 hidden semi-Markov model, 622 hidden units, 564 hidden variable, 312, 924 hidden variables, 319, 914 hierarchical adaptive lasso, 458 hierarchical Bayesian model, 171 hierarchical Bayesian models, 347 hierarchical clustering, 875, 893 hierarchical Dirichlet process, 621 hierarchical HMM, 624 hierarchical latent class model, 926 hierarchical mixture of experts, 344, 551 high throughput, 184, 421 high variance estimators, 550 highest density interval, 154 highest posterior density, 153 hill climbing, 920 hindsight, 607 hinge loss, 211, 477, 499 Hinton diagram, 592 Hinton diagrams, 399 histogram, 508 hit rate, 181 HMM, 312, 603 HMM ﬁlter, 640 HMMs, 685 Hoeffding’s inequality, 209 homogeneous, 589 homotopy, 442 Hopﬁeld network, 568, 669 horizon, 608 Horn clauses, 676 HPD, 153 HSMM, 622 Huber loss, 224, 561 Hugin, 722 Hungarian algorithm, 659, 810 hybrid MCMC, 868 hybrid Monte Carlo, 584 hybrid systems, 655\n",
      "\n",
      "INDEX TO KEYWORDS\n",
      "\n",
      "1057\n",
      "\n",
      "hyper-parameters, 74 hypothesis space, 66\n",
      "\n",
      "I-map, 324 I-projection, 733 ICA, 385, 409 ID3, 545 IDA, 936 identiﬁable, 346 identiﬁable in the limit, 70 iff, 68 iid, 51, 218, 320 ill-conditioned, 106, 129 image classiﬁcation, 7 image compression, 355 image denoising, 473 image inpainting, 14, 473 image segmentation, 671 image tagging, 968 IMM, 658 implicit feedback, 983 importance sampling, 820 importance weights, 821 impression log, 983 improper prior, 166, 168 imputation, 14 Imputation Posterior, 847 in-degree, 310 inclusion probabilities, 423 incremental EM, 365, 366 independence sampler, 848 independent and identically distributed, 51 independent component analysis, 409 indicator function, 17, 28, 976 induced width, 719 induction, 66, 77 inductive bias, 19, 582 infer.net, 799 inference, 320 inﬁnite hidden relational model, 977 inﬁnite HMM, 621 inﬁnite mixture models, 841, 879 inﬁnite relational model, 903, 973, 976 inﬂuence diagram, 328, 932 inﬂuence model, 628 infomax, 416 information, 27 information arc, 329, 331 information bottleneck, 405 information extraction, 688 information ﬁlter, 642 information form, 115, 305, 672, 711, 725 information gain, 547 Information inequality, 58 information projection, 733 information retrieval, 183, 300, 953 information theory, 56 inheritance model, 317 inner approximation, 779 innovation, 641 inside outside, 624 inside-outside algorithm, 689 instance-based learning, 17 integrate out, 156 integrated likelihood, 156 integrated risk, 195 intensive care unit, 313 inter-causal reasoning, 326 interaction effects, 421 interactive multiple models, 658\n",
      "\n",
      "interest point detector, 484 interpolate, 112 interpolated Kneser-Ney, 595 interpolator, 517 interval censored, 379 interventional data, 936 interventions, 931 intrinsic Gaussian random ﬁeld, 113 invariant, 8, 854 invariant distribution, 597 invariant features, 1004 inverse chi-squared distribution, 131 inverse Gamma, 130 inverse gamma, 42 inverse Gaussian, 448 inverse probability transform, 815 inverse problem, 317 inverse problems, 344 inverse reinforcement learning, 186 inverse Wishart, 126, 128 inverted index, 600 inverted indices, 1004 IP, 847 IPF, 682 iris, 6, 548 IRLS, 251 IRM, 976 irreducible, 598 Ising model, 668 isotropic, 46 iterated EKF, 650 iterative conditional modes, 669, 804, 929 iterative proportional ﬁtting, 682, 939 iterative scaling, 683 iterative shrinkage and thresholding algorithm, 445 iterative soft thresholding, 445 iteratively reweighted least squares, 251\n",
      "\n",
      "Jacobi, 710, 773 Jacobian, 151, 648, 649 Jacobian matrix, 50 JAGS, 847 JamBayes, 13 James Stein estimator, 174 James-Stein estimator, 173, 199 JC Penney, 603 Jeffreys prior, 166 Jeffreys-Lindley paradox, 165 Jensen’s inequality, 58, 363 Jensen-Shannon divergence, 57 Jeopardy, 4 jittered, 486 JJ bound, 761 joint distribution, 29, 307 joint probability distribution, 44 JTA, 720 jump Markov linear system, 655 junction tree, 722 junction tree algorithm, 720, 731 junction trees, 635\n",
      "\n",
      "K-centers, 887 K-means algorithm, 352 k-means++, 355 K-medoids algorothm, 490 k-spectrum kernel, 484 K2 algorithm, 920 Kalman ﬁlter, 122, 267, 632, 633, 640, 643 Kalman gain matrix, 637, 641 Kalman smoother, 633, 707\n",
      "\n",
      "1058\n",
      "\n",
      "INDEXES\n",
      "\n",
      "Kalman smoothing, 644, 712, 963 Karhunen Loeve, 387 Karl Popper, 77 KDE, 508, 510 Kendall’s τ , 304 kernel, 565, 600, 848 kernel density estimation, 127, 510 kernel density estimator, 508 kernel function, 479, 515 kernel machine, 486 kernel PCA, 494, 540, 892 kernel regression, 511 kernel smoothing, 511 kernel trick, 488 kernelised feature vector, 486 Kikuchi free energy, 784 kinect, 551 kinematic tracking, 344 kink, 372 KL divergence, 57, 732 Kleene star, 483 knee, 372 KNN, 16 knots, 537 knowledge base, 676 knowledge discovery, 2, 9 knowledge engineering, 313 Kolmogorov Smirnov, 864 kriging, 516 kronecker product, 253, 760 Kruskal’s algorithm, 912 Kullback-Leibler divergence, 57 kurtosis, 413, 415\n",
      "\n",
      "L-BFGS, 252 (cid:7)0 pseudo-norm, 424 (cid:7)0 regularization, 426 (cid:7)1 loss, 179 (cid:7)1 regularization, 430 L1-Adaboost, 563 L1VM, 488, 505 (cid:7)2 loss, 179 (cid:7)2 norm, 218 (cid:7)2 regularization, 226 L2boosting, 558 L2VM, 488 label, 176 label bias, 685 label switching, 341, 841 label taxonomy, 689 labeled LDA, 953, 969 lag, 608 Lagrange multiplier, 80 Lagrange multipliers, 289 Lagrangian, 80, 289 Lagrangian relaxation, 808 Lanczos algorithm, 398 language model, 300, 953 language modeling, 81, 568 language models, 591 Laplace, 223, 413, 429 Laplace approximation, 255, 468 Laplace distribution, 41 Laplace’s rule of succession, 77 LAR, 442, 562 large margin classiﬁer, 501 large margin principle, 259 LARS, 437, 442, 558, 562 lasso, 431, 470, 562, 936 latent, 11\n",
      "\n",
      "latent class model, 926 latent CRF, 701 latent Dirichlet allocation, 949, 950 latent factors, 11 latent semantic analysis, 12, 947 latent semantic indexing, 418, 947 latent SVMs, 702 latent variable models, 337 lattice, 668 Lauritzen-Spiegelhalter, 722 LBP, 767 LDA, 104, 927, 949, 950 LDA-HMM, 963 LDPC, 768 LDS, 631 leaf, 309 leak node, 315 leapfrog steps, 868 learning, 320 learning curve, 230 learning rate, 247 learning to learn, 296 learning to rank, 300 least favorable prior, 197 least mean squares, 265, 637 least squares, 219 least squares boosting, 428, 442, 558 leave one out cross validation, 207 leave-one out cross validation, 24 leaves, 895 left censored, 379 left-to-right, 612 left-to-right transition matrix, 590 LeNet5, 566 leptokurtic, 413 LETOR, 300 level sets, 47 Levenberg Marquardt, 250 Levinson-Durbin, 627 LG-SSM, 631 likelihood, 319 likelihood equivalence, 917 likelihood equivalent, 200 likelihood principle, 214 likelihood ratio, 67, 163 likelihood weighting, 822 limited memory BFGS, 252 limiting distribution, 598 line minimization, 248 line search, 248 linear discriminant analysis, 104 linear dynamical system, 631 linear Gaussian, 318 linear Gaussian system, 119 linear kernel, 482 linear program, 224 linear programming relaxtion, 800 linear regression, 19 linear smoother, 533 linear threshold unit, 252 linear trend, 660 linear-Gaussian CPD, 673 linear-Gaussian SSM, 631 linearity of expectation, 49 linearly separable, 22, 252, 266 link farms, 601 link function, 291 LISREL, 930 ListNet, 302 LMS, 265, 637\n",
      "\n",
      "INDEX TO KEYWORDS\n",
      "\n",
      "1059\n",
      "\n",
      "local consistency, 780 local evidence, 317, 671 local level model, 637 local prior parameter independence, 917 local variational approximation, 756 localist encoding, 984 locally decodable, 811 locally normalized, 686, 715 locally weighted regression, 512 LOESS, 512 log partition function, 282 log-linear, 667 log-loss, 210 log-odds ratio, 283 log-sum-exp, 86, 757 logic sampling, 822 logical reasoning problems, 726 logistic, 21, 295 logistic distribution, 413, 863 logistic normal, 402, 961 logistic regression, 21, 106 logit, 21 logitBoost, 560 long tail, 2, 296 long tails, 43 LOOCV, 24, 207 look-ahead RBPF, 832 loop, 310 loopy belief propagation, 691, 767, 889 Lorentz, 40 loss, 176 loss function, 261 loss matrix, 185 loss-augmented decoding, 699 loss-calibrated inference, 694 lossy compression, 354 low density parity check, 768 Low-level vision, 690 LOWESS, 512 LSA, 947, 1003 lse, 757 LSI, 947 LVM, 337\n",
      "\n",
      "M step, 350 M-projection, 733 M3nets, 693 machine learning, 1 macro-averaged F1, 183 Mahalanobis distance, 98 mammogram, 29 maneuvering target tracking, 832 manifest, 930 MAP estimate, 4, 178 MAR, 270 margin, 563 margin re-rescaling), 696 marginal distribution, 29 marginal likelihood, 156, 169 marginal polytope, 777 marginalizing out, 320 marginally independent, 30 marker, 317 market basket analysis, 15 Markov, 324 Markov assumption, 308 Markov blanket, 327, 662, 736, 838 Markov chain, 308, 589 Markov Chain Monte Carlo, 815 Markov chain Monte Carlo, 52, 600, 837\n",
      "\n",
      "Markov decision process, 331 Markov equivalence, 936 Markov equivalent, 915, 917 Markov logic network, 675 Markov mesh, 661 Markov model, 589 Markov models, 32 Markov network, 661 Markov random ﬁeld, 661 Markov switching models, 604 MARS, 538, 553, 562 MART, 562 master, 810 matching pursuit, 562 matching pursuits, 428 Matern kernel, 482 MATLAB, xxviii matrix completion, 14, 939 matrix determinant lemma, 118 matrix factorization, 948 matrix inversion lemma, 118, 144, 641 matrix permanent, 669 matrix tree theorem, 914 max ﬂow/min cut, 801 max margin Markov networks, 693 max pooling, 1005 max product linear programming, 810 max-product, 614, 713 max-product belief propagation, 800 maxent, 289 maximal branching, 913 maximal clique, 310 maximal information coefficient, 60 maximal weight bipartite matching, 659 maximizer of the posterior marginals, 612 maximum a posteriori, 4 maximum entropy, 39, 104, 289, 667 maximum entropy classiﬁer, 252 maximum entropy Markov model, 685 maximum expected utility principle, 177 maximum likelihood estimate, 69 maximum risk, 196 maximum weight spanning tree, 912 MCAR, 270 MCEM, 368 MCMC, 52, 596, 600, 815, 837 MDL, 162 MDP, 331 MDS, 496 mean, 33 mean absolute deviation, 511 mean average precision, 303 mean ﬁeld, 735, 756, 767, 989 mean ﬁeld energy functional, 779 mean function, 291 mean precision, 182 mean reciprocal rank, 303 mean squared error, 205, 218 Mechanical Turk, 10, 995 median, 33 median model, 423 MEMM, 685 memory-based learning, 17 Mendelian inheritance, 317 Mercer kernel, 481 Mercer’s theorem, 481, 539 message passing, 644, 800 metric, 691, 691, 803 metric CRF, 691 metric MRF, 803\n",
      "\n",
      "1060\n",
      "\n",
      "INDEXES\n",
      "\n",
      "Metropolis Hastings, 848, 922 Metropolis-Hastings algorithm, 869 MFCC, 1005 MH, 848 MI, 59 micro-averaged F1, 183 Microsoft, 983 mini-batch, 264, 571 minimal, 282 minimal I-map, 324 minimax rule, 196 minimum description length, 162 minimum entropy prior, 621 minimum mean squared error, 179 minimum spanning tree, 897 minorize-maximize, 369 misclassiﬁcation loss, 176 Misclassiﬁcation rate, 547 misclassiﬁcation rate, 22, 205 missed detection, 180 missing, 15 missing at random, 270, 372, 982 missing completely at random, 270 missing data, 14, 914, 974 missing data problem, 269 mixed directed graphs, 931 mixed membership model, 950 mixed membership stochastic block model, 973 mixed model, 298 mixing matrix, 408 mixing time, 857 mixing weights, 169, 338 mixture, 72 mixture density network, 344 mixture model, 164, 338 mixture of conjugate priors, 169 mixture of experts, 342, 563, 973, 984 mixture of factor analysers, 386 mixture of Gaussians, 339 mixture of Kalman ﬁlters, 831 mixture of trees, 914 mixture proposal, 853 MLE, 69 MLP, 563 MM, 369 MMSE, 179 MNIST, 7, 341 Mobious numbers, 784 mode, 4 model based clustering, 11 model selection, 10, 24, 156 model selection consistent, 439 model-based approach, xxvii model-based clustering, 879 moderated output, 260 modularity, xxviii MoE, 342 moment matching, 176, 287, 653, 658, 677 moment parameters, 115 moment projection, 733 momentum, 248 monks, 974 Monte Carlo, 52, 151, 192, 258, 815 Monte Carlo EM, 368 Monte Carlo integration, 53 Monte Carlo localization, 828 moralization, 663, 715 motes, 218 motif, 36 mPCA, 948\n",
      "\n",
      "MPE, 614 MPM, 612 MRF, 661 MSE, 218 multi label classiﬁcation, 970 multi net, 627 multi-armed bandit, 184 multi-class logistic regression, 104 multi-clust, 904 multi-grid techniques, 775 multi-information, 415 multi-label classiﬁcation, 3, 405 multi-layer perceptron, 563, 999 multi-level model, 171 multi-level modeling, 844 multi-stage, 186 multi-target tracking, 659 multi-task feature selection, 297 multi-task learning, 172, 231, 296, 449, 757 multiclass classiﬁcation, 3 multidimensional scaling, 496 multinomial, 35 multinomial coefficient, 35 multinomial logistic regression, 104, 252 multinomial PCA, 948, 951 multinomial probit, 295 multinomial regression LDA, 968 multinomial resampling, 826 multinoulli distribution, 35 multiple hypothesis testing, 184 multiple hypothesis tracking, 656 multiple imputation, 115 multiple kernel learning, 524, 543 multiple LDA, 276 multiple output model, 3 multiple random restarts, 348, 921 multiple restarts, 620 multivariate adaptive regression splines, 553 multivariate Bernoulli naive Bayes, 82 multivariate delta method, 763 multivariate Gamma function, 133 multivariate gamma function, 126 multivariate Gaussian, 46, 97, 339 multivariate normal, 46, 97 multivariate probit, 295 multivariate Student t, 46 mutual information, 46, 59, 87, 547, 912 mutual inhibition, 564 mutually independent, 62 MVN, 46, 97\n",
      "\n",
      "N-best list, 616 n-gram, 568 n-gram models, 591 Nadaraya-Watson, 511 naive Bayes classiﬁer, 82, 88, 311 naive Bayes classiﬁers, 32 named entity extraction, 688 NaN, 14 nats, 56 natural exponential family, 282 natural gradient, 411 natural parameters, 115, 282 NDCG, 304 nearest centroids classiﬁer, 102 nearest medoid classiﬁcation, 491 nearest neighbor, 16 nearest neighbor clustering, 897 nearest neighbor data association, 658 nearest shrunken centroids, 109\n",
      "\n",
      "INDEX TO KEYWORDS\n",
      "\n",
      "1061\n",
      "\n",
      "negative binomial, 624 negative binomial distribution, 214 negative examples, 65 negative log likelihood, 218, 349 negative transfer, 297 negentropy, 415 neighbors, 309 neocognitron, 566 nested plate, 321 Nesterov’s method, 446 Netﬂix, 15, 580, 979, 981, 987, 993 NETtalk, 569 neural network, 302, 969 neural networks, 344, 535 neutral process, 882 Newton’s algorithm, 249, 251 NHST, 213 NIW, 133 NIX, 136 NLL, 218, 349 NMAR, 270 NMF, 470, 949 no forgetting, 331 no free lunch theorem, 24, 582 nodes, 309 nodes that ﬁre together should wire together, 929 noise ﬂoor, 230 noisy-OR, 313, 928 nominal, 2 non-descendants, 327 non-factorial, 466 non-informative, 165 non-negative matrix factorization, 470, 949 non-negative sparse coding, 470 non-null recurrent, 599 non-parametric Bayes, 879 non-parametric bootstrap, 192 non-parametric BP, 712 non-parametric model, 16 non-parametric prior, 879 non-serial dynamic programming, 717 non-smooth, 432 non-terminals, 689 nonparanormal, 942 norm of a function, 539 normal, 20, 38 normal equation, 220 normal Gamma, 476 normal inverse chi-squared, 136 Normal-inverse-wishart, 133 normalized cut, 891 normalized discounted cumulative gain, 304 normalized mutual information, 879 not missing at random, 270 noun phrase chunking, 687 NP-complete, 920 NP-hard, 726 ν-SVM classiﬁer, 502 nuisance variables, 320 null hypothesis, 163, 213 null hypothesis signiﬁcance testing, 213 number game, 65 numerical underﬂow, 86\n",
      "\n",
      "object detection, 8 object localization, 8 observation, 603 observation model, 312, 631 observed data log likelihood, 348 observed information, 167\n",
      "\n",
      "observed information matrix, 193 Occam factor, 255 Occam’s razor, 67, 156, 399, 400 occasionally dishonest casino, 606 occupancy grid, 828 Octave, xxviii offline, 261 oil wild-catter, 328 OLS, 220 OMP, 428 one-armed bandit, 184 one-hot encoding, 35 one-of-C encoding, 252 one-shot decision problem, 186 one-standard error rule, 208 one-step-ahead predictive density, 609 one-versus-one, 503 one-versus-the-rest, 503 one-vs-all, 503 online EM, 365 online gradient descent, 262 online learning, 75, 241, 261 ontological uncertainty, 973 ontology, 977 open class, 596, 688 Open Directory Project, 600, 689 open universe, 676 optimal action, 177 optimism of the training error, 206 optimization, 218 ordered Markov property, 310, 327 ordinal, 295 ordinal regression, 2, 295, 301 ordinal variables, 876 ordinary least squares, 220 Ornstein-Uhlenbeck process, 483 orthodox statistics, 191 orthogonal least squares, 427 orthogonal matching pursuits, 428 orthogonal projection, 221 out-degree, 310 out-of-clique query, 722 outer approximation, 780 outliers, 179, 223 over-complete, 282, 1001 overcomplete, 469 overcounting number, 784 overdispersed, 859 overﬁt, 22 overﬁtting, 72 overrelaxed EM algorithm, 369\n",
      "\n",
      "p-value, 138, 163, 163, 213 PAC, 210 PageRank, 301, 596, 600, 601 paired t-test, 137 pairwise independent, 62 pairwise Markov property, 662 pairwise MRF, 666 parallel tempering, 858, 871, 922 parameter, 176 parameter expansion, 736 parameter modularity, 918 parameter sharing, 107 parameter tying, 107, 171, 589 parametric bootstrap, 192 parametric model, 16, 19 parents, 309, 310 Pareto distribution, 43 part of speech, 605, 966\n",
      "\n",
      "1062\n",
      "\n",
      "INDEXES\n",
      "\n",
      "Part of speech tagging, 605 partial dependence plot, 586 partial least squares, 406, 975 partially directed acyclic graph, 915 partially labeled LDA, 969 partially observed Markov decision process, 331 partially observed MRF, 672 Particle ﬁltering, 823 particle ﬁltering, 267, 648, 823, 887 partition function, 282, 666 partitional clustering, 875 partitioned inverse formula, 116 partitioning, 841 partitions of the integers, 885 Parzen window density estimator, 508 passing a ﬂow, 724 path, 310 path diagrams, 929 pathologies, 211 pattern, 915 pattern completion, 669 pattern recognition, 2 pattern search, 736, 783 PCA, 12, 387, 493, 947 PCFG, 689 PDAG, 936 pdf, 32 pedigree graph, 315 peeling algorithm, 715 Pegasos, 701 penalized least squares, 226 penalized log likelihood, 161 penalized splines, 537 penetrance model, 317 perception-action, 331 perceptron, 569 perceptron algorithm, 266 perceptual aliasing, 828 perfect intervention, 931 perfect map, 664 period, 598 permanent, 942 perplexity, 953, 953, 992 persistent CD, 991 persistent contrastive divergence, 680 personalized recommendation, 77 personalized spam ﬁltering, 296 perturbation theory, 892 phase, 317 phase transition, 671, 857 phenotypes, 317 phone, 624 phonemes, 1005 phylogenetic HMM, 317 phylogenetic tree, 925 piecewise polynomial, 537 pilot runs, 851 pipeline, 687 Pitman-Koopman-Darmois theorem, 286 Pitman-Yor process, 885 Plackett-Luce, 302 plates, 321 platykurtic, 413 PLS, 406 PLSI, 949 plug-in, 147 plug-in approximation, 72 plutocracies, 43 pmf, 28 PMTK, xxviii\n",
      "\n",
      "point estimate, 149, 150 pointwise approach, 301 pointwise marginal credibility intervals, 114 pointwise mutual information, 59 Poisson, 37 poisson regression, 292 polar, 51 policy, 177 Polya urn, 89, 884 Polyak-Ruppert averaging, 263 polynomial kernel, 481 polynomial regression, 20 polynomial time approximation schemes, 728 polysemy, 951 polytree, 310 POMDP, 331 pooled, 171 pooled empirical variance, 108 population minimizer, 556 positive deﬁnite, 125, 222 positive deﬁnite kernel, 481 positive examples, 65 posterior expected loss, 177 posterior mean, 179 posterior median, 179 posterior mode, 178 posterior predictive density, 608 posterior predictive distribution, 66, 71, 234 potential function, 665 Potts model, 671, 856 power law, 43 power method, 603 PPCA, 381, 387 precision, 38, 182 precision at k, 303, 702 precision matrix, 46, 100 precision recall curve, 182 predict-update cycle, 609 predict-update-project, 653 predictive, 2 preferences, 185 preposterior risk, 195 prevalence, 183 Prim’s algorithm, 912 primal variables, 492, 499 principal component, 388 principal components, 1000 principal components analysis, 12, 387 principal components regression, 230 principle of insufficient reason, 58 probabilistic decision tree, 551 probabilistic expert system, 313 probabilistic inference, 319 probabilistic latent semantic indexing, 949 probabilistic matrix factorization, 337, 980 probabilistic PCA, 387 probabilistic principal components analysis, 381 probabilistic relational modeling, 675, 976 probability density function, 32 probability mass function, 28 probability of the evidence, 319, 609, 717 probability product kernel, 485 probability simplex, 47, 79 probability theory, xxvii, 1 probably approximately correct, 210 probe, 583 probit, 260, 655 probit regression, 293, 362, 380, 795, 864 product of experts, 983 product rule, 29\n",
      "\n",
      "INDEX TO KEYWORDS\n",
      "\n",
      "1063\n",
      "\n",
      "production rules, 689 proﬁle HMM, 606 proﬁle log likelihood, 401 projected gradient descent, 444, 445 projection, 262 projection pursuit, 415 Prolog, 676 proposal distribution, 817, 848, 869 propose, 848 prosecutor’s fallacy, 61 Protein sequence alignment, 606 protein-protein interaction networks, 970 prototype, 341 proximal operator, 443 pruning, 549 pseudo counts, 75 pseudo likelihood, 678 pseudo marginals, 780 pseudo random number generator, 816 pseudo-likelihood, 943 pure, 546, 548 purity, 877 pushing sums inside products, 715 pyramid match kernel, 484\n",
      "\n",
      "QALY, 186 QMR, 313 QP, 431 qq-plot, 260 QR decomposition, 228 quadratic discriminant analysis, 102 quadratic loss, 179 quadratic program, 431, 498, 499 quantile, 33 quantize, 59 quartiles, 33 Quasi-Newton, 251 query logs, 301 query variables, 320 quick medical reference, 313\n",
      "\n",
      "radar, 658 radial basis function, 480 Rand index, 878 random accelerations model, 633 random effects, 298 random effects mixture of experts, 969 random forests, 551, 554 random probability measure, 880 random utility model, 294 random walk Metropolis algorithm, 848 random walk on the integers, 599 random walk proposal, 869 Rank correlation, 304 rank one update, 118 ranking, 87, 601, 702 RankNet, 302 Rao-Blackwell, 841 Rao-Blackwellisation, 841 Rao-Blackwellised particle ﬁltering, 831 Rao-Blackwellized particle ﬁltering, 659 rare event, 182, 820 rate, 355 rational behavior, 177 RBF, 480 RBF kernel, 517 RBF network, 486 RBM, 983, 996 RBPF, 831 real AdaBoost, 559\n",
      "\n",
      "recall, 181, 182 receiver operating characteristic, 181 receptive ﬁelds, 565 recognition weights, 410, 986 recombination model, 317 reconstruction error, 354, 387 recurrent, 599 recurrent neural network, 568, 669 recurrent neural networks, 591 recursive, 929 recursive least squares, 265, 636 reﬂecting pair, 553 regime switching, 660 regime switching Markov model, 626 regression, 2 regression spline, 537 regret, 262 regular, 598 regularization, 227 regularization path, 436, 442, 562 regularized discriminant analysis, 107 regularized estimation, 130 regularized particle ﬁlter, 827 regularized risk minimization, 206 reinforcement learning, 2, 186 reject action, 178 rejection sampling, 817 rejuvenation, 825 relation, 975 relational probabilistic models, 676 relational topic model, 974 relative entropy, 57 relative importance of predictor variables, 586 relative risk, 531 relevance network, 908 relevance vector machine, 463, 488 Rephil, 928 replicated softmax model, 992 representer theorem, 539 reproducing kernel Hilbert space, 539 reproducing property, 539 rerank, 616 resample-move, 827 residual, 641 residual analysis, 260 residual belief propagation, 774 residual error, 19 residual resampling, 826 residual sum of squares, 218 response variable, 2 responsibility, 340, 351 restricted Boltzmann machine, 983 reverse KL, 733 reversible jump MCMC, 370, 399, 855 reward, 2 Ricatti equations, 642 rich get richer, 755, 885 ridge regression, 203, 226 right censored, 379 risk, 195, 261 risk averse, 4, 178 RJMCMC, 855 RKHS, 539 RLS, 636 Robbins-Monro, 263, 366, 701 robust, 179 robust priors, 168 robustness, 223 ROC, 181 rocking, 261\n",
      "\n",
      "1064\n",
      "\n",
      "INDEXES\n",
      "\n",
      "root, 309, 895 root mean square error, 979 Rosenblatt, 266 rotamers, 690 RTS smoother, 644 rule of iterated expectation, 141 rule of total probability, 29 rules, 550 RUM, 294 running intersection property, 722 RVM, 488, 505\n",
      "\n",
      "saddle point approximation, 255 sample impoverishment, 826 sample standard deviation, 136 samples, 52 sampling distribution, 191, 191 sampling importance resampling, 823 sampling period, 633 satisfying assignment, 727 saturated model, 428 SBL, 463 scalar product, 19 scale invariant prior, 168 scale of evidence, 163 scatter plot, 6 SCFGs, 624 schedule, 263 Schur complement, 116 scientiﬁc method, 71 scope, 328 score function, 167, 193 score matching, 1001 score vector, 485 scores, 382 scree plot, 400 screening, 87 search engine optimization, 603 second order, 249 second order Markov chain, 312 second-order Markov model, 591 self loops, 309 semantic hashing, 1003 semantic network, 977 semantic role labeling, 576 semi-conjugate, 132 semi-continuous HMM, 630 semi-Markov model, 622 semi-metric, 691 semi-parametric model, 298, 524 semi-supervised, 405 semi-supervised embedding, 576 semi-supervised learning, 268, 270 sensible PCA, 387 sensitivity, 29, 181 sensitivity analysis, 166 sensor fusion, 122 sentiment analysis, 967 separating set, 723 separation oracle, 699 sequence logo, 36 sequential, 186 sequential minimal optimization, 499 sequential TRBP, 801 SGD, 262 Shafer-Shenoy, 722 shallow parsing, 687 shared, 103 Sherman-Morrison-Woodbury formula, 118 shooting, 441, 940\n",
      "\n",
      "shrinkage, 122, 174, 230, 557 shrinkage estimation, 130 shrinkage factor, 437 side chains, 690 side information, 982 SIFT, 484 sifting property, 39 sigma points, 650, 651 sigmoid, 21, 105 sigmoid belief net, 313, 996 sigmoid belief nets, 763 sigmoid kernel, 482 signal detection theory, 106 signal processing, 421 signal-to-noise ratio, 122 signal-to-symbol, 1007 similar, 66, 875 similarity-based clustering, 875 simple cells, 413 Simple linear regression, 241 simplex factor model, 949 Simpon’s paradox, 933 Simulated annealing, 869 simulated annealing, 262, 348, 853, 921 simulation based, 823 simultaneous localization and mapping, 635 single best replacement, 427 single link clustering, 897 single site updating, 847 singular value decomposition, 392 singular values, 392 SIR, 823 size principle, 67 skewness, 413 skip arcs, 568 skip-chain CRF, 688 slack re-scaling, 696 slack variables, 498 SLAM, 635, 834 slaves, 810 slice sampling, 865 sliding window detector, 8 slippage, 635 slot machine, 184 small N , large D, 421 SmartASS, 4 SML, 680 SMO, 499 Smoothing, 607 smoothing kernel, 507, 507 Smoothing splines, 536 social networks, 970 soft clustering, 340, 973 soft margin constraints, 501 soft thresholding, 434, 435 soft weight sharing, 575 softmax, 104, 283 source coding, 56 SpAM, 553 spam, 5 spanning tree polytope, 786 SpaRSA, 445 sparse, 15, 421, 621, 945, 979 sparse Bayesian learning, 463 sparse boosting, 562 sparse coding, 469 sparse data problem, 77 sparse kernel machine, 421 sparse matrix factorization, 469, 470 sparse PCA, 469\n",
      "\n",
      "INDEX TO KEYWORDS\n",
      "\n",
      "1065\n",
      "\n",
      "sparse representation, 421 sparse vector machine, 488 sparsity, 41 sparsity-promoting prior, 297 spectral, 445 spectral clustering, 891 spectral graph theory, 891 speech recognition, 590, 1005 sphereing, 142 spherical, 46 spike and slab, 424 spin, 668 spline, 298 split merge, 621 split variable, 224 square root ﬁlter, 642 squared error, 179 squared exponential kernel, 480, 517 squared loss, 176 squashing function, 21 SSM, 631 SSVMs, 693 stability selection, 439 stable, 936 stacked denoising auto-encoder, 1001 stacking, 580 standard deviation, 34 standard error, 56 standard error of the mean, 137, 208 standard errors, 194 standard model, 995 standard normal, 38 standard overcomplete representation, 776 standardized, 352 Standardizing, 142 state, 176 state estimation, 313 state space, 28 state space model, 631 state transition diagram, 590, 606 state transition matrix, 308 stationary, 589, 631 stationary distribution, 596, 597 statistical learning theory, 209 statistical relational AI, 675 statistical relational learning, 976 statistically signiﬁcant, 213 steepest descent, 247, 264 Stein’s paradox, 199 stemming, 81 step size, 247 stepping out, 866 stepwise EM, 365 stick-breaking construction, 883 sticky, 850 stochastic algorithm, 869 stochastic approximation, 368 stochastic approximation EM, 368 stochastic automaton, 590 stochastic block model, 972 stochastic context free grammars, 624 stochastic EM, 368 stochastic gradient boosting, 584 stochastic gradient descent, 262, 570, 868, 981, 987 stochastic matrix, 307, 589 stochastic maximum likelihood, 680, 990 stochastic optimization, 262 stochastic process, 953 stochastic processes, 589 stochastic search, 429\n",
      "\n",
      "stochastic volatility, 831 stop words, 81, 480, 952 stopping rule, 214 stratiﬁed CV, 206 stratiﬁed sampling, 826 streaming data, 261 StreetView, 8 strict, 197 strictly convex, 222 string kernel, 483 strong local optimum, 804 strong sampling assumption, 67 structural EM, 925 structural equation model, 929 structural equation models, 674 structural error, 230 structural risk minimization, 206 structural signatures, 926 structural support vector machines, 693 structural time series, 637 structural zeros, 672 structure learning, 621, 681 structured mean ﬁeld, 740 structured output, 684 structured perceptron algorithm, 700 structured-output classiﬁcation problems, 266 Student t, 359 Student t distribution, 39 sub-Gaussian, 413 subderivative, 432 subdifferential, 432 subgradient, 432, 432 subgraph, 310 subjective, 67 subjective probability, 310 submodular, 802 subsampling, 566 subspace method, 647 sufficiency principle, 214 sufficient statistics, 74, 79, 281, 282, 348 suffix trees, 483 sum of squared errors, 218 sum of squares, 220 sum rule, 29 sum-product, 614, 709 sum-product algorithm, 707 super efficient, 820 super-Gaussian, 413 supermodular, 802 supervised LDA, 967 supervised learning, 2 supervised PCA, 405 support, 426 support vector machine, 488, 496, 569 support vector machines, 211 support vectors, 496, 498, 499 surrogate loss, 304 surrogate loss function, 211 surrogate splits, 550 survival of the ﬁttest, 825 suspicious coincidence, 164 suspicious coincidences, 67 SVD, 107, 228, 392, 980 SVM, 211, 488, 496 SVMstruct, 698, 700 Swendsen Wang, 866 switching linear dynamical system, 655, 831 switching state space model, 655 symbol grounding, 1007 symmetric, 849\n",
      "\n",
      "1066\n",
      "\n",
      "INDEXES\n",
      "\n",
      "synchronous updates, 773 syntactic sugar, 321 synthesis view, 387 systematic resampling, 826 systems biology, 13 systems identiﬁcation, 646 systolic array, 710\n",
      "\n",
      "t statistic, 137 t-test, 137 tabula rasa, 165 tail area probabilities, 33 tail area probability, 213 TAN, 312, 914 TASA, 951 Taylor series, 255 Taylor series expansion, 648 Taylor’s theorem, 248 temperature, 104 template, 676 template matching, 543 tensor product, 553 tensor product basis, 538 terminals, 689 test statistic, 163, 213 TF-IDF, 480 thin junction tree ﬁlter, 635 thin junction trees, 944 thin plate spline, 538 thin SVD, 392 thinning, 862 Thompson sampling, 185 tied, 103, 565, 997 tied-mixture HMM, 630 Tikhonov regularization, 124 time reversible, 599 time-invariant, 589 time-series forecasting, 637, 673 Tobit model, 379 Toeplitz, 627 tokens, 945 topic, 946, 951 topic model, 757 topological ordering, 310, 310 total ordering, 310 trace, 99 trace plot, 859 trace trick, 99 traceback, 614, 717 tracking, 823 tracking by detection, 830 tractable substructure, 739 trail, 310 training set, 2 trans-dimensional MCMC, 855 transfer function, 563, 570 transfer learning, 296 transient, 599 transition matrix, 589, 590 transition model, 312, 631 translation invariance, 565, 1004 translation invariant, 472 translation invariant prior, 167 TRBP, 787 TRBP-S, 801 tree, 310 tree EP, 793 tree reparameterization, 774 tree reweighted belief propagation, 786 tree-augmented naive Bayes classiﬁer, 312\n",
      "\n",
      "treewidth, 320, 719, 800 trellis, 614 trellis diagram, 612 tri-cube kernel, 508 triangle inequality, 352, 875 triangulated, 722 tridiagonal, 114 trigram model, 591 true positive rate, 181 TrueSkill, 654, 793 truncated Gaussian, 362 truncated Gaussian potential, 691 truncated Newton, 250 truncated SVD, 393 TRW, 787 TRW-S, 801 tube, 497 tuples, 975 turbo codes, 768 two-ﬁlter smoothing, 646 two-slice marginal, 611 type I, 213 type I error rate, 181 type II maximum likelihood, 157 type-II maximum likelihood, 173\n",
      "\n",
      "U-shaped curve, 23 UCB, 185 UGM, 661 UKF, 650 unbiased, 200 uncertainty, 27 unclamped phase, 988 unclamped term, 677 unconditionally independent, 30 underﬁts, 23 undirected, 309 undirected graphical model, 661 undirected local Markov property, 662 unfaithful, 663 unidentiﬁable, 200, 278, 841 Uniﬁed Medical Language System, 977 uniform distribution, 32 unigram statistics, 591 unigrams, 953 uninformative, 165 union bound, 209 unit information prior, 236 universal approximator, 564 unk, 81, 596 unknown, 15, 81 unrolled, 321 unscented Kalman ﬁlter, 523, 650 unscented particle ﬁlter, 828 unscented transform, 650 unstable, 550 unsupervised learning, 2, 9, 337 up-down, 998 user rating proﬁle, 949 utilities, 294 utility function, 177 utility nodes, 328\n",
      "\n",
      "v-structure, 324, 326 validation set, 23 value nodes, 328 value of perfect information, 331 vanishing gradient, 999 Vapnik-Chervonenkis, 210 VAR, 673\n",
      "\n",
      "INDEX TO KEYWORDS\n",
      "\n",
      "1067\n",
      "\n",
      "variable duration HMM, 622 variable elimination, 318, 331, 715 variance, 33 variance stabilizing transform, 175 variation of information, 879 variational Bayes, 742 variational Bayes EM, 620, 750, 923 variational EM, 368 variational free energy, 733 variational inference, 281, 318, 731 variational message passing, 756 varimax, 385, 410 VB, 742 VBEM, 750 VC, 210 VC dimension, 206 vector auto-regressive, 673 vector quantization, 354 version space, 67 vertices, 309 VIBES, 756 views, 904 visible, 349 visible nodes, 313 visible variables, 319 visual words, 1007 visualizing, 12 Viterbi, 612, 701 Viterbi decoding, 608 Viterbi training, 620 VMP, 756 Voronoi tessellation, 18 VQ, 354\n",
      "\n",
      "Zipf’s law, 43\n",
      "\n",
      "Wald, 448 Wald interval, 212 warm starting, 442 WARP, 304 Watson, 4 wavelet, 469 wavelet transforms, 413 weak conditionality, 215 weak learner, 554 weak marginalization, 658 web crawling, 600 web spam, 603 weight decay, 226, 572, 987 weight function, 533 weight vector, 19 weighted approximate-rank pairwise, 304 weighted average, 71 weighted least squares, 358 weighted least squares problem, 251 Whitening, 142 whitening, 410 Widrow-Hoff rule, 265 Wishart, 125 working response, 250 World Health Organization, 60 wrapper method, 427\n",
      "\n",
      "Xbox, 654, 795 xor, 486\n",
      "\n",
      "Zellner’s g-prior, 405 zero avoiding, 733 zero count problem, 77 zero forcing, 733 zero temperature limit, 800 zig-zag, 248\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from filetype import guess\n",
    "\n",
    "def detect_document_type(document_path):\n",
    "    \n",
    "    guess_file = guess(document_path)\n",
    "    file_type = \"\"\n",
    "    image_types = ['jpg', 'jpeg', 'png', 'gif']\n",
    "    \n",
    "    if(guess_file.extension.lower() == \"pdf\"):\n",
    "        file_type = \"pdf\"\n",
    "        \n",
    "    elif(guess_file.extension.lower() in image_types):\n",
    "        file_type = \"image\"\n",
    "        \n",
    "    else:\n",
    "        file_type = \"unkown\"\n",
    "        \n",
    "    return file_type\n",
    "research_paper_path = \"Model/Snap_ai/ML.pdf\"\n",
    "print(f\"Research Paper Type: {detect_document_type(research_paper_path)}\")\n",
    "from langchain.document_loaders.image import UnstructuredImageLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "\"\"\"\n",
    "YOU CAN UNCOMMENT THE CODE BELOW TO UNDERSTAND THE LOGIC OF THE FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \n",
    "    loader = UnstructuredFileLoader(pdf_file)\n",
    "    documents = loader.load()\n",
    "    pdf_pages_content = '\\n'.join(doc.page_content for doc in documents)\n",
    "    \n",
    "    return pdf_pages_content\n",
    "\n",
    "def extract_file_content(file_path):\n",
    "    \n",
    "    file_type = detect_document_type(file_path)\n",
    "    \n",
    "    if(file_type == \"pdf\"):\n",
    "        loader = UnstructuredFileLoader(file_path)\n",
    "        \n",
    "    elif(file_type == \"image\"):\n",
    "        loader = UnstructuredImageLoader(file_path)\n",
    "        \n",
    "    documents = loader.load()\n",
    "    documents_content = '\\n'.join(doc.page_content for doc in documents)\n",
    "    \n",
    "    return documents_content\n",
    "\n",
    "#research_paper_content = extract_text_from_pdf(research_paper_path)\n",
    "#article_information_content = extract_text_from_image(article_information_path)\n",
    "research_paper_content = extract_file_content(research_paper_path)\n",
    "print(research_paper_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File ORGANISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved '.DS_Store' to 'ds_store' directory\n",
      "Moved 'Integration.py' to 'py' directory\n",
      "Moved '.env' to 'env' directory\n",
      "Moved 'requirments.txt' to 'txt' directory\n",
      "Moved 'test.ipynb' to 'ipynb' directory\n",
      "Moved 'Discord_server.py' to 'py' directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Function to create a directory if it doesn't exist\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Function to organize files by extension\n",
    "def organize_files_by_extension(source_dir):\n",
    "    # List all files in the source directory\n",
    "    files = os.listdir(source_dir)\n",
    "\n",
    "    # Create a dictionary to store file extensions and their corresponding directories\n",
    "    extension_dirs = {}\n",
    "\n",
    "    for filename in files:\n",
    "        if os.path.isfile(os.path.join(source_dir, filename)):\n",
    "            # Get the file extension\n",
    "            file_extension = filename.split('.')[-1].lower()\n",
    "\n",
    "            # Define a directory based on the file extension\n",
    "            target_dir = os.path.join(source_dir, file_extension)\n",
    "\n",
    "            # Create the directory if it doesn't exist in the source directory\n",
    "            create_directory(target_dir)\n",
    "\n",
    "            # Move the file to the corresponding directory\n",
    "            source_file = os.path.join(source_dir, filename)\n",
    "            target_file = os.path.join(target_dir, filename)\n",
    "\n",
    "            # Check if the file already exists in the target directory\n",
    "            if not os.path.exists(target_file):\n",
    "                shutil.move(source_file, target_file)\n",
    "                print(f\"Moved '{filename}' to '{file_extension}' directory\")\n",
    "\n",
    "# Specify the source directory where the files are located\n",
    "source_directory = './'\n",
    "\n",
    "# Call the function to organize files by extension\n",
    "organize_files_by_extension(source_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved '.DS_Store' to '2023-10-03/.DS_Store'\n",
      "Moved 'doc.pdf' to '2023-10-03/doc'\n",
      "Moved 'a.py' to '2023-10-03/a'\n",
      "Moved '__init__.py' to '2023-10-01/__init__'\n",
      "Moved 'Role.py' to '2023-10-02/Role'\n",
      "Moved 'Bot.py' to '2023-10-03/Bot'\n",
      "Moved 'ML.pdf' to '2023-10-03/ML'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "# Function to create a directory if it doesn't exist\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Function to organize files by creation date and file name\n",
    "def organize_files_by_creation_date(source_dir):\n",
    "    files = os.listdir(source_dir)\n",
    "\n",
    "    for filename in files:\n",
    "        if os.path.isfile(os.path.join(source_dir, filename)):\n",
    "            creation_date = datetime.datetime.fromtimestamp(os.path.getctime(os.path.join(source_dir, filename)))\n",
    "            date_str = creation_date.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            # Use the original file name (without extension) as the subdirectory name\n",
    "            subdirectory_name = os.path.splitext(filename)[0]\n",
    "\n",
    "            date_dir = os.path.join(source_dir, date_str)\n",
    "            subdirectory_path = os.path.join(date_dir, subdirectory_name)\n",
    "\n",
    "            create_directory(date_dir)\n",
    "\n",
    "            # Move the file to the final directory\n",
    "            source_file = os.path.join(source_dir, filename)\n",
    "            target_file = os.path.join(subdirectory_path, filename)\n",
    "\n",
    "            # Check if the file already exists in the target directory\n",
    "            if not os.path.exists(target_file):\n",
    "                create_directory(subdirectory_path)\n",
    "                shutil.move(source_file, target_file)\n",
    "                print(f\"Moved '{filename}' to '{date_str}/{subdirectory_name}'\")\n",
    "\n",
    "# Specify the source directory where the files are located\n",
    "source_directory = 'Model/Snap_ai'\n",
    "\n",
    "# Call the function to organize files by creation date and file name\n",
    "organize_files_by_creation_date(source_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for the given keywords.\n",
      "No search results found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Function to search for items on the Internet Archive\n",
    "def search_items(keywords):\n",
    "    base_url = \"https://archive.org\"\n",
    "    search_url = f\"{base_url}/search.php?query={'+'.join(keywords)}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            search_results = soup.find_all(\"div\", class_=\"item-ia\")\n",
    "\n",
    "            if not search_results:\n",
    "                print(\"No items found for the given keywords.\")\n",
    "                return []\n",
    "\n",
    "            items = []\n",
    "\n",
    "            for i, result in enumerate(search_results, start=1):\n",
    "                title_element = result.find(\"div\", class_=\"ttl\")\n",
    "                creator_element = result.find(\"div\", class_=\"creator\")\n",
    "\n",
    "                title = title_element.text.strip()\n",
    "                creator = creator_element.text.strip() if creator_element else \"Unknown\"\n",
    "                identifier = result.find(\"div\", class_=\"item-ttl\").find(\"a\")[\"href\"].split(\"/\")[-1]\n",
    "\n",
    "                items.append((title, creator, identifier))\n",
    "\n",
    "                print(f\"{i}. Title: {title}\")\n",
    "                print(f\"   Creator: {creator}\")\n",
    "                print(f\"   Identifier: {identifier}\\n\")\n",
    "\n",
    "            return items\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: Unable to fetch search results. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to download an item from the Internet Archive\n",
    "def download_item(item_identifier, output_directory):\n",
    "    base_url = \"https://archive.org/download\"\n",
    "    item_url = f\"{base_url}/{item_identifier}/{item_identifier}.zip\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(item_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            # Determine the file name from the URL\n",
    "            file_name = f\"{item_identifier}.zip\"\n",
    "            file_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "            # Create the 'Document' directory if it doesn't exist\n",
    "            if not os.path.exists(output_directory):\n",
    "                os.makedirs(output_directory)\n",
    "\n",
    "            # Save the file to the 'Document' directory\n",
    "            with open(file_path, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "\n",
    "            print(f\"Downloaded: {file_name}\")\n",
    "        else:\n",
    "            print(f\"Error: Unable to download item. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    keywords = input(\"Enter keywords to search for items (e.g., 'science fiction books'): \")\n",
    "    output_directory = \"Document\"  # Default output directory\n",
    "\n",
    "    items = search_items(keywords)\n",
    "\n",
    "    if items:\n",
    "        choice = input(\"Enter the index of the item you want to download (e.g., '1'): \")\n",
    "        try:\n",
    "            choice_index = int(choice) - 1\n",
    "            if 0 <= choice_index < len(items):\n",
    "                item_title, _, item_identifier = items[choice_index]\n",
    "                download_item(item_identifier, output_directory)\n",
    "            else:\n",
    "                print(\"Invalid choice.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid index.\")\n",
    "    else:\n",
    "        print(\"No search results found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfiletype\u001b[39;00m \u001b[39mimport\u001b[39;00m guess\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummarization\u001b[39;00m \u001b[39mimport\u001b[39;00m summarize\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlanguage_tool_python\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from filetype import guess\n",
    "from gensim.summarization import summarize\n",
    "import re\n",
    "import language_tool_python\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove extra whitespaces and newline characters\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def detect_document_type(document_path):\n",
    "    guess_file = guess(document_path)\n",
    "    file_type = \"\"\n",
    "    image_types = ['jpg', 'jpeg', 'png', 'gif']\n",
    "    \n",
    "    if guess_file.extension.lower() == \"pdf\":\n",
    "        file_type = \"pdf\"\n",
    "    elif guess_file.extension.lower() in image_types:\n",
    "        file_type = \"image\"\n",
    "    else:\n",
    "        file_type = \"unknown\"\n",
    "    \n",
    "    return file_type\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    loader = UnstructuredFileLoader(pdf_file)\n",
    "    documents = loader.load()\n",
    "    pdf_pages_content = '\\n'.join(doc.page_content for doc in documents)\n",
    "    return pdf_pages_content\n",
    "\n",
    "def extract_file_content(file_path):\n",
    "    file_type = detect_document_type(file_path)\n",
    "    \n",
    "    if file_type == \"pdf\":\n",
    "        loader = UnstructuredFileLoader(file_path)\n",
    "    elif file_type == \"image\":\n",
    "        loader = UnstructuredImageLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    documents_content = '\\n'.join(doc.page_content for doc in documents)\n",
    "    \n",
    "    return documents_content\n",
    "\n",
    "def summarize_text(text, ratio=0.2):\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Summarize the text\n",
    "    summarized_text = summarize(cleaned_text, ratio=ratio)\n",
    "    \n",
    "    return summarized_text\n",
    "\n",
    "def correct_grammar(text):\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    corrected_text = tool.correct(text)\n",
    "    return corrected_text\n",
    "\n",
    "research_paper_path = \"Model/Snap_ai/doc.pdf\"\n",
    "research_paper_content = extract_file_content(research_paper_path)\n",
    "\n",
    "# Summarize the research paper content\n",
    "summary = summarize_text(research_paper_content)\n",
    "\n",
    "# Correct grammar in the summary\n",
    "corrected_summary = correct_grammar(summary)\n",
    "\n",
    "print(\"Original Content:\")\n",
    "print(research_paper_content)\n",
    "print(\"\\nSummary:\")\n",
    "print(corrected_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Paper Type: pdf\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'stopwords' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cleaned_text\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# Extract the text from the research paper\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m research_paper_content \u001b[39m=\u001b[39m extract_file_content(research_paper_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# Identify the key sentences in the text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m key_sentences \u001b[39m=\u001b[39m extract_key_sentences(research_paper_content)\n",
      "\u001b[1;32m/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m(file_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpdf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     loader \u001b[39m=\u001b[39m UnstructuredFileLoader(file_path)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     cleaned_text \u001b[39m=\u001b[39m clean_text(loader\u001b[39m.\u001b[39mload())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39melif\u001b[39;00m(file_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     loader \u001b[39m=\u001b[39m UnstructuredImageLoader(file_path)\n",
      "\u001b[1;32m/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_text\u001b[39m(text):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39m# Remove stop words\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     stopwords \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     cleaned_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39m# Remove punctuation\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'stopwords' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from filetype import guess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import pipeline\n",
    "bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def extract_key_sentences(text):\n",
    "    # Use BART to extract the key sentences from the text\n",
    "    return bart_summarizer.extract_key_sentences(text)\n",
    "\n",
    "def generate_summary(key_sentences):\n",
    "    # Use BART to generate a summary from the key sentences\n",
    "    return bart_summarizer.generate_summary(key_sentences)\n",
    "\n",
    "def detect_document_type(document_path):\n",
    "    \n",
    "    guess_file = guess(document_path)\n",
    "    file_type = \"\"\n",
    "    image_types = ['jpg', 'jpeg', 'png', 'gif']\n",
    "    \n",
    "    if(guess_file.extension.lower() == \"pdf\"):\n",
    "        file_type = \"pdf\"\n",
    "        \n",
    "    elif(guess_file.extension.lower() in image_types):\n",
    "        file_type = \"image\"\n",
    "        \n",
    "    else:\n",
    "        file_type = \"unkown\"\n",
    "        \n",
    "    return file_type\n",
    "\n",
    "research_paper_path = \"doc.pdf\"\n",
    "print(f\"Research Paper Type: {detect_document_type(research_paper_path)}\")\n",
    "\n",
    "from langchain.document_loaders.image import UnstructuredImageLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \n",
    "    loader = UnstructuredFileLoader(pdf_file)\n",
    "    documents = loader.load()\n",
    "    pdf_pages_content = '\\n'.join(doc.page_content for doc in documents)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(pdf_pages_content)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_file_content(file_path):\n",
    "    \n",
    "    file_type = detect_document_type(file_path)\n",
    "    \n",
    "    if(file_type == \"pdf\"):\n",
    "        loader = UnstructuredFileLoader(file_path)\n",
    "        cleaned_text = clean_text(loader.load())\n",
    "        \n",
    "    elif(file_type == \"image\"):\n",
    "        loader = UnstructuredImageLoader(file_path)\n",
    "        cleaned_text = clean_text(loader.load())\n",
    "        \n",
    "    return cleaned_text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove stop words\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    cleaned_text = \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "    # Remove punctuation\n",
    "    cleaned_text = cleaned_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_text = \" \".join([lemmatizer.lemmatize(word) for word in cleaned_text.split()])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Extract the text from the research paper\n",
    "research_paper_content = extract_file_content(research_paper_path)\n",
    "\n",
    "# Identify the key sentences in the text\n",
    "key_sentences = extract_key_sentences(research_paper_content)\n",
    "\n",
    "# Generate a summary from the key sentences\n",
    "summary = generate_summary(key_sentences)\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af62ca2502ff474a9417ed074373e32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/bj/cl8384wj78953hsy9c7v9dw40000gn/T/ipykernel_929/619283835.py\", line 6, in <module>\n",
      "    nlp(\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/transformers/pipelines/document_question_answering.py\", line 243, in __call__\n",
      "    The maximum length of the question after tokenization. It will be truncated if needed.\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1074, in __call__\n",
      "    final_iterator = PipelineIterator(model_iterator, self.postprocess, postprocess_params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 1080, in run_single\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/transformers/pipelines/document_question_answering.py\", line 249, in preprocess\n",
      "    Additional flags to pass to tesseract while running OCR.\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/transformers/image_utils.py\", line 59, in load_image\n",
      "    ImageInput = Union[\n",
      "                        \n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/PIL/Image.py\", line 3283, in open\n",
      "    raise UnidentifiedImageError(msg)\n",
      "PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x280cca2f0>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/theory903/anaconda3/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\n",
    "    \"document-question-answering\",\n",
    "    model=\"impira/layoutlm-document-qa\",\n",
    ")\n",
    "nlp(\n",
    "    \"https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.gkget.com%2F2023%2F04%2FTop-100-gk-questions-answers-class-5th.html&psig=AOvVaw1tvt_XVQluh4V1cElG2hbu&ust=1696424512149000&source=images&cd=vfe&opi=89978449&ved=0CBEQjRxqFwoTCJjS1fX32YEDFQAAAAAdAAAAABAE\",\n",
    "    \"What is the capital of India?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Pix2StructProcessor' from 'transformers' (/Users/theory903/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Pix2StructProcessor, Pix2StructForConditionalGeneration\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/theory903/Desktop/Models/my_ai_bot/test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Pix2StructProcessor' from 'transformers' (/Users/theory903/anaconda3/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = Pix2StructProcessor.from_pretrained('google/deplot')\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, text=\"Generate underlying data table of the figure below:\", return_tensors=\"pt\")\n",
    "predictions = model.generate(**inputs, max_new_tokens=512)\n",
    "print(processor.decode(predictions[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Category=%{x}<br>Frequency of Purchases=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Footwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Footwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Footwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Footwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Footwear",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Outerwear",
          "Footwear",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Footwear",
          "Footwear",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Footwear",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Outerwear",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Outerwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Accessories",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Footwear",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Outerwear",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Outerwear",
          "Footwear",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Clothing",
          "Accessories",
          "Footwear",
          "Clothing",
          "Accessories",
          "Clothing",
          "Footwear",
          "Clothing",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Clothing",
          "Clothing",
          "Accessories",
          "Accessories",
          "Footwear",
          "Accessories"
         ],
         "xaxis": "x",
         "y": [
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Weekly",
          "Weekly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Monthly",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Annually",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Annually",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Fortnightly",
          "Monthly",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Annually",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Annually",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Monthly",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Weekly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Monthly",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Weekly",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Fortnightly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Monthly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Weekly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Quarterly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Annually",
          "Annually",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Annually",
          "Monthly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Annually",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Annually",
          "Weekly",
          "Annually",
          "Monthly",
          "Monthly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Bi-Weekly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Weekly",
          "Annually",
          "Quarterly",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Fortnightly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Weekly",
          "Monthly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Annually",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Monthly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Annually",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Quarterly",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Annually",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Annually",
          "Monthly",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Quarterly",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Weekly",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Monthly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Weekly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Monthly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Annually",
          "Monthly",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Monthly",
          "Monthly",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Monthly",
          "Fortnightly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Quarterly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Monthly",
          "Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Bi-Weekly",
          "Weekly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Annually",
          "Annually",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Weekly",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Weekly",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Monthly",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Weekly",
          "Fortnightly",
          "Quarterly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Annually",
          "Annually",
          "Monthly",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Quarterly",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Annually",
          "Annually",
          "Monthly",
          "Annually",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Quarterly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Annually",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Annually",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Monthly",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Weekly",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Annually",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Annually",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Annually",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Annually",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Annually",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Monthly",
          "Weekly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Annually",
          "Monthly",
          "Weekly",
          "Annually",
          "Annually",
          "Annually",
          "Annually",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Weekly",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Annually",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Annually",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Monthly",
          "Bi-Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Weekly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Monthly",
          "Annually",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Fortnightly",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Annually",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Quarterly",
          "Quarterly",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Annually",
          "Annually",
          "Annually",
          "Monthly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Monthly",
          "Monthly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Annually",
          "Annually",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Bi-Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Quarterly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Weekly",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Monthly",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Annually",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Monthly",
          "Monthly",
          "Annually",
          "Annually",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Annually",
          "Weekly",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Weekly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Annually",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Quarterly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Monthly",
          "Weekly",
          "Monthly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Monthly",
          "Bi-Weekly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Monthly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Monthly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Quarterly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Annually",
          "Annually",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Annually",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Every 3 Months",
          "Weekly",
          "Annually",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Weekly",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Every 3 Months",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Monthly",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Bi-Weekly",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Quarterly",
          "Bi-Weekly",
          "Weekly",
          "Bi-Weekly",
          "Monthly",
          "Quarterly",
          "Weekly",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Monthly",
          "Fortnightly",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Every 3 Months",
          "Annually",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Fortnightly",
          "Annually",
          "Bi-Weekly",
          "Monthly",
          "Every 3 Months",
          "Fortnightly",
          "Annually",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Quarterly",
          "Every 3 Months",
          "Monthly",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Annually",
          "Fortnightly",
          "Bi-Weekly",
          "Quarterly",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Every 3 Months",
          "Monthly",
          "Quarterly",
          "Bi-Weekly",
          "Weekly",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Bi-Weekly",
          "Weekly",
          "Monthly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Quarterly",
          "Every 3 Months",
          "Weekly",
          "Monthly",
          "Every 3 Months",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Bi-Weekly",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Bi-Weekly",
          "Fortnightly",
          "Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Weekly",
          "Every 3 Months",
          "Annually",
          "Every 3 Months",
          "Monthly",
          "Bi-Weekly",
          "Annually",
          "Fortnightly",
          "Quarterly",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Quarterly",
          "Fortnightly",
          "Weekly",
          "Annually",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Monthly",
          "Weekly",
          "Every 3 Months",
          "Monthly",
          "Every 3 Months",
          "Weekly",
          "Weekly",
          "Fortnightly",
          "Annually",
          "Weekly",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Annually",
          "Monthly",
          "Bi-Weekly",
          "Fortnightly",
          "Monthly",
          "Weekly",
          "Annually",
          "Quarterly",
          "Weekly",
          "Annually",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Bi-Weekly",
          "Every 3 Months",
          "Fortnightly",
          "Fortnightly",
          "Annually",
          "Monthly",
          "Quarterly",
          "Quarterly",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Quarterly",
          "Fortnightly",
          "Annually",
          "Fortnightly",
          "Weekly",
          "Fortnightly",
          "Every 3 Months",
          "Every 3 Months",
          "Weekly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Quarterly",
          "Fortnightly",
          "Monthly",
          "Monthly",
          "Every 3 Months",
          "Quarterly",
          "Annually",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Every 3 Months",
          "Annually",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Quarterly",
          "Quarterly",
          "Every 3 Months",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Monthly",
          "Monthly",
          "Fortnightly",
          "Fortnightly",
          "Every 3 Months",
          "Fortnightly",
          "Monthly",
          "Annually",
          "Annually",
          "Every 3 Months",
          "Every 3 Months",
          "Monthly",
          "Weekly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Monthly",
          "Annually",
          "Quarterly",
          "Monthly",
          "Weekly",
          "Fortnightly",
          "Fortnightly",
          "Weekly",
          "Quarterly",
          "Quarterly",
          "Quarterly",
          "Bi-Weekly",
          "Annually",
          "Annually",
          "Quarterly",
          "Fortnightly",
          "Bi-Weekly",
          "Every 3 Months",
          "Weekly",
          "Bi-Weekly",
          "Quarterly",
          "Weekly",
          "Quarterly"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "line"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Category"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Frequency of Purchases"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def plot(file_path, x_column, y_column, GraphNAME, plot_type=\"line\"):\n",
    "    \"\"\"\n",
    "    Read data from a file, clean it using pandas, and visualize a 2D graph in dark mode using Plotly.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input data file.\n",
    "        x_column (str): Name of the column to use for the X-axis.\n",
    "        y_column (str): Name of the column to use for the Y-axis.\n",
    "        GraphNAME (str): Title of the graph.\n",
    "        plot_type (str): Type of plot to create (\"line\", \"scatter\", \"bar\", etc.).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Read data from the file using pandas\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)  # Assuming a CSV file, adjust as needed\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"The file {file_path} is empty or not in the expected format.\")\n",
    "        return\n",
    "\n",
    "    # Clean the data (you can add specific data cleaning steps here)\n",
    "\n",
    "    # Create a Plotly figure based on the chosen plot type with a dark template\n",
    "    if plot_type == \"line\":\n",
    "        fig = px.line(df, x=x_column, y=y_column, title=GraphNAME)\n",
    "    elif plot_type == \"scatter\":\n",
    "        fig = px.scatter(df, x=x_column, y=y_column, title=GraphNAME)\n",
    "    elif plot_type == \"bar\":\n",
    "        fig = px.bar(df, x=x_column, y=y_column, title=GraphNAME)\n",
    "    else:\n",
    "        print(f\"Unsupported plot type: {plot_type}\")\n",
    "        return\n",
    "\n",
    "    # Set a dark mode template\n",
    "    fig.update_layout(template=\"plotly_dark\")\n",
    "\n",
    "    # Show the interactive Plotly graph\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"doc.csv\"  # Replace with the path to your data file\n",
    "x_column = \"Category\"  # Replace with the actual X column name\n",
    "y_column = \"Frequency of Purchases\"  # Replace with the actual Y column name\n",
    "GraphNAME = \"line\"\n",
    "\n",
    "# Visualize the cleaned data from the file using Plotly\n",
    "visualize_2d_graph_from_file_dark(file_path, x_column, y_column, GraphNAME, plot_type=\"line\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
